{"paper_id": "2403-00530", "version": "2403-00530v1", "root_file_path": "d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\data_raw\\2403-00530\\tex\\2403-00530v1\\main.tex", "metadata": {"total_length": 41291, "merged_count": 1, "merged_files": ["main.tex"], "missing_files": []}, "content": "\\documentclass[\ntwocolumn,\nsuperscriptaddress,\namsmath,amssymb,\naps,prb\n]{revtex4-2}\n\\bibliographystyle{apsrev4-2}\n\\usepackage{appendix} \n\\usepackage{graphicx}\n\\usepackage[caption=false,labelformat=simple]{subfig}\n\\captionsetup[subfloat]{position=top ,font={bf},justification=raggedright,singlelinecheck=false}\n\\renewcommand{\\thesubfigure}{(\\alph{subfigure})}\n\n\\usepackage{dcolumn}\n\\usepackage{braket}\n\\usepackage{threeparttable}\n\\usepackage{longtable}\n\\setlength{\\LTcapwidth}{0.47\\textwidth}\n\\makeatletter\n\\def\\LT@LR@e{\\LTleft\\z@   \\LTright\\z@}\n\\makeatother\n\n\\usepackage{bm}\n\\usepackage{multirow}\n\\usepackage[ colorlinks, linkcolor=blue, anchorcolor=blue, citecolor=blue ]{hyperref}\n\\usepackage{cleveref}\n\\crefname{figure}{fig.}{figures}\n\\Crefname{figure}{Fig.}{Figures}\n\\crefname{subfigure}{fig.}{figures}\n\\Crefname{subfigure}{Fig.}{Figures}\n\\crefname{table}{table}{tables}\n\\Crefname{Table}{Table}{Tables}\n\\Crefname{equation}{Eq.}{Equations}\n\n\n\\begin{document}\n\n\\title{Extended Hubbard corrected tight-binding model for rhombohedral few-layer graphene}\n\\author{Dongkyu Lee}\n\\affiliation{Department of Physics, University of Seoul, Seoul 02504, Korea}\n\\affiliation{Department of Smart Cities, University of Seoul, Seoul 02504, Korea}\n\\author{Wooil Yang}\n\\affiliation{Korea Institute for Advanced Study, Seoul 02455, Korea}\n\\author{Young-Woo Son}\n\\affiliation{Korea Institute for Advanced Study, Seoul 02455, Korea}\n\\author{Jeil Jung}\n\\email{jeiljung@uos.ac.kr}\n\\affiliation{Department of Physics, University of Seoul, Seoul 02504, Korea}\n\\affiliation{Department of Smart Cities, University of Seoul, Seoul 02504, Korea}\n\n\\date{\\today}\n\n\\begin{abstract}\nRhombohedral multilayer graphene (RnG) featuring partially flat bands has emerged as an important platform to probe strong Coulomb correlation effects. \nTheoretical consideration of local electron-electron interactions are of particular importance for electronic eigenstates with a tendency to spatially localize. \nWe present a method to incorporate mean-field electron-electron interaction corrections in the tight-binding hopping parameters of the band Hamiltonian within the extended Hubbard model \n\nthat incorporates {\\em ab initio} estimates of on-site ($U$) and inter-site ($V$) Hubbard interactions for the $\\pi$ bands of RnG.\n\n\nOur Coulomb-interaction renormalized band structures feature electron-hole asymmetry, band flatness, band gap, and anti-ferromagnetic ground states in excellent agreement with available experiments for $n \\geq 4$. \n\nWe reinterpret the putative gaps proposed in $n=3$ systems \nin terms of shifting electron and hole density of states peaks depending on the range of the Coulomb interaction models.  \n\n\\end{abstract}\n\n\\maketitle\n\n\\section{Introduction}\\label{intro}\n\nNearly flat bands confined within a small energy range that are spread in momentum space often lead to localized states in real space. \nTo elucidate the characteristics of flat bands, it is imperative to incorporate local interactions. The eigenstates of flat bands, known as compact localized states, exhibit values only within a finite range~\\cite{rhim_singular_2021}. Notable examples of the states include the $AA$ region of magic-angle twisted bilayer graphene (MATBG)~\\cite{choi_electronic_2019, koshino_maximally_2018, kerelsky_maximized_2019} and the coherent regions of moiré structures of rhombohedral multilayer graphene on hexagonal boron nitride~\\cite{chen_evidence_2019, chen_signatures_2019, gonzalez_topological_2021, park_topological_2023} and the $A$ sublattice of the first layer with its symmetric counterpart $B$ sublattice of the last layer in rhombohedral stacked $n$-layer graphene (\\textbf{RnG}) which has partially flat bands~\\cite{pamuk_magnetic_2017, Kerelsky_moireless_2021, hagymasi_observation_2022}. Owing to their localized nature, flat-band materials exhibit heightened sensitivity to variations in local interactions, leading to notable alterations in band flatness, band gap, correlation phase, and beyond~\\cite{Kerelsky_moireless_2021, guinea_electrostatic_2018, vahedi_magnetism_2021, gonzalez_magnetic_2021}. On the other hand, outcomes derived from calculations employing the local density approximation (LDA)~\\cite{kohn_self-consistent_1965} and generalized gradient approximation (GGA)~\\cite{perdew_generalized_1996} functional, which neglect local interactions, run the risk of predicting physics that deviate from the actual properties of flat-band materials.\n\nRecent experimental observations have underscored the need for more sophisticated calculations in flatband materials. The reported many-body phenomena in flatband, such as quantum anomalous Hall effect, unconventional superconductivity, and charge density wave~\\cite{chen_signatures_2019, cao_unconventional_2018, bhowmik_broken-symmetry_2022, tseng_anomalous_2022, han_large_2023}, stem from symmetry breaking induced by external perturbations such as (proximity) spin-orbit coupling and gating. As the properties are dictated by the ratio of external parameters to hopping parameters, an accurate hopping model that incorporates the appropriate interaction strength would effectively mitigate computational complexity and bolster result reliability.\n\nHowever, a proper model explaining flat band experiments has yet to be proposed, particularly for rhombohedral stacked n-layer graphene (RnG for $n=3, 4, \\dotsb$) which has partially flat bands near the high-symmetry points $K$ and $K'$. The rhombohedral stacked 3-layer graphene (ABC stacked graphene, R3G) exhibits band gap ranging from $0$ to $42$ meV~\\cite{khodkov_direct_2015, van_elferen_fine_2013, bao_stacking-dependent_2011, lee_competition_2014, lee_gate-tunable_2022}. The estimated Hubbard $U$ value is $4.8t$ from the $42$ meV gap where $t$ is the nearest-neighbor hopping parameter~\\cite{lee_gate-tunable_2022}. This value significantly surpasses the $U$ value $3.5t$ from the constrained random phase approximation (cRPA)~\\cite{wehling_strength_2011} and even the anti-ferromagnetic critical value of monolayer graphene $2.2t$~\\cite{sorella_semi-metal-insulator_1992}. On the computational front, gapless states were obtained for all RnG within the GGA~\\cite{adamo_toward_1999}. Despite the PBE0 hybrid functional adequately estimating the gapped states with a 39 meV band gap for R3G~\\cite{pamuk_magnetic_2017}, it faltered in predicting experimental gaps of 10 meV for ABCA stacked graphene (R4G)~\\cite{Kerelsky_moireless_2021, liu_spontaneous_2023}. Consequently, an appropriate interaction model capable of elucidating experimental results for RnG remains unproposed.\n\nIn this paper, we propose realistic extended Hubbard corrected tight-binding models (TB+$U$+$V$) for RnG ($n=1, 2, \\dotsb, 8$), and discuss their ground states and flat band structures under neutral conditions. The model incorporates three extended Hubbard parameters ($U$, $V_1$, $V_2$) from a newly developed self-consistent density functional theory~\\cite{lee_first-principles_2020, yang_ab_2021} and hopping parameters obtained from the $\\pi$-band maximally localized Wannier functions. The local interaction corrected hopping parameters provide a Fermi velocity that matches experimental values well, in contrast to LDA functional density functional theory (DFT) which underestimates it~\\cite{jung_tight-binding_2013, trevisanutto_ab_2008}. The ground states of the TB+$U$+$V$ Hamiltonian accurately predict the particle-hole asymmetry, antiferromagnetic band gap, flat bandwidth, and critical temperature for $n \\geq 4$. It also provides a new interpretation of the experimental gapped states in R3G, explaining the variations in measured band gap size depending on substrate condition. We expect that these models, being self-consistently determined without empirical variables, will reduce computational complexity and enhance result reliability, in subsequent calculations involving external variables.\n\n\n\\section{Extended Hubbard corrected tight-binding model}\\label{sec2}\n\\subsection{Interacting correction}\\label{sec2-1}\n\n\\begin{figure}[tb!] \n\\includegraphics[width=0.90\\columnwidth]{Figures/Fig1/diagram.pdf}\n\\caption{A flowchart diagram summarizing the construction and solution process of the TB+$U$+$V$ model. Hopping coefficients extracted from DFT+$U$+$V$ include contributions from extended Hubbard interactions. The additional step to calculate $\\rho^0$ is introduced to correct the interaction. The degrees of freedom associated with $U$ and $V$ are eliminated using the extended Hubbard functional method.}\\label{Fig:fig1} \n\\end{figure} \n\nThe physics of interacting models emerges from the competition between hopping parameters and interactions, while effective hopping parameters are renormalized by interactions~\\cite{lee_hartree-fock_2018}. Since parameters obtained from DFT are renormalized hopping parameters, careful definition and subtraction of the interaction are necessary to avoid double counting. However, an issue arises when using the LDA functional, as it does not provide information about the magnitude of interactions for the Wannier function basis. Therefore, we turned our attention to DFT+$U$+$V$ functional~\\cite{leiria_campo_jr_extended_2010}. Since the electron-electron interaction potential is restricted to the extended Hubbard interaction, the onsite potential $\\epsilon^{UV}$ and the hopping parameters $t^{UV}$ extracted from the DFT+$U$+$V$ can be defined as follows:\n\\begin{align} \n\\epsilon^{UV}_{i} &\\sim \\epsilon_{i\\sigma} +U_i \\rho^0_{i\\tilde{\\sigma}} + \\sum_j V_{ij} \\rho^0_{j} \\\\\nt^{UV}_{ij} &\\sim t_{ij\\sigma} - V_{ij} \\rho^0_{ij\\sigma}\n\\end{align}\nwhere the $\\epsilon_{i\\sigma}$ and the $t_{ij\\sigma}$ are the non-interacting tight-binding parameters between the orbitals $i$ and $j$ of spin $\\sigma$, the $\\tilde{\\sigma}$ denotes the opposite spin of $\\sigma$, the $U_i$ and the $V_{ij}$ are the extended Hubbard parameters which were used in the DFT+$U$+$V$ calculation, and the $\\rho^0$ is a mean-field density matrix ($ \\rho_{ij\\sigma} = \\braket{c^\\dagger_{j\\sigma} c_{i\\sigma}} $) obtained from the non-corrected Hamiltonian,\n\\begin{align} \\label{Eq:eq_H0}\nH^{0} &= \\sum_{i\\sigma} \\epsilon^{UV}_{i}c^\\dagger_{i\\sigma} c_{i\\sigma}  +\\sum_{ij\\sigma} t^{UV}_{ij}c^\\dagger_{i\\sigma} c_{j\\sigma} .\n\\end{align}\nHere, we have abbreviated the notation for the density matrix by denoting $\\rho_{i\\sigma} = \\rho_{ii\\sigma}$ and $\\rho_{i}=\\rho_{i\\uparrow}+\\rho_{i\\downarrow}$ removing the indices.\n\nThe interaction correction involves excluding the original electron-electron interaction from the Hamiltonian and introducing a new one based on the updated density matrix. In other words, the interaction corrected Hamiltonian used in the $(N+1)$th step, where the density matrix obtained from the $N$th self-consistent iteration step is $\\rho^N$, is given by,\n\\begin{equation}\nH_{TB+UV}^{(N+1)}(\\rho^N) = H^{0} + H^{UV}(\\rho^N-\\rho^0)\n\\end{equation} \nwhere the extended Hubbard correction,\n\\begin{align*} \nH^{UV}(\\Delta \\rho) =& \\sum_{i\\sigma} (U_i \\Delta \\rho_{i\\tilde\\sigma} + \\sum_j V_{ij} \\Delta \\rho_{j}) c^\\dagger_{i\\sigma} c_{i\\sigma} \\\\\n&- \\sum_{ij\\sigma}V_{ij} \\Delta \\rho_{ij\\sigma} c^\\dagger_{i\\sigma} c_{j\\sigma} . \n\\end{align*} \nHere, the non-collinear interaction has been ignored. \n\nIn typical DFT approaches with Hubbard interactions~\\cite{Anisimov1991PRB, Anisimov1997JPC}, the determination of the parameters relies on empirical fitting procedures. Recently, however, there have been significant developments in computing those parameters {\\it ab initio}~\\cite{Kulik2006PRL, Cococcioni2005PRB, Miyake2008PRB, Miyake2009PRB, Aichhorn2009PRB, Mosey2007PRB, Mosey2008JCP, agapito_reformulation_2015, Rubio2017PRB, Timrov2018PRB, lee_first-principles_2020, tancogne-dejean_parameter-free_2020, Timrov2021PRB}. \nAmong them, we used a newly developed first-principles method~\\cite{lee_first-principles_2020,yang_ab_2021} for on-site and intersite Hubbard interactions by incorporating the Agapito–Curtarolo–Buongiorno Nardelli (ACBN0) pseudohybrid functional for on-site Coulomb interactions~\\cite{agapito_reformulation_2015}. The present method turns out to be very efficient and accurate in obtaining various physical parameters such as bands gaps, atomic forces, phonon dispersions and magnetic moments of  correlated solids~\\cite{lee_first-principles_2020, tancogne-dejean_parameter-free_2020, yang_ab_2021, Jang2023PRL}. Moreover, this method can self-consistently determine the strength of inter-site Hubbard interactions between a pair of orbitals with arbitrary spatial range to handle long-ranged correlations in low dimensional solids~\\cite{lee_first-principles_2020}. Therefore, our TB+$U$+$V$ method differs from typical mean-field Hubbard model calculations~\\cite{hubbard_electron_1963, fernandez-rossier_magnetism_2007} in that the long-ranged Hubbard parameters can be determined self-consistently and that a step to solve tight-binding Hamiltonian of $H^0$ is added subsequently. We note that this approach also helps prevent the double-counting of interactions and enhances accuracy significantly. \\Cref{Fig:fig1} shows a diagram explaining the process of constructing and solving the $H_{TB+UV}$.\n\n\n\\subsection{Rhombohedral stacked N-layer graphene}\\label{rhomboG}\n\n\\begin{table}[tb!] \n\\begin{threeparttable}\n\\caption{\\label{tab:table1}\nCalculated extended Hubbard parameters and extracted Dirac velocity of monolayer graphene~(R1G) in the DFT+$U$+$V$ step. We compare our results with those in other methods. The Hubbard parameters are given in $eV$}\n\\begin{ruledtabular}\n\\def\\arraystretch{1.2}\n\\begin{tabular}{ccccc}\n & This work & ref\\tnote{a} & ref\\tnote{b} & ref\\tnote{c}\\\\\n\\hline\n$U$ & 6.20 & 7.56 & 10.16 &-\\\\\n$V_1$ & 3.22 & 4.02 & 5.68 &-\\\\\n$V_2$ & 2.09 & 2.57 & 4.06&-\\\\\n\\hline\n$v_{\\text{f}}$~[$10^6 m/s$] & 1.09 & 1.43 & -& 0.84 \\\\\n$t_{\\text{eff}}$~[$eV$] & 3.40 & 4.46 & -& 2.58 \\\\\n\\end{tabular}     \n\\begin{tablenotes}\n\\item [a] Extended Hubbard functional DFT+$U$+$V$~\\cite{tancogne-dejean_parameter-free_2020}\n\\item [b] cRPA~\\cite{schuler_optimal_2013}\n\\item [c] LDA~\\cite{jung_tight-binding_2013}\n\\end{tablenotes}\n\\end{ruledtabular}\n\\end{threeparttable}\n\\end{table} \n\n\\begin{figure*}[bth!] \n\\begin{minipage}[h]{.40\\textwidth}\n\\subfloat[\\label{Fig:fig2_a}]{\\includegraphics[width=\\columnwidth]{Figures/Fig2/RnG.pdf}}\n\\end{minipage}\n\\qquad\\qquad\n\\begin{minipage}[h]{.40\\textwidth}\n\\subfloat[\\label{Fig:fig2_b}]{\\includegraphics[width=\\columnwidth]{Figures/Fig2/F2G2.pdf}}\n\\\\\n\\subfloat[\\label{Fig:fig2_c}]{\\includegraphics[width=\\columnwidth]{Figures/Fig2/kpoint.pdf}}\n\\end{minipage}\n\\caption{(a)~Schematic sideview(left) and topview(right) of rhombohedral stacked few-layer graphene structure. The label $A_i$ or $B_i$ ($i = 1, 2, \\dotsb, n$) represents the $A$ or $B$ sublattice orbital of the $i$-th layer. The dashed lines denote the unit cell of the RnG. (b)~Relation classification for F2G2 truncation relative to a central reference position(yellow circle). The F2G2 model truncates hopping parameters up to the second-nearest orbital for the non-zero in-plane displacement between each label. The left panel illustrates the scenario when the in-plain position of the reference orbital is located at the $A$ or $B$ position in the layer, while the right panel depicts the case when it is positioned at the $C$ site. The $g_0$ represents onsite energy for the same label or perpendicular hopping for different labels. (c)~An example of the partially dense $k$-point sampling used to solve our TB+$U$+$V$ Hamiltonians of the RnG. The points at $K$($K'$) position are replaced by dense grids. The size of each point corresponds to its weight. The illustrated example consists of a $18 \\times 18$ coarse grid with $32 \\times 32$ partially dense points resulting in an effective sampling of 576 × 576 points near the valleys.}\\label{Fig:fig2}\n\\end{figure*} \n\nWe applied the TB+$U$+$V$ models for RnG which have partially flat bands. We have considered rigid structures of which the in-plain lattice constant is $a=2.46 \\text{~\\AA}$ and the interlayer distance is $c=3.35 \\text{~\\AA}$ in this work. \\Cref{Fig:fig2_a} illustrates the unit cell of rhombohedral stacked graphene. DFT calculations were performed with modified {\\textsc{Quantum ESPRESSO}}~\\cite{giannozzi_quantum_2009,lee_first-principles_2020,yang_ab_2021} for the extended Hubbard functional DFT+$U$+$V$. We used a projector augmented wave (PAW)~\\cite{blochl_projector_1994} LDA pseudopotential parameterized by Perdew and Zunger~\\cite{perdew_self-interaction_1981} in {\\textsc{PSlibrary}}~\\cite{dal_corso_pseudopotentials_2014}. The Brillouin zone integrations in this step were performed with $60 \\times 60 \\times 1$ Monkhorst-Pack mesh points. The extended Hubbard correction considered interaction range up to 2.46~\\AA, including interactions with the second-nearest neighbor orbitals. The onsite parameter $U$, nearest inter-sublattice parameter $V_1$, and nearest intra-sublattice parameter $V_2$ were self-consistently determined for this interaction range. Since there was no significant difference in these Hubbard parameters depending on layer, sublattice, or structure, the same values were used for all basis and every RnG. The non-corrected tight-binding parameters $\\epsilon^{UV}$ and $t^{UV}$ were obtained by constructing the maximally localized Wannier functions (MLWF) for $\\pi$-bands of the DFT+$U$+$V$ calculations. The Wannierization was performed using {\\textsc{Wannier90}}. The obtained hopping parameters were truncated for use in a model called the F2G2 model~\\cite{jung_accurate_2014}, which considers up to second-nearest hopping for each sublattice relation as illustrated in \\Cref{Fig:fig2_b}. The hoppings between the basis differing by three or more layers were excluded, as they were negligible in magnitude. The F2G2 hopping parameters and the non-corrected electronic structure for RnG are explained in \\Cref{Appendix1}.\n\n\\Cref{tab:table1} summarizes the parameters obtained during the DFT+$U$+$V$ step. Our Hubbard parameters show smaller values compared to other references. Due to $\\pi$-bonding, the $\\pi$-band orbitals exhibit a broader spread compared to the $p_z$ orbital, and this spread is calculated to increase with the density of the reciprocal space grid~\\cite{jung_tight-binding_2013}. As the spread of orbitals increases, it reduces the magnitude of local interactions. Hence, our results yield smaller values compared not only to cRPA but also to the same extended Hubbard functional DFT+$U$+$V$ method. The Fermi velocity $v_\\textrm{f}$ obtained using the slope of the Dirac cone in the monolayer graphene (R1G) confirms a higher value compared to the velocity obtained within the LDA~\\cite{jung_tight-binding_2013, jung_accurate_2014}, and our value is in good agreement with the experimental value~\\cite{hwang_fermi_2012}. Furthermore, the effective hopping value $t_{\\textrm{eff}}$ calculated numerically from the Fermi velocity also shows improvement over the LDA calculation.\n\nThe final step in constructing the TB+$U$+$V$ Hamiltonian for RnG involved solving the $H^0$, which includes the F2G2 hopping parameters, to obtain $\\rho^0$. (The energy dispersions near $K$ point of the non-corrected Hamiltonians can be found in \\Cref{fig:figA1}.) During the self-consistent TB+$U$+$V$ Hamiltonian calculation, a partially dense $k$-point sampling was employed to prevent overweighted Dirac points, as illustrated in~\\Cref{Fig:fig2_c}. This approach involves creating a coarse grid of dimensions $N_\\textrm{coarse}\\times N_\\textrm{coarse}\\times 1$ across the entire Brillouin zone (BZ) and replacing the representative zone at the $K$ point with a dense grid of dimensions $N_\\textrm{dense}\\times N_\\textrm{dense}\\times 1$. In this paper, we used $N_\\textrm{coarse}=24$ and $N_\\textrm{dense}=64$, resulting in an effective $k$-grid density of $1536 \\times 1536 \\times 1$ near the Dirac points. The density matrix was mixed using the modified Broyden algorithm~\\cite{johnson_modified_1988}, and iterations were continued until the distance between the vectors of the steps was below $10^{-6}$.\n \n\n\n\n\\section{Realistic gapped states and Flatbands}\\label{sec3}\n\n\\begin{figure*}[bt!] \n\\subfloat[\\label{Fig:fig3_a}]{\\begin{tabular}{c}\n\\includegraphics[width=0.325\\textwidth]{Figures/Fig3/R3G.pdf}\n\\includegraphics[width=0.325\\textwidth]{Figures/Fig3/R4G.pdf}\n\\includegraphics[width=0.325\\textwidth]{Figures/Fig3/R5G.pdf}\\\\\n\\includegraphics[width=0.325\\textwidth]{Figures/Fig3/R6G.pdf}\n\\includegraphics[width=0.325\\textwidth]{Figures/Fig3/R7G.pdf}\n\\includegraphics[width=0.325\\textwidth]{Figures/Fig3/R8G.pdf}\n\\end{tabular}}\\\\\n\\subfloat[\\label{Fig:fig3_b}]{\\includegraphics[width=0.325\\textwidth]{Figures/Fig3/width.pdf}}\n\\subfloat[\\label{Fig:fig3_c}]{\\includegraphics[width=0.325\\textwidth]{Figures/Fig3/Egap.pdf}}\n\\subfloat[\\label{Fig:fig3_d}]{\\includegraphics[width=0.325\\textwidth]{Figures/Fig3/gap_vs_temp.pdf}}\n\\caption{(a) TB+$U$+$V$ band structure and density of states for the ground states of the RnG ($n=3, 4, \\dotsb, 8$). The bands show 3D~($k_x,k_y,E$) electronic dispersions projected to 2D~($k_x,E$) plane near the $K$ point. The insets of each panel show the contour plots of the conduction band(left insets) and the valence band(right insets). (b) Bandwidths extracted from the peaks of the density of states. The red straight line is obtained using \\Cref{eq:width_fitting}, which is a fitting equation of the valence band flatness for $4 \\leq n \\leq 8$. (c) Band gaps depending on the number of layers $n$. The $\\Delta$ represents the difference between the valence band minima and conduction band maxima, while The $\\Delta_{peak}$ show the difference in the positions of the two peaks in the density of states. We also list other gaps from PBE(diamond) and PBE0(square)~\\cite{pamuk_magnetic_2017,campetella_hybrid-functional_2020}. The cross marks denote experimental band gaps~\\cite{khodkov_direct_2015, zhang_direct_2009, van_elferen_fine_2013,bao_stacking-dependent_2011,lee_gate-tunable_2022,lee_competition_2014,Kerelsky_moireless_2021, liu_spontaneous_2023}. (d) Temperature dependence of the energy gap $\\Delta$. The star marks indicate transition temperatures $T_c$, which represent the lowest temperature points among the data showing zero band gap within an error margin of $1$ K.}\\label{fig:fig3}\n\\end{figure*} \n\nTo validate our model, we compare the self-consistent states of RnG under the neutral state with experimental results. \\Cref{fig:fig3} present the electronic dispersions and the density of states~(DOS) near the Dirac $K$ valley of the RnG for ($n \\leq 8$). Including unremarkable gapless states in R1G and R2G, we observed a gapless state in R3G and gapped states for $n \\geq 4$. In all RnG, the electron pocket (see left insets of the band structures) exhibits a 3-fold rotation symmetry spreading from K to M direction. The hole pocket (right insets of the band structures) appears at the zone boundary of $K \\rightarrow M$ for $n\\leq 5$. On the other hand from R6G onward, they appear in the $K \\rightarrow \\Gamma$ direction, displaying strong electron-hole asymmetry along with changes in bandwidth.\n\nWe first examined the flat bandwidth $W$ for quantitative analysis, as shown in \\Cref{Fig:fig3_b}. We used the standard deviation $\\sigma$ of fitted Gaussian functions to calculate the width as $W = 2\\sqrt{2\\ln2}\\sigma$. The flatness of the conduction band remained nearly constant at around $2$ meV for all of the gapped RnG, while the width of the valence band exhibited a linear increase given by \n\\begin{equation} \\label{eq:width_fitting}\n    W = 1.96 n - 6.52\n\\end{equation} \nwith the stacked layer number $n$. Our results not only account for the flatness in the experimental results for R4G~\\cite{Kerelsky_moireless_2021} but also excellently explain the 25 meV bandwidth observed in angle-resolved photoemission spectroscopy (ARPES) measurements for R14G~\\cite{henck_flat_2018}. Additionally, the narrow flat band of about 2 meV widths in R4G and R5G suggests the potential for various many-body phenomena.\n\nNext, we examined the size of the band gap. Two definitions of the band gap are considered in \\Cref{Fig:fig3_c}. The first is the rigorous definition, the difference between the conduction band minima and valence band maxima, denoted as $\\Delta$. The second is the distance between the two peaks in the density of states (DOS), denoted as $\\Delta_{peak}$. As observed in the electronic band structure, the band gap emerges for $n \\geq 4$. The bandgap $\\Delta$ increases with the $n$ and converges to 18 meV. Excluding the case of R3G, the $\\Delta_{peak}$ shows a similar value to the $\\Delta$, but due to increasing of the bandwidth of the valence band, there is a trend of a slight deviation from $\\Delta$. Our results are in good agreement with a gapless R2G~\\cite{zhang_direct_2009} and the experimental gaps for R4G~\\cite{Kerelsky_moireless_2021, liu_spontaneous_2023}, while previous studies using PBE and PBE0~\\cite{pamuk_magnetic_2017, campetella_hybrid-functional_2020} failed to explain the magnitude of the bandgaps. On the other hand, Our results reveal a gapless state in R3G, whereas experiments suggest various bandgaps spanning from $0$ to $42$ meV, with most cases indicating finite bandgaps. Specifically, R3G on substrate exhibits gaps ranging from $0.38$ to $6$ meV~\\cite{van_elferen_fine_2013, bao_stacking-dependent_2011}, while suspended R3G shows either $0$ meV~\\cite{khodkov_direct_2015} or $42$ meV~\\cite{lee_competition_2014, lee_gate-tunable_2022} in experiments.\n\nTemperature dependence of the band gaps is shown in \\Cref{Fig:fig3_d}. These calculations were performed by adjusting the temperature values of the Fermi-Dirac distribution in the density matrix integration process. We observed that the RnG~($n \\geq 4$), which had gapped ground states, transition to metallic states above certain temperatures. The transition temperatures $T_c$ were determined as the lowest temperature among the results with zero gap. The $T_c$ measured to be 38 K for R4G and 50 K for R5G, increasing with the number of layers and converging to around 65 K. The value of $T_c = 50$ K for R5G is consistent with the experimental result at which the correlated insulator state transitions to a semi-metallic behavior under neutral conditions~\\cite{han_correlated_2023}.\n\n\\begin{figure}[tb!] \n\\includegraphics[width=0.98\\columnwidth]{Figures/Fig4/Mag.pdf}\n\\caption{Calculated spin magnetic moments of the sublattices in $\\mu_B$. There is a symmetric relation, $\\mu_{Ai} = -\\mu_{B{n+1-i}}$, about the opposite site orbital. }\\label{Fig:fig4}\n\\end{figure} \n\nWe observed the emergence of spin ordering for $n \\geq 4$. The spin magnetic moments of the sublattices are illustrated in \\Cref{Fig:fig4}. The spin magnetic moments for the unlisted sublattices can be obtained by a symmetric relation $\\mu_{Ai} = -\\mu_{B(n+1-i)}$. The magnetic moments are most large in the surface layer and decrease rapidly with distance from the surface. For a layer index $i$, $A_i$ and $B_i$ exhibit magnetic moments with differing signs and magnitudes. Consequently, the net spin magnetic moment of the $i$-th layer is positive for ($i < n/2$) and negative for ($i > n/2$). This indicates that the origin of the gapped ground states is the layer antiferromagnetic (LAF) phase.\n\n\n\\begin{figure}[bt!] \n\\subfloat[\\label{Fig:fig5_a}]{\\includegraphics[width=0.90\\columnwidth]{Figures/Fig5/R3G_longrange_DOS.pdf}}\\\\\n\\subfloat[\\label{Fig:fig5_b}]{\\includegraphics[width=0.90\\columnwidth]{Figures/Fig5/Finitesizescaling.pdf}}\n\\caption{(a) Comparison of density of states of rhombohedral trilayer graphene for different interaction ranges $L_{UV}$. Note that the untruncated full hopping parameters were used for this result. (b) Interaction range dependence of the distance between two peaks of the DOS. The red dashed line fitting the data for $L_{UV} \\geq 8.5$ exhibits a $y$-intercept of 33.9 meV. }\\label{Fig:fig5}\n\\end{figure} \n\nHere, we discuss the experimental gapped states in R3G that do not match our results. Contrary to our results indicating the semi-metallic state for R3G, numerous prior experimental studies suggest that the ground state of R3G possesses a bandgap~\\cite{van_elferen_fine_2013, bao_stacking-dependent_2011,lee_competition_2014, lee_gate-tunable_2022}. Specifically, suspended R3G~\\cite{lee_competition_2014, lee_gate-tunable_2022} tends to exhibit a larger bandgap compared to R3G on substrates~\\cite{van_elferen_fine_2013, bao_stacking-dependent_2011}. However, we have found a clue that can explain the larger and varying bandgaps from our TB+$U$+$V$ model which includes longer-range interaction. \\Cref{Fig:fig5} shows the DOS obtained from our model as the extended Hubbard interaction range $L_{UV}$ is increased. In this calculation, we used the full hopping parameters without F2G2 truncation to assess the effects of long-range interactions. Similar to the divergence in the Fermi velocity observed in monolayer graphene~\\cite{elias_dirac_2011}, we observed band reshaping in R3G. In our results, the renormalization reduces the DOS near the Fermi level and increases the distance between peaks. \n\nWe found that the $\\Delta_{peak}$ exhibits a linear relationship with $L_{UV}^{-1}$ for $L_{UV} \\geq 8.5$, where $8.5$ is greater than the distance involving a interaction between $A_1$ and $B_3$. Through a finite-size scaling as shown in \\Cref{Fig:fig5_b}, we obtained a $y$-intercept of 33.9 meV. It can be confirmed that long-range interactions can increase the $\\Delta_{peak}$ to a value similar to the size of the experimental gap in the suspended R3G. This suggests that the reduced DOS could lead to the $\\Delta_{peak}$ being measured as a bandgap. This hypothesis can also explain why the band gap observed for the R3G on substrate is smaller than in suspended R3G. If the range of interaction is reduced due to substrate screening, the DOS near the Fermi level could be measurable. While the 42 meV antiferromagnetic gap would require an excessively large $U$~\\cite{lee_competition_2014}, our model demonstrates that the the band gap size can be explained within realistic interaction strength.\n\n\n\n\\section{Conclusion}\\label{conclusion}\n\nIn this paper, we introduced the TB+$U$+$V$ model, a self-consistent tight-binding model that incorporates the mean-field electron-electron interaction correction. We have applied our model to RnG ($n=1, 2, \\dotsb, 8$), reporting the model parameters and the ground states in neutral conditions. We confirmed that using the extended Hubbard functional DFT+$U$+$V$ yields reasonable interaction parameters and energy dispersions whose Fermi velocity was closer to experiments than other methods~\\cite{jung_tight-binding_2013,schuler_optimal_2013,tancogne-dejean_parameter-free_2020}. The neutral ground states from the extended Hubbard corrected tight-binding Hamiltonian for RnG reported the semi-metallic phases for $n \\leq 3$ and the LAF gapped phases for $n \\geq 4$. Our results achieved excellent agreements between their electron-hole asymmetry, bandwidth, bandgap, and transition temperature, and those from experiments for RnG. Lastly, we observed the changes in the DOS near the Fermi level and the distance between the peaks due to the renormalization of the Fermi velocity when adjusting the interaction range in the R3G model. From this observation, we claim the hypothesis that the experimental gap in R3G may be the distance between the DOS peaks, not a real gap. This hypothesis allowed us to explain the origin and the difference in the experimental band gaps in R3G without the unnaturally large interaction. \n\nIn conclusion, we have confirmed that our model can describe naturalistic flat band physics using self-consistently determined interaction parameters. The absence of empirical variables in the model construction step suggests that our method can be easily extended to other flat band materials. Moreover, the determined interaction parameters raise expectations for effectively reducing computational complexity in extended calculations that consider external perturbations. Hence, we expect that our method may overcome the difficulties of exploring many-body phenomena of flat-band materials.\n\n\n\\begin{acknowledgments}\nY.-W.S. was supported by NRF of Korea (Grant No. 2017R1A5A1014862, SRC program: vdWMRC center) and KIAS individual Grant (No. CG031509). W.Y. was supported by KIAS individual Grant (No. 6P090103).\nA part of computations were supported by the CAC of KIAS.\n\\end{acknowledgments}\n\n\n\n\\newpage\n\n\n\\appendix\n\\counterwithin{table}{section}\n\\counterwithin{figure}{section}\n\n\\section{Non-corrected Tight-binding Hamiltonian}\\label{Appendix1}\n\nIn this appendix, we list the F2G2 truncated hopping parameters $\\epsilon^{UV}$ and $t^{UV}$ for RnG in \\Cref{tab:F2G2}. The Hamiltonian $H^0$ in \\Cref{Eq:eq_H0} using the hopping parameters exhibits the ground state under neutral conditions in a spin-unpolarized case. \\Cref{fig:figA1} show the electronic band and DOS of the $H^0$.\n\n\n\\begin{longtable}[e]{@{\\extracolsep{\\fill}}cccccc}\n   \\\\ \\caption{F2G2 truncated hopping parameters in eV from the Wannierization step. Two sites in the label column denote the hopping between those sites. Note that inversion symmetry leads to the relations between the labels, $(A_iA_j) \\leftrightarrow (B_{n+1-i}B_{n+1-j})$ and $(A_iB_j) \\leftrightarrow (B_{n+1-i}A_{n+1-j})$.} \\label{tab:F2G2} \\vspace{0.05 in} \\\\\n  \n  \\hline\n  \\hline\n  \\multicolumn{1}{c}{RnG} & \n  \\multicolumn{1}{c}{label} & \n  \\multicolumn{1}{c}{g/f} &\n  \\multicolumn{1}{c}{$g_0$}&\n  \\multicolumn{1}{c}{$g_1/f_1$}&\n  \\multicolumn{1}{c}{$g_2/f_2$} \\\\ \\hline\n  \\endfirsthead\n  \n  \\multicolumn{6}{c}{{\\tablename\\ \\thetable{} (continued)}} \\\\\n  \\hline\n  \\multicolumn{1}{c}{RnG} & \n  \\multicolumn{1}{c}{label} & \n  \\multicolumn{1}{c}{g/f} &\n  \\multicolumn{1}{c}{$g_0$}&\n  \\multicolumn{1}{c}{$g_1/f_1$}&\n  \\multicolumn{1}{c}{$g_2/f_2$} \\\\ \n  \\hline\n  \\endhead\n  \n  \\multicolumn{6}{r}{{Continued on next table...}} \\\\ \\hline\n  \\endfoot\n  \n  \\hline\n  \\hline\n  \\endlastfoot\n \\multirow{2}{*}{R1G}\n&$A_1 A_1$ &g & -3.5564 & 0.2185 & 0.0521\\\\\n&$A_1 B_1$ &f & & -3.7339 & -0.1665\\\\\n\\hline\n \\multirow{3}{*}{R2G}\n&$A_1 A_1$ &g & -2.8384 & 0.2214 & 0.0512\\\\\n&$B_1 B_1$ &g & -2.8110 & 0.2133 & 0.0514\\\\\n&$A_1 B_1$ &f & & -3.7396 & -0.1629\\\\\n&$A_1 A_2$ &f & & 0.0848 & -0.0311\\\\\n&$A_1 B_2$ &f & & 0.1308 & -0.0692\\\\\n&$B_1 A_2$ &g & 0.3328 & -0.0055 & -0.0008\\\\\n\\hline\n \\multirow{3}{*}{R3G}\n&$A_1 A_1$ &g & -2.0493 & 0.2219 & 0.0513\\\\\n&$B_1 B_1$ &g & -2.0247 & 0.2142 & 0.0515\\\\\n&$A_2 A_2$ &g & -2.0728 & 0.2161 & 0.0493\\\\\n&$A_1 B_1$ &f & & -3.7375 & -0.1716\\\\\n&$A_2 B_2$ &f & & -3.7432 & -0.1751\\\\\n&$A_1 A_2$ &f & & 0.0881 & -0.0345\\\\\n&$A_1 B_2$ &f & & 0.1357 & -0.0746\\\\\n&$A_1 A_3$ &f & & 0.0065 & -0.0017\\\\\n&$A_1 B_3$ &g & 0.0046 & 0.0016 & -0.0014\\\\\n&$B_1 A_2$ &g & 0.3321 & -0.0051 & -0.0008\\\\\n&$B_1 B_2$ &f & & 0.0862 & -0.0354\\\\\n&$B_1 A_3$ &f & & 0.0068 & -0.0058\\\\\n\\hline\n \\multirow{3}{*}{R4G}\n&$A_1 A_1$ &g & -1.2643 & 0.2219 & 0.0522\\\\\n&$B_1 B_1$ &g & -1.2384 & 0.2133 & 0.0521\\\\\n&$A_2 A_2$ &g & -1.2899 & 0.2147 & 0.0501\\\\\n&$B_2 B_2$ &g & -1.2876 & 0.2156 & 0.0503\\\\\n&$A_1 B_1$ &f & & -3.7361 & -0.1616\\\\\n&$A_2 B_2$ &f & & -3.7417 & -0.1615\\\\\n&$A_1 A_2$ &f & & 0.0882 & -0.0313\\\\\n&$A_1 B_2$ &f & & 0.1336 & -0.0685\\\\\n&$A_1 A_3$ &f & & 0.0063 & -0.0010\\\\\n&$A_1 B_3$ &g & 0.0052 & 0.0014 & -0.0013\\\\\n&$B_1 A_2$ &g & 0.3415 & -0.0045 & -0.0012\\\\\n&$B_1 B_2$ &f & & 0.0848 & -0.0320\\\\\n&$B_1 A_3$ &f & & 0.0069 & -0.0041\\\\\n&$B_1 B_3$ &f & & 0.0061 & -0.0012\\\\\n&$A_2 A_3$ &f & & 0.0864 & -0.0313\\\\\n&$A_2 B_3$ &f & & 0.1331 & -0.0681\\\\\n&$B_2 A_3$ &g & 0.3416 & -0.0039 & -0.0011\\\\\n\\hline\n \\multirow{3}{*}{R5G}\n&$A_1 A_1$ &g & -1.2639 & 0.2220 & 0.0522\\\\\n&$B_1 B_1$ &g & -1.2384 & 0.2133 & 0.0522\\\\\n&$A_2 A_2$ &g & -1.2908 & 0.2147 & 0.0500\\\\\n&$B_2 B_2$ &g & -1.2869 & 0.2157 & 0.0503\\\\\n&$A_3 A_3$ &g & -1.2883 & 0.2150 & 0.0501\\\\\n&$A_1 B_1$ &f & & -3.7361 & -0.1618\\\\\n&$A_2 B_2$ &f & & -3.7417 & -0.1618\\\\\n&$A_3 B_3$ &f & & -3.7419 & -0.1615\\\\\n&$A_1 A_2$ &f & & 0.0883 & -0.0313\\\\\n&$A_1 B_2$ &f & & 0.1337 & -0.0687\\\\\n&$A_1 A_3$ &f & & 0.0064 & -0.0010\\\\\n&$A_1 B_3$ &g & 0.0051 & 0.0014 & -0.0013\\\\\n&$B_1 A_2$ &g & 0.3414 & -0.0045 & -0.0012\\\\\n&$B_1 B_2$ &f & & 0.0849 & -0.0320\\\\\n&$B_1 A_3$ &f & & 0.0068 & -0.0043\\\\\n&$B_1 B_3$ &f & & 0.0062 & -0.0010\\\\\n&$A_2 A_3$ &f & & 0.0867 & -0.0313\\\\\n&$A_2 B_3$ &f & & 0.1335 & -0.0681\\\\\n&$A_2 A_4$ &f & & 0.0063 & -0.0009\\\\\n&$A_2 B_4$ &g & 0.0049 & 0.0014 & -0.0013\\\\\n&$B_2 A_3$ &g & 0.3416 & -0.0040 & -0.0012\\\\\n&$B_2 B_3$ &f & & 0.0862 & -0.0314\\\\\n&$B_2 A_4$ &f & & 0.0069 & -0.0041\\\\\n\\hline\n \\multirow{3}{*}{R6G}\n& $A_1 A_1$ &g & -0.6406 & 0.2234 & 0.0539\\\\\n&$B_1 B_1$ &g & -0.6157 & 0.2139 & 0.0537\\\\\n&$A_2 A_2$ &g & -0.6671 & 0.2161 & 0.0520\\\\ \n&$B_2 B_2$ &g & -0.6662 & 0.2164 & 0.0522\\\\\n&$A_3 A_3$ &g & -0.6667 & 0.2160 & 0.0520\\\\\n&$B_3 B_3$ &g & -0.6653 & 0.2163 & 0.0520\\\\\n&$A_1 B_1$ &f & & -3.7340 & -0.1611\\\\\n&$A_2 B_2$ &f & & -3.7389 & -0.1602\\\\\n&$A_3 B_3$ &f & & -3.7391 & -0.1604\\\\\n&$A_1 A_2$ &f & & 0.0882 & -0.0312\\\\\n&$A_1 B_2$ &f & & 0.1326 & -0.0683\\\\\n&$A_1 A_3$ &f & & 0.0063 & -0.0016\\\\\n&$A_1 B_3$ &g & 0.0054 & 0.0014 & -0.0012\\\\\n&$B_1 A_2$ &g & 0.3432 & -0.0040 & -0.0012\\\\\n&$B_1 B_2$ &f & & 0.0845 & -0.0316\\\\\n&$B_1 A_3$ &f & & 0.0069 & -0.0039\\\\\n&$B_1 B_3$ &f & & 0.0061 & -0.0017\\\\\n&$A_2 A_3$ &f & & 0.0866 & -0.0310\\\\\n&$A_2 B_3$ &f & & 0.1327 & -0.0671\\\\\n&$A_2 A_4$ &f & & 0.0061 & -0.0017\\\\\n&$A_2 B_4$ &g & 0.0053 & 0.0013 & -0.0012\\\\\n&$B_2 A_3$ &g & 0.3455 & -0.0034 &-0.0012\\\\\n&$B_2 B_3$ &f & & 0.0863 & -0.0310\\\\\n&$B_2 A_4$ &f & & 0.0070 & -0.0038\\\\\n&$B_2 B_4$ &f & & 0.0062 & -0.0017\\\\\n&$A_3 A_4$ &f & & 0.0864 & -0.0310\\\\\n&$A_3 B_4$ &f & & 0.1327 & -0.0671\\\\\n&$B_3 A_4$ &g & 0.3450 & -0.0035 & -0.0012\\\\\n\\hline\n \\multirow{3}{*}{R7G}\n&$A_1 A_1$ &g & -0.0155 & 0.2251 & 0.0557\\\\\n&$B_1 B_1$ &g & 0.0066 & 0.2146 & 0.0554\\\\\n&$A_2 A_2$ &g & -0.0444 & 0.2180 & 0.0540\\\\ \n&$B_2 B_2$ &g & -0.0449 & 0.2175 &0.0540\\\\\n&$A_3 A_3$ &g & -0.0444 & 0.2177 &0.0539\\\\\n&$B_3 B_3$ &g & -0.0438 & 0.2176 &0.0539\\\\\n&$A_4 A_4$ &g & -0.0442 & 0.2176 &0.0539\\\\\n&$A_1 B_1$ &f & & -3.7315 & -0.1615\\\\\n&$A_2 B_2$ &f & & -3.7362 & -0.1613\\\\\n&$A_3 B_3$ &f & & -3.7365 & -0.1618\\\\\n&$A_4 B_4$ &f & & -3.7365 & -0.1618\\\\\n&$A_1 A_2$ &f & & 0.0880 & -0.0322\\\\\n&$A_1 B_2$ &f & & 0.1325 & -0.0696\\\\\n&$A_1 A_3$ &f & & 0.0061 & -0.0022\\\\\n&$A_1 B_3$ &g & 0.0060 & 0.0013 &-0.0013\\\\\n&$B_1 A_2$ &g & 0.3461 & -0.0036 &-0.0012\\\\\n&$B_1 B_2$ &f & & 0.0856 & -0.0320\\\\\n&$B_1 A_3$ &f & & 0.0071 & -0.0030\\\\\n&$B_1 B_3$ &f & & 0.0059 & -0.0022\\\\\n&$A_2 A_3$ &f & & 0.0864 & -0.0323\\\\\n&$A_2 B_3$ &f & & 0.1316 & -0.0694\\\\\n&$A_2 A_4$ &f & & 0.0060 & -0.0022\\\\\n&$A_2 B_4$ &g & 0.0058 & 0.0012 &-0.0013\\\\\n&$B_2 A_3$ &g & 0.3481 & -0.0028 &-0.0012\\\\\n&$B_2 B_3$ &f & & 0.0864 & -0.0321\\\\\n&$B_2 A_4$ &f & & 0.0071 & -0.0029\\\\\n&$B_2 B_4$ &f & & 0.0060 & -0.0022\\\\\n&$A_3 A_4$ &f & & 0.0863 & -0.0323\\\\\n&$A_3 B_4$ &f & & 0.1318 & -0.0693\\\\\n&$A_3 A_5$ &f & & 0.0060 & -0.0022\\\\\n&$A_3 B_5$ &g & 0.0057 & 0.0012 &-0.0013\\\\\n&$B_3 A_4$ &g & 0.3483 & -0.0028 &-0.0013\\\\\n&$B_3 B_4$ &f & & 0.0865 & -0.0322\\\\\n&$B_3 A_5$ &f & & 0.0071 & -0.0029\\\\\n\\hline\n \\multirow{3}{*}{R8G}\n&$A_1 A_1$ &g & 0.6084 & 0.2265 &0.0575\\\\\n&$B_1 B_1$ &g & 0.6288 & 0.2155 &0.0570\\\\\n&$A_2 A_2$ &g & 0.5789 & 0.2200 & 0.0561\\\\\n&$B_2 B_2$ &g & 0.5767 & 0.2181 & 0.0558\\\\\n&$A_3 A_3$ &g & 0.5789 & 0.2190 & 0.0557\\\\\n&$B_3 B_3$ &g & 0.5783 & 0.2187 & 0.0555\\\\\n&$A_4 A_4$ &g & 0.5783 & 0.2188 & 0.0556\\\\\n&$B_4 B_4$ &g & 0.5784 & 0.2188 & 0.0556\\\\\n&$A_1 B_1$ &f & & -3.7291 & -0.1628\\\\\n&$A_2 B_2$ &f & & -3.7336 & -0.1636\\\\\n&$A_3 B_3$ &f & & -3.7341 & -0.1642\\\\\n&$A_4 B_4$ &f & & -3.7340 & -0.1641\\\\\n&$A_1 A_2$ &f & & 0.0872 & -0.0333\\\\\n&$A_1 B_2$ &f & & 0.1320 & -0.0714\\\\\n&$A_1 A_3$ &f & & 0.0059 & -0.0022\\\\\n&$A_1 B_3$ &g & 0.0066 & 0.0011 & -0.0014\\\\\n&$B_1 A_2$ &g & 0.3475 & -0.0033 & -0.0012\\\\\n&$B_1 B_2$ &f & & 0.0871 & -0.0323\\\\\n&$B_1 A_3$ &f & & 0.0072 & -0.0025\\\\\n&$B_1 B_3$ &f & & 0.0058 & -0.0023\\\\\n&$A_2 A_3$ &f & & 0.0858 & -0.0334\\\\\n&$A_2 B_3$ &f & & 0.1298 & -0.0723\\\\\n&$A_2 A_4$ &f & & 0.0058 & -0.0022\\\\\n&$A_2 B_4$ &g & 0.0064 & 0.0011 & -0.0014\\\\\n&$B_2 A_3$ &g & 0.3517 & -0.0022 & -0.0013\\\\\n&$B_2 B_3$ &f & & 0.0873 & -0.0328\\\\\n&$B_2 A_4$ &f & & 0.0073 & -0.0025\\\\\n&$B_2 B_4$ &f & & 0.0058 & -0.0022\\\\\n&$A_3 A_4$ &f & & 0.0864 & -0.0331\\\\\n&$A_3 B_4$ &f & & 0.1300 & -0.0720\\\\\n&$A_3 A_5$ &f & & 0.0059 & -0.0022\\\\\n&$A_3 B_5$ &g & 0.0064 & 0.0010 & -0.0014\\\\\n&$B_3 A_4$ &g & 0.3519 & -0.0022 & -0.0014\\\\\n&$B_3 B_4$ &f & & 0.0866 & -0.0331\\\\\n&$B_3 A_5$ &f & & 0.0073 & -0.0025\\\\\n&$B_3 B_5$ &f & & 0.0059 & -0.0022\\\\\n&$A_4 A_5$ &f & & 0.0865 & -0.0331\\\\\n&$A_4 B_5$ &f & & 0.1300 & -0.0720\\\\\n&$B_4 A_5$ &g & 0.3518 & -0.0022 & -0.0014\\\\\n\\end{longtable} \n\n\\begin{figure*}[bth!] \n\\includegraphics[width=0.32\\textwidth]{Figures/FigA1/R3G.pdf}\n\\includegraphics[width=0.32\\textwidth]{Figures/FigA1/R4G.pdf}\n\\includegraphics[width=0.32\\textwidth]{Figures/FigA1/R5G.pdf}\n\\\\\n\\includegraphics[width=0.32\\textwidth]{Figures/FigA1/R6G.pdf}\n\\includegraphics[width=0.32\\textwidth]{Figures/FigA1/R7G.pdf}\n\\includegraphics[width=0.32\\textwidth]{Figures/FigA1/R8G.pdf}\n\\caption{$H^0$ band structure and density of states for the RnG ($n=3, 4, \\dotsb, 8$)}\\label{fig:figA1}\n\\end{figure*} \n\n\\end{document}\n"}
{"paper_id": "2403-00530", "version": "2403-00530v2", "root_file_path": "d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\data_raw\\2403-00530\\tex\\2403-00530v2\\main_rerev2025.tex", "metadata": {"total_length": 51454, "merged_count": 1, "merged_files": ["main_rerev2025.tex"], "missing_files": []}, "content": "\\documentclass[\ntwocolumn,\nsuperscriptaddress,\namsmath,amssymb,\naps,prb\n]{revtex4-2}\n\\usepackage{appendix} \n\\usepackage{graphicx}\n\\usepackage[caption=false,labelformat=simple]{subfig}\n\\captionsetup[subfloat]{position=top ,font={bf},justification=raggedright,singlelinecheck=false}\n\\renewcommand{\\thesubfigure}{(\\alph{subfigure})}\n\n\\usepackage{dcolumn}\n\\usepackage{braket}\n\\usepackage{threeparttable}\n\\usepackage{longtable}\n\\setlength{\\LTcapwidth}{0.47\\textwidth}\n\\makeatletter\n\\def\\LT@LR@e{\\LTleft\\z@   \\LTright\\z@}\n\\makeatother\n\n\\usepackage{bm}\n\\usepackage{multirow}\n\\usepackage[ colorlinks, linkcolor=blue, anchorcolor=blue, citecolor=blue ]{hyperref}\n\\usepackage{cleveref}\n\\usepackage{verbatim}\n\\crefname{figure}{fig.}{figures}\n\\Crefname{figure}{Fig.}{Figures}\n\\crefname{subfigure}{fig.}{figures}\n\\Crefname{subfigure}{Fig.}{Figures}\n\\crefname{table}{table}{tables}\n\\Crefname{Table}{Table}{Tables}\n\\Crefname{equation}{Eq.}{Equations}\n\n\n\\begin{document}\n\n\n\n\\title{Self-consistent tight-binding calculations with extended Hubbard interactions in rhombohedral multilayer graphene}\n\\author{Dongkyu Lee}\n\\affiliation{Department of Physics, University of Seoul, Seoul 02504, Korea}\n\\author{Wooil Yang}\n\\affiliation{Korea Institute for Advanced Study, Seoul 02455, Korea}\n\\author{Young-Woo Son}\n\\affiliation{Korea Institute for Advanced Study, Seoul 02455, Korea}\n\\author{Jeil Jung}\n\\email{jeiljung@uos.ac.kr}\n\\affiliation{Department of Physics, University of Seoul, Seoul 02504, Korea}\n\n\\date{\\today}\n\n\\begin{abstract}\nWe study the mean-field broken symmetry phases of charge neutral multilayer rhombohedral graphene within tight-binding approximations including self-consistent extended Hubbard interactions.\n\nWe used on-site and inter-site Hubbard interactions obtained from a newly developed first-principles calculation method.\n\n\nOur calculations for systems up to eight layers \ngive rise to electron-hole asymmetries, band flatness, band gaps, and layer anti-ferromagnetic ground states in keeping with available experiments.\n\nBy including the intersite Hubbard interactions up to the next-nearest neighboring sites, the band gaps are shown to open when the number of layers is larger than three, while the trilayer system maintains its metallic nature with two low energy density of state peaks near the Fermi energy whose separation increases with the range of inter-site Hubbard parameters.\nWithin our framework, the calculated band gaps reflect mean-field ground states with extended Hubbard interactions, in closer agreement with experimental estimates. The tight-binding formulation further enables efficient treatment of large rhombohedral chiral systems, including twisted multilayer graphene.\n\\end{abstract}\n\n\\maketitle\n\n\\section{Introduction}\\label{intro}\n\n\n\nPhysical systems with nearly flat bands in momentum space have often spatially localized states in real space~\\cite{rhim_singular_2021, park_quasi-localization_2024} that makes them susceptible to local Coulomb interactions. \n\nExample graphene systems with such nearly flat bands include the zigzag nanoribbons~\\cite{wakabayashi_electronic_1999, son_energy_2006, jung_theory_2009, park_quasi-localization_2024},  \nrhombohedral $n$-layer graphene (here we abbreviate as \\textrm{RnG} and $n \\ge 3$)~\\cite{min_chiral_2008,pamuk_magnetic_2017, Kerelsky_moireless_2021, hagymasi_observation_2022},\nand magic angle twisted bilayer graphene~\\cite{choi_electronic_2019, koshino_maximally_2018, kerelsky_maximized_2019, bistritzer_moire_2011}\nfor which a variety of interaction driven phases have been studied in the literature.\n\n\nRecent advances in near-field optical identification of stacking domains of RnG have sparked a wave of research in RnG, in particular the tetralayer~\\cite{liu_spontaneous_2023} and pentalayer~\\cite{han_correlated_2024} systems where a variety of ordered phases have been observed, including the fractional quantum anomalous Hall effect, unconventional superconductivity.\nWe note that for these systems external system parameters such as the (proximity) spin-orbit coupling and gating are able to sensitively alter its properties~\\cite{han_large_2024, lu_fractional_2024, zhou_fractional_2024, dong_theory_2024, choi_superconductivity_2025, waters_chern_2025, lu_extended_2025}.\n\nFrom the perspective of modeling the RnG, the interplay between the band parameters and Coulomb interactions sensitively influences the onset of the ordered phases.\n\n\n\nWhereas it is expected that an infinitesimally small Coulomb interaction strength is sufficient to trigger broken symmetry phases for $n \\geq 2$ in a minimal $n$-chiral two-dimensional electron gas (2DEG) model~\\cite{min_chiral_2008,zhang_spontaneous_2011},\nactual calculation results sensitively depend both on the specific details in the band Hamiltonian and the Coulomb interaction model.\n\nIn particular, for Bernal stacked bilayer graphene, a number of studies predicted the possibility of various broken symmetry phases~\\cite{lemonik_spontaneous_2010,vafek_many-body_2010,nandkishore_quantum_2010,zhang_spontaneous_2010,jung_lattice_2011}.\nFor instance, the Hartree-Fock calculations with long-range Coulomb interactions screened with a dielectric constant value  of $\\varepsilon_r = 4$ gives rise to band gaps of the order of 20~meV for the bilayer system~\\cite{jung_lattice_2011},\nwhile the experimental signatures of Coulomb correlations~\\cite{martin_local_2010, seiler_quantum_2022} \nand gaps in suspended bilayer graphene at charge neutrality turned out to be smaller than 2~meV~\\cite{velasco_transport_2012, bao_evidence_2012}.\n\n\n\nThe ABC stacked graphene or R3G is expected to have a larger band gap than the bilayer graphene within the same minimal model, although the actual gap magnitude turns out to be sensitive to details of the band's trigonal warping~\\cite{jung_gapped_2013}.\n\n\nIn experiments, the R3G gaps have been reported to be between $0$ to $42$~meV~\\cite{khodkov_direct_2015, van_elferen_fine_2013, bao_stacking-dependent_2011, lee_competition_2014, lee_gate-tunable_2022}. To achieve this gap within a $U$-only mean-field calculation we need $U = 4.8t$, where $t$ is the nearest-neighbor hopping parameter~\\cite{lee_gate-tunable_2022}, which significantly surpasses the $U = 3.5t$ estimate of the constrained random phase approximation (cRPA)~\\cite{wehling_strength_2011} and even the anti-ferromagnetic critical value of monolayer graphene $U = 2.2t$~\\cite{sorella_semi-metal-insulator_1992}. \n\nOn the computational front, gapless states were obtained for all RnG from first-principles calculations based on the density functional theory (DFT) with the local density approximation (LDA) or generalized gradient approximations (GGA) for exchange-correlation functional~\\cite{adamo_toward_1999}, indicating the relative weakness of Coulomb interaction corrections in commonly used local and semi-local DFT approximations. \n\nOn the contrary, inclusion of non-local Fock terms as in PBE0 hybrid functional allows to adequately estimate the 39~meV band gap for R3G~\\cite{pamuk_magnetic_2017}, but overestimates the experimental gaps for ABCA stacked graphene or R4G of the order of $\\sim 10$~meV~\\cite{Kerelsky_moireless_2021, liu_spontaneous_2023}. \nConsequently, appropriate interaction models that predict results consistent with experiments for various RnG systems is needed. \n\n\n\n\n\n\n\n\n\n\n\n\nIn this work, we obtain the interacting mean-field ground state of charge neutral RnG ($n=1, 2, \\dotsb, 8$) based on the unified calculation framework of the self-consistent tight-binding approximations with the onsite ($U$) and intersite ($V$) Hubbard interactions (TB+$U$+$V$). \nOur main proposed model incorporates $U$ and $V$ from a newly developed {\\it ab initio} DFT calculation method for the self-consistent extended Hubbard interactions (DFT+$U$+$V$)~\\cite{lee_first-principles_2020,yang_ab_2021,Yang2024PRB}.\n\nWe extracted nearest-neighboring and next nearest-neighboring intersite Hubbard interactions ($V_1$ and $V_2$) as well as onsite $U$ interactions from our {\\it ab initio} calculations.\nThe calculated renormalized hopping parameters within TB+$U$+$V$ provide a Fermi velocity ($v_F$) of single layer graphene that is enhanced by almost 30\\% with respect to the computed value using DFT-LDA~\\cite{jung_tight-binding_2013} for our choice of cutoff range up to $V_2$. We note that the $v_F$ sensitively depends on the dielectric screening from  substrates, e.g., it is larger than the DFT-LDA velocity by 10$\\sim$30\\% if deposited on top of SiO$_2$~\\cite{dorgan_mobility_2010, knox_spectromicroscopy_2008,hwang_fermi_2012} and up to almost 50\\% when deposited on top of hBN~\\cite{yu_interaction_2013, muzzio_momentum-resolved_2020, zhang_experimental_2005, hwang_fermi_2012}.\n\n\nUsing a newly developed computational method, we aim to provide a more realistic description of the band structure that incorporates the band renormalization due to Coulomb interaction effects in RnG systems. Within our TB+$U$+$V$ framework, our calculated band gaps correspond to the truncated-range mean-field Hartree-Fock ground states, that are more closely comparable to available experimental estimates.\n\nAdditionally, the TB-based description of the bands allows for a more flexible and efficient calculation framework that can eventually be used to study larger number of atoms with rhombohedral stacking orders such as twisted multilayer graphene systems~\\cite{gonzalez_magnetic_2021,Nakatsuji2023PRX,Park2025Nature}.  \n\n\n\n\n\n\n\n\\section{Self-consistent tight-binding extended Hubbard model}\\label{sec2}\n\nIn this section we first briefly introduce a DFT+$U$+$V$ method to obtain self-consistent extended Hubbard interactions. Then, we present our newly developed TB+$U$+$V$ method for RnG systems where efficient calculations are possible thanks to the reduced Hilbert space of the tight-binding orbital basis.\n\n\n\n\\subsection{DFT+$U$+$V$ for $U$, $V$ parameters calculations}\\label{sec2-0}\nIn typical DFT approaches with Hubbard interactions, the $U$ and $V$ parameters are found by empirical fitting procedures~\\cite{Anisimov1991PRB, Anisimov1997JPC}. Recently, however, there have been significant developments in computing those parameters {\\it ab initio}~\\cite{Kulik2006PRL, Cococcioni2005PRB, Miyake2008PRB, Miyake2009PRB, Aichhorn2009PRB, Mosey2007PRB, Mosey2008JCP, agapito_reformulation_2015, Rubio2017PRB, Timrov2018PRB, lee_first-principles_2020, tancogne-dejean_parameter-free_2020, Timrov2021PRB,yang_ab_2021,Yang2024PRB}. \nAmong them, we used a first-principles calculations based method~\\cite{lee_first-principles_2020,yang_ab_2021,Yang2024PRB} \ndeveloped by extending to $V$ parameters the Agapito–Curtaolo–Buongiorno Nardelli pseudohybrid functional for on-site Coulomb $U$ interactions~\\cite{agapito_reformulation_2015}. \nThis method turns out to be very efficient and accurate for obtaining various physical parameters such as bands gaps, atomic forces, phonon dispersions and magnetic moments of correlated solids~\\cite{lee_first-principles_2020,yang_ab_2021,Yang2024PRB,tancogne-dejean_parameter-free_2020, Jang2023PRL}. \nMoreover, this method can self-consistently determine the strength of inter-site Hubbard interactions between a pair of orbitals with arbitrary spatial range to handle \nlocal and nonlocal Coulomb interactions in low dimensional solids~\\cite{lee_first-principles_2020}. \nAn advantage of our implementation with respect to other mean-field Hubbard model calculations~\\cite{hubbard_electron_1963, fernandez-rossier_magnetism_2007} is that the extended Hubbard $V$ parameters are determined self-consistently.  We refer the previous related studies~\\cite{lee_first-principles_2020,yang_ab_2021,Yang2024PRB,tancogne-dejean_parameter-free_2020, Jang2023PRL} for further details on self-consistent calculations of extended Hubbard interactions. \n\n\n\n\\subsection{Self-consistent TB+$U$+$V$ calculations}\\label{sec2-1}\n\n\n\\begin{figure}[b] \n\\includegraphics[width=0.90\\columnwidth]{Figures/Fig1/diagram.pdf}\n\\caption{A flowchart diagram summarizing the construction and solution process of the TB+$U$+$V$ model. The hopping parameters of the TB band Hamiltonian are extracted from the DFT+$U$+$V$, where the $U$ and $V$ parameters are obtained in a self-consistent manner~\\cite{lee_first-principles_2020,yang_ab_2021,Yang2024PRB},\nand whose associated ground state density is $\\rho^0$. \nThe degrees of freedom associated with $U$ and $V$ are eliminated using the extended Hubbard functional method.}\\label{Fig:fig1} \n\\end{figure} \n\n\nIn the following, we explain the calculation procedure of our self-consistent TB+$U$+$V$ bands\nin search of broken-symmetry solutions starting from the reference Hamiltonian resulting from the\nDFT+$U$+$V$ calculations. \n\n\n\n\\Cref{Fig:fig1} shows a diagram explaining the construction process and resolution of $\\hat{H}^{{\\rm TB}+U+V}$ Hamiltonian.\nThe procedure is as follows. We start with the DFT+$U$+$V$ calculations to obtain the corresponding tight-binding Hamiltonian $\\hat{H}^0$, its ground state density matrix $\\rho^0$, the associated tight-binding and the extended Hubbard Coulomb interaction parameters. \n\nThrough the DFT+$U$+$V$ calculations we obtain self-consistently the tight-binding hopping terms and the Hubbard $U$ and $V$ parameters. \nWe have carried out valley, spin, and inversion symmetry preserving self-consistent DFT+$U$+$V$ calculations to provide the reference renormalized tight-binding Hamiltonian $\\hat{H}^0$.\n\nThen, this symmetry preserving ground-state of $\\hat{H}^0$ is taken as the reference starting point of the self-consistent TB+$U$+$V$ calculations. At this stage we seek for broken symmetry solutions  where we use an initial condition a density matrix $\\rho^{1}$ with broken symmetry until we converge the calculations to self-consistency.\n\n\nThe Kohn-Sham (KS) equations of a DFT+$U$+$V$ calculation can be cast onto a tight-binding Wannier orbital basis that provides the reference point Hamiltonian $\\hat{H}_0$ of our self-consistent TB+$U$+$V$ calculation,\n\\begin{align} \\label{Eq:eq_H0}\n\\hat{H}^{0} &= \\sum_{i\\sigma} h^{\\rm KS}_{ij} \\hat{c}^\\dagger_{i\\sigma} \\hat{c}_{j\\sigma} = \\sum_{i j \\sigma} \n\\epsilon^{UV}_{i} \\hat{c}^\\dagger_{i\\sigma} \\hat{c}_{i\\sigma}  +\\sum_{ij\\sigma} t^{UV}_{ij} \\hat{c}^\\dagger_{i\\sigma} \\hat{c}_{j\\sigma},\n\\end{align}\nwhere $\\epsilon^{UV}_i$ is the onsite potential of the $i$-th site and $t^{UV}_{ij}$ the hopping parameters\nbetween $i$- and $j$-th sites.\nFor simplicity, all our calculations have started from the non-magnetic spinless form of $\\hat{H}^0$\nand use abbreviated notations for the density matrix $\\rho^{\\sigma \\sigma^{\\prime}}_{ij} = \\langle \\hat{c}^{\\dag}_{i \\sigma} \\hat{c}_{j \\sigma^{\\prime}} \\rangle$ of the form $\\rho_{i\\sigma} = \\rho^{\\sigma \\sigma}_{ii}$ and $\\rho_{i}=\\rho_{i\\uparrow}+\\rho_{i\\downarrow}$ by removing repeated site indices and spins.\n\nThe $\\epsilon^{UV}$ and $t^{UV}$ are renormalized by the extended Hubbard Coulomb interactions, that removes the double counting of the Coulomb interactions implied in DFT+$U$+$V$~\\cite{Cococcioni2005PRB}:\n\\begin{align} \n\\epsilon^{UV}_{i\\sigma}  &= \\epsilon^{\\text{NI}}_{i\\sigma} + \\sum_{j\\neq i} V^H_{ij} \\rho^0_{j}  + v_{xc, \\,i \\sigma} + U_i \\rho^0_{i\\tilde{\\sigma}}~, \\label{eq:onsite_UV} \\\\ \nt^{UV}_{ij\\sigma}  &= t^{\\text{NI}}_{ij\\sigma} - V_{ij} \\rho^0_{ij\\sigma} \\label{eq:t_UV},\n\\end{align}\n\nwhere the $\\rho^{0}_{ij}$ density matrix elements corresponds to the DFT+$U$+$V$ ground-state in \\Cref{Eq:eq_H0}, $\\epsilon^{\\text{NI}}$ and $t^{\\text{NI}}$ represent the non-interacting parts of the tight-binding parameters. See \\Cref{Appendix_TBUV} for further details. \n\nThe second term on the right hand side of \\Cref{eq:onsite_UV} is the Hartree term proportional to the bare Coulomb potential $V^H_{ij}$ between sites $i$ and $j$. The third is the exchange-correlation potential at site $i$ for spin $\\sigma$. The last terms of \\Cref{eq:onsite_UV} and \\Cref{eq:t_UV} are the extended Hubbard corrections where $U_i$ and the $V_{ij}$ parameters are obtained from the DFT+$U$+$V$ calculation. \n\n\n\n\n\nOur self-consistent TB+$U$+$V$ calculations take as starting point reference the interaction-less tight-binding Hamiltonian,\n\n\\begin{align} \n\\hat{H}^{{\\rm TB}0}[\\rho_0] =& \\hat{H}^{0} - \\hat{H}^{UV}[\\rho_0],\n\\end{align}\n\nwhere we remove the $U+V$ terms included in $\\hat{H}^0$.\n\n\n\n\n\n\n\n\n\n\n\nThe self-consistent $\\rho$ dependent TB+$U$+$V$ Hamiltonian is\n\\begin{align} \n\\hat{H}^{{\\rm TB}+U+V}\\left[ \\rho \\right] \n=& \\hat{H}^{{\\rm TB}0}[\\rho_0] \n+ \\hat{H}^{UV}\\left[\\rho \\right] ,  \\label{selfcon}\n\n\\end{align} \n\nwhere the $U+V$ correction is given by \n\\begin{align*} \n\\hat{H}^{UV}\\left[ \\rho \\right] =& \\sum_{i\\sigma} (U_i \\rho_{i\\tilde\\sigma} + \\sum_{j\\neq i} V^H_{ij} \\rho_{j}) \\hat{c}^\\dagger_{i\\sigma} \\hat{c}_{i\\sigma} \\\\\n&- \\sum_{ij\\sigma}V_{ij} \\rho_{ij\\sigma} \\hat{c}^\\dagger_{i\\sigma} \\hat{c}_{j\\sigma}.~ \n\\end{align*} \nHere in $\\hat{H}^{UV}[\\rho]$ we have neglected the corrections proportional to $v_{xc}$ in ~\\Cref{eq:onsite_UV}, and the equation would reduce to the truncated-range Hartree-Fock form if $V^{H}_{ij} = V_{ij}$,\nbut in general they can be defined differently such that $V^{H}_{ij} \\neq V_{ij}$. \n\nWe note that the Hamiltonians in \\Cref{Eq:eq_H0} and \\Cref{selfcon} are the same if $\\rho = \\rho^0$,\nnamely when the self-consistent TB+$U$+$V$ calculation does not develop a new solution. \n\n\n\n\\subsection{Rhombohedrally stacked n-layer graphene}\\label{rhomboG}\n\n\\begin{table}[bt!] \n\\begin{threeparttable}\n\\caption{\nCalculated in-plane extended Hubbard parameters $U$, $V_1$ and $V_2$ and the Fermi velocity of single layer graphene~(R1G) in the DFT+$U$+$V$ calculation and comparison with other methods.\nThe Hubbard parameters are given in $eV$ units. \n}\\label{tab:table1}\n\\begin{ruledtabular}\n\\def\\arraystretch{1.2}\n\\begin{tabular}{ccccc}\n & This work & ref\\tnote{a} & ref\\tnote{b} & ref\\tnote{c}\\\\\n\\hline\n$U$ & 6.20 & 7.56 & 10.16 &-\\\\\n$V_1$ & 3.22 & 4.02 & 5.68 &-\\\\\n$V_2$ & 2.09 & 2.57 & 4.06&-\\\\\n\\hline\n$v_{\\text{f}}$~[$10^6 m/s$] & 1.10 & 1.43 & -& 0.84 \\\\\n$t_{\\text{eff}}$~[$eV$] & $-$3.42 & $-$4.46 & -& $-$2.58 \\\\\n\\end{tabular}     \n\\begin{tablenotes}\n\\item [a] Extended Hubbard functional DFT+$U$+$V$~\\cite{tancogne-dejean_parameter-free_2020}\n\\item [b] cRPA~\\cite{schuler_optimal_2013}\n\\item [c] LDA~\\cite{jung_tight-binding_2013}\n\\end{tablenotes}\n\\end{ruledtabular}\n\\end{threeparttable}\n\\end{table} \n\n\\begin{figure}[bth] \n\\subfloat[\\label{Fig:fig2_a}]{\\includegraphics[width=0.90\\columnwidth]{Figures/Fig2/RnG.pdf}}\\\\\n\\subfloat[\\label{Fig:fig2_b}] {\\includegraphics[width=0.90\\columnwidth]{Figures/Fig2/kpoint.pdf}} \n\\caption{(a)~Schematic sideview(left) and topview(right) of rhombohedral stacked few-layer graphene structure. The label $A_i$ or $B_i$ ($i = 1, 2, \\dotsb, n$) represents the $A$ or $B$ sublattice orbital of the $i$-th layer. The dashed lines denote the unit cell of the RnG. (b)~An example of the partially dense $k$-point sampling used to solve our TB+$U$+$V$ Hamiltonians of the RnG. The points at $K$($K'$) position are replaced by dense grids. The size of each point corresponds to its weight. The illustrated example consists of a $18 \\times 18$ coarse grid with $32 \\times 32$ partially dense points resulting in an effective sampling of 576 × 576 points near the valleys.}\\label{Fig:fig2}\n\\end{figure} \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe carried out the TB+$U$+$V$ calculations for RnG which are systems with partially flat bands. We have considered in this work rigid structures of which the in-plane lattice constant is $a=2.46 \\text{~\\AA}$ and the interlayer distance is $d=3.35 \\text{~\\AA}$. \\Cref{Fig:fig2_a} illustrates the unit cell of rhombohedral multilayer graphene. The DFT calculations were performed with modified {\\textsc{Quantum ESPRESSO}}~\\cite{giannozzi_quantum_2009,lee_first-principles_2020,yang_ab_2021} for the extended Hubbard functional DFT+$U$+$V$. The codes can be found on github~\\cite{dftuv_github}. \nWe used a projector augmented wave (PAW)~\\cite{blochl_projector_1994} LDA pseudopotential parameterized by Perdew and Zunger~\\cite{perdew_self-interaction_1981} in {\\textsc{PSlibrary}}~\\cite{dal_corso_pseudopotentials_2014}. The Brillouin zone integrations in this step were performed with $60 \\times 60 \\times 1$ Monkhorst-Pack mesh points. In these calculations, we have introduced a extended Hubbard correction cutoff of $L_{UV} = 2.46~\\text{\\AA}$ that gives finite $V_1$ and $V_2$ values.\nThe onsite parameter $U$, nearest inter-sublattice parameter $V_1$, and nearest intra-sublattice parameter $V_2$ were self-consistently determined for this interaction range. \nThe calculated Hubbard parameters were similar for the different systems considered and thus have used the same values in all cases.  \n\n\n\n\nIn our calculations we did not consider the inter-layer Coulomb interaction $V$ terms.\n\nThe DFT+$U$+$V$ tight-binding parameters $\\epsilon^{UV}$ and $t^{UV}$ were obtained by constructing the maximally localized Wannier functions (MLWF) using {\\textsc{Wannier90}}~\\cite{pizzi_wannier90_2020}. The obtained in-plane hopping parameters were truncated with the $F_2G_2$ method~\\cite{jung_accurate_2014} up to the second same-sublattice nearest neighbors. \nLikewise, we keep the interlayer hopping parameters up to the second nearest out-of-plane layers.\n\nThe truncation method and the truncated hopping models for RnG band Hamiltonian are explained in~\\Cref{Appendix_FnGn}.\n\n\\Cref{tab:table1} summarizes the parameters obtained during the DFT+$U$+$V$ step for single layer graphene. Our Hubbard parameters are smaller than similar earlier calculations~\\cite{tancogne-dejean_parameter-free_2020}\nor the cRPA~\\cite{schuler_optimal_2013}, likely due to differences in implementation details such as $k$-point grid density, resulting also in a smaller predicted Fermi velocity.\n\n\n\nThe Fermi velocity $v_\\textrm{F}$ obtained using the slope of the Dirac cone in monolayer graphene (R1G) is larger than the LDA velocity~\\cite{jung_tight-binding_2013, jung_accurate_2014}, and in reasonable agreement with $\\upsilon_{\\rm F} \\sim 1.05 \\cdot 10^6$m/s commonly used in modeling of graphene on SiO$_2$~\\cite{dorgan_mobility_2010, knox_spectromicroscopy_2008,hwang_fermi_2012} and somewhat smaller than $\\upsilon_{\\rm F} \\sim 1.2 \\cdot 10^6$m/s used in graphene on hBN~\\cite{yu_interaction_2013, muzzio_momentum-resolved_2020, zhang_experimental_2005, hwang_fermi_2012}. The effective hopping value $t_{\\textrm{eff}}$ calculated numerically from the Fermi velocity is also enhanced over the LDA calculation. We also observed that as $L_{UV}$ increases, the Fermi velocity becomes larger (e.g., $v_F = 1.25 \\cdot 10^6$~m/s for $L_{UV} = 3a$). This behavior is related to the logarithmic divergence of the Fermi velocity in graphene, which arises due to long-range exchange interactions~\\cite{elias_dirac_2011}.\n\nThe final step in the construction of the TB+$U$+$V$ Hamiltonian for RnG involved solving the $H^0$, which includes the hopping parameters, to obtain $\\rho^0$. (The energy dispersions near the $K$ point for the non-corrected Hamiltonians can be found in \\Cref{fig:figB2}.) \nFor the Hartree term, we adopt the bare Hartree potential~\\cite{min_ab_2007, jung_enhancement_2011} \n\\begin{equation}\\label{eq:Hartree}\n  V^H_{ij} = \\frac{1}{N_{ij}A}\\sum_{\\bm G}e^{i \\bm G \\cdot(\\tau_i-\\tau_j)}|f(|\\bm G|)|^2V_q(|\\bm G|)  \n\\end{equation}\nwhere $N_{ij}$ is the number of nearest $j$ orbitals with respect to $i$ orbital, $A$ is the system area, and $\\tau_i$ is the position of $i$ orbital in the unitcell. In our calculations, we have used the bare Coulomb interactions $V_q(q) = 2\\pi e^2/q$ for intra-layer interaction, $V_q(q) = 2\\pi e^2\\textrm{exp}(-qd)/q$ for inter-layer interaction and the form factor $f(q) = [1-(r_0q)^2]/[1+(r_0q)^2]^4$ with $r_0 = a/(6\\sqrt{10})~\\text{\\AA}$, a somewhat extended radial distribution of the 2p electrons following ~\\cite{jung_enhancement_2011}. \n\n\n\nFor RnG at charge neutrality, the details in modeling the interlayer Hartree potential is not critical, while it is expected to be more important in systems with a perpendicular electric field.\n\n\n\nDuring the self-consistent TB+$U$+$V$ Hamiltonian calculation, adaptive $k$-point sampling was employed to resolve details near the Dirac points, as illustrated in~\\Cref{Fig:fig2_b}. This approach involves creating a coarse grid of dimensions $N_\\textrm{coarse}\\times N_\\textrm{coarse}\\times 1$ across the entire Brillouin zone (BZ) and replacing the representative zone at the $K$ point with a dense grid of dimensions $N_\\textrm{dense}\\times N_\\textrm{dense}\\times 1$. In this paper, we used $N_\\textrm{coarse}=24$ and $N_\\textrm{dense}=64$, resulting in an effective $k$-grid density of $1536 \\times 1536 \\times 1$ near the Dirac points. The density matrix was mixed using the modified Broyden algorithm~\\cite{johnson_modified_1988}, and iterations were continued until the distance between the normalized  eigenstate vectors between successive steps was below $10^{-6}$.\n \n\n\n\\section{Low energy nearly flat bands and Coulomb-interaction driven gaps}\\label{sec3}\n\n\\begin{figure*}[tbh!] \n\\subfloat[\\label{Fig:fig3_a}]{\\begin{tabular}{c}\n\\includegraphics[width=0.325\\textwidth]{Figures/Fig3/R3G.pdf}\n\\includegraphics[width=0.325\\textwidth]{Figures/Fig3/R4G.pdf}\n\\includegraphics[width=0.325\\textwidth]{Figures/Fig3/R5G.pdf}\\\\\n\\includegraphics[width=0.325\\textwidth]{Figures/Fig3/R6G.pdf}\n\\includegraphics[width=0.325\\textwidth]{Figures/Fig3/R7G.pdf}\n\\includegraphics[width=0.325\\textwidth]{Figures/Fig3/R8G.pdf}\n\\end{tabular}}\\\\\n\\subfloat[\\label{Fig:fig3_b}]{\\includegraphics[width=0.325\\textwidth]{Figures/Fig3/width.pdf}}\n\\subfloat[\\label{Fig:fig3_c}] {\\includegraphics[width=0.325\\textwidth]{Figures/Fig3/Egap.pdf}} \n\\subfloat[\\label{Fig:fig3_d}]{\\includegraphics[width=0.325\\textwidth]{Figures/Fig3/gap_vs_temp.pdf}}\n\\caption{(a) TB+$U$+$V$ band structure and density of states for the ground states of the RnG ($n=3, 4, \\dotsb, 8$). The bands show 3D~($k_x,k_y,E$) electronic dispersions projected to 2D~($k_x,E$) plane near the $K$ point. The insets of each panel show the contour plots of the conduction band(left insets) and the valence band(right insets). (b) Bandwidths extracted from the peaks of the density of states. The red straight line is obtained using \\Cref{eq:width_fitting}, which is a fitting equation of the valence band flatness for $4 \\leq n \\leq 8$. (c) Band gaps depending on the number of layers $n$. The gap $\\Delta$ represents the difference between the valence band minima and conduction band maxima, while The $\\Delta_{peak}$ show the difference in the positions of the two peaks in the density of states. We also list other gaps from PBE(diamond) and PBE0(square)~\\cite{pamuk_magnetic_2017,campetella_hybrid-functional_2020}. The cross marks denote experimental band gaps~\\cite{khodkov_direct_2015, velasco_transport_2012, bao_evidence_2012, zhang_direct_2009, van_elferen_fine_2013,bao_stacking-dependent_2011,lee_gate-tunable_2022,lee_competition_2014,Kerelsky_moireless_2021, liu_spontaneous_2023}. (d) Temperature dependence of the energy gap $\\Delta$. The star marks indicate transition temperatures $T_c$, which represent the lowest temperature points among the data showing zero band gap within an error margin of 1~K.}\\label{fig:fig3}\n\\end{figure*} \n\nTo validate our model, we compare our calculations for RnG against available experiments.\n\nThe \\Cref{fig:fig3} presents the electronic dispersions and the density of states~(DOS) near the Dirac $K$ valley of RnG for ($n \\leq 8$). \nWe do not observe a band gap in R1G, R2G, R3G but see Coulomb-interaction driven finite gaps for $n \\geq 4$.\n\n\nWe first examined the flattening bandwidth $W$ in the vicinity of the Dirac points for quantitative analysis, as shown in \\Cref{Fig:fig3_b}. We estimated this bandwidth near the band edges from the full width at half maximum of the DOS peaks,\nusing the standard deviation $\\sigma$ of fitted Gaussian functions to calculate the width as $W = 2\\sqrt{2\\ln2}\\sigma$. The flatness of the conduction band remained nearly constant at around 2~meV for all of the gapped RnG, while the width of the valence band exhibited a linear increase that could be fitted by\n\\begin{equation} \\label{eq:width_fitting}\n    W = 1.96 n - 6.52\n\\end{equation} \nwhere $n$ is the layer number. Our results can account for the flatness of the experimental results in R4G~\\cite{Kerelsky_moireless_2021} and is in keeping with the $\\sim$25~meV bandwidth observed in angle-resolved photoemission spectroscopy (ARPES) measurements for R14G~\\cite{henck_flat_2018}. \nThe Fermi surface geometry also varied notably. While the electron pockets consistently showed a three-fold rotational symmetry along the $K \\rightarrow M$ direction for all RnG systems, in contrast the hole pockets showed a similar symmetry for $n \\leq 4$, but became annular at R5G and shifted toward the $K \\rightarrow \\Gamma$ direction for $n \\geq 6$. The observed variations in bandwidth and Fermi surface shape provides an explanation for the pronounced electron–hole asymmetry in rhombohedral graphite~\\cite{shi_electronic_2020}. The narrow band widths on the order of $\\sim$2~meV near the $K$ and $K'$ points in R4G and R5G suggest optimal scenarios for the observation of many-body phenomena.\n\n\nNext, we examine the size of the band gaps as a function of layer number $n$. Two definitions of band gaps were used in ~\\Cref{Fig:fig3_c}. The first one is the proper definition, namely the difference between the conduction band minimum and valence band maximum, denoted as $\\Delta$. In our calculations, a finite gap $\\Delta$ is found from R4G onwards until it saturates to 18~meV for $n=8$ layers.\nThe second definition is the density of states (DOS) peak to peak distance $\\Delta_{peak}$ between the valence and conduction nearly flat band regions. \nThese two values are generally closely similar\n\n\nwith an exception for R3G that shows a particularly strong difference between $\\Delta$ and $\\Delta_{peak}$. While this $\\Delta_{peak}$ could potentially be misinterpreted as a band gap in  spectroscopic measurements, in the case of R3G the gap $\\Delta$ remains zero while a finite $\\Delta_{peak}$ develops due to the trigonal warping introduced by remote hopping terms that pushes the flattened band regions away from the charge neutral point.   \n\n\nExperiments in the literature have reported a wide range of results with band gaps spanning from 0 to 42~meV \\cite{khodkov_direct_2015, van_elferen_fine_2013, bao_stacking-dependent_2011, zhou_half-_2021, lee_competition_2014, lee_gate-tunable_2022} sensitively depending on experimental conditions such as sample preparation method, choice of substrates or  external electric or magnetic fields. Specifically, the gaps tend to be zero in general when on a substrate even for high quality R3G on misaligned hBN~\\cite{zhou_half-_2021}. For R3G on SiO$_2$ a magnetic field induced gap of 0.38~meV was found~\\cite{van_elferen_fine_2013}, while in suspended samples the gaps were reported to vary widely, between 0~\\cite{khodkov_direct_2015} to 6~meV~\\cite{bao_stacking-dependent_2011} or up to 42~meV~\\cite{lee_competition_2014}. \n\n\n\n\n\n\n\nThe Coulomb interaction driven band gap of the layer antiferromagnetic phase we found in R4G is of $\\Delta \\sim 12$~meV, comparable in magnitude with the experimental gap estimates of $\\sim$10~meV~\\cite{Kerelsky_moireless_2021} and $\\sim 15$~meV~\\cite{liu_spontaneous_2023}. Previous DFT-based calculations either predicts a zero gap within GGA-PBE~\\cite{pamuk_magnetic_2017}, or reaches values of $\\sim 50$~meV when calculated through the hybrid PBE0 functional~\\cite{campetella_hybrid-functional_2020}, comparable to Hartree-Fock estimates on a tight-binding basis~\\cite{jung_gapped_2013}. \n\n\nThe temperature dependence of the gaps in \\Cref{Fig:fig3_d} is introduced through the Fermi-Dirac distribution in the density matrix integration process. The RnG~($n \\geq 4$) systems that are gapped transition to gapless metallic phases above their respective critical temperatures $T_c$ of\n\n38~K for R4G, 50~K for R5G, steadily increasing with layer number $n$ and saturating around 65~K for R8G. \nThe value of $T_c = 50$~K for R5G is in keeping with the experimental result in Ref.~\\cite{han_correlated_2024} where the correlated insulator phase shows a semi-metallic behavior at charge neutrality.\n\n\n\\begin{figure}[tb] \n\\includegraphics[width=0.95\\columnwidth]{Figures/Fig4/Mag.pdf}\n\\caption{Calculated spin magnetic moments of the sublattices in $\\mu_B$ as a function of layer number $n$. There is a symmetric relation of the $i^{th}$ layer sublattice with its counterpart from the opposite surface such that $\\mu_{Ai} = -\\mu_{B{n+1-i}}$.\nThe sublattice label notation follows \\Cref{Fig:fig2}(a).}\\label{Fig:fig4}\n\\end{figure} \n\n\n\nStable gapped layer antiferromagnetic (LAF) solutions can be achieved in our calculations calculations for $n \\geq 4$, in agreement with earlier work~\\cite{myhro_large_2018, Kerelsky_moireless_2021, pamuk_magnetic_2017}. \nWe show in \\Cref{Fig:fig4} the spin magnetic moments as a function of layer number, where the unlisted sublattices are related by the relation $\\mu_{Ai} = -\\mu_{B(n+1-i)}$. The magnetic moments are largest at the surface layers and decrease rapidly away from the surface. For a given layer index $i$, \nthe $A_i$ and $B_i$ sites exhibit magnetic moments with different signs and magnitudes. Consequently, the net spin magnetic moment of the $i^{th}$ layer is positive for ($i < n/2$) and negative for ($i > n/2$) in the LAF phase. \n\n\nFurther discussions on the role of longer range Coulomb interactions in reshasping the DOS peak positions can be found in \\Cref{Appendix_UVrange}, where the increase of $\\Delta_{peak}$ upon inclusion of longer-ranged  Coulomb interactions can give a misleading appearance of band gap increase in the case of R3G. \n\n\n\n\n\\section{Conclusion}\\label{conclusion}\n\nIn this paper, we introduced the TB+$U$+$V$ model, a self-consistent tight-binding calculation framework to efficiently obtain DFT+$U$+$V$ results using $U$ and $V$ previously obtained from {\\em ab initio} calculations. \n\nOur self-consistent TB+$U$+$V$ calculation for the broken-symmetry solutions uses as the reference non-interacting tight-binding band Hamiltonian the corresponding one for DFT+$U$+$V$ with unbroken spin symmetry, where we subtract the $U$+$V$ contribution.   \n\n\n\nThe increased efficiency of TB+$U$+$V$ has allowed the calculation of the ground states of RnG structures drastically reducing the computation time that would be required in a direct DFT+$U$+$V$ calculation. \n\nWe have applied our model to charge neutral RnG ($n=1, 2, \\dotsb, 8$) systems to obtain the extended Hubbard interaction-driven broken symmetry electronic structures. \n\nWe find semimetallic phases for $n \\leq 3$ and LAF gapped phases for $n \\geq 4$, in excellent agreement with experiments of RnG on hBN susbstrates for the band gap magnitudes and the transition temperatures to the metallic phase.\n\nThe results in this work are closer to experiments of RnG systems on substrates than previous calculations in the literature in particular for the reported band gap magnitudes~\\cite{jung_tight-binding_2013,schuler_optimal_2013,tancogne-dejean_parameter-free_2020}. \n\nIt is expected that similar TB+$U$+$V$ approaches can be applied for other 2D materials in situations where more efficient computation is required, for instance in typical moir\\'e materials with large supercells.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{acknowledgments}\nD.L. was supported by the National Research Foundation of Korea (NRF) through grant RS-2024-00413481. Y.-W.S. was supported by KIAS individual Grant (No. CG031509). W.Y. was supported by KIAS individual Grant (No. 6P090103). J.J. was supported by the Basic Study and Interdisciplinary R\\&D Foundation Fund of the University of Seoul (2025).\nThe authors acknowledge the Urban Big Data and AI Institute of the University of Seoul supercomputing resources~(\\url{http://ubai.uos.ac.kr}). A part of computations were supported by the CAC of KIAS. \n\\end{acknowledgments}\n\n\n\n\\appendix\n\\counterwithin{table}{section}\n\\counterwithin{figure}{section}\n\n\\section{Self-consistent tight-binding Extended Hubbard model}\\label{Appendix_TBUV}\nThe Hamiltonian corresponding to the DFT+$U$+$V$ can be cast into the tight-binding Hamiltonian $\\hat{H}^0$ in \\Cref{Eq:eq_H0}. This Hamiltonian serves as a reference point for the self-consistent TB+$U$+$V$ calculation.\nHere, we provide additional details for the derivation of the onsite and intersite hopping terms $\\epsilon^{UV}$ and $t^{UV}$ of the Hamiltonian $\\hat{H}^0$ stemming from the \nDFT+$U$+$V$ equations presented in Ref.~\\cite{Timrov2018PRB}. \n\n\n\n\nWe use the localized orbital basis  $\\phi_i(\\mathbf{r})$ at atomic $i$ sites for our tight-binding model. Hence, the density operators \n    $\\hat{\\rho}(\\mathbf{r}) = \\hat{\\psi}^\\dagger(\\mathbf{r}) \\hat{\\psi}(\\mathbf{r})$\nsuch that the field operators\n    $ \\hat{\\psi}^\\dagger(\\mathbf{r}) = \\sum_i \\phi_i^*(\\mathbf{r}) \\hat{c}_i^\\dagger $\n    and     $\\hat{\\psi}(\\mathbf{r}) = \\sum_j \\phi_j(\\mathbf{r}) \\hat{c}_j $\nare defined in terms of atomic orbital creation and annihilation operators. \n\nThe Kohn-Sham (KS) DFT+$U$+$V$ Hamiltonian\n\n\\begin{equation}\n    \\hat{H}^{\\text{DFT}+U+V} = \\sum_{i,j} h_{ij}^{\\text{KS}} \\hat{c}_i^\\dagger \\hat{c}_j\n\\end{equation}\nconsists of matrix elements\n\\begin{align*}\n    &h_{ij}^{\\text{KS}}  \\\\ &= \\int d^3r \\, \\phi_i^*(\\mathbf{r}) \\left[ -\\frac{\\hbar^2}{2m} \\nabla^2 + V^{\\text{DFT}}[\\rho](\\mathbf{r})+ V^{UV}[\\rho](\\mathbf{r}) \\right] \\phi_j(\\mathbf{r})\n\\end{align*}\nwhere the kinetic term gives rise to the DFT interatomic hopping terms, and the KS potential is\n\\begin{align}\n    V^{\\text{DFT}}[\\rho]({\\bf r}) \n    &=  \n   V_{\\rm ext}(\\mathbf{r}) + V^{\\text{H}}[\\rho] (\\mathbf{r}) + v_{\\text{xc}}[\\rho](\\mathbf{r}).\n\\end{align}\nApart from the standard KS terms a widely used way to deal with the double-counting corrected form of the $U$+$V$  potential\\cite{leiria_campo_jr_extended_2010} is the so-called ‘fully localized limit’ (FLL). In the restriction for the single $p_z$ orbitals at the carbon atoms\n\n\n\n\n\n\n\n\\begin{align*}\n    \\hat{V}^{UV}\n    =& \\hat{V}^{\\text{Hub}} - \\hat{V}^{\\text{dc}}\\\\\n    =&  \\sum_{i, \\sigma} \\frac{U_i}{2}  \\left( 1 - 2 \\rho^{\\sigma }_{i i}  \\right)  \\hat{c}^{\\dagger}_{i \\sigma} \\hat{c}_{i \\sigma}\\\\\n    &- \\sum_{\\langle i, j \\rangle, \\sigma} V_{ij}  \\rho^{\\sigma }_{i j } \\hat{c}^{\\dagger}_{i \\sigma} \\hat{c}_{j \\sigma}, \\nonumber\n\\end{align*}\nwhere $\\langle i, j \\rangle$ represents sums over all possible unequal sites within a given truncation range.\nThe inter-site interactions $V_{ij}$ are assumed to be the same as the\nCoulomb interaction averaged over all the states, just one orbital per atomic site in our case, such that \n\n\n\n\n\n\n\n\n\n\n\n\n$\nV_{ij} = \\langle \\phi_{i} \\phi_{j}\\mid V_{ee} \\mid \\phi_{i} \\phi_{j} \\rangle.\n$ The same consideration applies when incorporating the Hubbard correction into hopping parameters obtained from DFT~\\cite{hourahine_self-interaction_2007}.\n\nIn contrast, in this work, we employ hopping parameters obtained from DFT+U+V calculations rather than those obtained from the conventional DFT. By utilizing the parameters from which the double-counting interaction contained in $V^{\\text{DFT}}$ has been eliminated, we ensure that only the Hubbard-model-like potential term $V^{\\text{Hub}}$ corresponding to the variations in the density matrix need to be considered in subsequent tight-binding calculations. The DFT+U+V potential can be reformulated by decomposing it into a density-independent non-interacting part~$\\hat{V}^{\\text{NI}}$, an onsite interaction~$\\hat{V}^{\\text{on}}$, and an intersite interaction~$\\hat{V}^{\\text{inter}}$.\n\\begin{align*}\n    \\hat{V}^{\\text{DFT}+U+V}[\\rho]\n    =& \\hat{V}^{\\text{DFT}}[\\rho] +\\hat{V}^{\\text{Hub}}[\\rho] - \\hat{V}^{\\text{dc}}[\\rho]\\\\\n    =& \\hat{V}^{\\text{NI}} +\\hat{V}^{\\text{on}}[\\rho] + \\hat{V}^{\\text{inter}}[\\rho]\n\\end{align*}\nIn DFT+U+V, the onsite component of $V^{\\text{DFT}}$ is effectively removed by the onsite $V^{\\text{dc}}$, such that the remaining onsite term $V^{\\text{on}}$ corresponds to the Hubbard U term of the Hubbard potential $V^{\\text{Hub}}$. Similarly, since the intersite $V^{\\text{dc}}$ corresponds to the Hartree part of $V^{\\text{Hub}}$, the total intersite interaction can be expressed as the interaction part of $V^{\\text{DFT}}$, equally $V^\\text{H} + v_{xc}$, supplemented by the Fock (exchange) part of $V^{\\text{Hub}}$:\n\\begin{align*}\n    \\hat{V}^{\\text{on}}[\\rho] &= \\sum_i U_i \\rho^{\\tilde \\sigma}_i \\hat{c}^{\\dagger}_{i \\sigma} \\hat{c}_{i \\sigma}\\\\\n    \\hat{V}^{\\text{inter}}[\\rho] &= (\\sum_{ij} V^H_{ij}\\rho_j +v_{xc,i\\sigma}) \\hat{c}^\\dagger_{i\\sigma} \\hat{c}_{i\\sigma} -\\sum_{ij\\sigma}V_{ij} \\rho^\\sigma_{ij}\\hat{c}^\\dagger_{i\\sigma} \\hat{c}_{j\\sigma} \n\\end{align*}\nBased on this, the tight-binding parameters from the DFT+U+V can be derived as follows:\n\\begin{align}\n\\epsilon^{UV}_{i\\sigma} :=& h^{\\text{KS}}_{ii\\sigma} \\nonumber \\\\\n=& \\epsilon^{\\text{NI}}_{i\\sigma} + U_i \\rho_{i\\tilde\\sigma} + \\sum_j V^{\\rm H}_{ij} \\rho_j + v_{xc,i\\sigma},\\\\\nt^{UV}_{ij\\sigma} :=& h^{\\text{KS}}_{ij\\sigma} \\nonumber\\\\\n=& t^{\\text{NI}}_{ij\\sigma} - V_{ij} \\rho_{ij\\sigma},\n\\end{align}\nwhere $\\epsilon^{\\text{NI}}$ and $t^{\\text{NI}}$ represent the non-interacting parts of the onsite and hopping parameters, respectively. These are obtained from the kinetic term and the non-interacting potential~$V^{\\rm NI}$.\n\n\n\\section{Hopping range truncation and simplified effective tight-binding model}\\label{Appendix_FnGn}\n\n\\begin{table}[bt!] \n\\begin{threeparttable}\n\\caption{Hopping parameters of the simple model for RnG in $eV$. This model is defined by hopping parameters $t^{\\Delta l}_{\\alpha\\beta}$ for the layer difference $\\Delta l$ and sublattices $\\alpha,\\beta$, along with diagonal site potentials $u_{\\alpha}$~[For an example of application to R4G, see \\Cref{Fig:figB1_b}]. }\\label{tab:simplemodel}\n\\begin{ruledtabular}\n\\def\\arraystretch{1.3}\n\\begin{tabular}{cccc}\n$u_{A_1}/u_{B_n}$ & $u_{B_1}/u_{A_n}$ & $u_{other}$ & $t^0_{AB}$ \\\\\n\\hline\n0.018 & 0.067 & 0.0 & $-$3.415\\\\\n\\hline \\hline\n$t^1_{AA}/t^1_{BB}$ & $t^1_{AB}$ & $t^1_{BA}$ & \\\\\n\\hline\n0.161 & 0.279 & 0.343 & \\\\\n\\hline \\hline\n$t^2_{AA}/t^2_{BB}$ & $t^2_{AB}$ & $t^2_{BA}$ & \\\\\n\\hline\n0.009 & $-$0.009 & 0.019 & \\\\\n\\end{tabular}\n\\end{ruledtabular}\n\\end{threeparttable}\n\\end{table} \n\n\\begin{figure*}[bth!] \n\\subfloat[\\label{Fig:figB1_a}]{\\begin{tabular}{c}\n\\includegraphics[width=0.300\\textwidth]{Figures/FigB1/R3G_H0.pdf}\n\\includegraphics[width=0.300\\textwidth]{Figures/FigB1/R4G_H0.pdf}\\\\\n\\includegraphics[width=0.300\\textwidth]{Figures/FigB1/R3G_HUV.pdf}\n\\includegraphics[width=0.300\\textwidth]{Figures/FigB1/R4G_HUV.pdf}\n\\end{tabular}}\n\\subfloat[\\label{Fig:figB1_b}]{\\includegraphics[width=0.33\\textwidth]{Figures/FigB1/Simplemodel.pdf}}\n\\caption{ (a) Comparison of energy dispersions near the $K$ point for $\\hat{H}^0$ and self-consistent $\\hat{H}^{{\\rm TB}+U+V}$ in R3G and R4G using different models. Models shown include full tight-binding(FTB) model, the truncated models with F1G0 and F2G2, and the simple model in \\Cref{tab:simplemodel}. (b) Schematic representation of hopping processes when applying our simple model to R4G.}\\label{fig:figB1}\n\\end{figure*} \n\nThe $F_nG_n$ truncation~\\cite{jung_accurate_2014,jung_tight-binding_2013} determines the hopping model to preserve the zeroth and first-order $\\vec{k} \\cdot \\vec{p}$ expansion coefficients in the vicinity of the Dirac cone. For a truncation order $n$, the effective hopping parameters $t$ of orders less than $n$ are retained identical to those of the full tight-binding(FTB) model $t^{\\text{FTB}}$, while the $n$-th order truncated hopping coefficient is given by:\n\\begin{equation}\nt_{\\alpha\\beta,n} = \\frac{\\sum_{m=n}^\\infty c^{f/g}_m t^{\\text{FTB}}_{\\alpha\\beta,m}}{c^{f/g}_n}\n\\end{equation}\nwhere $\\alpha$ and $\\beta$ denote the sublattice indices, and the coefficients $c^{f/g}$ are associated with the $F/G$ categories and are derived from the structure factors at the Dirac point.\nFor the $F$ category, which encompasses sublattice pairs with distinct planar positions (e.g., $\\alpha\\beta = A_1 B_1, A_1A_2$), the coefficients are given by:\n\\begin{equation}\n    (c^f_1, c^f_2, \\dots) = (-1, +2, +1, -5, -4, +7,  \\dots).\n\\end{equation}\nConversely, when two sublattices occupy same planar positions, exemplified by $\\alpha\\beta = A_1A_1, A_1B_3, \\dots$, they fall under the $G$ category. For the cases, the coefficients are \n\\begin{equation}\n    (c^g_0, c^g_1, \\dots) = (1, -3, +6, -3, -6, +6, \\dots).\n\\end{equation}\nThe $t_0$ term in the $G$ category represents the onsite potential for $\\alpha = \\beta$, while for $\\alpha \\neq \\beta$, it denotes the perpendicular hopping.\n\nWe validated the $F_nG_n$ truncation for RnG systems. We examined the energy dispersion near the Dirac point for R3G and R4G using the FTB model, as well as the $F_2G_2$ and $F_1G_0$ models for both the $\\hat{H}^0$ and the $\\hat{H}^{{\\rm TB}+U+V}$. As illustrated in \\Cref{Fig:figB1_a}, we confirmed that not only $F_2G_2$ but also $F_1G_0$ terms listed in \\Cref{tab:simplemodel} can significantly simplify the model without substantial loss of accuracy, both for $\\hat{H}^0$ and the self-consistent $\\hat{H}^{{\\rm TB}+U+V}$. Furthermore, we observed that applying the simplified parameters of the $F_1G_0$ model for R3G to R4G resulted in no significant differences in band structure or ground state near the Dirac point and therefore can be used for all RnG models. \nThe full tight-binding parameters in HR-format of {\\textsc{Wannier90}}~\\cite{pizzi_wannier90_2020} for all RnG systems, along with a Python script for performing $F_nG_n$ truncation, are available in our GitHub repository~\\cite{hopping_github}.\n\n\\begin{figure*}[bt!] \n\\includegraphics[width=0.30\\textwidth]{Figures/FigB2/R3G.pdf}\n\\includegraphics[width=0.30\\textwidth]{Figures/FigB2/R4G.pdf}\n\\includegraphics[width=0.30\\textwidth]{Figures/FigB2/R5G.pdf}\n\\\\\n\\includegraphics[width=0.30\\textwidth]{Figures/FigB2/R6G.pdf}\n\\includegraphics[width=0.30\\textwidth]{Figures/FigB2/R7G.pdf}\n\\includegraphics[width=0.30\\textwidth]{Figures/FigB2/R8G.pdf}\n\\caption{$\\hat{H}^0$ band structure near the $K$ point and density of states of the $F_2G_2$ models for the RnG ($n=3, 4, \\dotsb, 8$).}\\label{fig:figB2}\n\\end{figure*} \n\nThe non-corrected Hamiltonian $\\hat{H}^0$ in \\Cref{Eq:eq_H0} yields the same band structure as in the DFT+$U$+$V$ step, representing the ground state under spin non-polarized conditions. Consequently, for R1G, R2G, and R3G, which exhibit non-magnetic ground states in $\\hat{H}^{{\\rm TB}+U+V}$, the band structures of $\\hat{H}^0$ and $\\hat{H}^{{\\rm TB}+U+V}$ are identical. \\Cref{fig:figB2} show the electronic band and DOS of the $F_2G_2$ model of the $\\hat{H}^0$ for RnG.\n\n\n\\section{Role of Coulomb correction cutoff in the extended Hubbard model}\\label{Appendix_UVrange}\n\n\\begin{figure*}[bt!] \n\\subfloat[\\label{Fig:figC1_a}]{\\includegraphics[width=0.85\\columnwidth]{Figures/FigC1/R3G_longrange_DOS.pdf}}\n\\subfloat[\\label{Fig:figC1_b}]{\\includegraphics[width=0.85\\columnwidth]{Figures/FigC1/Finitesizescaling.pdf}}\n\\caption{(a) Comparison of the density of states of TB+$U$+$V$ models in rhombohedral trilayer graphene with varying correction ranges $L_{UV}$.  For this calculation we have used all the distant hopping parameters in the band Hamiltonian. \n(b) Interaction range dependence of the peak to peak DOS distance $\\Delta_{peak}$. \nIn systems with long-range Coulomb interactions, where screening is suppressed, the separation between DOS peaks becomes larger, which can give the misleading appearance of an increasing band gap. The red dashed line fitting the data for $L_{UV} \\geq 8.5$ exhibits a $y$-intercept of 33.9 meV. }\\label{Fig:figC1}\n\\end{figure*} \n\nIn our calculation we have truncated the range of the Coulomb interaction up to the $V_2$ term for most of the reported results. \nHere we discuss the evolution of the band structure shapes in R3G for increasing Coulomb interaction range in our TB+$U$+$V$ calculation.\n\nIt is known that the Fermi velocity of single layer graphene depends on the range of the Coulomb interactions~\\cite{elias_dirac_2011}, and it is also expected to reshape the band structure of R3G with increasing $L_{UV}$ where the absence of the band gap $\\Delta$ and increased trigonal warping leads zero and finite values of the gap and the peak to peak distance of the DOS $\\Delta_{peak}$ respectively.\n\n\\Cref{Fig:figC1} shows the DOS obtained from our model for increasing extended Hubbard correction cutoff $L_{UV}$. In this calculation we used the full tight-binding hopping parameters for the band Hamiltonian instead of the $F_2G_2$ truncation. \n\n\n\n\n\n\n\n\nThe main effect we find is the reduction of the DOS near the Fermi level and increase of  $\\Delta_{peak}$.\nWe extrapolate the $\\Delta_{peak}$ value to the limit of infinite range by using a linear fit with respect to $L_{UV}^{-1}$ for some values of $L_{UV} \\geq 8.5~\\text{\\AA}$.\n\nThis extrapolation from finite-size scaling in \\Cref{Fig:figC1_b} yields a $y$-intercept of 33.9~meV. \n\n\n\n\n\n\n\n\\begin{comment}\n\n\\section{Hartree potential in biased system}\\label{Appendix_Hartree}\n\n\n\\begin{figure}[bt] \n\\includegraphics[width=0.95\\columnwidth]{Figures/FigC1/gap_biG.pdf}\n\\caption{External field dependence of the band gap $\\Delta$ in bilayer graphene using different approaches. The red cross marks denote experimental band gaps~\\cite{zhang_direct_2009} }\\label{Fig:bilayerG}\n\\end{figure} \n\nAs shown in \\Cref{Fig:bilayerG}, we tested the validity of the TB+$U$+$V$ model for describing the band gap in biased bilayer graphene. For the DFT(+$U$+$V$) calculations in this test, a $90 \\times 90 \\times 1$ $k$-space grid was employed. We first confirmed that DFT+$U$+$V$ can correct the band gap underestimation typically found in LDA and yields results in good agreement with experimental data~\\cite{zhang_direct_2009}. In contrast, TB+$U$+$V$ and DFT+$U$+$V$ results did not initially agree, and we found that a modification to the Hartree potential was necessary to reconcile the two approaches.\n\nIn \\Cref{eq:Hartree}, the $G_0$ ($\\bm{G}=0$) component is generally neglected under the assumption of charge neutrality. However, in a biased system, the $G_0$ term in the interlayer Hartree interaction needs to be considered. This term can be obtained by taking the limit $|q|\\to0$ as\n\\begin{equation} \\label{eq:G0}\nV_{G_0} = \\lim_{|q|\\to 0} V_q(|q|) = -2\\pi e^2 d .\n\\end{equation}\nHowever, the TB+$U$+$V$ results using \\Cref{eq:G0} tended to overestimate the band gap compared to DFT+$U$+$V$. Interestingly, we found that when the $G_0$ term in the Hartree potential is doubled, i.e.,\n\\begin{equation}\nV_{G_0} = -4\\pi e^2 d,   \n\\end{equation}\nthe TB+$U$+$V$ results precisely mimic those of DFT+$U$+$V$. Notably, Hartree model calculation based on tight-binding parameters from previously LDA-based study~\\cite{jung_tight-binding_2013} also produced consistent with the LDA when the doubled $G_0$ term was used. This suggests that the doubling of the $G_0$ component is general with respect to the transformation of the Kohn-Sham Hamiltonian into tight-binding bases. While we have not determined the exact origin of this doubling, one possible explanation could be the neglect of exchange-correlation potential. \n\n\n\\end{comment}\n\n\\newpage\n\n\n\\end{document}\n"}
{"paper_id": "2403-00531", "version": "2403-00531v1", "root_file_path": "d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\data_raw\\2403-00531\\tex\\2403-00531v1\\main.tex", "metadata": {"total_length": 62735, "merged_count": 1, "merged_files": ["main.tex"], "missing_files": []}, "content": "\\documentclass[a4paper,11pt]{article}\n\\pdfoutput=1 \n             \n\n\\usepackage{jcappub} \n                     \n\n\\usepackage{longtable}\n\\usepackage[toc]{appendix}\n\\usepackage[T1]{fontenc} \n\n\\usepackage[dvipsnames]{xcolor}\n\\usepackage{hyperref}\n\\usepackage{subcaption}\n\\usepackage{parskip}\n\\usepackage[backend=biber, style=numeric, sorting=none]{biblatex}\n\\addbibresource{ref.bib}\n\n\n\\title{\\boldmath Phenomenology of renormalization group improved gravity from the kinematics of SPARC galaxies. \n\n}\n\n\n\n\n\n\n\n\n\\author[a]{Esha Bhatia,}\n\\author[a]{Sayan Chakrabarti,}\n\\author[a]{Sovan Chakraborty}\n\n\n\n\n\n\\affiliation[a]{Indian Institute of Technology, Guwahati, India}\n\n\n\n\n\\emailAdd{b.esha@iitg.ac.in}\n\\emailAdd{sayan.chakrabarti@iitg.ac.in}\n\\emailAdd{sovan@iitg.ac.in}\n\n\n\n\n\n\\abstract{\n\nRenormalization Group correction to General Relativity (RGGR) proposes a logarithmic running of the gravitational coupling $\\left(G\\right)$, resulting \nin a modified description of gravity. This has the potential to explain the observed kinematics of the galaxies including the missing-mass problem. We, for the first time, based on the galaxy morphological types, investigate the dynamics of a diverse collection of galaxies present in the Spitzer Photometry for Accurate Rotation Curve (SPARC) catalog. We phenomenologically constrain the RGGR model parameter $\\left(\\bar\\nu\\right)$ along with the mass-to-light ratio for a sample of 100 SPARC galaxies, selected from four different morphological types, viz. early, spiral, late, and starburst. Our statistical analysis finds RGGR to fit the observed galaxy kinematics consistently. The constrained RGGR model parameter also supports the claim that it has a near-linear dependence on the galactic baryonic mass. From our morphology study, we find that the parameter $\\bar\\nu$ decreases from the early-type to the starburst galaxies. Finally, the renormalization group improved gravity is tested against the two established empirical relations for the SPARC catalog, viz., the Radial Acceleration Relation (RAR) and the Baryonic Tully Fisher relation (BTFR),\nboth are found to satisfy consistently.  \n\n}\n\n\n\n\\begin{document}\n\\maketitle\n\\flushbottom\n\\hbadness=999999\n\\vbadness=99999\n\\vfuzz=1pt\n\\tolerance=500\n\n\\section{Introduction} \n\\label{sec:intro}\n\nThe proposal to modify the action of the general theory of relativity (GR) has been found to be successful \\cite{Clifton:2011jh, Nojiri:2006ri} in mitigating the missing-mass problem \\cite{Zwicky:1937zza, Rubin:1978kmz, Rubin:1980zd}. This leads to the introduction of additional components such as scalar, vector, or tensor fields in the action of gravity \\cite{Clifton:2011jh, Nojiri:2006ri, Nojiri:2017ncd}. \nOn the other hand, the dark matter (DM) explanation to these galactic kinematic issues is found to have wider implications in cosmology and astroparticle physics \\cite{Perivolaropoulos:2021jda, Green:2021jrr, Khelashvili:2022ffq}. Although both modified gravity and DM scenarios are appealing, they suffer from certain challenges and shortcomings. \nOn one hand, we have $\\Lambda$CDM \\cite{Perivolaropoulos:2021jda}, which has issues such as core-cusp problem \\cite{de2010core}, coincidence problem \\cite{Velten:2014nra}, missing satellite problem \\cite{Klypin:1999uc, Kauffmann:1993gv} and many more \\cite{Perivolaropoulos:2021jda, Weinberg:1988cp, Ostriker:2003qj, Moore:1994yx}. \nSimilarly, any modified theory of gravity must demonstrate stability throughout all epochs of cosmic evolution without exhibiting any internal instabilities within its framework \\cite{Clifton:2011jh, Tsujikawa:2010zza, DeFelice:2006pg}. Consequently, the detection of gravitational waves has led to the exclusion of several alternative theories that were inconsistent with the obtained phenomenological constraint on the mass of the graviton \\cite{LIGOScientific:2016dsl, Yunes:2013dva}. Keeping all the past references in mind, our work in this paper discusses an alternative gravity scenario, which can explain the rotation curve for a large selection of galaxies. Specifically, we look into a model that studies the effect of the renormalization group (RG) running on the parameters under quantum field theory in curved spacetime. \\\\\n\\newline\n\n\n\n\n\nRenormalization Group correction to General Relativity (RGGR) proposes running of the gravitational coupling ($G$) with the energy scale \\cite{Reuter:2004nx, Shapiro:2004ch, Farina:2011me} and has \nbeen found to provide an alternative description to the galactic kinematic discrepancies \\cite{Rodrigues:2009vf, Rodrigues:2012qm, Rodrigues:2012wk}. The previous studies on the RG effect showed that a logarithmic running of $G$ can phenomenologically explain the kinematics of galaxies \\cite{Reuter:2004nx, Fabris:2012wg}. Additionally, the energy scale of the running parameter is related to the observable potential with the condition that it must satisfy the Tully-Fisher relation and regain the correct Newtonian limit on Solar system scales \\cite{Shapiro:2000dz, Rodrigues:2014xka} as well as on astrophysical scales \\cite{Domazet:2010bk}.  Similarly, earlier work on the RG study also accounts for the renormalization running of the cosmological constant ($\\Lambda$) \\cite{Shapiro:2000dz, Shapiro:2003ui}. However, due to the smallness of $\\Lambda$, any modification arising out of the running of $\\Lambda$ can be neglected on the galactic scales \\cite{Rodrigues:2012qm, Rodrigues:2012wk, Rodrigues:2014xka}. The running of the gravitational constant results in a phenomenological parameter ($\\bar\\nu$) for the RGGR model. Studies on the kinematics of spiral and elliptical galaxies show that the $\\bar{\\nu}$ is of the order of $10^{-7}$ \\cite{Rodrigues:2012qm, Rodrigues:2014xka, Oliveira2015TestingTA}. The parameter $\\bar{\\nu}$, which determines the strength of the running, has been found to have an almost linear relation with the baryonic mass of the galaxy and has been studied for a number of galaxies \\cite{Rodrigues:2014xka}. In the following, we analyze the RGGR model with an even larger selection of galactic rotation curve (RC), as compiled in the Spitzer Photometry for Accurate Rotation Curve (SPARC) \\cite{Lelli:2016zqa}. \\\\\n\\newline \nSPARC \\cite{Lelli:2016zqa} is a collection of 175 rotationally supported galaxies that contain the radial variation of net circular velocity, measured at near-infrared region. This catalog has been extensively used for the phenomenological study of a large number of DM \\cite{Ren:2018jpt, Khelashvili:2022ffq} and alternative gravity models \\cite{ Naik:2019moz, deAlmeida:2018kwq, Green:2019cqm}. In addition to the net circular velocity, the catalog compiles the mass modeling of individual baryonic components, viz., disk, bulge, and gas of the galaxy. The SPARC encompasses galaxies belonging to different morphological types. This includes galaxies ranging from Hubble type $0-12$ that consist of Early, Spiral, Late-type, and Starburst galaxies. Our analysis of the RGGR model extends to all $4$ morphological types. We aim to study the dependence of the alternative gravity parameter on the galaxy morphology, and in this regard, we, for the first time, based on the galaxy morphological types, investigate the dynamics of a diverse collection of galaxies. \n\nSPARC imposes certain selection criteria on the quality of observational data and the inclination of the galaxies from our reference frame, which reduces the number of relevant galaxies to $153$. Additionally, we impose a $\\chi^2$ cutoff to ensure that our analysis of the RGGR model shows a consistent fit with the observations. This further reduces the number of relevant galaxies to $100$. For each of these 100 galaxies, we constrain the RGGR parameter $\\bar\\nu$ from the observed RC data. The linear dependence of the parameter $\\bar{\\nu}$ for a wide range of masses in SPARC galaxies $(10^8-10^{11}~M_{\\odot})$ is also determined in our analysis. The model parameters of the individual galaxies are statistically constrained using an algorithm based on the Bayesian technique known as Markov Chain Monte Carlo (MCMC) \\cite{foreman2013emcee}. We also report the goodness of fit of individual galaxies by measuring the $\\chi^2$ value to explain the consistency of the gravity model. \\\\\n\\newline\nWe extend the phenomenology of the RGGR model further to the well-known empirical relations, the Radial Acceleration Relation (RAR) and the Baryonic Tully-Fisher Relation (BTFR).  Independent of the underlying gravity model, an analytical relation exists between the observed and baryonic accelerations for all the SPARC galaxies. This relation is known as RAR \\cite{mcgaugh2016radial}, which hints towards modified kinematics that govern the motion on galactic scales. Another such relation, known as the BTFR \\cite{Tully:1977fu, Lelli:2019igz} signifies that the baryonic mass contained within all the galaxies present in the SPARC catalog has a power-law dependence on the flat part of the circular velocity ($V_f$) far from the galactic center.\n Using the best-fit values obtained from the RC analysis with the emcee sampler, we establish the above two relations of the SPARC catalog. As both the relations were evaluated from the observational data, having no model dependence, such relations become a consistency check for the gravity model used. Our elaborate analysis aims to check the consistency of the RGGR model for a rather large selection of rotationally supported galaxies to date. \\\\\n\\newline\nThe layout of the paper is as follows. Section \\ref{sec:rggr} gives a detailed account of the RGGR model. The next Section \\ref{sec:gal} discusses in detail the galaxy catalog SPARC adopted for studying the alternative gravity model. The methodology and the computational packages used to sample the parameter space of the model are discussed in Section \\ref{sec:method}. The following Section \\ref{sec:res} and Section \\ref{sec:conc} discuss the results obtained and the conclusion for our study, respectively.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{RGGR gravity model}\n\\label{sec:rggr}\nRenormalization Group correction to General Relativity is a quantum gravity model that studies the variation of the gravitational coupling parameter ($G$) with the energy scale of the Universe \\cite{Reuter:2004nx, Farina:2011me, Rodrigues:2009vf}. The observations in the far infrared region established $G$ to be a constant in nature. However, this might not be true if one looks at GR as a field theory in curved spacetime. The beta function for the gravitational coupling constant $G$ has been studied to have a logarithmic dependence on the energy scale ($\\mu$) of the Universe. From the dimensional argument, one unique choice for the beta function has been taken to be of the following form \\cite{Farina:2011me,Rodrigues:2009vf}\n\\begin{equation}\\label{bet}\n    \\beta=\\mu\\frac{dG^{-1}}{d\\mu}=2\\nu\\frac{M^2_{planck}}{c\\hbar}=2\\nu G_0^{-1},\n\\end{equation}\nwhere $\\nu$ is a phenomenological parameter that is fixed from observational data, and $G_0$ is the bare value of the gravitational coupling parameter. The logarithmic dependence of $G$ with the energy scale as obtained from the solution of Eq.\\ref{bet} becomes\n\\begin{equation}\\label{gmu}\n    G(\\mu)=\\frac{G_0}{1+\\nu \\ln\\left(\\frac{\\mu^2}{\\mu_0^2}\\right)},\n\\end{equation}\nhere $\\mu_0$ is the energy scale defined such that $G(\\mu_0)=G_0$. As the variation in $G$ is small, the exact value of $\\mu_0$ is inconsequential. For the far-infrared (IR) region, where the coupling parameter $G$ is known to be a constant, the GR limit is regained by substituting $\\nu=0$. Thus, the coupling parameter $G$ in the Einstein-Hilbert action of gravity follows the RG flow given in Eq.\\ref{gmu}. For observations in galactic scales the variation in $\\nu$ is of the order $10^{-7}$ \\cite{Rodrigues:2014xka}. A similar $\\beta$ function can also be defined for the cosmological constant ($\\Lambda(\\mu)$) present in the action of gravity. However, due to the negligible effect on the astrophysical scales, it is ignored from the RC analysis of the galaxies \\cite{Rodrigues:2009vf}.\\\\\n\\newline\nAdditionally, to formulate a consistent theory with an observational basis, it is required to correlate the energy scale with some observable parameter. For example, on the cosmological scales, $\\mu$ is shown to have a relation with the energy scale of the Universe i.e., Hubble constant ($H$) \\cite{Fabris:2012wg}. Similarly, for the galactic scales, $\\mu$ has a functional dependence on the potential energy of the system. The relation between the energy scale dependence of the parameter with the potential is defined such that it regains the correct Newtonian limit and satisfies the Tully Fisher relation. Thus, the functional form that satisfies the mentioned condition can be assumed to be \\cite{Fabris:2012wg} \n\\begin{equation}\n    \\frac{\\mu}{\\mu_0}=\\left(\\frac{\\phi}{\\phi_0}\\right)^{\\alpha},\n\\end{equation}\nwhere $\\alpha$ is a phenomenological parameter constrained from the observations. The analysis of galactic dynamics with RGGR \\cite{Rodrigues:2012qm,Rodrigues:2014xka} suggests that the parameter $\\nu\\alpha\\equiv\\bar{\\nu}$ follows a close to linear relation with the baryonic mass of the galaxy. Such a linear relation has been shown from the study of RC of spiral galaxies \\cite{Rodrigues:2014xka} and from the fundamental plane of elliptical galaxies \\cite{Rodrigues:2012qm}.\nSolving for the weak-field limit, the effective circular velocity and acceleration take the following forms \\cite{Rodrigues:2009vf, Rodrigues:2012qm}\n\\begin{equation}\\label{vrgr}\n    V_{RGGR}(r)^2=V_N(r)^2\\left(1-\\frac{c^2\\bar{\\nu}}{\\phi_N(r)}\\right),\n\\end{equation}\n \n\\begin{equation}\\label{arggr}\n    a_{RGGR}(r)\\approx a_N(r)\\left(1-\\frac{c^2\\bar{\\nu}}{\\phi_N(r)}\\right),\n\\end{equation}\nwhere $V_N(r)$ and $a_N(r)$ are the Newtonian contribution to the velocity and acceleration, respectively, $c$ is the speed of light.  \nThus, on the galactic scales, the net circular velocity and acceleration of an object in addition to the Newtonian (baryonic) part have the extra contribution due to the modified gravity. This extra contribution depends on the model parameter $\\bar\\nu$ and the potential $\\phi_N(r)$. The potential $\\phi_N(r)$ arises from all the baryonic contents of the galaxy and is crucial to evaluate the effect of RGGR gravity. This requires mass modeling, i.e., modeling the radial dependence of the baryonic mass of the galaxy. In general, the baryonic structure of the galaxy is composed of three components, i.e., disk, gas, and bulge. We assume a radial exponential profile \\cite{Freeman:1970mx} for the disk and gas component. For the galaxies having bulge, the mass modeling is done assuming a spheroidal Hernquist profile \\cite{Hernquist1990AnAM}. Thus, for the given mass profiles, one may estimate or constrain the phenomenological parameter $\\bar{\\nu}$ from observations. For example, rotation curve studies \\cite{Rodrigues:2014xka} estimate $\\bar{\\nu}$ to be of the order of $10^{-7}$ to be consistent with the observations.\n\n \n Additionally, the constraints obtained on the solar system scales suggest the parameter $\\bar{\\nu}$ to be of the order of $10^{-17}$ \\cite{Farina:2011me}.\nThe limit is much smaller than the galaxy rotation curve estimates. This, however, is expected as the parameter $\\bar\\nu$ is found to have a linear dependence on the baryonic mass of the system. In comparison to the spiral galaxies, the mass contained within the Solar system is about ten times less, thus making the $\\bar{\\nu} \\sim 10^{-17}$ limit consistent. Similarly, in the case of Ultra-diffuse galaxies, whose masses are comparatively smaller than the rotationally supported galaxies, the evaluated $\\bar \\nu$ turns out to be $10^{-8}$ \\cite{Bhatia:2023pts}. \n \n In what follows, we analyze the consistency of the RGGR model for a large selection of galaxies from the SPARC catalog. These galaxies belonging to different morphologies with different characteristics give a measure of the model parameter's variation over the nature of the galaxies.  \n\n\n\n\\section{Galaxy catlogue}\n\\label{sec:gal}\n\nSpitzer Photometry for Accurate Rotation Curve (SPARC) contains the collection of $175$ rotationally supported galaxies measured at near-IR photometry \\cite{Lelli:2016zqa}. The catalog contains the mass models of galaxies covering a broad range with varying luminosities, morphologies, rotation velocity, gas content, etc. \\cite{Lelli:2016zqa}. The dominant contribution for a rotationally supported galaxy comes from the disk, which extends up to a few kiloparsecs. The additional baryonic contribution arises from the bulge (if present) superimposed at the disk's center and gas diffused throughout the galaxy. Depending upon the size of the bulge and structure of spiral arms present, the SPARC is majorly divided into $4$ morphological types. The different morphologies are represented via Hubble type ($H$), which for the catalog ranges between $H: 0-12$ \\cite{mihalas1981galactic}. The Hubble type is a representation of the evolutionary stage of the galaxy. In general, the size of the bulge and tightness of spiral arms for a rotationally supported galaxy reduces as we move up in the Hubble type.  The first Hubble category ranging from $H: 0-2$ belongs to the Early type, which contains galaxies of type S0, Sa, and Sab, respectively. Such galaxies are distinguished by the presence of a prominent bulge at the center and tightly woven spiral arms. The second category consists of Spiral galaxies ($H: 3-6$) where the features such as bulge and tightness of spiral arms start decreasing. The next two categories include Late-type ($H: 7-9$) and Starburst ($H: 10-12$). These galaxies contain almost no visible bulge present at the center of the galaxy. The last category i.e., Starburst galaxies are known to have no spiral structure in the outer parts and are characterized by their diffused shape. The Starburst galaxies are also known to have a higher star formation rate. Thus, this catalog represents a wide spectrum of galaxies and  makes a versatile ground to test our alternative gravity model.\\\\\n\\newline\n\n\\textbf{\\textit{Rotation Curve:}} Given the radial variation of the velocity for individual baryonic components of a galaxy, the total Newtonian contribution can be written as:\n\\begin{equation}\n    v_{N}^2(r)=\\gamma_d~ v_{disk}^2(r)+\\gamma_b v_{bulge}^2(r)+|v_{gas}(r)|~v_{gas}(r);\n\\end{equation}\nwhere $v_{disk}(r)$, $v_{bulge}(r)$ and $v_{gas}(r)$ represent the disk, bulge, and gas velocity components for a particular galaxy in the SPARC catalog. The radial variation of the individual baryonic component for each rotationally supported galaxy is provided within the SPARC. The catalog additionally contains the accurate HI and H$\\alpha$ measurement of the total circular velocity $v_{obs}(r)$, along with the error bars $\\sigma(r)$ for every galaxy. Also, the two baryonic components are scaled by a factor $\\gamma_d$ and $\\gamma_b$, which measure the mass-to-light ratio for the disk and bulge part, respectively. The data from the catalog clearly shows that the net contribution of velocity coming from the baryonic component is insufficient to explain the observed velocity, $v_{obs}(r)$ of the galaxy. This leaves room for the additional components, such as DM or MOG which can be added to the Newtonian part to explain the overall rotational stability of the galaxy.  \\\\\n\\newline \nUnder the assumption that the gravity on the galactic scale is RGGR, the modified kinematics of objects within the galaxy is expressed as given in Eq.\\ref{vrgr}. The equation shows that the net circular velocity has an additional contribution from the RGGR gravity.\n\nTherefore, comparing the total observed circular velocity from the SPARC catalog with the RGGR velocity form (Eq.\\ref{vrgr}) helps to check the consistency of the gravity model. This requires the variation of the model parameter of the theory within the allowed parameter space. This analysis aims to look for the parameters that give a consistent fit to the observed RC of the galaxy sample. The best-fit values obtained in the analysis are also used to construct the empirical relation RAR and BTFR.  \\\\\n\\newline\n\\textbf{\\textit{RAR:}} The analysis of SPARC data shows that the total baryonic acceleration $(a_{bar})$ cannot explain the net observed acceleration $(a_{obs})$ and follow a certain analytical relation known as RAR \\cite{mcgaugh2016radial}. This analysis has been found to be true for the 153 galaxies of the catalog irrespective of their morphological types, thus indicating a new dynamical law governing galaxy kinematics. The empirical RAR relationship obtained from SPARC is defined as \\cite{mcgaugh2016radial}\n\\begin{equation}\\label{rar}\n    a_{obs}(R)=\\frac{a_{bar}(R)}{1-\\exp(-\\sqrt{a_{bar}(R)/a_*})},\n\\end{equation}\nwhere $a_*$ is the acceleration scale parameter and has best-fit value $a_*=1.2\\times 10^{-10}$ ms$^{-2}$. \nFor a rotationally supported galaxy, the net centripetal acceleration is defined in terms of observed velocity in SPARC catalog\n\\begin{equation}\\label{acc}\n    a_{obs}(R)=\\frac{v_{obs}^2(R)}{R}=\\left|\\frac{\\partial \\phi_{tot}(R)}{\\partial R}\\right|,\n\\end{equation}\nwhere $\\phi_{tot}$ is the total potential i.e., total force per unit mass acting on a point particle. Similarly, $a_{bar}(R)$ is the linear sum of the acceleration for different baryonic components (disk, gas, and bulge) within the galaxy and can be estimated from the SPARC data using\n\\begin{equation}\\label{bar}\n    a_{bar}(R)=\\frac{v_{N}^2(R)}{R}=\\frac{\\gamma_d~v_{disk}^2(R)+\\gamma_b~ v_{bulge}^2(R)+|v_{gas}(R)|~v_{gas}(R)}{R}= \\left|\\frac{\\partial \\phi_{bar}(R)}{\\partial R}\\right|.\n\\end{equation}\n\nBoth the net circular velocity ($v_{obs}(r)$) and individual baryonic components mentioned in Eq.\\ref{acc} and \\ref{bar} are known observationally in SPARC. This makes the relation an empirical one as it assumes no prior knowledge about the DM or MOG model and is purely from the observational data in SPARC.  \\\\\n\\newline\nFor our analysis, we aim to probe RAR in the context of the RGGR gravity. The analytical expression for the net acceleration (Eq.\\ref{arggr}) in RGGR includes a $\\bar{\\nu}$ dependent extra component to the Newtonian part. As explained in the previous discussion in the context of RC, the best-fit value of this $\\bar\\nu$ parameter for each galaxy can be estimated from the circular velocity fitting with the SPARC data. Thus, the consistency of the RGGR predicted net acceleration ($a_{RGGR}$) with respect to the $a_{obs}(R)$ can be probed by the RAR. In the following, we compare the observed RAR with the same empirical relation Eq.\\ref{rar} in the context of the RGGR model for each data point of the qualifying SPARC galaxies. \\\\\n\\newline\n\\textbf{\\textit{BTFR:}} SPARC also shows a tight correlation between the dynamics of the galaxy with the baryonic distribution. BTFR suggests that the stellar mass of the galaxy has a power law dependence on the flat part of the circular velocity, \n\\begin{equation}\\label{btfr}\n    M_{bar}=A V_f^x.\n\\end{equation}\nHere $M_{bar}$ refers to the baryonic mass contained within the galaxy and $V_f$ is the velocity measured along the flat part of the RC. \nThe optimal value of the free parameters ($A, x$) are obtained from the study of the SPARC catalog, and are estimated to be $A=50$ M$_{\\odot}$km$^{-4}$s$^4$, $x=4$ \\cite{McGaugh:2005qe}. Similar to RAR, BTFR is also known to be empirical in nature as it assumes no underlying gravity model and is strictly obtained from the SPARC data. For every galaxy in SPARC, the baryonic mass is the linear sum of disk, bulge, and gas components.  For the RGGR model, the total circular velocity is estimated by $V_{RGGR}$ (Eq.\\ref{vrgr}) and is dependent on the free parameters ($\\gamma_d$, $\\gamma_b$, $\\bar{\\nu}$) which are already constrained using the observed RC of the galaxy. Thus, the $V_f$ for the RGGR will also get modified. The relation between the $M_{bar}$ and the RGGR predicted flat velocity for each qualifying SPARC galaxy should follow observed BTFR. This additional check for RGGR is crucial for the gravity model to remain phenomenologically consistent.  \n\\newline\n\n\n\n\n\n\n\\section{Methodology}\n\\label{sec:method}\nUsing the RC data for the qualifying SPARC galaxies, we fit the model parameters considering RGGR as the underlying gravity model. These free parameters are constrained using the publicly available $emcee$ package \\cite{foreman2013emcee} in PYTHON. This package works on the principle of Markov Chain Monte Carlo (MCMC), which samples the posterior distribution of the free model parameters. For a given observational data set $\\mathcal{D}$, the posterior probability $\\mathcal{P}$($\\theta$|$\\mathcal{D}$) for unknown set of parameters $\\theta$ is defined as\n\\begin{equation}\\label{sampler}\n    \\mathcal{P}(\\theta|\\mathcal{D})\\propto \\mathcal{L}(D|\\theta)\\pi(\\theta),\n\\end{equation}\nwhere $\\mathcal{L}(D|\\theta)$ is the likelihood which determines the probability of a data for a given model with free parameters, and $\\pi(\\theta)$ represents the priors imposed on the free parameters. Assuming that the errors on the observed circular velocity follow a Gaussian distribution, the likelihood for each galaxy is written as\n\\begin{equation}\n    \\mathcal{L}_g=(2\\pi)^{-n/2}\\left\\{\\prod_{i=1}^n \\sigma(r_i)^{-1}\\right\\}\\exp\\left\\{-\\frac{1}{2}\\sum_{i=1}^n\\left(\\frac{v_{g,obs}(r_i)-v_{tot}(r_i,\\theta)}{\\sigma(r_i)}\\right)^2\\right\\},\n\\end{equation}\nhere $n$ represents the number of observational data points over which the likelihood is summed. Also, $v_{obs}(r)$ and $\\sigma(r)$ are the total circular velocity and error on the observations defined at each radial point within the SPARC catalog. The analytical velocity $v_{tot}(r,\\theta)$ computed at a certain radius for a given set of free parameters is given in Eq.\\ref{arggr}. The velocity contribution $v_{tot}(r,\\theta)$ is dependent on free parameters, which are phenomenologically constrained by comparing with the observational circular velocity. The unknown parameters include the normalization mass-to-light factor ($\\gamma$) introduced for the baryonic component i.e., disk and bulge of the galaxy, respectively. $v_{tot}$ is also dependent on the model parameter $\\bar{\\nu}$. Thus, $\\theta$ is composed of \\{$\\gamma_d$, $\\gamma_b$, $\\bar{\\nu}$\\} which are to be estimated independently for each galaxy. \\\\\n\\newline\nRegarding the $\\pi(\\theta)$, we assume flat priors for the model parameters. The mass-to-light ratio for both disk ($\\gamma_d$) and bulge ($\\gamma_b$) are assumed to have no radial dependence on the galaxy and are varied in the range [$0.3$, $0.8$] \\cite{deAlmeida:2018kwq, schombert2014stellar, Meidt:2014mqa}. Additionally, for the mass-dependent parameter $\\bar{\\nu}$, a wide range of parameter space is looked into and is varied within $10^{-9}$ $\\le$ $\\bar{\\nu}$ $\\le$ $10^{-6}$. This range is motivated from the previous analysis of spiral and elliptical galaxies where $\\bar{\\nu}$ is found to vary in order of $10^{-7}$ \\cite{Rodrigues:2014xka}. To ensure the convergence of the chain, we run the sampler for a sufficient number of steps such that the acceptance fraction lies within the range $0.3-0.5$ \\cite{foreman2013emcee}. Additionally, by estimating the autocorrelation time ($\\tau$) for each galaxy, we discard $\\tau$ number of steps as burn-in before performing posterior analysis. We run a sufficient number of steps i.e., $50~\\tau$ as specified in $emcee$ \\cite{foreman2013emcee}, such that convergence is achieved. \\\\\n\\newline\n\n\n \n\n\nCertain selection criteria are also adopted before choosing a galaxy from the SPARC catalog. This includes galaxies with asymmetrical RC i.e., $Q > 2$, and face-on galaxies having $i<30^{\\circ}$ which are exempted from our analysis \\cite{Lelli:2016zqa}. Additionally, to ensure that we have a consistent RC fit, we reject galaxies whose goodness of fit i.e., $\\chi^2_{red} \\ge 6$. The last quality cut is imposed manually to ensure a consistent fit for the model with the observations.\\\\\n\n\n\n\n\\section{Results}\n\\label{sec:res}\nIn the following, we analyze the consistency of the RGGR framework with the observational circular velocity for our selection of SPARC galaxies. The RC analysis is done for galaxies belonging to different morphological types. The free model parameters are constrained using the emcee sampler scanning the parameter space to evaluate the best-fit values. For this, we look into the rotation curves of individual galaxies present within SPARC. Additionally, we also study the mass-dependent nature of the phenomenological parameter $\\bar{\\nu}$. These best-fit values must also satisfy the fundamental relationships RAR and BTFR, hence we compare our results for the RGGR model with both these relationships. \n\n\n\n\n\\begin{longtable}{ |p{2.8cm}||c|c|c|c|c|  }\n\\caption{The best-fit parameters obtained from the emcee sampler. Corresponding to the RGGR gravity model, for every galaxy, there are $3$ parameters ($\\gamma_d$, $\\gamma_b$, $\\bar{\\nu}$) that are statistically constrained from the observed circular velocity.} \\label{par_sparc} \\\\\n \\hline\n \\multicolumn{1}{|c|}{\\textbf{Galaxy Name}} & \\multicolumn{1}{c|}{\\textbf{Galaxy type}} & \\multicolumn{1}{c|}{\\textbf{$\\gamma_d$}} & \\multicolumn{1}{c|}{\\textbf{$\\gamma_b$}} & \\multicolumn{1}{c|}{\\textbf{$\\bar{\\nu} \\times 10^7$}} & \\multicolumn{1}{c|}{\\textbf{$\\chi^2_{red}$}}  \\\\ \\hline  \n \\endfirsthead\n\\multicolumn{6}{c}\n{{\\bfseries \\tablename\\ \\thetable{} -- continued from previous page}} \\\\\n\\hline\n\\multicolumn{1}{|c|}{\\textbf{Galaxy Name}} & \\multicolumn{1}{c|}{\\textbf{Galaxy type}} & \\multicolumn{1}{c|}{\\textbf{$\\gamma_d$}} & \\multicolumn{1}{c|}{\\textbf{$\\gamma_b$}} & \\multicolumn{1}{c|}{\\textbf{$\\bar{\\nu} \\times 10^7$}} & \\multicolumn{1}{c|}{\\textbf{$\\chi^2_{red}$}}\n\\endhead\n \\hline \\multicolumn{6}{r}{\\textit{Continued on next page}} \\\\ \\hline\n    \\endfoot\n    \\hline \\hline\n    \\endlastfoot \n NGC7814 & Early & $0.53$ & $0.31$ & $7.23$ & $1.18$\\\\\n \\hline\n NGC4138 & Early& $0.58$ & $0.50$ & $1.58$ & $0.56$\\\\\n \\hline\n UGC02487 & Early & $0.80$ & $0.79$ & $8.26$ & $4.07$ \\\\\n \\hline\n UGC06614 & Early & $0.52$ & $0.32$ & $3.60$ & $1.00$\\\\\n \\hline\n UGC03546 & Early & $0.59$ & $0.34$ & $2.15$ & $0.90$\\\\\n \\hline\n UGC03205 & Early & $0.55$ & $0.80$ & $3.42$ & $4.82$ \\\\\n \\hline\n \n \n UGC08699 & Early & $0.69$ & $0.51$ & $1.61$ & $1.40$ \\\\\n \\hline\n UGC11914 & Early & $0.30$ & $0.74$ & $6.11$ & $1.68$\\\\\n \\hline\n  UGC06786 & Early & $0.39$ & $0.55$ & $4.33$ & $2.28$\\\\\n \\hline\n NGC1090 & Spiral & $0.54$ & - & $1.65$ & $2.85$\\\\ \n\\hline\nNGC2683 & Spiral  & $0.56$ & $0.54$ & $1.88$ & $0.69$\\\\\n\\hline\nNGC2841 & Spiral & $0.78$ &$0.79$ & $5.61$ &$1.86$ \\\\\n\\hline\nNGC3769 & Spiral & $0.33$ &-& $1.41$ & $0.91$\\\\\n\\hline\nNGC2955 & Spiral & $0.32$ &$0.56$ & $4.69$ &$4.89$\\\\\n\\hline\nNGC4013 & Spiral &$0.30$ &$0.37$ & $2.53$ &$2.38$\\\\\n\\hline\nNGC0801 & Spiral &$0.57$& $~~-~~$&$1.88$&$5.80$\\\\\n\\hline\nNGC5005 & Spiral & $0.46$ &$0.35$ &$3.68$ &$0.13$\\\\\n\\hline\nNGC6195 & Spiral & $0.31$ & $0.56$ & $4.16$ &$2.17$\\\\\n\\hline\nNGC7331 & Spiral & $0.30$&$0.36$&$3.61$&$2.00$\\\\\n\\hline\nUGC12506 & Spiral & $0.79$ & - & $4.40$ & $3.74$\\\\\n\\hline\nUGC11455 & Spiral & $0.41$ & - & $5.09$ & $4.16$\\\\\n\\hline\nUGC09037 & Spiral & $0.30$ & - & $1.83$ & $3.18$\\\\\n\\hline\nUGC07151 & Spiral & $0.78$ & - & $0.39$ &$2.39$\\\\\n\\hline\nUGC06983 & Spiral & $0.79$ & - & $1.09$ & $2.42$\\\\\n\n\n\\hline\nNGC4559 & Spiral & $0.41$ & - & $0.70$ & $1.02$\\\\\n\\hline\nNGC4100 & Spiral & $0.63$ & - & $1.89$ & $2.11$ \\\\\n\\hline\nNGC4088 & Spiral & $0.31$ &-& $1.62$ & $1.10$\\\\\n\\hline\nNGC4085 & Spiral & $0.32$ & - & $1.13$ & $3.05$\\\\\n\\hline\nNGC3972 & Spiral & $0.68$ & - & $1.26$ & $2.01$\\\\\n\\hline\nNGC3893 & Spiral & $0.38$ & - & $2.48$ & $0.53$\\\\\n\\hline\nNGC2998 & Spiral & $0.67$ & - & $2.53$ & $2.47$\\\\\n\\hline\n\n\nNGC0100 & Spiral & $0.32$ & - & $1.05$ & $0.63$\\\\\n\\hline\n\n\nESO079-G014 & Spiral & $0.77$ & - & $2.21$ & $3.85$\\\\\n\\hline\nNGC0289 & Spiral & $0.65$ & - & $2.27$ & $1.89$\\\\\n\\hline\nNGC2976 & Spiral & $0.54$ & - & $0.93$ & $0.50$\\\\\n\\hline\nNGC3521 & Spiral & $0.44$ & - & $2.22$ & $0.76$\\\\\n\\hline\nNGC3949 & Spiral & $0.39$ & - & $1.39$ & $0.52$\\\\\n\\hline\nNGC3953 & Spiral & $0.75$ & - & $0.76$ & $0.55$\\\\\n\\hline\nNGC3992 & Spiral & $0.76$ & - & $3.65$ & $1.87$\\\\\n\\hline\nNGC4051 & Spiral & $0.64$ & - & $0.31$ & $1.30$\\\\\n\\hline\nNGC4183 & Spiral & $0.79$ & - & $0.88$ & $1.61$\\\\\n\\hline\n\n\nNGC4010 & Late-type & $0.36$ & - & $1.42$ & $1.44$\\\\\n\\hline\nNGC0300 & Late-type & $0.69$ & - & $0.74$ & $0.76$\\\\\n\\hline\nUGC04278 & Late-type & $0.79$ & - & $0.75$ & $3.46$\\\\\n\\hline\nUGC05721 & Late-type & $0.76$ & - & $0.71$ & $2.26$\\\\\n\\hline\nNGC7793 & Late-type & $0.64$ & - & $0.53$ & $0.64$\\\\\n\\hline\nUGC06446 & Late-type & $0.79$ & - & $0.77$ & $4.34$\\\\\n\\hline\nUGC07603 & Late-type & $0.37$ & - & $0.48$ & $0.61$\\\\\n\\hline\nUGC06930 & Late-type & $0.77$ & - & $0.84$ & $1.54$ \\\\\n\\hline\nUGC08550 & Late-type & $0.79$ & - & $0.30$ & $3.87$\\\\\n\\hline\nUGC12732 & Late-type & $0.79$ & - & $0.89$ & $5.93$\\\\\n\\hline\nUGC11557 & Late-type & $0.35$ & - & $0.38$ & $0.84$\\\\\n\\hline\nUGC08490 & Late-type & $0.78$ & - & $0.61$ & $0.54$\\\\\n\\hline\nUGC07089 & Late-type & $0.45$ & - & $0.51$ & $0.13$\\\\\n\\hline\nUGC07261 & Late-type & $0.68$ & - & $0.59$ & $0.92$\\\\\n\\hline\nUGC07323 & Late-type & $0.60$ & - & $0.56$ & $0.21$\\\\\n\\hline\nUGC07399 & Late-type & $0.75$ & - & $1.21$ & $2.64$\\\\\n\\hline\nUGC06399 & Late-type & $0.73$ & - & $0.71$ & $0.51$\\\\\n\\hline\nUGC06917 & Late-type & $0.75$ & - & $0.94$ & $1.63$\\\\\n\\hline\nUGC05986 & Late-type & $0.39$ & - & $1.79$ & $1.02$\\\\\n\\hline\nUGC04499 & Late-type & $0.76$ & - & $0.39$ & $1.89$ \\\\\n\\hline\n\n\nNGC3109 & Late-type & $0.51$ & - & $0.53$ & $0.89$\\\\\n\\hline\nNGC0055 & Late-type & $0.34$ &-& $0.51$ & $1.15$\\\\\n\\hline\nESO116-G012 & Late-type & $0.66$ & - & $0.90$ & $1.88$\\\\\n\\hline\nIC2574 & Late-type & $0.42$ & - & $0.44$ & $2.73$\\\\\n\\hline\nUGCA444 & Starburst & $0.70$ & - & $0.10$ & $0.81$\\\\\n\\hline\nUGC08837 & Starburst & $0.43$ &  - & $0.17$ & $0.48$\\\\\n\\hline\nUGC07559 & Starburst & $0.46$ & - & $0.12$ & $0.30$ \\\\\n\\hline\n\n\nUGC07690 & Starburst & $0.71$ & - & $0.25$ & $0.38$ \\\\\n\\hline\nUGC07866 & Starburst & $0.56$ & - & $0.12$ & $0.21$\\\\\n\\hline\nUGC06923 & Starburst &$0.48$ & - & $0.49$ & $0.77$ \\\\\n\\hline\nUGC05414 & Starburst & $0.50$ & - & $0.36$ & $0.08$\\\\\n\\hline\nUGC05829 & Starburst & $0.77$ &-& $0.37$ & $2.14$\\\\\n\\hline\nNGC4068 & Starburst & $0.41$ & - & $0.12$ & $0.23$\\\\\n\\hline\nNGC3741 & Starburst & $0.73$ & - & $0.30$ & $2.62$ \\\\\n\\hline\nESO444-G084 & Starburst & $0.66$ & - &$0.32$ & $1.08$\\\\\n\\hline\nKK98-251 & Starburst & $0.60$ & - & $0.10$ &$0.36$ \\\\\n\\hline\n\n\\end{longtable}\n\n\\subsection{Fit to the observed Rotation Curve}\n\\label{sec:rc}\n\nFitting the RC of each SPARC galaxy with the RGGR gravity model constrains the free parameters. It includes two mass modeling parameters defined for the baryonic component of a galaxy i.e., $\\gamma_d$ and  $\\gamma_b$. The additional model parameter, i.e., $\\bar{\\nu}$, comes from the choice of RGGR gravity and has been found to have an almost linear dependence on the luminous mass of the galaxy. We analyze the behavior of the phenomenological parameter $\\bar{\\nu}$ with the baryonic mass for the large sample of SPARC galaxies. In particular, we study the consistency of the RGGR governed model for all four morphological types of the \ngalaxies present in SPARC. This compares the phenomenological consistency of the RGGR gravity for all the different galaxy types.\n\n\n\n\\begin{figure}[t]\n    \\centering\n\\begin{subfigure}[t]{0.45\\textwidth}\n\n\n\\includegraphics[width=\\linewidth,height=0.9\\linewidth]{7814.pdf} \n\n\\end{subfigure}\n\n\\begin{subfigure}[t]{0.45\\textwidth}\n\n\n\\includegraphics[width=\\linewidth,height=0.9\\linewidth]{4138.pdf} \n\\end{subfigure}\n\n\\begin{subfigure}[t]{0.45\\textwidth}\n\n\\includegraphics[width=\\linewidth,height=0.9\\linewidth]{6614.pdf} \n\n\\end{subfigure}\n\n\\begin{subfigure}[t]{0.45\\textwidth}\n\n\\includegraphics[width=\\linewidth,height=0.9\\linewidth]{3546.pdf} \n\n\\end{subfigure}\n\\caption{For Early-type galaxies the RC for $4$ specimen galaxies are shown in the four panels above. The black solid line is the circular velocity when RGGR gravity contributes in addition to the baryonic part. The black dots with green error bars are the circular velocity data obtained from SPARC \\cite{Lelli:2016zqa}. The dashed grey line represents the gaseous component of the galaxy. The dot-dashed line shows the variation of disk velocity within the galaxy, and the bulge part is plotted via the dotted line. }\n\\label{Fig:early}\n\\end{figure}\n\\paragraph{Early-type galaxies}\n\\hfill \\break\n\nThe first category belongs to the early-type galaxies, which can be identified from the presence of a bulge at the center and tight indistinguishable spiral arms in the outer parts of the disk. From the whole set of early-type, we analyze $9$ galaxies that satisfy the selection criteria. Out of these $9$ galaxies, we illustrate the RC fitting for $4$ of them in Fig.\\ref{Fig:early}. This includes NGC7814 shown in the upper left panel, NGC4138 in the upper right, UGC06614 in the lower left, and UGC03546 in the lower right panel of Fig.\\ref{Fig:early}. For each galaxy shown, the grey dots with green error bars represent the observational total circular velocity traced by the HI component within the galaxy. We also plot the individual baryonic components i.e., disk, bulge, and gas, for each galaxy using the gray lines. The dashed gray line represents the radial variation of the gas velocity. The disk and bulge components of the galaxy scaled by their respective mass-to-light ratio are shown via. the dashed-dotted and dotted lines, respectively.\n\nIn particular, the first panel at the top left shows the RC for NGC7814. The constraints on the free parameters ($\\gamma_d, \\gamma_b $ and  $\\bar{\\nu} \\times 10^7$) obtained from our analysis give $0.53$, $0.31$ and $7.23$, respectively. The radial variation of the circular velocity (Eq.\\ref{vrgr}) in the RGGR model with these best-fit parameters is plotted using a solid black line. The obtained value of $\\chi^2_{red}$ for the galaxy is $1.18$ indicating a good fit to the observational data of NGC7814. Similarly, the obtained value of parameters and goodness of fit for the other early-type galaxies in the panel can be found in Table.\\ref{par_sparc}. The measured goodness of fit for the early-type galaxies as compiled in Table.\\ref{par_sparc} points to positive results towards the RGGR model.  \n\n\\begin{figure}[t]\n    \\centering\n\\begin{subfigure}{0.45\\textwidth}\n\n\n\\includegraphics[width=\\linewidth,height=0.9\\linewidth]{1090.pdf} \n\n\\end{subfigure}\n\n\\begin{subfigure}{0.45\\textwidth}\n\n\n\\includegraphics[width=\\linewidth,height=0.9\\linewidth]{2683.pdf} \n\n\\end{subfigure}\n\n\\begin{subfigure}{0.45\\textwidth}\n\n\\includegraphics[width=\\linewidth,height=0.9\\linewidth]{2841.pdf} \n\n\\end{subfigure}\n\n\\begin{subfigure}{0.45\\textwidth}\n\n\\includegraphics[width=\\linewidth,height=0.9\\linewidth]{3769.pdf} \n\n\\end{subfigure}\n\\caption{For Spiral-type galaxies, the RC for $4$ specimen galaxies are shown in the panel. The black solid line is the circular velocity when RGGR gravity contributes in addition to the baryonic part. The black dots with green error bars are the circular velocity data obtained from SPARC \\cite{Lelli:2016zqa}. The dashed grey line represents the gaseous component of the galaxy. The dot-dashed line shows the variation of disk velocity within the galaxy, and the bulge part is plotted via the dotted line.}\n\\label{Fig:spiral}\n\\end{figure}\n\n\\paragraph{Spiral-type galaxies}\n\\hfill\\break\nThe second class of galaxies mentioned in SPARC belongs to the Spiral type. One example of such a galaxy is our own Milky Way. Such galaxies are characterized by the presence of distinct spiral structures at the outer parts of the disk. Additionally, the bulge at the center is of comparatively smaller size. SPARC has a large collection of spiral galaxies and our selection criteria gives us 39 such galaxies. Similar to Fig.\\ref{Fig:early} we show $4$ representative galaxies i.e., NGC1090, NGC2683, NGC2841 and NGC3769 in Fig.\\ref{Fig:spiral}. Out of the $4$ galaxies shown, only two have a visible bulge component present and are represented by a dotted gray line in the plot. The gray points with error bars represent the total circular velocity observed in SPARC. Similarly, the gray lines represent the individual baryonic components in a galaxy.\\\\\n\\newline\n The best fit parameters for the first subplot for NGC1090 shown at the top left panel in Fig.\\ref{Fig:spiral} are $\\gamma_d = 0.54$ and $~\\bar{\\nu} \\times 10^7 = 1.65$ with $\\chi^2_{red}$ equal to $2.85$. For the obtained parameters, the net contribution to the total circular velocity, which is the sum of baryonic and RGGR components, is shown by a solid black line. The goodness of fits evaluated for each galaxy as given in Table.\\ref{par_sparc} show that the RGGR model is a consistent choice in explaining the observed circular velocity of the spiral SPARC galaxies. \n\\begin{figure}\n  \\centering\n\\begin{subfigure}{0.45\\textwidth}\n\n\n\\includegraphics[width=\\linewidth,height=0.9\\linewidth]{4010.pdf} \n\\end{subfigure}\n\n\\begin{subfigure}{0.45\\textwidth}\n\n\n\\includegraphics[width=\\linewidth,height=0.9\\linewidth]{300.pdf} \n\\end{subfigure}\n\n\\begin{subfigure}{0.45\\textwidth}\n\n\\includegraphics[width=\\linewidth,height=0.9\\linewidth]{4278.pdf} \n\n\\end{subfigure}\n\n\\begin{subfigure}{0.45\\textwidth}\n\n\\includegraphics[width=\\linewidth,height=0.9\\linewidth]{5721.pdf} \n\n\\end{subfigure}\n\\caption{For Late-type galaxies, the RC for $4$ specimen galaxies are shown in the panel. The black solid line is the circular velocity when RGGR gravity contributes in addition to the baryonic part. The black dots with green error bars are the circular velocity data obtained from SPARC \\cite{Lelli:2016zqa}. The dashed grey line represents the gaseous component of the galaxy. The dot-dashed line shows the variation of disk velocity within the galaxy, and the bulge part is plotted via the dotted line. }\n\\label{Fig.late}\n\\end{figure}\n\\paragraph{Late-type dwarf}\n\\hfill\\break\nThe third category of galaxies belongs to late-type dwarf, where the bulge is too faint to be observed. Thus, the baryonic component of the galaxy mostly constitutes disk and gas only. From the total late-type dwarf galaxies in the catalog, $34$ galaxies fulfill the selection criteria. Similar to the above two categories, we plot four late-type dwarf galaxies in Fig.\\ref{Fig.late}. The galaxies that are illustrated include NGC4010 (top-left), NGC0300 (top-right), UGC04278 (bottom-left), and UGC05721 (bottom-right). The details on the plot corresponding to the observed circular velocity of a galaxy and its baryonic components follow the same convention as mentioned in the discussion of the previous two types of galaxies.\n\\newline\nThe circular velocity in the RGGR framework can be obtained by substituting the best-fit values of the model parameters i.e., ($\\gamma_d$, $\\gamma_b$, $\\bar{\\nu}$) as given in Table.\\ref{par_sparc} in Eq.\\ref{vrgr}. This radial variation is shown using the solid black line in Fig.\\ref{Fig:spiral}. \nThe $\\chi_{red}^2$ computed in Table.\\ref{par_sparc} for all the galaxies shows that the late-type dwarfs are a good fit with the RGGR model. This consistent behavior can also be visualized from the four sample plots shown in Fig.\\ref{Fig.late}. \n\\begin{figure}[t]\n    \\centering\n\\begin{subfigure}{0.45\\textwidth}\n\n\n\\includegraphics[width=\\linewidth,height=0.9\\linewidth]{8837.pdf}\n\\end{subfigure}\n\n\\begin{subfigure}{0.45\\textwidth}\n\n\n\\includegraphics[width=\\linewidth,height=0.9\\linewidth]{7559.pdf} \n\n\\end{subfigure}\n\n\\begin{subfigure}{0.45\\textwidth}\n\n\\includegraphics[width=\\linewidth,height=\\linewidth,height=0.9\\linewidth]{7690.pdf} \n\n\\end{subfigure}\n\n\\begin{subfigure}{0.45\\textwidth}\n\n\\includegraphics[width=\\linewidth,height=0.9\\linewidth]{7866.pdf} \n\n\\end{subfigure}\n\\caption{For Starburst galaxies, the RC for $4$ specimen galaxies are shown in the panel. The black solid line is the circular velocity when RGGR gravity contributes in addition to the baryonic part. The black dots with green error bars are the circular velocity data obtained from SPARC \\cite{Lelli:2016zqa}. The dashed grey line represents the gaseous component of the galaxy. The dot-dashed line shows the variation of disk velocity within the galaxy, and the bulge part is plotted via the dotted line.}\n\\label{Fig:star}\n\\end{figure}\n\\paragraph{Starburst galaxy}\n\\hfill\\break\n\nThe fourth morphological type belongs to starburst galaxies which are relatively young with a high star formation rate. Such galaxies are dominated by gas and thus make a versatile region to look for the signature of the alternative gravity model. The morphology of such systems shows a diffused structure with no visible bulge and spiral arms. We studied $18$ galaxies belonging to this particular morphological type and found that RGGR can give consistent fits with the observed circular velocities. As an illustration following the convention used in other morphological types, we plot 4 starburst galaxies i.e., UGC08837, UGC07559, UGC07690, and UGC07866 in Fig.\\ref{Fig:star}. The baryonic contribution for these types of galaxies comes from the gas and disk component only, as starbursts have no visible bulge present within. This is represented in the above plot by gray lines following the same convention used in preceding cases. Also, the total observed circular velocity with error bars is shown using gray points.\\\\\n\\newline\n \n The constrained free parameters ($\\gamma_d$, $\\gamma_b$, $\\bar{\\nu}~ \\times~ 10^{7}$) obtained for the aforementioned galaxies can be referred from Table.\\ref{par_sparc}. From the obtained values of parameters, the total circular velocity evaluated for each galaxy is shown by a solid black line in Fig.\\ref{Fig:star}. As can be seen from the plots, the observational circular velocity fits well with the analytical velocity in all $4$ cases. Similarly, this consistent nature of the gravity model can also be quantified for all the starburst galaxies from the goodness of fit as mentioned in Table.\\ref{par_sparc}.  \n\nFor the four different morphological types of galaxies discussed above, we find that RGGR is a phenomenologically consistent theory of gravity. The best fit values for the $\\bar\\nu$ parameter lies in the range $10^{-6}-10^{-8}$. The parameters $\\gamma_d$ and $\\gamma_b$ fall in the range $0.3-0.8$, consistent with the choices of our priors. We also observe from Table.\\ref{par_sparc} that the values of $\\bar\\nu$ are larger (in the range close to $10^{-6}$) for the early-type galaxies compared to the other types. Interestingly, the $\\bar\\nu$ values decrease as we go across the Hubble type towards the late-type galaxies with $\\bar\\nu$ around $10^{-8}$. This relation between the parameter $\\bar\\nu$ and the galaxy type can be understood from the baryonic mass content of the galaxies and is discussed in the next subsection.  \n\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=.99\\linewidth, height=.7\\textwidth]{gamma_rel.pdf}\n    \\caption{The plot shows the variation of $\\gamma_d$. The varying morphological types are represented using different markers. The square markers represent the early type, and the circles are for the spiral galaxies. Additionally, $\\gamma_d$ for late-type and starburst are shown via triangle and diamond markers.}\n    \\label{Fig:gama}\n\\end{figure}\n The mass modeling parameters constrained for each galaxy in our analysis also include $\\gamma_d$ and $\\gamma_b$ and are assumed to be a constant having no radial dependence. The bulge component is prominent and can be majorly seen only in the early-type galaxies. Therefore, we utilize the disk normalization factor to show the variation of $\\gamma_d$ for the galaxies of different morphological types. The plot containing the values of $\\gamma_d$ for the $100$ galaxies present in our analysis is shown in Fig.\\ref{Fig:gama}. The behavior of $\\gamma_d$ represents that this parameter varies throughout the range of $[0.3,0.8]$ for all galaxies. The different markers on the plot belong to the different morphological types of galaxies present in SPARC. The pink points give the value of $\\gamma_d$ for early-type galaxies.\nSimilarly, green square, grey triangles, and violet diamond shapes represent $\\gamma_d$ for the spiral, late-type, and starburst galaxies respectively. Additionally, looking into the behavior of $\\bar{\\nu}$ in Table.\\ref{par_sparc}, shows that as we go from early type to starburst galaxies, the magnitude of the phenomenological parameter decreases. Such behavior of $\\bar{\\nu}$ with the stellar mass of galaxies is studied in detail in the next subsection.\n\n\n\\subsection{Relation of \\texorpdfstring{$\\bar{\\nu}$}{TEXT} with baryonic matter}\n\nTo investigate the RGGR model parameter dependence on the baryonic mass, we rely on the same galaxies from the SPARC catalog. The baryonic mass of a rotationally supported galaxy consists of the bulge, disk, and gas components, as mentioned earlier. Each component of the galaxy (stellar$+$gas) is assumed to follow a distinct density distribution. The radial variation of matter distribution for disk and gas is assumed to vary exponentially \\cite{Lelli:2016zqa, Freeman:1970mx}. Similarly, for the galaxies having bulges, the density variation can be fitted with the Hernquist profile \\cite{Hernquist1990AnAM}. The total baryonic content for a galaxy is a linear mass sum of stellar and gas components. The galaxies in SAPRC constitute a wide range of masses varying from $10^8-10^{11}~ M_{\\odot}$ \\cite{Lelli:2016zqa}.\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=.99\\linewidth, height=.7\\textwidth]{mass_rel.pdf}\n    \\caption{The plot relates the phenomenological parameter $\\bar{\\nu}$ with the baryonic mass of the galaxy. The individual points are the representation of the $\\bar{\\nu}$ evaluated for each galaxy studied in SPARC. The almost linear relation shown in the plot affirms the previous mass dependence claim for the RGGR parameter. The different markers in the plot show the different morphological types of galaxies, with the circle representing early-type, a square belonging to spiral, a triangle showing late-type dwarfs and a diamond for starburst galaxies.}\n    \\label{Fig:massrel}\n\\end{figure}\nFor the scenario where the underlying gravity is assumed to be RGGR, the velocity contribution is the sum of the Newtonian part plus an additional term dependent on a free parameter ($\\bar{\\nu}$). This phenomenological parameter varies independently for each galaxy and is constrained from the study of RC as discussed in the previous section. A comparison of the constrained $\\bar{\\nu}$ obtained along with the stellar mass of the galaxy is represented via an individual datapoint in Fig.\\ref{Fig:massrel}. The figure shows that the magnitude of the phenomenological parameter $\\bar{\\nu}$ increases almost linearly with the increasing mass of the galaxies. Our analysis is thus justified with the previous study \\cite{Rodrigues:2014xka} done for the same gravity model. Different representations are used in the plot to categorize galaxies belonging to various morphological types. The pink round pointers belong to the early-type, the green square shows the spiral, the gray triangle is for the late-type, and the purple diamond represents the starburst galaxies. Clearly from the figure, we can see that the baryonic mass decreases from the early-type galaxies to the late-type ones, and accordingly, the value of $\\bar \\nu$ reduces linearly: a feature which can also be seen from the data provided in Table.\\ref{par_sparc}, i.e. $\\bar\\nu$ is smaller for the relatively younger galaxies. Such a linear relation of the model parameter with the mass of the system points towards the non-fundamental behavior of the RGGR model. However, as can be seen from the analysis of RC, the gravity model is consistent with the observations on astrophysical scales.\n\\subsection{Empirical relations for SPARC}\n\n\\subsubsection{Radial Acceleration Relation (RAR)}\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=.99\\linewidth, height=.6\\textwidth]{RAR.pdf}\n    \\caption{The plot compares the RAR relation where the total observed acceleration is evaluated in the RGGR framework. The black solid line shows the empirical relation Eq.(\\ref{rar}) as obtained from the observations in SPARC. The green scatter points refer to the individual data points of all the galaxies analyzed where the net contribution to the acceleration gets modified as given by Eq.(\\ref{arggr}) }\n    \\label{Fig:rar}\n\\end{figure}\nThe data points of the SPARC galaxies projecting the relation between the observed Eq.\\ref{acc} and baryonic acceleration Eq.\\ref{bar} is shown to follow an empirical relation as mentioned in Eq.\\ref{rar}. This shows that in weak-gravity regions, the observed and baryonic acceleration follows the relation $a_{obs}\\propto \\sqrt{a_{bar}}$ and depends linearly in high acceleration scales as is expressed by the empirical relation. Thus, if RGGR is a consistent gravity model, it must satisfy RAR for the SPARC galaxies. This relation has been looked into and verified for a large sample of galaxies present in the SPARC catalog without any prior assumptions for the DM or alternative gravity model. In the alternative gravity scenario, total circular velocity and acceleration are modified according to Eq.\\ref{vrgr} and Eq.\\ref{arggr} respectively. The net acceleration is the sum of contribution coming from different baryonic components of a galaxy ($a_{bar}(r)$) with an additional part dependent on $\\phi_N(r)$ and $\\bar{\\nu}$ as expressed in Eq.\\ref{arggr}. In our analysis, we compare the modified acceleration $(a_{RGGR}(r))$ with $a_{bar}(r)$ to study the behavior of RAR in an alternative gravity framework. \\\\\n\\newline\nThe evaluation of the empirical relation requires the knowledge of free parameters, which is taken from the RC analysis in Sec.\\ref{sec:rc}. The RAR behavior as determined for the individual datapoint of all the selected galaxies in the RGGR framework is shown in Fig.\\ref{Fig:rar}. The gray solid line is the analytical relation fitted from the observational data and has the form as given in Eq.\\ref{arggr}. Also, for each galaxy in our analysis, we determine the baryonic acceleration ($a_{bar}(r)$) at every radial point specified in the SAPRC catalog. Similarly, we also compute the RGGR acceleration ($a_{RGGR}(r)$) at every radial point. Thus, each green dot on the plot refers to the RGGR and baryonic acceleration at a specific radius for a given galaxy. The collection of points in Fig.\\ref{Fig:rar} contains the contribution for all the 100 galaxies selected in our analysis.\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.95\\linewidth,height=13.0cm]{cornerplot2.pdf}\n    \\caption{The posterior distribution of the parameter space in BTFR Eq.\\ref{btfr_lin} evaluated using BayesLineFit \\cite{Lelli:2019igz}.}\n    \\label{fig:btfrplot2}\n\\end{figure}\nResidual computed from the comparison of the relation with the 1840 data points, as illustrated in the figure, turns out to be $0.33$ dex. Thus, from the best-fit parameters from the RC analysis, RAR behaves satisfactorily in the context of the RGGR model. \n\n\n\n\n\n \n\n\n\n\\subsubsection{Baryonic Tully Fisher Relation (BTFR)}\nSPARC galaxies are also found to follow the BTFR (Eq.\\ref{btfr}), which provides a power law relation between the baryonic mass and the velocity measured at the flat part of the rotation curve. This relation holds irrespective of the gravity model. Therefore, for RGGR to be a consistent choice as an alternative gravity model, BTFR must be satisfied by the RGGR-predicted dynamics for the SPARC galaxies. To probe the BTFR,\n\\begin{equation}\\label{btfr_lin}\n    \\log(M_{bar})=x\\log(V_f)+\\log~A,\n\\end{equation}\n\n\nwe consider the $V_f$ corresponding to the RC fit with RGGR for each galaxy. The flat velocity ($V_f$) for each galaxy in the RGGR framework can be evaluated from the fit of the RC (Sec. \\ref{sec:rc}) at a given radius. The baryonic mass $M_{bar}$ for a galaxy is a sum of stellar and gas components. The velocities of these different components are connected to their respective mass profile. For a given mass profile, $M_{bar}$ can be estimated, particularly for the SPARC galaxies, the bulge and disk are assumed to have spheroidal and exponential distribution respectively \\cite{Lelli:2016zqa}. BTFR has also been found to be sensitive on the radial choice \\cite{Lelli:2019igz} at which $V_f$ is measured for SPARC galaxies. We evaluate the relation at $3.2~R_d$, which has shown an optimal behavior to the relation. The choice of $3.2~R_d$ \\cite{ Lelli:2019igz,romanowsky2012angular} ensures that $80\\%$ of the stellar matter is encompassed within the radius. Almost for all the SPARC galaxies we select, the distance of $3.2~R_d$ from the center of the galaxy corresponds to the flat region of the RC. \n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.95\\linewidth,height=10.0cm]{btfr_3.2rd.pdf}\n    \\caption{The BTFR with the flat circular velocity evaluated in RGGR model. The above plot estimates $V_{f}$ for the RGGR model at $3.2~R_d$. The gray data points are the measure of flat velocity evaluated for individual galaxies. The green solid line shows the analytical best fit, consistent with the observed $V_f$ in the RGGR framework.}\n    \\label{fig:btfrplot1}\n\\end{figure}\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\nWe use BayesLineFit package \\cite{Lelli:2019igz} which is based on \\textit{emcee} algorithm to find ($x$, $\\log A$). The best-fit value obtained for the parameters ($x$, $\\log A$) is found to be ($3.55$, $2.70$) with an orthogonal scatter of $0.08$ dex. We also report the vertical scatter given by \\cite{Lelli:2019igz}:\n\\begin{equation}\n   \\sigma_0=\\sqrt{\\frac{1}{n} \\sum^n [\\log(M_{bar})-x\\log(V_f)-\\log A]^2},\n\\end{equation}\nwhere $n$ is the number of galaxies that are selected in our analysis. The observed $\\sigma_0$ for the case of $3.2~R_d$ comes out to be $0.54$. The posterior distribution for the fitting parameters i.e., slope ($x$), intercept ($\\log~A$), and intrinsic scatter, are shown in Fig.\\ref{fig:btfrplot2}. The solid red lines in the plot point towards the maximum likelihood values for the parameters. Given the best-fit parameters, BTFR for all the galaxies included in our analysis is illustrated in Fig.\\ref{fig:btfrplot1}. The solid gray circles represent the flat velocity for each SPARC galaxy evaluated at $r=3.2~R_d$. Additionally, the green solid line shows a fit to the linear equation Eq.\\ref{btfr_lin} for the best-fit parameters obtained in our analysis. The BTFR plot in Fig.\\ref{fig:btfrplot1} shows that the flat velocity evaluated at $3.2~R_d$ shows a consistent match with the observational results. \n\n\n\n\n\n\n\n \n  \n   \n    \n\n\\section{Conclusion}\n\\label{sec:conc}\n\nOur analysis focuses on the renormalization group improved gravity to explain the kinematics for a collection of rotationally supported galaxies compiled in the SPARC catalog. \nWe, for the first time, study the dependence of the alternative gravity parameter on the galaxy morphology. We have looked into four different morphological types of galaxies, viz. early, spiral, late, and starburst. We have constrained the model parameter $\\bar \\nu$ for each galaxy taken from all four morphological types and have found that the RGGR consistently fits the observed net circular velocity. Our statistical analysis has probed this consistency for the individual galaxies using the goodness of fit. We have also checked the linear dependence of the model parameter $\\bar \\nu$ on the baryonic mass of the galaxy.\n\nThe constrained values for the parameter $\\bar\\nu$ for our sample of the SPARC galaxies lie in the range $10^{-6}-10^{-8}$ and are consistent with the variation of the masses of the galaxies under consideration. This implies that the older and heavier galaxies lead to a larger $\\bar \\nu$. Indeed, we have found that the parameter $\\bar\\nu$ decreases from the older galaxies to the younger ones. \n\\newline\nWe have further verified our goodness of fit from the RC analysis in light of the well-known empirical relations: RAR and BTFR.\nIn particular, the RAR compares the radial variation of the observed and the baryonic acceleration. An additional factor comes in the baryonic acceleration due to the RGGR model. Our analysis has found that the RAR in the RGGR framework is in line with the established analytical relation obtained from previous observations. \nOn a similar note, BTFR compares the baryonic mass with the observed flat velocity ($V_f$) of a galaxy, suggesting a power law dependence between them. The exact power law index has been found to be sensitive to the choice of the flat velocity radius. We have selected a radius of $3.2 R_d$, which is known to show optimal behavior with the observations. Our analysis finds a tight correlation of the baryonic mass with flat velocity measured at $3.2R_d$ and has obtained a small orthogonal scatter of $0.08$. \nThis scatter may reduce even further for different choices of the radius of the $V_f$ and by relieving the assumption of universal exponential density profile for the diffused gas component of the galaxy which requires further study.\n\\\\\n\\newline\n\nAn important aspect of the RGGR model is that the model parameter $\\bar\\nu$ is dependent on the mass of the gravitating system. Many other alternative gravity models exhibit dependence of the model parameters on the scale of the system. Though this scale dependence of the phenomenological parameter may question the fundamental nature of these alternative gravity models, they may remain a consistent theory of gravity. Any alternative gravity model must satisfy the consistency criteria, i.e., it must be relativistic, self-consistent, and a complete theory free of ghosts and instabilities. The theory also requires to be able to explain the gravitational dynamics both in the strong and weak-field regimes as well as it should regain a valid Newtonian limit on the Solar System scales. RGGR thus remains a phenomenologically consistent theory of gravity and very importantly, satisfies the galactic kinematic observations for a large selection of SPARC galaxies. \n \n\n\n\n\n\n\n\n\n\\acknowledgments\nSovan Chakraborty acknowledges the support of the funding from DST-SERB\nprojects CRG/2021/002961 and MTR/2021/000540. Sayan\nChakrabarti would like to acknowledge the support from\nthe DST-SERB research grant MTR/2022/000318. All the\nauthors would like to acknowledge discussions and valuable inputs from Sayak Dutta. Esha Bhatia is indebted to the Ministry of Human Resource Development, Government of India, for assistance through a doctoral fellowship.\n\n\n\n\\end{document}"}
{"paper_id": "2403-00531", "version": "2403-00531v2", "root_file_path": "d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\data_raw\\2403-00531\\tex\\2403-00531v2\\apssamp.tex", "metadata": {"total_length": 72185, "merged_count": 1, "merged_files": ["apssamp.tex"], "missing_files": []}, "content": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\documentclass[\n reprint,\n\n\n\n\n\n\n\n\n\n\n amsmath,amssymb,\n aps,\npra,\n\n\n\n\nfloatfix,\n]{revtex4-2}\n\n\\usepackage{graphicx}\n\n\\usepackage{csvsimple}\n\\usepackage{dcolumn}\n\\usepackage{array} \n\n\n\\usepackage{bm}\n\\usepackage[dvipsnames]{xcolor}\n\n\\usepackage{subcaption}\n\\usepackage{multirow}\n\\usepackage{times}\n\\usepackage{placeins}\n\\usepackage{hyperref}\n\\usepackage{booktabs}\n\\bibliographystyle{apsrev4-1}\n\n\\begin{document}\n\\hbadness=99999\n\n\\preprint{APS/123-QED}\n\n\\title{Phenomenology of renormalization group improved gravity from the kinematics of SPARC galaxies.}\n\n\n\\author{Esha Bhatia}\n\\email{b.esha@iitg.ac.in}\n\\author{Sayan Chakrabarti}\n\\email{sayan.chakrabarti@iitg.ac.in}\n\\author{Sovan Chakraborty}\n\\email{sovan@iitg.ac.in}\n\n\n\n\\affiliation{Department of Physics, Indian Institute of Technology, Guwahati 781039, India}\n\n\n\n\n\\date{\\today}\n             \n\n\\begin{abstract}\nRenormalization Group correction to General Relativity (RGGR) proposes a logarithmic running of the gravitational coupling $\\left(G\\right)$, resulting \nin a modified description of gravity. Analogous to the proposal of dark matter, RGGR has the potential to explain the observed kinematics of the galaxies including the missing-mass problem. We, for the first time, based on the galaxy morphological types, investigate the dynamics of a diverse collection of galaxies present in the Spitzer Photometry for Accurate Rotation Curve (SPARC) catalog. We phenomenologically constrain the RGGR model parameter $\\left(\\bar\\nu\\right)$ along with the mass-to-light ratio for a sample of 100 SPARC galaxies, selected from four different morphological types, viz. early, spiral, late, and starburst. Our statistical analysis finds RGGR to fit the observed galaxy kinematics consistently. Additionally, we observe that apart from the galaxies where both RGGR and NFW fares equally well, RGGR is favored in more galaxies when compared to NFW. The constrained RGGR model parameter also supports the claim that it has a near-linear dependence on the galactic baryonic mass. From our morphology study, we find that the parameter $\\bar\\nu$ decreases from the early-type to the starburst galaxies. Finally, the renormalization group improved gravity is tested against the two established empirical relations for the SPARC catalog, viz., the Radial Acceleration Relation (RAR) and the Baryonic Tully Fisher relation (BTFR), both are found to satisfy consistently. Importantly, these results are \nare found to be nearly independent of the choice of the Bayesian priors on the model parameters. \n\n\\end{abstract}\n\n\n\\maketitle\n\n\n\n\\section{Introduction} \n\\label{sec:intro}\n\nThe proposal to modify the action of the general theory of relativity (GR) has been found to be successful \\cite{Clifton:2011jh, Nojiri:2006ri} in mitigating the missing-mass problem \\cite{Zwicky:1937zza, Rubin:1978kmz, Rubin:1980zd}. This leads to the introduction of additional components such as scalar, vector, or tensor fields in the action of gravity \\cite{Clifton:2011jh, Nojiri:2006ri, Nojiri:2017ncd}. \nOn the other hand, the dark matter (DM) explanation to these galactic kinematic issues is found to have wider implications in cosmology and astroparticle physics \\cite{Perivolaropoulos:2021jda, Green:2021jrr, Khelashvili:2022ffq}. Although both modified gravity and DM scenarios are appealing, they suffer from certain challenges and shortcomings. \nOn one hand, we have $\\Lambda$CDM \\cite{Perivolaropoulos:2021jda}, which has issues such as core-cusp problem \\cite{de2010core}, coincidence problem \\cite{Velten:2014nra}, missing satellite problem \\cite{Klypin:1999uc, Kauffmann:1993gv} and many more \\cite{Perivolaropoulos:2021jda, Weinberg:1988cp, Ostriker:2003qj, Moore:1994yx}. \nSimilarly, any modified theory of gravity must demonstrate stability throughout all epochs of cosmic evolution without exhibiting any internal instabilities within its framework \\cite{Clifton:2011jh, Tsujikawa:2010zza, DeFelice:2006pg}. Consequently, the detection of gravitational waves has led to the exclusion of several alternative theories that were inconsistent with the obtained phenomenological constraint on the mass of the graviton \\cite{LIGOScientific:2016dsl, Yunes:2013dva}. Keeping all the past references in mind, our work in this paper discusses an alternative gravity scenario, which can explain the rotation curve for a large selection of galaxies. Specifically, we look into a model that studies the effect of the renormalization group (RG) running on the parameters under quantum field theory in curved spacetime. \\\\\n\\newline\n\n\n\n\n\nRenormalization Group correction to General Relativity (RGGR) proposes running of the gravitational coupling ($G$) with the energy scale \\cite{Reuter:2004nx, Shapiro:2004ch, Farina:2011me} and has \nbeen found to provide an alternative description to the galactic kinematic discrepancies \\cite{Rodrigues:2009vf, Rodrigues:2012qm, Rodrigues:2012wk}. The previous studies on the RG effect showed that a logarithmic running of $G$ can phenomenologically explain the kinematics of galaxies \\cite{Reuter:2004nx, Fabris:2012wg}. Additionally, the energy scale of the running parameter is related to the observable potential with the condition that it must satisfy the Tully-Fisher relation and regain the correct Newtonian limit on Solar system scales \\cite{Shapiro:2000dz, Rodrigues:2014xka} as well as on astrophysical scales \\cite{Domazet:2010bk}.  Similarly, earlier work on the RG study also accounts for the renormalization running of the cosmological constant ($\\Lambda$) \\cite{Shapiro:2000dz, Shapiro:2003ui}. However, due to the smallness of $\\Lambda$, any modification arising out of the running of $\\Lambda$ can be neglected on the galactic scales \\cite{Rodrigues:2012qm, Rodrigues:2012wk, Rodrigues:2014xka}. The running of the gravitational constant results in a phenomenological parameter ($\\bar\\nu$) for the RGGR model. Studies on the kinematics of spiral and elliptical galaxies show that the $\\bar{\\nu}$ is of the order of $10^{-7}$ \\cite{Rodrigues:2012qm, Rodrigues:2014xka, Oliveira2015TestingTA}. The parameter $\\bar{\\nu}$, which determines the strength of the running, has been found to have an almost linear relation with the baryonic mass of the galaxy and has been studied for a number of galaxies \\cite{Rodrigues:2014xka}. In the following, we analyze the RGGR model with an even larger selection of galactic rotation curve (RC), as compiled in the Spitzer Photometry for Accurate Rotation Curve (SPARC) \\cite{Lelli:2016zqa}. \\\\\n\\newline \nSPARC \\cite{Lelli:2016zqa} is a collection of 175 rotationally supported galaxies that contain the radial variation of net circular velocity, measured at near-infrared region. This catalog has been extensively used for the phenomenological study of a large number of DM \\cite{Ren:2018jpt, Khelashvili:2022ffq} and alternative gravity models \\cite{ Naik:2019moz, deAlmeida:2018kwq, Green:2019cqm}. In addition to the net circular velocity, the catalog compiles the mass modeling of individual baryonic components, viz., disk, bulge, and gas of the galaxy. The SPARC encompasses galaxies belonging to different morphological types. This includes galaxies ranging from Hubble type $0-12$ that consist of Early, Spiral, Late-type, and Starburst galaxies. Our analysis of the RGGR model extends to all $4$ morphological types. We aim to study the dependence of the alternative gravity parameter on the galaxy morphology, and in this regard, we, for the first time, based on the galaxy morphological types, investigate the dynamics of a diverse collection of galaxies. \n\nSPARC imposes certain selection criteria on the quality of observational data and the inclination of the galaxies from our reference frame, which reduces the number of relevant galaxies to $153$. Additionally, we impose a $\\chi^2$ cutoff to ensure that our analysis of the RGGR model shows a consistent fit with the observations. This further reduces the number of relevant galaxies to $100$. For each of these 100 galaxies, we constrain the RGGR parameter $\\bar\\nu$ from the observed RC data. The linear dependence of the parameter $\\bar{\\nu}$ for a wide range of masses in SPARC galaxies $(10^8-10^{11}~M_{\\odot})$ is also determined in our analysis. The model parameters of the individual galaxies are statistically constrained using an algorithm based on the Bayesian technique known as Markov Chain Monte Carlo (MCMC) \\cite{foreman2013emcee}. We also report the goodness of fit of individual galaxies by measuring the $\\chi^2$ value to explain the consistency of the gravity model. \\\\\n\\newline\nWe extend the phenomenology of the RGGR model further to the well-known empirical relations, the Radial Acceleration Relation (RAR) and the Baryonic Tully-Fisher Relation (BTFR).  Independent of the underlying gravity model, an analytical relation exists between the observed and baryonic accelerations for all the SPARC galaxies. This relation is known as RAR \\cite{mcgaugh2016radial}, which hints towards modified kinematics that governs the motion on galactic scales. Another such relation, known as the BTFR \\cite{Tully:1977fu, Lelli:2019igz} signifies that the baryonic mass contained within all the galaxies present in the SPARC catalog has a power-law dependence on the flat part of the circular velocity ($V_f$) far from the galactic center.\n Using the best-fit values obtained from the RC analysis with the emcee sampler, we establish the above two relations of the SPARC catalog. As both the relations were evaluated from the observational data, having no model dependence, such relations became a consistency check for the gravity model used. Our elaborate analysis aims to check the consistency of the RGGR model for a rather large selection of rotationally supported galaxies to date. \\\\\n\\newline\nSimilar to the RGGR analysis, we look into the SPARC galaxies with an alternative DM scenario. In the DM paradigm, the net matter density of a galaxy, in addition to baryonic matter, has a contribution from DM \\cite{Bertone:2004pz}. In our study of RC for the SPARC galaxies, we assume a particular well-known density profile obtained from the N-body simulation named Navarro-Frenk-White (NFW) \\cite{Navarro:1995iw}. Here, similar to the RGGR analysis, we constrain the DM model parameter ($M_{200}$) using the Bayesian MCMC algorithm \\cite{foreman2013emcee}. Furthermore, we compare the fit of the RC for the two profiles, i.e., RGGR and DM, by evaluating the Bayesian Inference Criteria (BIC) \\cite{1978AnSta...6..461S} that helps to check the preferability of one model over the other. This analysis helps to check the consistency and the credibility of the RGGR model in comparison to an alternative DM (NFW) scenario.  \\\\\n\\newline\nThe layout of the paper is as follows. Section \\ref{sec:rggr} gives a detailed account of the RGGR model. The next Section \\ref{sec:gal} discusses in detail the galaxy catalog SPARC adopted for studying the alternative gravity model. The methodology and the computational packages used to sample the parameter space of the model are discussed in Section \\ref{sec:method}. The following Section \\ref{sec:res} and Section \\ref{sec:conc} discuss the results obtained and the conclusion for our study, respectively.\n\n\\section{The models}\n\\label{sec:rggr}\n\\subsection{RGGR gravity model}\n\nRenormalization Group correction to General Relativity is a quantum gravity model that studies the variation of the gravitational coupling parameter ($G$) with the energy scale of the Universe \\cite{Reuter:2004nx, Farina:2011me, Rodrigues:2009vf}. The observations in the far infrared region established $G$ to be a constant in nature. However, this might not be true if one looks at GR as a field theory in curved spacetime. The beta function for the gravitational coupling constant $G$ has been studied to have a logarithmic dependence on the energy scale ($\\mu$) of the Universe. From the dimensional argument, one unique choice for the beta function has been taken to be of the following form \\cite{Farina:2011me,Rodrigues:2009vf}\n\\begin{equation}\\label{bet}\n    \\beta=\\mu\\frac{dG^{-1}}{d\\mu}=2\\nu\\frac{M^2_{planck}}{c\\hbar}=2\\nu G_0^{-1},\n\\end{equation}\nwhere $\\nu$ is a phenomenological parameter that is fixed from observational data, and $G_0$ is the bare value of the gravitational coupling parameter. The logarithmic dependence of $G$ with the energy scale as obtained from the solution of Eq.\\ref{bet} becomes\n\\begin{equation}\\label{gmu}\n    G(\\mu)=\\frac{G_0}{1+\\nu \\ln\\left(\\frac{\\mu^2}{\\mu_0^2}\\right)},\n\\end{equation}\nhere $\\mu_0$ is the energy scale defined such that $G(\\mu_0)=G_0$. As the variation in $G$ is small, the exact value of $\\mu_0$ is inconsequential. For the far-infrared (IR) region, where the coupling parameter $G$ is known as a constant, the GR limit is regained by substituting $\\nu=0$. Thus, the coupling parameter $G$ in the Einstein-Hilbert action of gravity follows the RG flow given in Eq.\\ref{gmu}. For observations in galactic scales the variation in $\\nu$ is of the order $10^{-7}$ \\cite{Rodrigues:2014xka}. A similar $\\beta$ function can also be defined for the cosmological constant ($\\Lambda(\\mu)$) present in the action of gravity. However, due to the negligible effect on the astrophysical scales, it is ignored from the RC analysis of the galaxies \\cite{Rodrigues:2009vf}.\\\\\n\\newline\nAdditionally, to formulate a consistent theory with an observational basis, it is required to correlate the energy scale with some observable parameter. For example, on the cosmological scales, $\\mu$ is shown to have a relation with the energy scale of the Universe i.e., Hubble constant ($H$) \\cite{Fabris:2012wg}. Similarly, for the galactic scales, $\\mu$ has a functional dependence on the potential energy of the system. The relation between the energy scale dependence of the parameter with the potential is defined such that it regains the correct Newtonian limit and satisfies the Tully Fisher relation. Thus, the functional form that satisfies the mentioned condition can be assumed to be \\cite{Fabris:2012wg} \n\\begin{equation}\n    \\frac{\\mu}{\\mu_0}=\\left(\\frac{\\phi}{\\phi_0}\\right)^{\\alpha},\n\\end{equation}\nwhere $\\alpha$ is a phenomenological parameter constrained from the observations. The analysis of galactic dynamics with RGGR \\cite{Rodrigues:2012qm, Rodrigues:2014xka} suggests that the parameter $\\nu\\alpha\\equiv\\bar{\\nu}$ follows a close to linear relation with the baryonic mass of the galaxy. Such a linear relation has been shown from the study of RC of spiral galaxies \\cite{Rodrigues:2014xka} and from the fundamental plane of elliptical galaxies \\cite{Rodrigues:2012qm}.\nSolving for the weak-field limit, the effective circular velocity and acceleration take the following forms \\cite{Rodrigues:2009vf, Rodrigues:2012qm}\n\\begin{equation}\\label{vrgr}\n    v_{RGGR}^2(r)=v_N^2(r)\\left(1-\\frac{c^2\\bar{\\nu}}{\\phi_N(r)}\\right),\n\\end{equation}\n \n\\begin{equation}\\label{arggr}\n    a_{RGGR}(r)\\approx a_N(r)\\left(1-\\frac{c^2\\bar{\\nu}}{\\phi_N(r)}\\right),\n\\end{equation}\nwhere $V_N(r)$ and $a_N(r)$ are the Newtonian contribution to the velocity and acceleration, respectively, $c$ is the speed of light.  \nThus, on the galactic scales, the net circular velocity and acceleration of an object, in addition to the Newtonian (baryonic) part, have the extra contribution due to the modified gravity. This extra contribution depends on the model parameter $\\bar\\nu$ and the potential $\\phi_N(r)$. The potential $\\phi_N(r)$ arises from all the baryonic contents of the galaxy and is crucial to evaluate the effect of RGGR gravity. This requires mass modeling, i.e., modeling the radial dependence of the baryonic mass of the galaxy. In general, the baryonic structure of the galaxy is composed of three components, i.e., disk, gas, and bulge. We assume a radial exponential profile \\cite{Freeman:1970mx} for the disk and gas component. For the galaxies having bulge, the mass modeling is done assuming a spheroidal Hernquist profile \\cite{Hernquist1990AnAM}. Thus, for the given mass profiles, one may estimate or constrain the phenomenological parameter $\\bar{\\nu}$ from observations. For example, rotation curve studies \\cite{Rodrigues:2014xka} estimate $\\bar{\\nu}$ to be of the order of $10^{-7}$ to be consistent with the observations.\n\n \n Additionally, the constraints obtained on the solar system scales suggest the parameter $\\bar{\\nu}$ to be of the order of $10^{-17}$ \\cite{Farina:2011me}.\nThe limit is much smaller than the galaxy rotation curve estimates. This, however, is expected as the parameter $\\bar\\nu$ is found to have a linear dependence on the baryonic mass of the system. In comparison to the spiral galaxies, the mass contained within the Solar system is about ten times less, thus making the $\\bar{\\nu} \\sim 10^{-17}$ limit consistent. Similarly, in the case of Ultra-diffuse galaxies, whose masses are comparatively smaller than the rotationally supported galaxies, the evaluated $\\bar \\nu$ turns out to be $10^{-8}$ \\cite{Bhatia:2023pts}. \n \n In what follows, we analyze the consistency of the RGGR model for a large selection of galaxies from the SPARC catalog. These galaxies belonging to different morphologies with different characteristics give a measure of the model parameter's variation over the nature of the galaxies. \\\\ \n \\newline\n\n \\subsection{NFW dark matter}\nWe also compare the RGGR model with an alternative scenario where we assume that the missing mass problem on the astrophysical scales can be resolved by the presence of DM. For this, we look into a known DM profile, i.e., Navarro-Frenk-White (NFW) profile \\cite{Navarro:1995iw}, having the density $\\rho(r)=\\frac{\\rho_s}{\\frac{r}{r_s}\\left(1+\\frac{r}{r_s}\\right)^2}$, where $\\rho_s(r)$ and $r_s$ are the characteristic density and radius respectively. It has already been shown \\cite{Rodrigues:2014xka} that a comparison of the RGGR model with the alternative gravity model MOND \\cite{1983ApJ...270..365M} or NFW DM model \\cite{Navarro:1995iw} shows a better or an equally consistent fit for the observed circular velocity of the spiral galaxies. Along the same line, in this work, we compare the RGGR model with an NFW DM profile for the SPARC galaxies and check the favourability of one over the other. Our analysis for RGGR versus DM looks into a scenario where the number of free parameters of the model is similar. Thus, in contrast to the two-parameter fit for the DM profile \\cite{Rodrigues:2014xka}, we fit a single DM model parameter ($M_{200}$) by using a stellar-halo relation \\cite{Dutton:2014xda}. \nThe velocity contribution from the assumed NFW profile is \\cite{Navarro:1995iw}\n\\begin{equation}\\label{eq:nfw}\n    v^2_{NFW}(r)=\\frac{4\\pi Gr_s^3\\rho_s}{r}\\left[-\\frac{r}{r+r_s}+\\log \\left(1+\\frac{r}{r_s}\\right)\\right].\n\\end{equation}\nFor the DM halo scenario, the contribution to the net circular velocity for a galaxy comes from the sum of the baryonic component ($v_N$) which for SPARC is expressed by Eq.\\ref{eq:vn} and NFW profile ($v_{NFW}$) i.e.,\n\\begin{equation}\\label{eq:vtnfw}\n    v^2_{tot}(r)=v^2_N(r)+v^2_{NFW}(r),\n\\end{equation}\nBoth the model parameters of NFW ($\\rho_s(r), ~ r_s$) can be expressed in terms of concentration parameter $c$ and virial mass $M_{200}$ \\cite{Li:2020iib}. We assume an additional constraint which relates $c-M_{200}$ for a galaxy-sized halos as \\cite{Dutton:2014xda},\n\\begin{equation}\n    c(M_{200})=10^{0.905}\\left(\\frac{M_{200}}{10^{12}h^{-1}M_{\\odot}}\\right)^{-0.101},\n\\end{equation}\nwhere h=0.671 \\cite{Planck:2015fie}. This $c-M_{200}$ relation leaves only one parameter, i.e., $M_{200}$, that is constrained from the analysis of observed RC. A comparison of the RGGR model with an NFW DM halo helps to determine the favorability of the gravity model in comparison to a DM scenario. \n\n\\section{Galaxy catlogue}\n\\label{sec:gal}\n\nSpitzer Photometry for Accurate Rotation Curve (SPARC) contains the collection of $175$ rotationally supported galaxies measured at near-IR photometry \\cite{Lelli:2016zqa}. The catalog contains the mass models of galaxies covering a broad range with varying luminosities, morphologies, rotation velocity, gas content, etc. \\cite{Lelli:2016zqa}. The dominant contribution for a rotationally supported galaxy comes from the disk, which extends up to a few kiloparsecs. The additional baryonic contribution arises from the bulge (if present) superimposed at the disk's center and gas diffused throughout the galaxy. Depending upon the size of the bulge and structure of spiral arms present, the SPARC is majorly divided into $4$ morphological types. The different morphologies are represented via Hubble type ($H$), which for the catalog ranges between $H: 0-12$ \\cite{mihalas1981galactic}. The Hubble type is a representation of the evolutionary stage of the galaxy. In general, the size of the bulge and tightness of spiral arms for a rotationally supported galaxy reduces as we move up in the Hubble type.  The first Hubble category ranging from $H: 0-2$ belongs to the Early type, which contains galaxies of type S0, Sa, and Sab, respectively. Such galaxies are distinguished by the presence of a prominent bulge at the center and tightly woven spiral arms. The second category consists of Spiral galaxies ($H: 3-6$) where the features such as bulge and tightness of spiral arms start decreasing. The next two categories include Late-type ($H: 7-9$) and Starburst ($H: 10-12$). These galaxies contain almost no visible bulge present at the center of the galaxy. The last category i.e., Starburst galaxies are known to have no spiral structure in the outer parts and are characterized by their diffused shape. The Starburst galaxies are also known to have a higher star formation rate. Thus, this catalog represents a wide spectrum of galaxies and  makes a versatile ground to test our alternative gravity model.\\\\\n\\newline\n\n\\textbf{\\textit{Rotation Curve:}} Given the radial variation of the velocity for individual baryonic components of a galaxy, the total Newtonian contribution can be written as:\n\\begin{equation}\\label{eq:vn}\n    v_{N}^2(r)=\\gamma_d~ v_{disk}^2(r)+\\gamma_b v_{bulge}^2(r)+|v_{gas}(r)|~v_{gas}(r);\n\\end{equation}\nwhere $v_{disk}(r)$, $v_{bulge}(r)$ and $v_{gas}(r)$ represent the disk, bulge, and gas velocity components for a particular galaxy in the SPARC catalog. The radial variation of the individual baryonic component for each rotationally supported galaxy is provided within the SPARC. The catalog additionally contains the accurate HI and H$\\alpha$ measurement of the total circular velocity $v_{obs}(r)$, along with the error bars $\\sigma(r)$ for every galaxy. Also, the two baryonic components are scaled by a factor $\\gamma_d$ and $\\gamma_b$, which measure the mass-to-light ratio for the disk and bulge part, respectively. The data from the catalog clearly shows that the net contribution of velocity coming from the baryonic component is insufficient to explain the observed velocity, $v_{obs}(r)$ of the galaxy. This leaves room for the additional components, such as DM or MOG which can be added to the Newtonian part to explain the overall rotational stability of the galaxy.  \\\\\n\\newline \nUnder the assumption that the gravity on the galactic scale is RGGR, the modified kinematics of objects within the galaxy is expressed as given in Eq.\\ref{vrgr}. The equation shows that the net circular velocity has an additional contribution from the RGGR gravity.\n\nTherefore, comparing the total observed circular velocity from the SPARC catalog with the RGGR velocity form (Eq.\\ref{vrgr}) helps to check the consistency of the gravity model. This requires the variation of the model parameter of the theory within the allowed parameter space. This analysis aims to look for the parameters that give a consistent fit to the observed RC of the galaxy sample. The best-fit values obtained in the analysis are also used to construct the empirical relation RAR and BTFR.  \\\\\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\newline\n\\textbf{\\textit{RAR:}} The analysis of SPARC data shows that the total baryonic acceleration $(a_{bar})$ cannot explain the net observed acceleration $(a_{obs})$ and follow a certain analytical relation known as RAR \\cite{mcgaugh2016radial}. This analysis has been found to be true for the 153 galaxies of the catalog irrespective of their morphological types, thus indicating a new dynamical law governing galaxy kinematics. The empirical RAR relationship obtained from SPARC is defined as \\cite{mcgaugh2016radial}\n\\begin{equation}\\label{rar}\n    a_{obs}(R)=\\frac{a_{bar}(R)}{1-\\exp(-\\sqrt{a_{bar}(R)/a_*})},\n\\end{equation}\nwhere $a_*$ is the acceleration scale parameter and has best-fit value $a_*=1.2\\times 10^{-10}$ ms$^{-2}$. \nFor a rotationally supported galaxy, the net centripetal acceleration is defined in terms of observed velocity in SPARC catalog\n\\begin{equation}\\label{acc}\n    a_{obs}(R)=\\frac{v_{obs}^2(R)}{R}=\\left|\\frac{\\partial \\phi_{tot}(R)}{\\partial R}\\right|,\n\\end{equation}\nwhere $\\phi_{tot}$ is the total potential i.e., total force per unit mass acting on a point particle. Similarly, $a_{bar}(R)$ is the linear sum of the acceleration for different baryonic components (disk, gas, and bulge) within the galaxy and can be estimated from the SPARC data using\n\\begin{equation}\\label{bar}\n\\begin{split}\n    a_{\\text{bar}}(R) &= \\frac{v_{N}^2(R)}{R} \\\\\n    &= \\frac{\\gamma_d~v_{\\text{disk}}^2(R) + \\gamma_b~ v_{\\text{bulge}}^2(R) + |v_{\\text{gas}}(R)|~v_{\\text{gas}}(R)}{R} \\\\\n    &= \\left|\\frac{\\partial \\phi_{\\text{bar}}(R)}{\\partial R}\\right|.\n\\end{split}\n\\end{equation}\n\nBoth the net circular velocity ($v_{obs}(r)$) and individual baryonic components mentioned in Eq.\\ref{acc} and \\ref{bar} are known observationally in SPARC. This makes the relation an empirical one as it assumes no prior knowledge about the DM or MOG model and is purely from the observational data in SPARC.  \\\\\n\\newline\nFor our analysis, we aim to probe RAR in the context of the RGGR gravity. The analytical expression for the net acceleration (Eq.\\ref{arggr}) in RGGR includes a $\\bar{\\nu}$ dependent extra component to the Newtonian part. As explained in the previous discussion in the context of RC, the best-fit value of this $\\bar\\nu$ parameter for each galaxy can be estimated from the circular velocity fitting with the SPARC data. Thus, the consistency of the RGGR predicted net acceleration ($a_{RGGR}$) with respect to the $a_{obs}(R)$ can be probed by the RAR. In the following, we compare the observed RAR with the same empirical relation Eq.\\ref{rar} in the context of the RGGR model for each data point of the qualifying SPARC galaxies. \\\\\n\\newline\n\\textbf{\\textit{BTFR:}} SPARC also shows a tight correlation between the dynamics of the galaxy with the baryonic distribution. BTFR suggests that the stellar mass of the galaxy has a power law dependence on the flat part of the circular velocity, \n\\begin{equation}\\label{btfr}\n    M_{bar}=A V_f^x.\n\\end{equation}\nHere $M_{bar}$ refers to the baryonic mass contained within the galaxy and $V_f$ is the velocity measured along the flat part of the RC. \nThe optimal value of the free parameters ($A, x$) are obtained from the study of the SPARC catalog, and are estimated to be $A=50$ M$_{\\odot}$km$^{-4}$s$^4$, $x=4$ \\cite{McGaugh:2005qe}. Similar to RAR, BTFR is also known to be empirical in nature as it assumes no underlying gravity model and is strictly obtained from the SPARC data. For every galaxy in SPARC, the baryonic mass is the linear sum of disk, bulge, and gas components.  For the RGGR model, the total circular velocity is estimated by $v_{RGGR}$ (Eq.\\ref{vrgr}) and is dependent on the free parameters ($\\gamma_d$, $\\gamma_b$, $\\bar{\\nu}$) which are already constrained using the observed RC of the galaxy. Thus, the $V_f$ for the RGGR will also get modified. The relation between the $M_{bar}$ and the RGGR predicted flat velocity for each qualifying SPARC galaxy should follow observed BTFR. This additional check for RGGR is crucial for the gravity model to remain phenomenologically consistent.  \n\n\\section{Methodology}\n\\label{sec:method}\nUsing the RC data for the qualifying SPARC galaxies, we fit the model parameters considering RGGR as the underlying gravity model. These free parameters are constrained using the publicly available $emcee$ package \\cite{foreman2013emcee} in PYTHON. This package works on the principle of Markov Chain Monte Carlo (MCMC), which samples the posterior distribution of the free model parameters. For a given observational data set $\\mathcal{D}$, the posterior probability $\\mathcal{P}$($\\theta$|$\\mathcal{D}$) for unknown set of parameters $\\theta$ is defined as\n\\begin{equation}\\label{sampler}\n    \\mathcal{P}(\\theta|\\mathcal{D})\\propto \\mathcal{L}(D|\\theta)\\pi(\\theta),\n\\end{equation}\nwhere $\\mathcal{L}(D|\\theta)$ is the likelihood which determines the probability of a data for a given model with free parameters, and $\\pi(\\theta)$ represents the priors imposed on the free parameters. Assuming that the errors on the observed circular velocity follow a Gaussian distribution, the likelihood for each galaxy is written as\n\\begin{equation}\\label{eq:like}\n\\begin{split}\n    \\mathcal{L}_g &= (2\\pi)^{-n/2} \\left\\{ \\prod_{i=1}^n \\sigma(r_i)^{-1} \\right\\} \\\\\n    &\\quad \\times \\exp \\left\\{ -\\frac{1}{2} \\sum_{i=1}^n \\left( \\frac{v_{g,\\text{obs}}(r_i) - v_{\\text{tot}}(r_i,\\vec{\\theta})}{\\sigma(r_i)} \\right)^2 \\right\\},\n\\end{split}\n\\end{equation}\n\nhere, $n$ represents the number of observational data points over which the likelihood is summed. Also, $v_{obs}(r)$ and $\\sigma(r)$ are the total circular velocity and error on the observations defined at each radial point within the SPARC catalog. The analytical velocity $v_{tot}(r,\\vec{\\theta})$ computed at a certain radius for a given set of free parameters is model-dependent. \nWe fit both the RGGR and the NFW DM models. The net circular velocities for RGGR and NFW in terms of their respective free parameters are expressed by Eq.\\ref{vrgr} and  Eq.\\ref{eq:vtnfw}, respectively. These free parameters in velocity $v_{tot}(r,\\vec{\\theta})$ are phenomenologically constrained by comparing with the observational circular velocity. For both the models, i.e., RGGR and NFW, the normalization mass-to-light factors ($\\gamma$) introduced for the baryonic components, i.e., the disk ($\\gamma_d$) and the bulge ($\\gamma_b$) of the galaxy are common. Additionally, the RGGR model has a mass-dependent phenomenological parameter $\\bar\\nu$. Similarly, the NFW model has a singular free parameter $M_{200}$. Thus, in the alternative gravity scenario $\\vec{\\theta}$ is composed of \\{$\\gamma_d$, $\\gamma_b$, $\\bar{\\nu}$\\} which are to be estimated independently for each galaxy. For the case of NFW $\\vec{\\theta}$ is made up of \\{$\\gamma_d$, $\\gamma_b$, $M_{200}$\\} and are aagin constrained for each galaxy.\n\\newline\nRegarding the priors $\\pi(\\vec{\\theta})$, we assume both flat and Gaussian priors for the model parameters. In case of the flat priors, the mass-to-light ratio for both disk ($\\gamma_d$) and bulge ($\\gamma_b$) are assumed to have no radial dependence on the galaxy and are varied in the range [$0.3$, $0.8$] \\cite{deAlmeida:2018kwq, schombert2014stellar, Meidt:2014mqa}. In the case of RGGR, for the mass-dependent parameter $\\bar{\\nu}$, a wide range of parameter space is looked into and is varied within $10^{-9}$ $\\le$ $\\bar{\\nu}$ $\\le$ $10^{-6}$. This range is motivated by the previous analysis of spiral and elliptical galaxies where $\\bar{\\nu}$ is found to vary in order of $10^{-7}$ \\cite{Rodrigues:2014xka}. For the DM scenario, the NFW parameter $M_{200}$ is bound to lie within the range $10^9<M_{200}/M_{\\odot}<10^{14}$ \\cite{deAlmeida:2018kwq}. For the Gaussian priors both the parameters $\\gamma_d$ and $\\gamma_b$ are considered to have a mean value of $0.5$ and standard deviation of $0.1$. The mean and standard deviation for $\\log(\\bar\\nu/10^{-7})$ and $\\log(M_{200}/ M_{\\odot})$ are taken to be ($-0.3,~0.1$) and ($10.69,~0.1$),  respectively. To ensure the convergence of the chain, we run the sampler for a sufficient number of steps such that the acceptance fraction lies within the range $0.2-0.5$ \\cite{foreman2013emcee}. Additionally, by estimating the autocorrelation time ($\\tau$) for each galaxy, we discard $\\tau$ number of steps as burn-in before performing posterior analysis. We run a sufficient number of steps, i.e., $50~\\tau$ as specified in $emcee$ \\cite{foreman2013emcee}, to achieve convergence. \\\\\n\nAdditionally, to quantify the preference of one model over the other, we consider the Bayesian Inference Criteria (BIC), which is a simple approximation for evidence and is defined as \\cite{1978AnSta...6..461S}\n\\begin{equation}\\label{eq:bic}\n    BIC=-2\\log \\mathcal{L}_{max}(D|\\vec{\\theta})+2k\\log(n).\n\\end{equation}\n\nHere $k$ is the number of parameters for a model which is the same, i.e., $3$ parameters, for both the RGGR and NFW models considered here. Also, $n$ represents the number of data points in each galaxy. To compare the two models, i.e., RGGR and NFW, we evaluate,\n\\begin{equation}\n    \\Delta BIC=BIC_{NFW}-BIC_{RGGR}.\n\\end{equation}\nThe measure of $\\Delta BIC$, if less than $2$, implies inconclusive preference between the two models. Similarly, the value of $\\Delta BIC$ that is between $2-6$ implies a positive inclination towards RGGR. Values of $\\Delta BIC$ greater than $6$ are considered to have a strong inclination toward the RGGR model.\n\\newline\n\nCertain selection criteria are adopted before choosing a galaxy from the SPARC catalog. This includes galaxies with asymmetrical RC and poor quality factor ($Q$), i.e., $Q > 2$. We also exempt face-on galaxies having $i<30^{\\circ}$ from our analysis \\cite{Lelli:2016zqa}. To express definite $\\chi^2_{red}$, we assume galaxies with data points greater than $4$. Additionally, to ensure that we have a consistent RC fit with the RGGR, we reject galaxies whose goodness of fit with the gravity model, i.e., $\\chi^2_{red} \\ge 6$. The last quality cut is imposed manually to ensure a consistent fit for the model with the observations.\\\\\n\n\n\\begin{table*}[ht!] \n    \\caption{The best-fit parameters obtained from the emcee sampler. Corresponding to the RGGR gravity model, for every galaxy, 3 parameters ($\\gamma_d$, $\\gamma_b$, $\\bar{\\nu}$) are statistically constrained from the observed circular velocity. Column 2 of the table denotes the galaxy morphology types, abbreviated as E: Early, S: Spiral, L: Late-type, and SB: Starburst. In the table, the representation for the free parameter $\\bar{\\nu^*}=\\bar \\nu \\times 10^{-7}$ and $M^*_{200}=M_{200} \\times 10^{11} M_{\\odot}$.}\n    \\label{par_sparc}\n    \\centering\n    \\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\n        \\hline\n        \\textbf{Galaxy} & \\textbf{Type} & \\multicolumn{2}{c|}{\\textbf{$\\gamma_d$}} & \\multicolumn{2}{c|}{\\textbf{$\\gamma_b$}} & \\textbf{$\\bar{\\nu}^*$} & \\textbf{$M^*_{200}$} & \\multicolumn{2}{c|}{\\textbf{$\\chi^2_{red}$}} \\\\\n        \\hline\n        \\textbf{Name} &  & $\\gamma_d$ (RGGR) & $\\gamma_d$ (NFW) & $\\gamma_b$ (RGGR) & $\\gamma_b$ (NFW) &  &  & $\\chi^2_{red}$ (RGGR) & $\\chi^2_{red}$ (NFW) \\\\\n        \\hline\n        \\csvreader[head to column names]{sample.csv}{}\n        {\n            \\Name & \\Type & \\gamadrgr & \\gamadnfw & \\gamabrgr & \\gamabnfw & \\nubar & \\mpar & \\chirgr & \\chinfw \\\\\n            \\hline\n        }\n    \\end{tabular}\n\\end{table*}\n\n\\begin{table*}[ht!] \n    \\caption*{Continued from the previous page}\n    \n    \\centering\n    \\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\n        \\hline\n        \\textbf{Galaxy} & \\textbf{Type} & \\multicolumn{2}{c|}{\\textbf{$\\gamma_d$}} & \\multicolumn{2}{c|}{\\textbf{$\\gamma_b$}} & \\textbf{$\\bar{\\nu}^*$} & \\textbf{$M^*_{200}$} & \\multicolumn{2}{c|}{\\textbf{$\\chi^2_{red}$}} \\\\\n        \\hline\n        \\textbf{Name} &  & $\\gamma_d$ (RGGR) & $\\gamma_d$ (NFW) & $\\gamma_b$ (RGGR) & $\\gamma_b$ (NFW) &  &  & $\\chi^2_{red}$ (RGGR) & $\\chi^2_{red}$ (NFW) \\\\\n        \\hline\n        \\csvreader[head to column names]{sample2.csv}{}\n        {\n            \\Name & \\Type & \\gamadrgr & \\gamadnfw & \\gamabrgr & \\gamabnfw & \\nubar & \\mpar & \\chirgr & \\chinfw \\\\\n            \\hline\n        }\n    \\end{tabular}\n\\end{table*}\n\n\\section{Results}\n\\label{sec:res}\nIn the following, we analyze the consistency of the RGGR framework with the observational circular velocity for our selection of SPARC galaxies, we also draw a comparison of the RGGR model with NFW DM scenario. The RC analysis is done for galaxies belonging to different morphological types. The free model parameters are constrained using the emcee sampler, which scans the parameter space to evaluate the best-fit values. For this, we look into the rotation curves of individual galaxies present within SPARC. We also study the mass-dependent nature of the phenomenological parameter $\\bar{\\nu}$. These best-fit values must also satisfy the fundamental relationships RAR and BTFR, hence we compare our results for the RGGR model with both these relationships. \nFor galaxies consistent with the RGGR model, we alternatively look into their kinematics in a DM-dominated case. We employ a similar Bayesian technique to constrain the model parameter $M_{200}$ for the NFW profile. To measure the favorability of the RGGR model over the DM NFW profile, we also report the $\\Delta BIC$ value evaluated for each galaxy.\n\n\\subsection{Fit to the observed Rotation Curve}\n\\label{sec:rc}\n\nFitting the RC of each SPARC galaxy with the RGGR gravity model constrains the free parameters. It includes two mass modeling parameters defined for the baryonic component of a galaxy i.e., $\\gamma_d$ and  $\\gamma_b$. The additional model parameter, i.e., $\\bar{\\nu}$, comes from the choice of RGGR gravity and has been found to have an almost linear dependence on the luminous mass of the galaxy. We analyze the behavior of the phenomenological parameter $\\bar{\\nu}$ with the baryonic mass for the large sample of SPARC galaxies. In particular, we study the consistency of the RGGR-governed model for all four morphological types of the galaxies present in SPARC. This compares the phenomenological consistency of the RGGR gravity for all the different galaxy types. A similar RC analysis where instead of modified gravity, galaxies are assumed to be DM dominated with the radial density profile having the NFW form is also looked into. For this scenario, in addition to the mass model parameters $\\gamma_d$ and $\\gamma_b$, we fit $M_{200}$, which comes from the choice of NFW halo.\n\n\n\\begin{figure*}[t]\n    \\centering\n\\begin{subfigure}[t]{0.45\\textwidth}\n\n\n\\includegraphics[width=\\linewidth,height=0.9\\linewidth]{7814.pdf} \n\n\\end{subfigure}\n\n\\begin{subfigure}[t]{0.45\\textwidth}\n\n\n\\includegraphics[width=\\linewidth,height=0.9\\linewidth]{4138.pdf} \n\\end{subfigure}\n\n\\begin{subfigure}[t]{0.45\\textwidth}\n\n\\includegraphics[width=\\linewidth,height=0.9\\linewidth]{6614.pdf} \n\n\\end{subfigure}\n\n\\begin{subfigure}[t]{0.45\\textwidth}\n\n\\includegraphics[width=\\linewidth,height=0.9\\linewidth]{3546.pdf} \n\n\\end{subfigure}\n\\caption{For Early-type galaxies the RC for $4$ specimen galaxies are shown in the four panels above. The black solid line is the circular velocity when RGGR gravity contributes in addition to the baryonic part. The black dots with green error bars are the circular velocity data obtained from SPARC \\cite{Lelli:2016zqa}. The red dashed line is where the net velocity contribution is evaluated in an NFW paradigm.  The dashed grey line represents the gaseous component of the galaxy. The dot-dashed line shows the variation of disk velocity within the galaxy, and the bulge part is plotted via the dotted line. }\n\\label{Fig:early}\n\\end{figure*}\n\n\\paragraph{\\textbf{Early-type galaxies}}\n\\hfill \\break\nThe first category belongs to the early-type galaxies, which can be identified from the presence of a bulge at the center and tight, indistinguishable spiral arms in the outer parts of the disk. From the whole set of early-type, we analyze $9$ galaxies that satisfy the selection criteria. Out of these $9$ galaxies, we illustrate the RC fitting for $4$ of them in Fig.\\ref{Fig:early}. This includes NGC7814 shown in the upper left panel, NGC4138 in the upper right, UGC06614 in the lower left, and UGC03546 in the lower right panel of Fig.\\ref{Fig:early}. For each galaxy shown, the grey dots with green error bars represent the observational total circular velocity traced by the HI component within the galaxy. We also plot the individual baryonic components, i.e., disk, bulge, and gas, for each galaxy using the gray lines. The dashed gray line represents the radial variation of the gas velocity. The disk and bulge components of the galaxy scaled by their respective mass-to-light ratio are shown via. the dashed-dotted and dotted lines, respectively.\n\nIn particular, the first panel at the top left shows the RC for NGC7814. Under the assumption of flat priors, the constraints on the free parameters ($\\gamma_d, \\gamma_b $ and  $\\bar{\\nu} \\times 10^7$) obtained from our analysis give $0.53$, $0.31$ and $7.23$, respectively. However, when the priors are taken to be Gaussian we see almost no change in the best-fit value. As an example, for the case of NGC7814, when priors are assumed to be Gaussian ($\\gamma_d, \\gamma_b $ and  $\\bar{\\nu} \\times 10^7$) are found to be $0.53$, $0.28$ and $7.53$ respectively. The radial variation of the circular velocity (Eq.\\ref{vrgr}) in the RGGR model with these best-fit parameters is plotted using a solid black line. The obtained value of $\\chi^2_{red}$ for the galaxy is $1.18$, indicating a good fit to the observational data of NGC7814. \n\nA similar RC analysis of the $9$ early-type galaxies in the presence of DM with the NFW profile is also looked into. The statistical analysis to constrain the model parameters of the DM halo gives the best-fit values evaluated in the case of flat-priors for $\\gamma_d$, $\\gamma_b$, and $M_{200}\\times 10^{11} M_{\\odot}$ as $0.79$, $0.66$ and $24.13$, respectively. However, contrary to the case of RGGR we observe that the constrained values of the model parameters differ between the flat and Gaussian prior cases. In particular, for the Gaussian prior case the $\\gamma_d$ is not rigorously contained in the preferred range of $0.3-0.8$ as the Gaussian distribution allows values outside this range. Hence, we consider the flat prior results over the Gaussian ones.  In Fig.\\ref{Fig:early} for each galaxy, the radial variation of the circular velocity in the DM model obtained by substituting the best-fit value is shown via the red dashed line. Similarly, the obtained parameters and goodness of fit for the other early-type galaxies in the panel for both RGGR and NFW model can be found in Table.\\ref{par_sparc}. The measured goodness of fit for the early-type galaxies is compiled in Table.\\ref{par_sparc} points to a positive inclination towards the RGGR scenario. \n\\begin{figure*}[t]\n    \\centering\n\\begin{subfigure}{0.45\\textwidth}\n\n\n\\includegraphics[width=\\linewidth,height=0.9\\linewidth]{2683.pdf} \n\n\\end{subfigure}\n\n\\begin{subfigure}{0.45\\textwidth}\n\n\n\\includegraphics[width=\\linewidth,height=0.9\\linewidth]{1090.pdf} \n\n\\end{subfigure}\n\n\\begin{subfigure}{0.45\\textwidth}\n\n\\includegraphics[width=\\linewidth,height=0.9\\linewidth]{2841.pdf} \n\n\\end{subfigure}\n\n\\begin{subfigure}{0.45\\textwidth}\n\n\\includegraphics[width=\\linewidth,height=0.9\\linewidth]{3769.pdf} \n\n\\end{subfigure}\n\\caption{For Spiral-type galaxies, the RC for $4$ specimen galaxies are shown in the panel. The black solid line is the circular velocity when RGGR gravity contributes in addition to the baryonic part. The red dash line belongs to the case where the net velocity contribution is evaluated in an NFW paradigm. The black dots with green error bars are the circular velocity data obtained from SPARC \\cite{Lelli:2016zqa}. The dashed grey line represents the gaseous component of the galaxy. The dot-dashed line shows the variation of disk velocity within the galaxy, and the bulge part is plotted via the dotted line.}\n\\label{Fig:spiral}\n\\end{figure*}\n\n\\paragraph{\\textbf{Spiral-type galaxies}}\n\\hfill\\break\nThe second class of galaxies mentioned in SPARC belongs to the Spiral type. One example of such a galaxy is our own Milky Way. Such galaxies are characterized by the presence of distinct spiral structures at the outer parts of the disk. Additionally, the bulge at the center is of comparatively smaller size. SPARC has a large collection of spiral galaxies; our selection criteria gives us 39 such galaxies. Similar to Fig.\\ref{Fig:early} we show $4$ representative galaxies i.e., NGC1090, NGC2683, NGC2841 and NGC3769 in Fig.\\ref{Fig:spiral}. Out of the $4$ galaxies shown, only two have a visible bulge component present and are represented by a dotted gray line in the plot. The gray points with error bars represent the total circular velocity observed in SPARC. Similarly, the gray lines represent the individual baryonic components in a galaxy.\\\\\n\\newline\n The best-fit parameters for the first subplot for NGC2683 shown at the top left panel in Fig.\\ref{Fig:spiral}. For the RGGR model when priors are assumed to be flat the best values come out to be $\\gamma_d = 0.56$, $\\gamma_b = 0.54$ and $~\\bar{\\nu} \\times 10^7 = 1.88$ with $\\chi^2_{red}$ equal to $0.69$. Similar to the case of Early-type galaxies, the choice of priors yields little difference in the best-fit values evaluated. In the case of NGC2683, Gaussian priors results in $\\gamma_d = 0.58$, $\\gamma_b = 0.50$, $\\bar{\\nu} \\times 10^7 = 1.82$ with $\\chi^2_{red}$ equal to $0.71$. For the obtained parameters, the net contribution to the total circular velocity, which is the sum of baryonic and RGGR components, is shown by a solid black line. The goodness of fits evaluated for each galaxy as given in Table.\\ref{par_sparc} show that the RGGR model is a consistent choice in explaining the observed circular velocity of the spiral SPARC galaxies. Similar to early-type, RC analysis of the $39$ spiral galaxies in the presence of DM with the NFW profile is also looked into. The best-fit value for $\\gamma_d$, $\\gamma_b$, and $M_{200}\\times 10^{11}M_{\\odot}$ evaluated in the case of flat-priors for circular velocity with NFW halo results in ($0.7,0.36, 4.18$) with $\\chi^2_{red}$ equals $0.84$. However, similar to the case of Early-type we consider the flat prior results out of the two choices of priors. The radial variation of circular velocity in the NFW DM model obtained by substituting the best-fit value is shown via a red dashed line for all the $4$ galaxies in the Fig.\\ref{Fig:spiral}.\n\\begin{figure*}\n  \\centering\n\\begin{subfigure}{0.45\\textwidth}\n\n\n\\includegraphics[width=\\linewidth,height=0.9\\linewidth]{4010.pdf} \n\\end{subfigure}\n\n\\begin{subfigure}{0.45\\textwidth}\n\n\n\\includegraphics[width=\\linewidth,height=0.9\\linewidth]{300.pdf} \n\\end{subfigure}\n\n\\begin{subfigure}{0.45\\textwidth}\n\n\\includegraphics[width=\\linewidth,height=0.9\\linewidth]{4278.pdf} \n\n\\end{subfigure}\n\n\\begin{subfigure}{0.45\\textwidth}\n\n\\includegraphics[width=\\linewidth,height=0.9\\linewidth]{5721.pdf} \n\n\\end{subfigure}\n\\caption{For Late-type galaxies, the RC for $4$ specimen galaxies are shown in the panel. The black solid line is the circular velocity when RGGR gravity contributes in addition to the baryonic part. The red dash line belongs to the case where the net velocity contribution is evaluated in an NFW paradigm. The black dots with green error bars are the circular velocity data obtained from SPARC \\cite{Lelli:2016zqa}. The dashed grey line represents the gaseous component of the galaxy. The dot-dashed line shows the variation of disk velocity within the galaxy, and the bulge part is plotted via the dotted line. }\n\\label{Fig.late}\n\\end{figure*}\n\\paragraph{\\textbf{Late-type dwarf}}\n\\hfill\\break\nThe third category of galaxies belongs to late-type dwarf, where the bulge is too faint to be observed. Thus, the baryonic component of the galaxy mostly constitutes disk and gas only. From the total late-type dwarf galaxies in the catalog, $34$ galaxies fulfill the selection criteria. Similar to the above two categories, we plot four late-type dwarf galaxies in Fig.\\ref{Fig.late}. The galaxies that are illustrated include NGC4010 (top-left), NGC0300 (top-right), UGC04278 (bottom-left), and UGC05721 (bottom-right). The details on the plot corresponding to the observed circular velocity of a galaxy and its baryonic components follow the same convention as mentioned in the discussion of the previous two types of galaxies.\n\\newline\nThe circular velocity in the RGGR framework can be obtained by substituting the best-fit values of the model parameters, i.e., ($\\gamma_d$, $\\gamma_b$, $\\bar{\\nu}\\times10^7$) as given in Table.\\ref{par_sparc} in Eq.\\ref{vrgr}. This radial variation is shown using the solid black line in Fig.\\ref{Fig:spiral}. Similar to the above two morphological types, we observe a little variation in the choice of priors. In Table.\\ref{par_sparc}, we report the result for the flat priors. Similarly, we also analyze the $34$ late-type galaxies in the NFW framework. The best-fit parameters obtained show preference towards the flat choice of priors and are reported along with the RGGR model in Table.\\ref{par_sparc}. The $\\chi_{red}^2$ computed in Table.\\ref{par_sparc} for all the galaxies shows that the late-type dwarfs fit well with the RGGR model. This consistent behavior can also be visualized from the four sample plots shown in Fig.\\ref{Fig.late}. \n\\begin{figure*}[t]\n    \\centering\n\\begin{subfigure}{0.45\\textwidth}\n\n\n\\includegraphics[width=\\linewidth,height=0.9\\linewidth]{8837.pdf}\n\\end{subfigure}\n\n\\begin{subfigure}{0.45\\textwidth}\n\n\n\\includegraphics[width=\\linewidth,height=0.9\\linewidth]{7559.pdf} \n\n\\end{subfigure}\n\n\\begin{subfigure}{0.45\\textwidth}\n\n\\includegraphics[width=\\linewidth,height=\\linewidth,height=0.9\\linewidth]{7690.pdf} \n\n\\end{subfigure}\n\n\\begin{subfigure}{0.45\\textwidth}\n\n\\includegraphics[width=\\linewidth,height=0.9\\linewidth]{7866.pdf} \n\n\\end{subfigure}\n\\caption{For Starburst galaxies, the RC for $4$ specimen galaxies are shown in the panel. The black solid line is the circular velocity when RGGR gravity contributes in addition to the baryonic part. The red dash line belongs to the case where the net velocity contribution is evaluated in an NFW paradigm. The black dots with green error bars are the circular velocity data obtained from SPARC \\cite{Lelli:2016zqa}. The dashed grey line represents the gaseous component of the galaxy. The dot-dashed line shows the variation of disk velocity within the galaxy, and the bulge part is plotted via the dotted line.}\n\\label{Fig:star}\n\\end{figure*}\n\\paragraph{\\textbf{Starburst galaxy}}\n\\hfill\\break\n\nThe fourth morphological type belongs to starburst galaxies which are relatively young with a high star formation rate. Such galaxies are dominated by gas and thus make a versatile region to look for the signature of the alternative gravity model. The morphology of such systems shows a diffused structure with no visible bulge and spiral arms. We studied $18$ galaxies belonging to this particular morphological type and found that RGGR can give consistent fits with the observed circular velocities. As an illustration following the convention used in other morphological types, we plot 4 starburst galaxies i.e., UGC08837, UGC07559, UGC07690, and UGC07866 in Fig.\\ref{Fig:star}. The baryonic contribution for these galaxies comes from the gas and disk component only, as starbursts have no visible bulge within. This is represented in the above plot by gray lines following the same convention used in preceding cases. Also, the total observed circular velocity with error bars is shown using gray points.\\\\\n\\newline\nAs mentioned in the previous three morphological types, for the RGGR model, we see a little variation in our two choices of priors, i.e., flat and Gaussian.\nThe constrained free parameters ($\\gamma_d$, $\\gamma_b$, $\\bar{\\nu}~ \\times~ 10^{7}$) obtained for the aforementioned galaxies in reference to the flat priors can be referred from Table.\\ref{par_sparc}. From the obtained values of parameters, the total circular velocity evaluated for each galaxy is shown by a solid black line in Fig.\\ref{Fig:star}. As can be seen from the plots, the observational circular velocity fits well with the analytical velocity in all $4$ cases. Similarly, this consistent nature of the gravity model can also be quantified for all the starburst galaxies from the goodness of fit as mentioned in Table.\\ref{par_sparc}. We also studied the $18$ starburst galaxies concerning the NFW model for both flat and Gaussian priors and considered the flat prior results in preference to the Gaussian ones. The best-fit values along with the $\\chi^2_{red}$ for flat-priors are compiled in Table.\\ref{par_sparc}. \nFor the four different morphological types of galaxies discussed above, we find that RGGR is a phenomenologically consistent theory of gravity. The best fit values for the $\\bar\\nu$ parameter lies in the range $10^{-6}-10^{-8}$. The parameters $\\gamma_d$ and $\\gamma_b$ fall in the range $0.3-0.8$, consistent with the choices of our priors. We also observe from Table.\\ref{par_sparc} that the values of $\\bar\\nu$ are larger (in the range close to $10^{-6}$) for the early-type galaxies compared to the other types. Interestingly, the $\\bar\\nu$ values decrease as we go across the Hubble type towards the late-type galaxies with $\\bar\\nu$ around $10^{-8}$. This relation between the parameter $\\bar\\nu$ and the galaxy type can be understood from the baryonic mass content of the galaxies and is discussed in the next subsection.  \n\n\\begin{figure*}[t]\n    \\centering\n    \n    \\begin{subfigure}[b]{0.45\\textwidth}\n        \\includegraphics[width=\\textwidth,height=.9\\textwidth]{rgrgamma_rel.pdf}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.45\\textwidth}\n        \\includegraphics[width=\\textwidth,height=.9\\textwidth]{nfwgamma_rel.pdf}\n    \\end{subfigure}\n    \\caption{The plot shows the variation of $\\gamma_d$. The left panel represents the $\\gamma_d$ obtained for the RGGR analysis, and the right panel belongs to the case of NFW. The varying morphological types are represented using different markers. The square markers represent the early type, and the circles are for the spiral galaxies. Additionally, $\\gamma_d$ for late-type and starburst are shown via triangle and diamond markers.}\n    \\label{Fig:gama}\n\\end{figure*}\n\nTo summarise the comparison between the RGGR and the DM NFW scenario, we study $100$ galaxies that satisfy our selection criteria, as discussed previously. To measure the evidence against or in favor of our choice of models (NFW or RGGR), we compute $\\Delta BIC$ for each galaxy. The plot, which measures the number of galaxies belonging to different bin sizes of $\\Delta BIC$ favoring either RGGR or NFW, is shown in Fig.\\ref{Fig:bicrel}. It clearly shows that out of $100$ galaxies, $47$ favors the RGGR model, and the result is inconclusive for $13$ galaxies. However, the remaining $40$ galaxies prefer the choice of DM profile. \\\\\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=.99\\linewidth, height=.4\\textwidth]{BIC.pdf}\n    \\caption{The plot represents the $\\Delta BIC$ comparing RGGR with NFW. The pink histograms show the frequency of galaxies that favor RGGR over the DM model over different bin sizes. For $\\Delta BIC$ range within $0-2$, the claim over a preference of a model is inconclusive. In total, we observe that RGGR is favored by the majority of galaxies when compared with NFW.}\n    \\label{Fig:bicrel}\n\\end{figure}\n The mass modeling parameters constrained for each galaxy in our analysis include $\\gamma_d$ and $\\gamma_b$, which are assumed to be a constant having no radial dependence. The bulge component is prominent and can be majorly seen only in the early-type galaxies. Therefore, we utilize the disk normalization factor to show the variation of $\\gamma_d$ for the galaxies of different morphological types. The plot containing the values of $\\gamma_d$ for the $100$ galaxies present in our analysis is shown in Fig.\\ref{Fig:gama}. For RGGR (left-panel), the behavior of $\\gamma_d$ represents that this parameter varies throughout the range of $[0.3,0.8]$ for all galaxies. Also, in the NFW regime, we observe that $\\gamma_d$ varies throughout the range for spiral and late-type galaxies. However, for early-type galaxies, $\\gamma_d$ is concentrated towards the higher end of the prior range. Also, in the case of Starburst, we observe that $\\gamma_d$ is pointing toward the lower end of the range. The different markers on the plot in Fig.\\ref{Fig:gama} belong to the different morphological types of galaxies in SPARC. The pink points give the value of $\\gamma_d$ for early-type galaxies.\nSimilarly, green squares, grey triangles, and violet diamond shapes represent $\\gamma_d$ for the spiral, late-type, and starburst galaxies. Additionally, looking into the behavior of $\\bar{\\nu}$ in Table.\\ref{par_sparc} shows that as we go from early type to starburst galaxies, the magnitude of the phenomenological parameter decreases. Such behavior of $\\bar{\\nu}$ with the stellar mass of galaxies is studied in detail in the next subsection.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=.95\\linewidth, height=.4\\textwidth]{mass_rel.pdf}\n    \\caption{The plot relates the phenomenological parameter $\\bar{\\nu}$ with the baryonic mass of the galaxy. The individual points are the representation of the $\\bar{\\nu}$ evaluated for each galaxy studied in SPARC. The almost linear relation shown in the plot affirms the previous mass dependence claim for the RGGR parameter. The different markers in the plot show the different morphological types of galaxies, with the circle representing early-type, a square belonging to spiral, a triangle showing late-type dwarfs and a diamond for starburst galaxies.}\n    \\label{Fig:massrel}\n\\end{figure}\n\\subsection{Relation of \\texorpdfstring{$\\bar{\\nu}$}{TEXT} with baryonic matter}\nTo investigate the RGGR model parameter dependence on the baryonic mass, we rely on the same galaxies from the SPARC catalog. The baryonic mass of a rotationally supported galaxy consists of the bulge, disk, and gas components, as mentioned earlier. Each component of the galaxy (stellar$+$gas) is assumed to follow a distinct density distribution. The radial variation of matter distribution for disk and gas is assumed to vary exponentially \\cite{Lelli:2016zqa, Freeman:1970mx}. Similarly, for the galaxies having bulges, the density variation can be fitted with the Hernquist profile \\cite{Hernquist1990AnAM}. The total baryonic content for a galaxy is a linear mass sum of stellar and gas components. The galaxies in SAPRC constitute a wide range of masses varying from $10^8-10^{11}~ M_{\\odot}$ \\cite{Lelli:2016zqa}.\\\\\n\\newline\nFor the scenario where the underlying gravity is assumed to be RGGR, the velocity contribution is the sum of the Newtonian part plus an additional term dependent on a free parameter ($\\bar{\\nu}$). This phenomenological parameter varies independently for each galaxy and is constrained from the study of RC as discussed in the previous section. A comparison of the constrained $\\bar{\\nu}$ obtained along with the stellar mass of the galaxy is represented via an individual datapoint in Fig.\\ref{Fig:massrel}. The figure shows that the magnitude of the phenomenological parameter $\\bar{\\nu}$ increases almost linearly with the increasing mass of the galaxies. Our analysis is thus justified with the previous study \\cite{Rodrigues:2014xka} done for the same gravity model. Different representations are used in the plot to categorize galaxies belonging to various morphological types. The pink round pointers belong to the early type, the green square shows the spiral, the gray triangle is for the late type, and the purple diamond represents the starburst galaxies. The figure shows that the baryonic mass decreases from the early-type galaxies to the late-type ones. Accordingly, the value of $\\bar \\nu$ reduces linearly: a feature which can also be seen from the data provided in Table.\\ref{par_sparc}, i.e. $\\bar\\nu$ is smaller for the relatively younger galaxies. Such a linear relation of the model parameter with the mass of the system points towards the non-fundamental behavior of the RGGR model. However, as can be seen from the analysis of RC, the gravity model is consistent with the observations on astrophysical scales.\n\\subsection{Empirical relations for SPARC}\n\\FloatBarrier\n\\subsubsection{Radial Acceleration Relation (RAR)}\nThe data points of the SPARC galaxies projecting the relation between the observed Eq.\\ref{acc} and baryonic acceleration Eq.\\ref{bar} is shown to follow an empirical relation as mentioned in Eq.\\ref{rar}. This shows that in weak-gravity regions, the observed and baryonic acceleration follows the relation $a_{obs}\\propto \\sqrt{a_{bar}}$ and depends linearly in high acceleration scales as is expressed by the empirical relation. Thus, if RGGR is a consistent gravity model, it must satisfy RAR for the SPARC galaxies. This relation has been looked into and verified for a large sample of galaxies present in the SPARC catalog without any prior assumptions for the DM or alternative gravity model. In the alternative gravity scenario, total circular velocity and acceleration are modified according to Eq.\\ref{vrgr} and Eq.\\ref{arggr} respectively. The net acceleration is the sum of contribution coming from different baryonic components of a galaxy ($a_{bar}(r)$) with an additional part dependent on $\\phi_N(r)$ and $\\bar{\\nu}$ as expressed in Eq.\\ref{arggr}. In our analysis, we compare the modified acceleration $(a_{RGGR}(r))$ with $a_{bar}(r)$ to study the behavior of RAR in an alternative gravity framework. \\\\\n\\begin{figure}[t!]\n    \n    \\includegraphics[width=.94\\linewidth, height=.4\\textwidth]{RAR.pdf}\n    \\caption{The plot compares the RAR relation where the total observed acceleration is evaluated in the RGGR framework. The black solid line shows the empirical relation Eq.(\\ref{rar}) obtained from the SPARC observations. The green scatter points refer to the individual data points of all the galaxies analyzed where the net contribution to the acceleration gets modified as given by Eq.(\\ref{arggr}) }\n    \\label{Fig:rar}\n\\end{figure}\nThe evaluation of the empirical relation requires the knowledge of free parameters, which is taken from the RC analysis in Sec.\\ref{sec:rc}. The RAR behavior as determined for the individual datapoint of all the selected galaxies in the RGGR framework is shown in Fig.\\ref{Fig:rar}. The gray solid line is the analytical relation fitted from the observational data and has the form as given in Eq.\\ref{arggr}. Also, for each galaxy in our analysis, we determine the baryonic acceleration ($a_{bar}(r)$) at every radial point specified in the SAPRC catalog. Similarly, we also compute the RGGR acceleration ($a_{RGGR}(r)$) at every radial point. Thus, each green dot on the plot refers to the RGGR and baryonic acceleration at a specific radius for a given galaxy. The collection of points in Fig.\\ref{Fig:rar} contains the contribution for all the 100 galaxies selected in our analysis.\n\nResidual computed from the comparison of the relation with the 1840 data points, as illustrated in the figure, turns out to be $0.33$ dex. Thus, from the best-fit parameters from the RC analysis, RAR behaves satisfactorily in the context of the RGGR model. \n\\begin{figure}[t!]\n    \\includegraphics[width=0.96\\linewidth,height=8.0cm]{cornerplot2.pdf}\n    \\caption{The posterior distribution of the parameter space in BTFR Eq.\\ref{btfr_lin} evaluated using BayesLineFit \\cite{Lelli:2019igz}.}\n    \\label{fig:btfrplot2}\n\\end{figure}\n\\subsubsection{Baryonic Tully Fisher Relation (BTFR)}\nSPARC galaxies are also found to follow the BTFR (Eq.\\ref{btfr}), which provides a power law relation between the baryonic mass and the velocity measured at the flat part of the rotation curve. This relation holds irrespective of the gravity model. Therefore, for RGGR to be a consistent choice as an alternative gravity model, BTFR must be satisfied by the RGGR-predicted dynamics for the SPARC galaxies. To probe the BTFR,\n\\begin{equation}\\label{btfr_lin}\n    \\log(M_{bar})=x\\log(V_f)+\\log~A,\n\\end{equation}\nwe consider the $V_f$ corresponding to the RC fit with RGGR for each galaxy. The flat velocity ($V_f$) for each galaxy in the RGGR framework can be evaluated from the fit of the RC (Sec. \\ref{sec:rc}) at a given radius. The baryonic mass $M_{bar}$ for a galaxy is a sum of stellar and gas components. The velocities of these different components are connected to their respective mass profile. For a given mass profile, $M_{bar}$ can be estimated, particularly for the SPARC galaxies, the bulge and disk are assumed to have spheroidal and exponential distribution respectively \\cite{Lelli:2016zqa}. BTFR has also been found to be sensitive on the radial choice \\cite{Lelli:2019igz} at which $V_f$ is measured for SPARC galaxies. We evaluate the relation at $3.2~R_d$, which has shown an optimal behavior to the relation. The choice of $3.2~R_d$ \\cite{ Lelli:2019igz,romanowsky2012angular} ensures that $80\\%$ of the stellar matter is encompassed within the radius. Almost for all the SPARC galaxies we select, the distance of $3.2~R_d$ from the center of the galaxy corresponds to the flat region of the RC. \n\\begin{figure}[t!]\n    \\centering\n    \\includegraphics[width=0.95\\linewidth,height=8.0cm]{btfr_3.2rd.pdf}\n    \\caption{The BTFR with the flat circular velocity evaluated in the RGGR model. The above plot estimates $V_{f}$ for the RGGR model at $3.2~R_d$. The gray data points are the measure of flat velocity evaluated for individual galaxies. The green solid line shows the analytical best fit, consistent with the observed $V_f$ in the RGGR framework.}\n    \\label{fig:btfrplot1}\n\\end{figure}\nWe use BayesLineFit package \\cite{Lelli:2019igz}, which is based on \\textit{emcee} algorithm to find ($x$, $\\log A$). The best-fit value obtained for the parameters ($x$, $\\log A$) is found to be ($3.55$, $2.70$) with an orthogonal scatter of $0.08$ dex. We also report the vertical scatter given by \\cite{Lelli:2019igz}:\n\\begin{equation}\n   \\sigma_0=\\sqrt{\\frac{1}{n} \\sum^n [\\log(M_{bar})-x\\log(V_f)-\\log A]^2},\n\\end{equation}\nwhere $n$ is the number of galaxies selected in our analysis, the observed $\\sigma_0$ for the case of $3.2~R_d$ comes out to be $0.54$. The posterior distribution for the fitting parameters, i.e., slope ($x$), intercept ($\\log~A$), and intrinsic scatter, are shown in Fig.\\ref{fig:btfrplot2}. The solid red lines in the plot point towards the maximum likelihood values for the parameters. Given the best-fit parameters, BTFR for all the galaxies included in our analysis is illustrated in Fig.\\ref{fig:btfrplot1}. The solid gray circles represent the flat velocity for each SPARC galaxy evaluated at $r=3.2~R_d$. Additionally, the green solid line fits the linear equation Eq.\\ref{btfr_lin} for the best-fit parameters obtained in our analysis. The BTFR plot in Fig.\\ref{fig:btfrplot1} shows that the flat velocity evaluated at $3.2~R_d$ consistently matches the observational results.\n\n\\section{Conclusion}\n\\label{sec:conc}\nOur analysis focuses on the renormalization group improved gravity to explain the kinematics for a collection of rotationally supported galaxies compiled in the SPARC catalog. \nWe, for the first time, study the dependence of the alternative gravity parameter on the galaxy morphology. We have looked into four different morphological types of galaxies, viz. early, spiral, late, and starburst. We have constrained the model parameter $\\bar \\nu$ for each galaxy taken from all four morphological types and have found that the RGGR consistently fits the observed net circular velocity. Our statistical analysis has probed this consistency for the individual galaxies using the goodness of fit. We have also checked the linear dependence of the model parameter $\\bar \\nu$ on the baryonic mass of the galaxy.\nThe constrained values for the parameter $\\bar\\nu$ for our sample of the SPARC galaxies lie in the range $10^{-6}-10^{-8}$ and are consistent with the variation of the masses of the galaxies under consideration. This implies that the older and heavier galaxies lead to a larger $\\bar \\nu$. Indeed, we have found that the parameter $\\bar\\nu$ decreases from the older galaxies to the younger ones. \n\\newline\nWe have further verified our goodness of fit from the RC analysis in light of the well-known empirical relations: RAR and BTFR.\nIn particular, the RAR compares the radial variation of the observed and the baryonic acceleration. An additional factor comes in the baryonic acceleration due to the RGGR model. Our analysis has found that the RAR in the RGGR framework aligns with the established analytical relation obtained from previous observations. \nOn a similar note, BTFR compares the baryonic mass with the observed flat velocity ($V_f$) of a galaxy, suggesting a power law dependence between them. The exact power law index has been found to be sensitive to the choice of the flat velocity radius. We have selected a radius of $3.2 R_d$, which is known to show optimal behavior with the observations. Our analysis finds a tight correlation of the baryonic mass with flat velocity measured at $3.2R_d$ and has obtained a small orthogonal scatter of $0.08$. \nThis scatter may reduce even further for different choices of the radius of the $V_f$ and by relieving the assumption of universal exponential density profile for the diffused gas component of the galaxy, which requires further study.\n\\\\\n\\newline\nAdditionally, we compare the RGGR model with a DM scenario, where we assume the profile to be NFW. Our analysis clearly shows that, leaving aside the galaxies where both the models (RGGR and NFW) perform equally well, RGGR is favored in relatively larger number of galaxies than NFW. \nAn important aspect of the RGGR model is that the model parameter $\\bar\\nu$ is dependent on the mass of the gravitating system. Many other alternative gravity models exhibit dependence of the model parameters on the scale of the system. Though this scale dependence of the phenomenological parameter may question the fundamental nature of these alternative gravity models, they still remain a consistent theory of gravity. It is to be noted that such RG models of gravity face certain criticisms for the non-universal choice of the running scale parameter \\cite{Donoghue:2019clr}. However, for any alternative gravity model to be considered viable, it must satisfy the consistency criteria, i.e., relativistic, self-consistent, and a complete theory free of ghosts and instabilities. The theory also requires the ability to explain the gravitational dynamics both in the strong and weak-field regimes as well, and it should regain a valid Newtonian limit on the Solar System scales \\cite{will_1993, Shankaranarayanan:2022wbx}. RGGR, an alternative gravity model, follows the above consistency criteria and thus remains a phenomenologically consistent theory of gravity. Most importantly, in the context of our work, RGGR satisfies the galactic kinematic observations for a large selection of SPARC galaxies. \nFurther validation of the RGGR model as an alternative to the DM paradigm requires the model to be tested in expanded galactic mass scales, for example, the newly discovered massive galaxies  \\cite{Labb2022APO}. In the smaller galactic mass scales, the DM-dominated ultra-diffuse galaxies like DF44 \\cite{vanDokkum:2016uwg} will provide further tests \\cite{eshabh} to the RGGR gravity model.\n\\acknowledgments\nSovan Chakraborty acknowledges the support of the funding from DST-SERB\nprojects CRG/2021/002961 and MTR/2021/000540. Sayan\nChakrabarti would like to acknowledge the support from\nthe DST-SERB research grant MTR/2022/000318. All the\nauthors would like to acknowledge discussions and valuable inputs from Sayak Dutta. Esha Bhatia is indebted to the Ministry of Human Resource Development, Government of India, for assistance through a doctoral fellowship.\n\n\n\\nocite{*}\n\n\n\n\\end{document}\n\n\n"}
{"paper_id": "2403-00532", "version": "2403-00532v1", "root_file_path": "d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\data_raw\\2403-00532\\tex\\2403-00532v1\\file_arxiv.tex", "metadata": {"total_length": 179231, "merged_count": 1, "merged_files": ["file_arxiv.tex"], "missing_files": []}, "content": "\n\n\\documentclass[english,a4paper]{article}\n\n\n\\expandafter\\let\\csname equation*\\endcsname\\relax\n\n\\expandafter\\let\\csname endequation*\\endcsname\\relax\n\n\\usepackage{bm}\n\\usepackage{graphicx}\n\\usepackage{amsmath}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsthm}\n\\usepackage{amstext}\n\\usepackage{amsbsy}\n\\usepackage{amsopn}\n\\usepackage{amscd}\n\\usepackage{amsxtra}\n\\usepackage[colorlinks=true,allcolors=blue]{hyperref}\n\\usepackage{color}\n\\usepackage{soul}\n\\usepackage[dvipsnames]{xcolor}\n\\usepackage{listings}\n\n\n\n\n\n\n\n\n\n\\usepackage{booktabs} \n\\usepackage{enumitem}\n\\usepackage{makeidx}\n\n\n\\usepackage[most]{tcolorbox}\n\t\\tcbuselibrary{theorems}\n\t\\tcbuselibrary{breakable}\n\\usepackage[left=1cm,right=1cm,top=2cm,bottom=2cm]{geometry}\n\n\n\n\n\n\n\n\n\n\n\\newcommand{\\setim}{\\mathrm{Ran}}\n\n\n\\newcommand{\\Texp}{\\mathbb{T}\\mathrm{exp}}\n\\newcommand{\\Te}{\\mathbb{T}e}\n\\newcommand{\\T}{\\mathbb{T}}\n\\newcommand{\\Pexp}{\\mathbb{P}\\mathrm{exp}}\n\\newcommand{\\Pe}{\\mathbb{P}e}\n\\newcommand{\\slim}[1]{\\text{\\textnormal{s--}} \\hspace{-.40em} \\lim_{#1}}\n\\newcommand{\\wlim}[1]{\\text{\\textnormal{w--}} \\hspace{-.40em} \\lim_{#1}}\n\\newcommand{\\siglim}[1]{\\sigma \\hspace{-.15em} \\text{--} \\hspace{-.20em} \\lim_{#1}}\n\\newcommand{\\pdirac}{\\sqcup \\hspace{-0.42em} \\sqcup}\n\\newcommand{\\ad}{\\mathrm{Ad}}\n\\newcommand{\\addiff}{\\mathrm{ad}}\n\\newcommand{\\oiint}{\\bigcirc \\hspace{-1.15em} {\\int \\hspace{-0.9em} \\int}}\n\\newcommand{\\oiiint}{\\bigcirc \\hspace{-1.35em} {\\int \\hspace{-0.95em} \\int \\hspace{-0.95em} \\int}}\n\\newcommand{\\loiint}{o \\hspace{-0.6em} {\\int \\hspace{-0.55em} \\int}}\n\\newcommand{\\bivec}{\\overset{\\rotatebox{180}{$\\curvearrowleft$}}}\n\n\\newcommand{\\cvec}{\\bivec}\n\\renewcommand{\\div}{\\mathrm{div} }\n\\newcommand{\\grad}{\\overrightarrow{\\mathrm{grad}} }\n\\newcommand{\\rot}{\\cvec {\\mathrm{rot}} }\n\n\\newcommand{\\ihbar}{\\imath \\hbar}\n\\newcommand{\\norme}[1]{\\begin{array}{||c||} #1 \\end{array}}\n\\newcommand{\\dslash}{\\partial \\hspace{-0.8em} \\diagdown}\n\\newcommand{\\sumint}{\\Sigma \\hspace{-0.85em} \\int}\n\\newcommand{\\fig}[4]{\\begin{figure}[!h]\n                     \\begin{center}\n                     \\includegraphics[width=#4]{#2}\n                     \\end{center}\n                     \\caption{\\label{#1} \\textit{#3}}\n                     \\end{figure}}\n\\newcommand{\\figdbl}[5]{\\begin{figure}[!h]\n                       \\begin{center}\n\t\t       \\includegraphics[width=#5]{#2}\n\t\t       \\includegraphics[width=#5]{#3}\n\t\t       \\end{center}\n\t\t       \\caption{\\label{#1} \\textit{#4}}\n\t\t       \\end{figure}}\n\\newcommand{\\ket}[1]{| #1 \\rangle}\n\\newcommand{\\bra}[1]{\\langle #1 |}\n\\newcommand{\\braket}[2]{\\langle #1 | #2 \\rangle}\n\\newcommand{\\ketbra}[2]{| #1 \\rangle \\langle #2 |}\n\\newcommand{\\card}{\\mathrm{card}\\ }\n\\newcommand{\\lbrakint}{\\left[ \\hspace{-0.1em} [}\n\\newcommand{\\rbrakint}{] \\hspace{-0.1em} \\right]}\n\\newcommand{\\sgn}{\\mathrm{sgn}}\n\\newcommand{\\dg}{\\delta_{(g)}}\n\\newcommand{\\cupg}{\\cup_{(g)}}\n\\newcommand{\\bigveebar}{\\underline{\\bigvee}}\n\\newcommand{\\ppint}{\\smallsetminus \\hspace{-.90em} \\int}\n\\newcommand{\\dgpe}{\\mathring{\\delta}}\n\\newcommand{\\ssi}{si et seulement si }\n\\newcommand{\\dom}{\\mathrm{Dom}}\n\\newcommand{\\Sp}{\\mathrm{Sp}}\n\\newcommand{\\core}{\\mathrm{Cor}}\n\\newcommand{\\Res}{\\mathrm{Res}}\n\\newcommand{\\Ric}{\\mathrm{Ric}}\n\\newcommand{\\argsh}{\\mathrm{argsh}\\ }\n\\newcommand{\\llangle}{\\langle \\hspace{-0.2em} \\langle}\n\\newcommand{\\rrangle}{\\rangle \\hspace{-0.2em} \\rangle}\n\\newcommand{\\RE}{\\Re \\mathrm{e}}\n\\newcommand{\\IM}{\\Im \\mathrm{m}}\n\\newcommand{\\cotan}{\\mathrm{cotan}\\ }\n\\newcommand{\\sinc}{\\mathrm{sinc}}\n\\newcommand{\\lllangle}{\\langle \\hspace{-0.2em} \\langle \\hspace{-0.2em} \\langle}\n\\newcommand{\\rrrangle}{\\rangle \\hspace{-0.2em} \\rangle \\hspace{-0.2em} \\rangle}\n\\newcommand{\\id}{\\mathrm{id}}\n\\newcommand{\\au}{\\ \\mathrm{a.u.}}\n\n\\newcommand{\\Mc}[1]{\\mathcal{#1}}\n\\newcommand{\\setZ}{\\mathbb{Z}}\n\\newcommand{\\setN}{\\mathbb{N}}\n\\newcommand{\\setR}{\\mathbb{R}}\n\\newcommand{\\setQ}{\\mathbb{Q}}\n\\newcommand{\\setC}{\\mathbb{C}}\n\\newcommand{\\Id}{\\mathbb{I}}\n\n\\newcommand{\\ii}{\\imath}\n\\newcommand{\\A}{\\hat a}\n\\newcommand{\\Ad}{\\hat a^\\dagger}\n\\newcommand{\\sigp}{\\hat{\\sigma}_+}\n\\newcommand{\\sigm}{\\hat{\\sigma}_-}\n\\newcommand{\\sigz}{\\hat{\\sigma}_z}\n\\newcommand{\\sigx}{\\hat{\\sigma}_x}\n\\newcommand{\\sigy}{\\hat{\\sigma}_y}\n\n\n\n\\newcommand{\\red}[1]{{\\color{red} #1}}\n\n\n\n\n\n\n\n\n\n\\begin{document}\n\n\n\\newtcbtheorem[auto counter]{definition}\n  {Definition}{breakable,theorem style=plain,fonttitle=\\bfseries\\upshape, fontupper=\\itshape ,arc=0mm, boxrule=0.5mm,coltitle=black, colback=gray!10!white,colframe=gray!25!black, left=1mm,right=1mm,top=1mm,bottom=1mm}{def}\n\n\\newtcbtheorem[auto counter]{example}\n  {Example}{breakable,theorem style=plain,fonttitle=\\bfseries\\upshape ,\n  arc=0mm, boxrule=0.5mm,coltitle=black, colback=gray!10!white,colframe=white, left=1mm,right=1mm,top=1mm,bottom=1mm}{example}\n\n\n\n\n  \\newtcbtheorem[auto counter]{property}\n  {Property}{breakable,theorem style=plain,fonttitle=\\bfseries\\upshape, fontupper=\\itshape ,arc=0mm, boxrule=0.5mm,coltitle=black, colback=gray!10!white,colframe=gray!25!white, left=1mm,right=1mm,top=1mm,bottom=1mm}{property}\n\n  \\newtcbtheorem[auto counter]{proposition}\n  {Proposition}{breakable,theorem style=plain,fonttitle=\\bfseries\\upshape, fontupper=\\itshape ,arc=0mm, boxrule=0.5mm,coltitle=black, colback=gray!10!white,colframe=gray!50!white, left=1mm,right=1mm,top=1mm,bottom=1mm}{prop}\n\n\\newtcbtheorem[auto counter]{theorem}\n  {Theorem}{breakable,theorem style=plain,fonttitle=\\bfseries\\upshape, fontupper=\\itshape ,arc=0mm, boxrule=0.5mm,coltitle=black, colback=gray!10!white,colframe=gray!75!black, left=1mm,right=1mm,top=1mm,bottom=1mm}{thm}\n\n\n\\tcolorboxenvironment{proof}{\nblanker,breakable,left=5mm,\nbefore skip=10pt,after skip=10pt,\nborderline west={1mm}{0pt}{gray}}\n\n\n\\lstset{language=Mathematica,numbers=left, numberstyle=\\tiny, stepnumber=1, mathescape=true,frame=single}\n\n\\title{Introduction to Theoretical and Experimental aspects of Quantum Optimal Control}\n\n\n\\author{Q. Ansel\\footnote{Institut UTINAM,  CNRS UMR 6213, Universit\\'{e} de Franche-Comt\\'{e}, Observatoire des Sciences de l’Univers THETA, 41 bis avenue de l’Observatoire, F-25010, Besançon, France}, E. Dionis, F. Arrouas, B. Peaudecerf\\footnote{Laboratoire Collisions Agr\\'egats R\\'eactivit\\'e, UMR 5589, FeRMI, UT3, Universit\\'e de Toulouse, CNRS, 118 Route de Narbonne, 31062 Toulouse CEDEX 09, France}, S. Gu\\'erin\\footnote{Laboratoire Interdisciplinaire Carnot de Bourgogne, CNRS UMR 6303, Universit\\'{e} de Bourgogne, BP 47870, F-21078 Dijon, France}, D. Gu\\'ery-Odelin\\footnote{Laboratoire Collisions Agr\\'egats R\\'eactivit\\'e, UMR 5589, FeRMI, UT3, Universit\\'e de Toulouse, CNRS, 118 Route de Narbonne, 31062 Toulouse CEDEX 09, France}, D. Sugny\\footnote{Laboratoire Interdisciplinaire Carnot de Bourgogne, CNRS UMR 6303, Universit\\'{e} de Bourgogne, BP 47870, F-21078 Dijon, France, dominique.sugny@u-bourgogne.fr}}\n\n\\maketitle\n\n\\begin{abstract}\nQuantum optimal control is a set of methods for designing time-varying electromagnetic fields to perform operations in quantum technologies. This tutorial paper introduces the basic elements of this theory based on the Pontryagin maximum principle, in a physicist-friendly way. An analogy with classical Lagrangian and Hamiltonian mechanics is proposed to present the main results used in this field. Emphasis is placed on the different numerical algorithms to solve a quantum optimal control problem. Several examples ranging from the control of two-level quantum systems to that of Bose-Einstein Condensates (BEC) in a one-dimensional optical lattice are studied in detail, using both analytical and numerical methods. Codes based on shooting method and gradient-based algorithms are provided. The connection between optimal processes and the quantum speed limit is also discussed in two-level quantum systems. In the case of BEC, the experimental implementation of optimal control protocols is described, both for two-level and many-level cases, with the current constraints and limitations of such platforms. This presentation is illustrated by the corresponding experimental results.\n\\end{abstract}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Introduction to Quantum Control}\n\nThe design and development of quantum technologies requires the use of many advanced techniques in order to tackle the fragility of quantum information, and the difficulty of isolating and manipulating quantum entities~\\cite{raimond2006exploring,kurizki2015quantum,acin2018quantum,becher20232023,shore-book,ricebook,RMPsugny}. These challenges have led in particular to the development of Quantum Optimal Control (QOC)~\\cite{guerin2003,werschnik2007,brif2010,bonnard_optimal_2012,altafini2012,cat,\nkochroadmap,rembold2020introduction,PRXQuantumsugny,kuprov2023spin,dalessandro-book,stefanatos2021}, a branch of optimal control theory whose aim is to adapt and apply the tools of optimal control to quantum systems. As its name suggests, optimal control~\\cite{pontryaginbook,leemarkusbook,bryson1975applied,kirk2004optimal,trelat2012optimal,agrachev-book,bressan-piccoli,schaettler-book,liberzon-book,boscain-book,jurdjevic-book} is a mathematical theory that refers to the design of time-varying controls to manipulate dynamical systems in order to ideally achieve specific goals (encapsulated in a figure of merit). QOC was initiated in the eighties, and has since then seen an exponential development. It nowadays comprises a very powerful toolbox of analytical and numerical methods for designing control protocols under various experimental constraints and limitations. QOC is therefore not only a mature theoretical field, but also a very efficient approach from an experimental point of view on many quantum platforms. A key aspect of QOC is that it is an open-loop procedure that provides the optimal control process without any feedback from the experiment. Successful applications therefore require a very accurate modeling of the dynamical system and careful consideration of the controls that are experimentally available. Such QOC techniques have recently been successfully applied in many different areas of quantum technologies, ranging from quantum computing and simulation to sensing~\\cite{kochroadmap,rembold2020introduction}, making QOC a key tool of growing importance in the development of quantum technologies. This can be seen by its growing presence in the literature, as illustrated in Fig.~\\ref{fig:number_of_publications}. QOC is not the only control method that is available to manipulate quantum systems in an optimized fashion. Among others, we can mention adiabatic passage techniques~\\cite{vitanov2001,RMPstirap,guerin2011}, shortcut to adiabaticity approaches~\\cite{RMPSTA,STA,campo2013,whitty2020,torosov2021} or composite pulses~\\cite{vitanovCP,dridi2020,Ivanov2022,torosov2011}.\nFurthermore, the framework of the quantum speed limit (QSL)~\\cite{deffner2017quantum,frey2016,bukov2019,oconnor2021,poggi2019,campo2013b}, which seeks to establish lower bounds on the minimum time required to steer a system from a given initial state to a target state, is closely linked to QOC~\\cite{deffner2017quantum,calarco2009,diaz2020,hegerfeldt2013driving}. The corresponding time is expressed as a ratio between the distance to the target state and the dynamical speed of evolution. This approach, like QOC, has been the subject of intense development in recent years.\n\n\\begin{figure}[htbp]\n    \\centering\n    \\includegraphics[width=15cm]{fig1.pdf}\n    \\caption{Growth in the number of scientific articles $N_{QOC}$ (red curve) with the keywords ``quantum\" and ``optimal control\" published each year on the internet, during the period 1990-2022, and ratio between $N_{QOC}$ and the number of scientific articles $N_Q$ with the keywords ``quantum\" (blue curve). Despite an increasing number of annual publications, the ratio decreased during the  period 2000-2015 (the number of publications in quantum science grew very rapidly during these years), but since 2018 the trend has reversed. At the beginning of 2023 (not shown in the graph), the trend is confirmed with almost 1\\% of publications concerning QOC. Data were collected using Google Scholar.}\n    \\label{fig:number_of_publications}\n\\end{figure}\n\nSeveral textbooks and reviews have introduced and described the basic elements and provided a fairly complete picture of the field of QOC~\\cite{bonnard_optimal_2012,rembold2020introduction,\nPRXQuantumsugny,dalessandro-book,wilhelmreview}. Others have focused on its practical numerical implementation or on the description of various optimization algorithms~\\cite{werschnik2007,bonnans2006numerical,koch2012,borzi-book}. However, either their starting points are at a high mathematical level, or they only consider a specific aspect of the QOC toolbox, so that it can be difficult for a newcomer to get a complete overview of the field and to apply such techniques to their own system. This tutorial paper aims to fill that gap. This introduction covers a wide range of aspects, from the description of the Pontryagin Maximum Principle (PMP) which can be seen as the central mathematical result of the theory~\\cite{pontryaginbook,leemarkusbook,liberzon-book}, to the analytical or numerical computations of optimal control and their experimental implementation. Mathematical issues are treated with a minimum of rigor, but with several reminders of basic notions. The various numerical algorithms available in the literature are described. Special emphasis is given to two numerical methods, namely the shooting and the gradient-based algorithms, which allow solving low- and high-dimensional quantum optimal problems respectively. Numerical codes are provided in the supplementary material. However, due to the large number and diversity of optimization methods, some interesting topics are not covered in this tutorial. Among others, we can mention machine learning techniques~\\cite{HushScience2017,bukovPRX2018,dayPRL2019,mehtaPhysRep2019,carleoRMP2019,giannelli2022,khalid2023}, the Hamilton-Jacobi-Bellman approach~\\cite{bertsekasbook}, second-order optimization algorithms~\\cite{grape2,tannor2011, sherson2020,goodwin2023}, closed-loop control~\\cite{naturerouchon,egger2014,porotti2023}, controllability and accessibility of quantum systems~\\cite{infinity-1,sachkov_controllability_2000,schirmer_complete_2001,schirmer2003controllability,Albertini_notions_of_controllability_2003}, quantum control landscapes~\\cite{pechen2011there,moore2012exploring,chakrabarti2007quantum,pechen2012quantum,larocca2020}, robust optimal control~\\cite{kobzar2004exploring,k1,k2,kobzar2012exploring,daems:2013,chen2014sampling,Van_Damme_robust_2017,zeng2018,wu2019,dridi2020b,dong2021,Ansel_2021,propson2022,guerin2022,guerin2022b,harutyunyan2022robust,nelson2023,schirmer2023,carolan2023}, optimal control of linear systems~\\cite{liberzon-book,vardan2020,vardan2019,li2017,li2011,evangelakos2023b},\noptimal control and quantum sensing~\\cite{Degen_review_2017,poggiali2018,wittler2021,Lin_optimal_2021,ansel2023optimal,Liu_optimal_QM_review_2022} and the control of open quantum systems~\\cite{sugny07,dirr2009,bonnard2009,addis2016,Koch2016,fux2021,pechen2021,pechen2023}.\n\nThe success of QOC has been demonstrated theoretically in a large number of quantum systems~\\cite{kochroadmap}. Although such control protocols are interesting from a theoretical point of view, e.g. to know the physical limits of a dynamical system in terms of control time or fidelity, they are generally not the final answer to a control problem: the ultimate goal is to implement the calculated control process experimentally. Several difficulties have to be overcome in order to transfer the theoretical result to the experiment, which can be divided into two categories. The first problem comes from the model system which must be sufficiently precise in an open-loop framework to describe the physical system, but relatively simple to apply the optimization algorithms numerically. A second obstacle is related to the family of control pulses that can be implemented experimentally. Depending on the experimental setup, different constraints may arise, ranging from the control time and the maximum intensity of the control available, to its Fourier bandwidth or its time discretization. It is now possible to take these constraints into account in optimization algorithms. This development makes QOC more useful in\nterms of experimental applications and helps to bridge the gap between control theory and control\nexperiments. Such a project has been successfully carried out on various platforms such as superconducting circuits, NV centers, magnetic resonance and trapped atoms, ions and molecules~\\cite{kochroadmap}. The experimental implementation of optimal control protocols is highly dependent on the system under study, and we cannot cover all possible situations here. In this paper, we focus on a specific example for which we provide numerical solutions, namely the control of a Bose-Einstein Condensate (BEC) in a one-dimensional optical lattice~\\cite{RMP1999,RMP2008,gross2017}. This system has attracted a lot of interest in recent years from a control point of view~\\cite{Hohenester2007,Jager2014,Sorensen2018,bason2012,zhou2018,Weidner2018,BEC2021,BEC2023,BEC2024,frank2016}, especially for quantum simulation applications~\\cite{altman,noriRMP,cirac2012}, for which QOC can be  used to efficiently prepare the initial state of the system~\\cite{dupont2023}. It allows us to illustrate the implementation of optimal control on an infinite-size system, as well as to emulate a two-level system. We discuss the different steps of the experimental implementation from the modeling of the system dynamics, to the experimental constraints on the control design and the measurement of the final state of the system. The impact of various experimental limitations is highlighted and discussed. Experimental results based on optimal control protocols are presented.\n\nThis tutorial  paper is organized as follows. In Sec.~\\ref{chap:optimal control theory}, we introduce the optimal control theory based on the PMP. The analogy with classical Lagrangian and Hamiltonian systems is used to describe the basic concepts of this theory. In Sec.~\\ref{sec:QOC}, we show how such results can be adapted to quantum systems. The time-optimal control of a two-level quantum system is used as an illustrative example. The connection between QOC and QSL in this system is discussed. In Sec.~\\ref{sec:numerical_methods}, a detailed introduction to numerical methods is given with particular emphasis on two different  approaches, namely the shooting method and gradient-based optimization algorithms which are also illustrated in a two-level quantum system. Pseudo-codes describing the structure of the algorithms are provided in the main text, and Python codes for specific control processes can be found in the supplementary material. These numerical approaches are used in Sec.~\\ref{sectheoexp} to manipulate a BEC in an optical lattice. A complete description of an experimental implementation is given as well as the constraints and limitations of the experimental apparatus used. A number of examples in classical and quantum physics are given throughout the text. The simplest ones are placed in a gray box and can be ignored by a reader already experienced in optimal control. A conclusion is given and prospective views are suggested in Sec.~\\ref{sec:conclusion}.~\\ref{secappA} gives a list of mathematical symbols and acronyms used in the article. \\ref{app_lagrange} and~\\ref{appendixPMP} contain mathematical results used in the main text. A description of the numerical codes of the supplementary material is also provided in~\\ref{app_code}.\n\n\n\n\n\n\n\n\n\n\\section{Optimal Control Theory and Pontryagin Maximum Principle}\n\\label{chap:optimal control theory}\n\n\\subsection{Introduction}\n\nOptimal Control Theory (OCT) has its roots in the calculus of variations which is over 300 years old~\\cite{Courant_Hilber_vol_1,Courant_Hilber_vol_2,gelfand2000calculus}.\nInterest in this branch of mathematics grew rapidly with the advent of computer science in the early 1960s. A rigorous mathematical framework for OCT was given by L. Pontryagin and his co-workers in 1960 with the introduction of the Pontryagin Maximum Principle (PMP)~\\cite{pontryaginbook,leemarkusbook,bryson1975applied,kirk2004optimal,liberzon-book}, which then led to a variety of applications~\\cite{bryson1996optimal}. In particular, OCT was at the origin of optimal trajectory prediction in aeronautics~\\cite{bonnard_optimal_2012,trelat2012optimal}. Today, OCT is used in a wide range of fields ranging from economics, to physics and electronics, to name but a few. The PMP transforms the optimal control problem into a\ngeneralized Hamiltonian system subject to a maximization condition and some boundary conditions. In this framework,\nthe goal is to find the Hamiltonian trajectory that reaches the target state, while minimizing the cost functional which\ndefines the optimization procedure. A key advantage of the PMP is that it reduces the initial infinite-dimensional control\nlandscape to a finite low-dimensional space. This brief description also shows that optimal control is closely related  to classical mechanics and its Lagrangian or Hamiltonian formalism.\n\nBefore we present the PMP at the end of this section, let us take a step back to basics. As the above brief description of optimal control shows, a first observation is that optimal control problems are very similar to those in classical mechanics~\\cite{goldsteinbook,arnoldbook}, where the goal is to find the trajectory of a classical system that minimizes a certain quantity, the action.\nThe same direction is followed in OCT, but instead of the usual action, other quantities relevant to the control procedure are minimized. Another important modification is the presence of a time-dependent parameter in the dynamical equations which can be shaped to some extent by an external operator. In the case of an aeroplane, for example, the control parameters include all possible actions that can be performed on the aircraft, such as modifying the engine thrust or changing the orientation of elevators and ailerons. In quantum physics, the control agent is generally a shaped electromagnetic field. The formal concepts are illustrated in this section using a simple example, namely a point particle controlled by a time-dependent force. We begin by recalling some key elements of the calculus of variations applied to a point particle.\n\\begin{example}{}{ex1}\n A basic idea of the calculus of variation and of the principle of least action is to derive the equation of motion of a physical system from a single quantity, the action~\\cite{goldsteinbook}. Mathematically, the latter takes the form of a functional~\\cite{gelfand2000calculus} i.e. a function of functions. In the case of a free one-dimensional particle, the action $S$ can be expressed as\n\n\\begin{equation}\nS[x]= \\int_0^{t_f} \\frac{1}{2} m \\dot x^2(t)~dt,\n\\end{equation}\n\nwhere $x(t)\\in \\setR$ is the position of the particle at time $t$ with $t\\in [0,t_f]$, $t_f$ the duration of the dynamics, $m$ its mass, and the dot symbolizes the time derivative. The physical motion is associated with the least action $S$. From the condition $\\delta S=0$ where $\\delta S$ is the functional derivative of $S$ taken for two trajectories close to each other~\\cite{goldsteinbook}, it can be shown that the physical trajectory satisfies the Euler-Lagrange equation\n\n\\begin{equation}\n\\frac{\\partial \\Mc L}{\\partial x} -\\frac{d}{dt} \\frac{\\partial \\Mc L}{\\partial \\dot x} = 0,\n\\label{eq:Euler_Lagrange_point_particle}\n\\end{equation}\n\nwhere $\\Mc L = m\\dot x^2/2$ is the Lagrangian, here equal to the kinetic energy of the particle. An explicit calculation leads to the expected result $m \\Ddot x(t) = 0$.\n\nThe model system can be extended by introducing a control $f(t)\\in\\setR$, which takes the form of a  force applied to the particle. With this modification, the action becomes\n\n\\begin{equation}\nS[x]= \\int_0^{t_f} \\left( \\frac{1}{2} m \\dot x^2 (t) + f(t) x(t) \\right)~dt,\n\\end{equation}\nwhere the extra term $-f(t)x(t)$ is a potential energy. The Lagrangian is then defined as the difference between the kinetic energy and the potential energy. The equation of motion, calculated using Eq.~\\eqref{eq:Euler_Lagrange_point_particle} with the new Lagrangian is\n$m \\Ddot x(t) = f(t)$. The latter can be determined from Newton's law, but also from the Hamiltonian formalism, in which the Lagrangian is replaced by the Hamiltonian $H$. The Hamiltonian is a function on the phase space i.e. position and momentum ($\\Gamma = \\setR^2$ for a point particle), rather than a function of position and velocity for the Lagrangian. The phase space can be constructed by defining a conjugate variable, the momentum, as $p = \\tfrac{\\partial \\Mc L}{\\partial \\dot x}$, and $H$ can be expressed as $H= p \\dot x - \\Mc L$~\\cite{goldsteinbook}. This change of variables is called a Legendre transformation. The equations of motion are given by Hamilton's first-order differential equations\n\n\\begin{equation}\n\\dot x=\\frac{\\partial H}{\\partial p},~\\dot p=-\\frac{\\partial H}{\\partial x},\n\\label{eq:Hamilton_equation_point_particle}\n\\end{equation}\nwhich allow us to recover the system dynamics. In the case of a point particle, we have $p = m \\dot x$ and the Hamiltonian is $H = \\tfrac{p^2}{2m} - f x$, which leads from Eq.~\\eqref{eq:Hamilton_equation_point_particle} to $\\dot p = f$ and $m\\dot x = p$, and finally to $m\\Ddot x = f$ as expected.\n\nNow suppose that a particular time-dependent force can be generated. It is clear that the dynamics of the system are modified, the corresponding trajectory being solution of the equation $m\\Ddot x(t)=f(t)$. The particle is then steered from the point $x(0)$ to $x(t_f)$. This example can be reformulated in terms of optimal control. The idea is to reverse the procedure by fixing the initial and final states of the system a priori and by designing the corresponding control. Since this problem can have a very large number of solutions, a specific control is selected based on an additional criterion as described below.\n\\end{example}\n\nAn optimal control problem is usually defined as follows. The first step is to introduce the system to be controlled, whose state $X(t) = \\{ X_a(t)\\}_{a=1,\\cdots,n}$ is a real vector, $X(t)\\in \\setR^n$ with coordinates $X_a$. We assume that the system dynamics are described by a real first-order differential equation of the form\n\n\\begin{equation}\\label{eqcontrol}\n\\dot X(t) = F (X(t),u(t),t),\n\\end{equation}\n\nas provided e.g. by Eq.~\\eqref{eq:Hamilton_equation_point_particle}, where $u(t) \\in U \\subset \\setR^m$ is the control with $m$ real components and $F = \\{ F_a\\}_{a=1,\\cdots,n}$ is a vector function that should be smooth enough. The set $U$ corresponds to the admissible values of the control and is determined by the operator. This choice is usually dictated by the experimental limitations of the device or by the hypotheses used to derive the model system. Note that $U=\\mathbb{R}^m$ if there is no specific constraint. A standard example for a one-dimensional control parameter is $U=[u_{\\min},u_{\\max}]$ where $u_{\\min}$ and $u_{\\max}$ are respectively the minimum and maximum of allowed control values.\nA solution to Eq.~\\eqref{eqcontrol} is well-defined from a mathematical point of view if the control $u$ belongs to a particular set of functions called the \\textbf{\\textit{admissible}} controls. In many cases, continuous or piecewise continuous functions are sufficient to guarantee the existence of a solution, but not of the optimal control, as discussed in~\\cite{PRXQuantumsugny}. We also emphasize that there are problems for which the optimal control is not a piecewise continuous function but presents for instance a chattering process, characterized by an infinite number of switchings between two extreme values in a finite time interval~\\cite{schaettler-book,fuller}. Note that this phenomenon can also be observed in QOC as recently shown in~\\cite{Robin2022}.\n\n\n\nThe optimal control protocol can be found by introducing a cost functional $\\mathcal{C}$ that should be minimized. Note that a maximization can also be considered by minimizing $-\\mathcal{C}$. The functional $\\mathcal{C}$ can be expressed as the sum of a terminal cost $G$ and a running cost $F_0$ depending respectively on the final state and the trajectory followed by the system:\n\n\\begin{equation}\n\\mathcal{C} = G(X(t_f),t_f) + \\int_0^{t_f} F_0 (X(t),u(t),t)dt ,\n\\label{eq:def_cost_fun}\n\\end{equation}\nwhere $t_f$ is the control time, which can be fixed or free. In standard applications, the terminal cost $G$ can describe the distance to the target state, while the second term can penalize either the control time or the energy of the control. Such running costs correspond to $F_0=1$ and $F_0=\\frac{1}{2}u^2(t)$, respectively, up to a constant factor.\n\\begin{example}{}{ex2}\nWe consider the problem of steering a one-dimensional point particle with minimum energy consumption between the points $x=0$ and $x=1$ in a given time $t_f$, such that the initial and final velocities are zero. The equations of motion in the phase space $\\mathbb{R}^2$ are\n\\begin{equation}\n\\frac{d}{dt} \\left(\\begin{array}{c}\nx \\\\\np\n\\end{array} \\right) = \\left( \\begin{array}{c}\np/m \\\\\nf\n\\end{array} \\right),\n\\end{equation}\nwith initial $(0,0)$ and final $(1,0)$ states respectively. The cost functional associated with this optimal control problem is chosen to be\n\n\\begin{equation}\n\\mathcal{C} = (x(t_f)-1)^2+p(t_f)^2+ \\int_0^{t_f} \\frac{1}{2}\\alpha f^2(t) ~dt,\n\\end{equation}\nwhere $\\alpha$ is a constant factor such that the second term has an energy dimension. Note that $\\alpha$ can also be used to weight the relative importance of the two terms in the cost functional. The minimum of $\\mathcal{C}$ corresponds to a compromise between the distance of the final state to the target and the energy consumed along the trajectory to reach the final state.\n\\end{example}\n\n\n\nIn summary, the  task in an optimal control problem is to find the control $u^*$ that minimizes the cost functional $\\mathcal{C}$ under the constraint that $\\dot X=F(X,u,t)$. It can be mathematically described as an infinite dimensional constrained optimization problem since all amplitudes $u(t)$ in a continuous time interval are optimized. As in a finite dimensional constrained optimization problem, the main difficulty comes from the condition~\\eqref{eqcontrol} to be satisfied at any time $t$. The functional $\\mathcal{C}$ cannot be minimized directly due to this additional constraint. The idea is then to increase the dimension of the state of the system to obtain an unconstrained optimization problem~\\cite{bryson1975applied,ito2008lagrange,contreras_dynamic_2017}. An extended space is defined by doubling the number of variables of the system state. The adjoint state $\\Lambda (t)\\in\\setR^n$ is introduced and the new state can be written as $(X,\\Lambda,u)\\in\\setR^{2n+m}$. The state $\\Lambda(t)$ plays the same role as Lagrange multipliers in a finite dimensional problem, except that the static constraints and a finite number of Lagrange multipliers are respectively replaced by a dynamical constraint~\\eqref{eqcontrol} and a time-dependent function. We refer the reader to~\\ref{app_lagrange} for details on this approach.\n\nWe then consider the action associated with the optimal control problem:\n\n\\begin{definition}{Action of the optimal control problem}{Action of the optimal control problem}\nLet $X(t) \\in \\setR^n$ be the state of a physical system at time $t$, $\\Lambda(t)\\in \\setR^n$ its adjoint state, and $u(t) \\in U \\subset \\setR^m$ a control. Let $G$ be a terminal cost function, $F_0$ a running cost, and $F$ a vector function describing the system dynamics. The optimal control action is defined as\n\\begin{equation}\\label{eq_def_action}\nS = G(X(t_f),t_f) + \\int_0^{t_f} dt \\underbrace{\\left[ F_0 (X(t),u(t),t) +\\Lambda(t)\\cdot \\left(\\dot X(t)-F(X(t),u(t),t)\\right)\\right]}_{\\Mc L (X,\\dot{X},u,t,\\Lambda)}.\n\\end{equation}\nThe function in the integral is the Lagrangian $\\Mc L$ of the optimal control problem.\n\\end{definition}\nFrom this definition, it is straightforward to deduce that $\\Lambda$ is the conjugate coordinate of $X$ using\n$$\n\\frac{\\partial \\Mc L}{\\partial \\dot X} = \\Lambda.\n$$\n\n\n\\begin{example}{}{ex3}\nA direct application of Def.~\\ref{def:Action of the optimal control problem} to the system defined in Example~\\ref{example:ex2} leads us to the following Lagrangian:\n\n\\begin{equation}\n\\Mc L = \\frac{1}{2}\\alpha f^2 + \\Lambda_x( \\dot x - p/m) + \\Lambda_p(\\dot p -f),\n\\end{equation}\n\nwhere $\\Lambda_x$ and $\\Lambda_p$ are respectively the adjoint states of $x$ and $p$, $\\Lambda=(\\Lambda_x,\\Lambda_p)$.\n\\end{example}\n\n\n\n\n\\subsection{First-order variation and Pontryagin Maximum Principle}\n\\label{sec:first order variation}\n\n\\subsubsection{Lagrangian formulation.}\n\\label{sec:Lagrangian approach}\n\nThe action introduced in Def.~\\ref{def:Action of the optimal control problem} is very similar to the action of a usual classical system and it contains all the information needed to find the solutions of the optimal control problem. We use the same approach as in classical mechanics by calculating the conditions that the extremals of $S$ must fulfill when considering a small variation of the control~\\cite{Courant_Hilber_vol_1,Courant_Hilber_vol_2,gelfand2000calculus}. The analysis is more demanding than in classical mechanics because the set $U$ can be closed as in the case $U=[u_{\\min},u_{\\max}]$. One must be careful at the boundary of $U$, because it leads to extremals that are not specified by functional derivatives of $S$. This is similar to the case of a function $f$ defined on a closed interval which has extrema on the boundary of the interval that are not given by a zero of its derivative.\n\nHere, we examine a system with unconstrained controls. The case of a closed set is discussed below with the Hamiltonian formalism. Extremals are characterized by the condition $\\delta S=0$ for small variations $\\delta X$, $\\delta \\Lambda$, and $\\delta u$ which are assumed to be independent. These variations are chosen to be as general as possible, and they are not a priori restricted to satisty $\\dot X = F$.\n\nThe functional derivative of $S$ with respect to the three variables, $X$, $\\Lambda$ and $u$ is\n$$\n\\delta S=\\frac{\\partial G}{\\partial X(t_f)}\\delta X(t_f)+\\int_0^{t_f}\\left[\\frac{\\partial F_0}{\\partial X}\\delta X+\\frac{\\partial F_0}{\\partial u}\\delta u\n+\\delta \\Lambda\\cdot (\\dot{X}-F)+\\Lambda\\cdot\\left(\\delta \\dot{X}-\\frac{\\partial F}{\\partial X}\\delta X-\\frac{\\partial F}{\\partial u}\\delta u\\right)\\right].\n$$\nIntegrating by part the term $\\Lambda\\cdot\\delta \\dot{X}$, we obtain\n\\begin{align}\n&\\delta S=\\left(\\frac{\\partial G}{\\partial X(t_f)}+\\Lambda(t_f)\\right)\\delta X(t_f)+\\Lambda(0)\\delta X(0)\\nonumber \\\\\n& +\\int_0^{t_f}dt\\left(\\left[\\frac{\\partial F_0}{\\partial X}-\\dot{\\Lambda}-\\Lambda\\frac{\\partial F}{\\partial X}\\right]\\delta X+\\left[\\dot{X}-F\\right]\\delta \\Lambda+\n\\left[\\frac{\\partial F_0}{\\partial u}-\\Lambda\\frac{\\partial F}{\\partial u}\\right]\\delta u \\right).\\nonumber\n\\end{align}\nNext, we deduce that the extremals of $S$ fulfill the following conditions\n\\begin{align}\n& \\dot{\\Lambda}=\\frac{\\partial F_0}{\\partial X}-\\Lambda\\frac{\\partial F}{\\partial X},~\\Lambda(t_f)=-\\frac{\\partial G}{\\partial X(t_f)},\\nonumber \\\\\n& \\dot{X}=F,~X(0)=X_0,\\nonumber \\\\\n& \\Lambda\\frac{\\partial F}{\\partial u}-\\frac{\\partial F_0}{\\partial u}=0.\\nonumber\n\\end{align}\nThe initial condition on $X$ comes from the fact that $\\delta X(0)=0$, i.e. we consider trajectories with the same initial state.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\noindent The results are summarized in the following theorem.\n\n\\begin{theorem}{Extremals of the action}{Extremals of the action}\n\\label{th:extremal}\nExtremals of the action of the optimal control problem  at a fixed final time with the initial state $X(0)=X_0$ satisfy the following conditions:\n\n\\begin{align}\n\\label{eq_S_p}\n& \\dot X_a (t)  = F_a (t) ~\\textrm{(Euler-Lagrange equation for $\\Lambda$)},\\\\\n\\label{eq_S_x}\n& \\dot \\Lambda_a (t)  = \\frac{\\partial F_0}{\\partial X_a(t)} - \\Lambda(t)\\cdot \\frac{\\partial F}{\\partial X_a(t)}~\\textrm{(Euler-Lagrange equation for $X$)},\\\\\n& \\Lambda_a (t_f) = -\\frac{\\partial G}{\\partial X_a(t_f)}~\\textrm{(Boundary condition for $X$)},\\\\\n\\label{eq_S_u}\n& \\frac{\\partial F_0}{\\partial u_a}  = \\Lambda(t)\\cdot \\frac{\\partial F}{\\partial u_a(t)}~\\textrm{(Euler-Lagrange equation for $u$)}.\n\\end{align}\n\\end{theorem}\n\nSeveral comments on these results can be made. Equation~\\eqref{eq_S_p} is the equation of motion, and thus, any solution of $\\delta S =0$ must correspond to a physical trajectory of the system. Equation~\\eqref{eq_S_x} is derived from the Euler-Lagrange equation $\\frac{d}{dt}\\frac{\\partial \\mathcal{L}}{\\partial \\dot{X}}-\\frac{\\partial \\mathcal{L}}{\\partial X}=0$ for $X$. It returns a differential equation for the dynamics of $\\Lambda(t)$, whose value at final time is given by the gradient of the terminal cost $G$ over the final state $X(t_f)$. Equation~\\eqref{eq_S_u} leads to a strong condition for the control $u$. As can be seen in the examples, Eq.~\\eqref{eq_S_u} can often be used to express the control as a function of $X$ and $\\Lambda$, i.e. $u(t) = u(X(t),\\Lambda(t))$. The control is then completely determined by the dynamics of the state and its adjoint state. Using the $n$ initial conditions of $X$ and the $n$ final conditions of $\\Lambda$ parameterizing $X(t)$ and $\\Lambda(t)$, we obtain that the optimal control is a function of a finite number of parameters, transforming thus an infinite-dimensional optimization problem into a finite one.\n\nWe emphasize that identities of Th.~\\ref{thm:Extremals of the action} are satisfied by all the extremals of the action. It is therefore only a necessary optimality condition that selects trajectory candidates to be optimal.\nIn practice, it is straightforward to establish equations of Th.~\\ref{thm:Extremals of the action} which can be expressed as a set of non-linear coupled differential equations with two-side boundary conditions. The difficult task is to find the solutions of such equations. This can be done analytically in the simplest cases, otherwise numerically. Some examples of applications to quantum systems are given in Sec.~\\ref{sec:QOC}.\n\n\\begin{example}{}{ex4}\n\nTheorem~\\ref{thm:Extremals of the action} can be applied directly  to the optimal control Lagrangian $\\Mc L =\\frac{1}{2}\\alpha f^2 + \\Lambda_x( \\dot x - p/m) + \\Lambda_p(\\dot p -f)$ of Example~\\ref{example:ex3}. The derivatives of $\\Mc L$ with respect to the states $(x,p)$ and the adjoint states $(\\Lambda_x,\\Lambda_p)$ give the equations of motion\n\n\\begin{equation}\n\\begin{split}\n& \\dot x = p/m, \\\\\n& \\dot  p = f, \\\\\n& \\dot \\Lambda_x = 0, \\\\\n& \\dot \\Lambda_p = -\\frac{\\Lambda_x}{m},\n\\end{split}\n\\end{equation}\nwhile control can be obtained from the derivative of $\\Mc L$ with respect to the control $f$\n$$\nf=\\frac{\\Lambda_p}{\\alpha}.\n$$\nBoundary conditions can also be deduced using the terminal cost $G=(x(t_f)-1)^2+p(t_f)^2$, introduced in Example~\\ref{example:ex2}. We obtain:\n\\begin{align*}\n    \\Lambda_x(t_f)& = - 2(x(t_f)-1),\\\\\n    \\Lambda_p(t_f) &= - 2p(t_f).\n\\end{align*}\nFor an optimal trajectory reaching exactly the target, we have $x(t_f)=1$ and $p(t_f)=0$, and thus $\\Lambda_x(t_f) = \\Lambda_p(t_f)=0$.\n\n\\end{example}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsubsection{Hamiltonian formulation.}\n\nAs in classical mechanics~\\cite{goldsteinbook}, we can adopt either a Lagrangian or a Hamiltonian formalism to derive the equations of motion. The Hamiltonian structure can be derived from the optimal control action introduced in Def.~\\ref{def:Action of the optimal control problem}. Since $\\Lambda$ is the conjugate momentum of $X$, the Hamiltonian $H_P$, called the Pontryagin Hamiltonian, can be defined as $H_P = \\Lambda\\cdot \\dot X - \\Mc L = \\Lambda\\cdot  F - F_0$. When $X$ and $\\Lambda$ satisfy the extremal equations, the functional derivative of $S$ given by\n$$\n\\delta S=\\int_0^{t_f}\\left(\\frac{\\partial F_0}{\\partial u}-\\Lambda\\cdot \\frac{\\partial F}{\\partial u}\\right)\\cdot \\delta u(t) dt\n$$\ncan be then written as\n\\begin{equation}\\label{eqSPont}\n\\delta S=-\\int_0^{t_f}\\frac{\\partial H_P}{\\partial u}\\cdot \\delta u(t) dt .\n\\end{equation}\nEquation~\\eqref{eqSPont} allows us, for an open set $U$, to find the extremal condition for the control, $\\frac{\\partial H_P}{\\partial u}=0$. Here only the \\textbf{\\textit{normal}} extremals are obtained. Under certain conditions, control processes that do not depend on the running cost $F_0$ can also be solutions of the optimal control problem. Such extremals are called \\textbf{\\textit{abnormal}}. To take them into account, a negative constant $\\Lambda_0\\leq 0$ is added to the definition of the Hamiltonian $H_P$ which is given in its final form as follows~\\cite{bonnard_optimal_2012,PRXQuantumsugny,kirk2004optimal}.\n\n\\begin{definition}{Pontryagin Hamiltonian}{Pontryagin Hamiltonian}\nThe Pontryagin Hamiltonian $H_P$ is given by:\n\\begin{equation}\nH_P = \\Lambda\\cdot F+ \\Lambda_0 F_0.\n\\label{eq:def_Hamiltonian_OC}\n\\end{equation}\n\\end{definition}\nFor $\\Lambda_0<0$, we find the previous definition of $H_P$ by noting that $\\Lambda_0$ can be normalized to -1 without loss of generality. The Pontraygin Hamiltonian does not depend on $F_0$ when $\\Lambda_0=0$, which leads to the family of abnormal solutions. We stress that abnormal solutions also occur in finite-dimensional optimization problems as a special case of Lagrange multipliers. This point is discussed in~\\ref{app_lagrange} which shows that abnormal extremals are not restricted to optimal control problems. An example of abnormal control is described in Example~\\ref{example:ex8}.\n\\begin{example}{}{ex5}\nThe Pontryagin Hamiltonian for the optimal control Lagrangian $\\Mc L =\\frac{1}{2}\\alpha f^2 + \\Lambda_x( \\dot x - p/m) + \\Lambda_p(\\dot p -f)$ can be written as\n\n\\begin{equation}\nH_P = \\Lambda_x\\frac{p}{m} + \\Lambda_p f +\\frac{\\Lambda_0}{2}\\alpha f^2,\n\\end{equation}\nwhere $\\Lambda_0$ is a negative constant. In the abnormal case, the Pontryagin Hamiltonian is given by\n$$\nH_P=\\Lambda_x\\frac{p}{m} + \\Lambda_p f.\n$$\n\\end{example}\nOptimal trajectories are given by Hamilton's equations, once again highlighting the link between classical mechanics and optimal control. They can be written as:\n\n\\begin{equation}\n\\begin{split}\n& \\dot \\Lambda_a = -\\frac{\\partial H_P}{\\partial X_a},  \\\\\n& \\dot X_a = \\frac{\\partial H_P}{\\partial \\Lambda_a},  \\\\\n& \\frac{\\partial H_P}{\\partial u_a} =0,\n\\end{split}\n\\label{eq:weak_PMP}\n\\end{equation}\n\nA straightforward calculation reveals that these equations are equivalent to dynamical equations of Th.~\\ref{thm:Extremals of the action}. Equations~\\eqref{eq:weak_PMP} that can be used with unconstrained control ($U$ is an open set) correspond to \\textit{\"the weak Pontryagin principle\"}. The theory can be extended to consider the general case. This leads to the \\textit{Pontryagin Maximum Principle} (PMP) which can be stated as follows~\\cite{bonnard_optimal_2012,PRXQuantumsugny,pontryaginbook,bryson1975applied,\nkirk2004optimal,trelat2012optimal,liberzon-book}.\n\n\\begin{theorem}{Pontryagin Maximum Principle}{Pontryagin Maximum Principle}\nWe consider the dynamical system defined by\n$$\n\\dot X(t) = F(X(t),u(t)) ,\n$$\nwhere $F$ is a smooth vector function and  $u:[0,t_f]\\rightarrow U \\subset \\setR^m$ the control. The goal of the control protocol is to steer the system from $X_0$ to $X_f$ at time $t_f$ which is fixed or free. The optimal control problem $u^\\star$ is defined from the cost functional $\\mathcal{C}$ to minimize\n$$\n\\mathcal{C}= G(X(t_f)) + \\int_0^{t_f} F_0 (X(t'),u(t'))dt',\n$$\nwhere $F_0$ and $G$ are two smooth functions. The Pontryagin Hamiltonian $H_P$ is defined as\n$$\nH_P(X,\\Lambda,\\Lambda_0,u) = \\Lambda\\cdot F(X,u)+ \\Lambda_0F_0(X,u),\n$$\nwhere $\\Lambda(t) \\in \\setR^n$ is the adjoint state, and the abnormal multiplier $\\Lambda_0\\leq 0$ a constant. The pair $(X,u^\\star)$ is optimal if there exists a non zero continuous pair $(\\Lambda,\\Lambda_0)$ such that the trajectories of the extended system are given by Hamilton's equations $\\dot \\Lambda = -\\partial_{X} H_P$ and $\\dot X = \\partial_{\\Lambda}H_P$, and the maximization condition can be written almost everywhere on $[0,t_f]$  as:\n\\begin{equation}\\label{eqmaxPMP}\nH_P (X,\\Lambda,\\Lambda_0,u^\\star) = \\max_{u \\in U} H_P (X,\\Lambda,\\Lambda_0,u).\n\\end{equation}\nThe state and adjoint state satisfy respectively the initial and final conditions\n$$\nX(0)=X_0,~\\Lambda(t_f)=\\Lambda_0\\frac{\\partial G(X(t_f)}{\\partial X(t_f)}.\n$$\n\\end{theorem}\nSome mathematical details and a geometric interpretation of the PMP can be found in~\\ref{appendixPMP}. We observe that the first-order condition $\\frac{\\partial H_P}{\\partial u}=0$ is replaced by a stronger maximization condition~\\eqref{eqmaxPMP} of the Pontryagin Hamiltonian along the optimal trajectory. This modification is crucial to treat the case of a closed set $U$ such as $U=[u_{\\textrm{min}},u_{\\textrm{max}}]$ since the maximum of $H_P$ can be defined on an open or a closed set. A schematic description of this maximization is given in Fig.~\\ref{fignew}.\n\\begin{figure}[htp]\n\\begin{center}\n\\includegraphics[width=7.5cm]{fig2.pdf}\n\\end{center}\n\\caption{Schematic plot of the Pontryagin Hamiltonian $H_P$ as a function of the control $u$ in the interval $U=[u_{\\textrm{min}},u_{\\textrm{max}}]$ represented by the vertical dashed lines. The dots indicate the position of the extremal values of $H_P$. In this example, the global maximum of $H_P$ lies on the edge of $U$, while a local maximum lies inside $U$.}\n\\label{fignew}\n\\end{figure}\n\nSome additional comments and extensions can be made on Theorem~\\ref{thm:Pontryagin Maximum Principle}. The Hamiltonian $H_P$ is generally called a \\textbf{\\textit{pseudo-Hamiltonian}} because it depends on the control $u$. It can be shown that the Hamiltonian $H_P$ is constant in time and it is constantly zero if $t_f$ is free. A brief description of this condition is given in~\\ref{appendixPMP}. A solution $X$ of this optimal control problem is called an extremal trajectory and is candidate to be optimal. This means that the PMP is only a necessary condition for optimality and several extremal trajectories may exist. Additional work is required to select the optimal solution among such trajectories. As in a finite-dimensional optimization problem (see~\\ref{app_lagrange} for details), two different sets of trajectories can be extremals, namely the \\textbf{\\textit{normal}} and the \\textbf{\\textit{abnormal}}, respectively for $\\Lambda_0< 0$ and $\\Lambda_0=0$. The pair $(\\Lambda,\\Lambda_0)$ is defined up to a constant factor, but cannot simultaneously be equal to 0. This degree of freedom comes from the fact that $H_P$ and the adjoint states are abstract quantities which have no physical meaning. Multiplying $(\\Lambda,\\Lambda_0)$ by a constant factor amounts to multiplying $H_P$ by the same factor without changing the optimal control problem. This allows in the normal case to normalize the constant $\\Lambda_0$ to a specific value such as $-1/2$ or $-1$. The target state here is only a point, but a set of points or a subset of $\\mathbb{R}^n$ can be chosen as target. The final condition for the adjoint state must then be adapted. It is also possible to consider the trajectories that reach exactly the state $X_f$ at time $t_f$. In this case, the final conditions are satisfied by the state as $X(t_f)=X_f$ and not by the final adjoint state $\\Lambda(t_f)$. In a general situation, the optimal solutions of the Hamiltonian's equations of the PMP are defined from $2n$ conditions given at initial or final times on the state or the adjoint state.\n\nBased on Th.~\\ref{thm:Pontryagin Maximum Principle}, a systematic way to solve the optimal equations given by the PMP can be formulated. The different steps must be followed for both normal and abnormal extremals.\nThe first objective is to use the maximization condition to express the control in terms of the state and adjoint state as $u=v(X,\\Lambda)$. If this is possible, the control is said to be \\textbf{\\textit{regular}}, otherwise it is \\textbf{\\textit{singular}}. Note that the two situations can be mixed for a given trajectory in the sense that the control can be regular at some times and singular at others. Different quantum optimal control problems in which singular extremals are optimal have been found in the literature~\\cite{PRXQuantumsugny,lapert2010singular,Lapert_exploring_2012}. In the regular situation, the second step is to insert the expression of the control into the Hamiltonian's equation. If the function $v$ is smooth, we get a well-defined Hamiltonian system as in classical mechanics except that the desired trajectory is defined by two-side boundary conditions on $X(0)$ and $\\Lambda(t_f)$. A straightforward way to solve this problem is to use a shooting technique which consists of finding the initial value $\\Lambda(0)$ such that the final condition on the adjoint state at time $t_f$ is satisfied. This approach faces two main difficulties due to the possible complexity of the system dynamics. The first is related to the non uniqueness of the solution and the second to the potentially strong sensitivity to initial conditions in the case of chaotic dynamics. The latter is found especially in high-dimensional systems. This observation justifies the use of shooting techniques only for simple control problems of low dimension. Other numerical optimization methods have been developed to solve more complicated issues. Such optimization algorithms are described in Sec.~\\ref{sec:numerical_methods}.\n\nThe following examples illustrate different situations that can be encountered with the PMP.\n\n\\begin{example}{}{ex6}\n\nWe consider the control of a point particle in the energy minimum case from state $(0,0)$ to $(1,0)$ in a fixed time $t_f$. The cost functional to minimize is given by\n$$\n\\mathcal{C}=\\frac{1}{2}(x(t_f)-1)^2+\\frac{1}{2}p(t_f)^2+\\frac{\\alpha}{2}\\int_0^{t_f}f^2(t)dt.\n$$\nThe Pontryagin Hamiltonian can be expressed as\n$$\nH_P=\\Lambda_x\\frac{p}{m}+\\Lambda_p f-\\frac{\\alpha f^2}{2},\n$$\nwhere $\\Lambda_0$ has been set to -1 (the abnormal case plays no role in this problem). Hamilton's equations can then be written as $\\dot x = p/m$, $\\dot p= f$, $\\dot \\Lambda_x =0$, $\\dot \\Lambda_p = - \\Lambda_x/m$, and the maximization condition reads $f=\\Lambda_p/\\alpha$. The extremal trajectories are therefore regular. Note that we find the same expressions as in Example~\\ref{example:ex4}.\n\nPlugging the expression of $f$ into $H_P$, a true Hamiltonian $H$ is obtained\n$$\nH=\\Lambda_x\\frac{p}{m}+\\frac{\\Lambda_p^2}{2\\alpha},\n$$\nand the optimal trajectories are given by the Hamiltonian's equations derived from $H$. Since $\\Lambda_x$ is a constant of motion, it is straightforward to show that\n$$\nf(t)=\\frac{\\Lambda_p(t)}{\\alpha}=-\\frac{\\Lambda_x}{\\alpha m}t+\\frac{\\Lambda_p(0)}{\\alpha},\n$$\nand the system dynamics read\n\\begin{equation}\n\\begin{split}\np(t)&=-\\frac{\\Lambda_x}{2\\alpha m}t^2 +  \\frac{\\Lambda_p(0)}{\\alpha} t, \\\\\nx(t)&=-\\frac{\\Lambda_x}{6\\alpha m^2}t^3+\\frac{\\Lambda_p(0)}{2\\alpha m}t^2,\n\\end{split}\n\\end{equation}\nwhere we use the initial state $(x(0),p(0))=(0,0)$. The optimal trajectory is derived from the final condition $\\Lambda_x(t_f)=1-x(t_f)$ and $\\Lambda_p(t_f)=-p(t_f)$. A linear system of equations then allows to find $\\Lambda_x$ and $\\Lambda_p(0)$\n\\begin{equation}\n\\begin{split}\n& \\left(1-\\frac{t_f^3}{6\\alpha m^2}\\right)\\Lambda_x+\\frac{t_f^2}{2\\alpha m}\\Lambda_p(0)=1, \\\\\n& \\left(-\\frac{t_f^2}{2\\alpha m}-\\frac{t_f}{m}\\right)\\Lambda_x+\\left(\\frac{t_f}{\\alpha}+1\\right)\\Lambda_p(0)=0.\n\\end{split}\n\\end{equation}\nOptimal trajectories are represented in Fig.~\\ref{fig3} for different values of the parameters.\n\\end{example}\n\n\\begin{figure}[htbp]\n\\begin{center}\n\\includegraphics[width=7.5cm]{fig3a.pdf}\n\\includegraphics[width=7.5cm]{fig3b.pdf}\n\\end{center}\n\\caption{Plot of the optimal trajectories in the space $(x,p)$ and of the corresponding control $f(t)$ for $\\alpha=0.25$ (black) and $\\alpha=1$ (red). Numerical parameters are set to $t_f=10$ and $m=1$.}\n\\label{fig3}\n\\end{figure}\n\n\n\n\n\\begin{example}{}{ex7}\nWe consider the same control problem but in minimum time. To simplify the description of the optimal solution, we exchange the roles of the initial and target states which in this case are respectively $(1,0)$ and $(0,0)$. There is no constraint on the control energy, only on its amplitude, $f(t)\\in [-f_0,f_0]$ where $f_0$ is the maximum force allowed. For time-optimal problems, it is often preferable to reach the target exactly, the cost functional is defined from a running cost as $\\mathcal{C}=\\int_0^{t_f}dt=t_f$. Again, the abnormal extremals are not relevant and the parameter $\\Lambda_0$ is normalized to -1. The Pontryagin Hamiltonian can be written as\n$$\nH_P=\\Lambda_x\\frac{p}{m}+\\Lambda_pf-1,\n$$\nand Hamilton's equations are the same as in the preceding example. Since the final time is free, the Pontryagin Hamiltonian is zero at any time $t$. We deduce that $\\Lambda_x\\frac{p}{m}+\\Lambda_pf=1$ is a constant along the optimal trajectory. In the open interval, the maximization condition $\\frac{\\partial H_P}{\\partial f}=0$ leads to $\\Lambda_p(t)=0$ on a non-zero time interval and thus to $\\dot{\\Lambda}_p(t)=0=-\\Lambda_x/m$. We deduce that the condition $\\Lambda_x\\frac{p}{m}+\\Lambda_pf=1$ cannot be satisfied and that the optimal control takes values on the boundary of the interval, i.e. $f(t)=\\pm f_0$. The sign of $f$ has to be chosen to maximize $H_P$ at any time $t$. The only Hamiltonian term depending on $f$ being $\\Lambda_p f$, we deduce that the optimal control can be expressed as $f(t)=\\textrm{sign}[\\Lambda_p]f_0$. The latter is a square wave  with switchings at times for which $\\Lambda_p(t)=0$. Such a solution is referred to as \\emph{bang-bang} in the control literature. The Hamiltonian's equations can be directly integrated which leads for a trajectory in the interval $[t_i,t]$ to\n\\begin{equation}\n\\begin{split}\nx(t)&=f\\frac{(t-t_i)^2}{2m}+\\frac{p(t_i)}{m}(t-t_i)+x(t_i), \\\\\np(t)&=f(t-t_i)+p(t_i),\\nonumber\n\\end{split}\n\\end{equation}\nwhere $f$ is a constant equal to $+f_0$ or $-f_0$. Note that $p$ is an increasing (resp. decreasing) function of time when $f=+f_0$ (resp. $f=-f_0$).\nWe also deduce that the optimal trajectories in the space $(x,p)$ are parabolas. For the adjoint state, we arrive at\n\\begin{equation}\n\\begin{split}\n\\Lambda_x(t)&=\\Lambda_x, \\\\\n\\Lambda_p(t)&=-\\frac{\\Lambda_x}{m}(t-t_i)+\\Lambda_p(t_i),\\nonumber\n\\end{split}\n\\end{equation}\nwhere $\\Lambda_x$ is a constant. $\\Lambda_p$ is a linear function of time with at most one zero. The optimal control therefore has at most one switching and the candidates to optimality are of the form:\\\\\n- $f(t)=+f_0$ for $t\\in[0,t_f]$,\\\\\n- $f(t)=-f_0$ for $t\\in[0,t_f]$,\\\\\n- $f(t)=+f_0$ for $t\\in[0,t_s[$ and $f(t)=-f_0$  for $t\\in ]t_s,t_f]$,\\\\\n- $f(t)=-f_0$ for $t\\in[0,t_s[$ and $f(t)=+f_0$  for $t\\in ]t_s,t_f]$,\\\\\nwhere $t_s$ is the switching time to be determined. The two bang solutions, i.e. the trajectories without switching, correspond to the arcs of parabolas passing through the target state $(0,0)$ as shown in Fig.~\\ref{fig4}. We denote this set of points by $\\mathcal{P}$. If the initial state belongs to $\\mathcal{P}$ then the time-optimal solution is a constant control equal to $+f_0$ when $x(0)>0$ and $-f_0$ otherwise. In the general case, two arcs must be concatenated. This is the situation of the example for which $(x(0),p(0))=(1,0)$. A first bang with $f=-f_0$ is used to reach $\\mathcal{P}$ and then $f=+f_0$ to attain the target state, as depicted in Fig.~\\ref{fig4}.\n\\end{example}\n\\begin{figure}[htbp]\n\\begin{center}\n\\includegraphics[width=7.5cm]{fig4.pdf}\n\\end{center}\n\\caption{\nPlot of the time-optimal trajectory to go from the state $(1,0)$ to $(0,0)$ represented as black dots. The optimal control is bang-bang with first a control equal to $-f_0$ and then to $+f_0$. The red curve depicts the set $\\mathcal{P}$.\n}\\label{fig4}\n\\end{figure}\n\n\n\n\\begin{example}{}{ex8}\nWe now give an example of abnormal control. We follow the example presented in~\\cite{Arutyunov}. The control problem is almost the same as in Example~7 where the minimum time $t^\\star$ to reach the origin is derived. The unique optimal control switches at time $t_s$ from $-f_0$ to $+f_0$. The time $t_f$ is set to $t^\\star$ and we consider a running cost $\\mathcal{C}$ to minimize\n$$\n\\mathcal{C}=\\int_0^{t^\\star}f(t)\\sqrt{|t-t_s|}dt.\n$$\nSince the control time is equal to the minimum time, the constraints on the control being the same, it is clear that the only solution to the optimal control problem is the one previously derived and that this process does not depend on the chosen cost functional.\n\nUsing the PMP in this case, we arrive at the following Pontryagin Hamiltonian\n$$\nH_P=\\Lambda_x\\frac{p}{m}+\\Lambda_pf(t)+\\Lambda_0f(t)\\sqrt{|t-t_s|}.\n$$\nThe Hamiltonian equations are the same as before, but the maximization condition is modified due to the new cost functional. Note that $\\Lambda_p(t)$ is a linear function of time. The control $f$ is chosen to maximize the expression $f(t)(\\Lambda_p+\\Lambda_0\\sqrt{|t-t_s|})$. Since the optimal solution has a switching at $t=t_s$, we deduce that $\\Lambda_p(t_s)=0$. If $\\Lambda_0\\neq 0$ then there is a small interval around $t=t_s$ such that $\\Lambda_p(t)+\\Lambda_0\\sqrt{|t-t_s|})\\neq 0$ because a square root function grows faster than a linear function at the origin. There is a contradiction because $t_s$ is a switching time. We conclude that $\\Lambda_0=0$ and that the optimal solution is an abnormal extremal of the control problem.\n\\end{example}\n\n\\section{Quantum Optimal Control}\n\\label{sec:QOC}\n\n\\subsection{From the Schr\\\"odinger equation to the Pontryagin Hamiltonian}\n\\label{sec:from_QM_to_PMP}\n\nWe are now interested in applying OCT to a quantum system. Let $\\Mc H$ be the Hilbert space of the system, and assume that the quantum Hamiltonian operator of the system can be written as\n\n\\begin{equation}\n\\hat H(t) = \\hat H_0 + \\sum_{k = 1}^m u_k(t) \\hat H_k ,\n\\end{equation}\nwith real control parameters $u_k$. The quantum operators are denoted by a hat in the rest of the paper. We restrict the discussion to bilinear systems whose dynamics are linear with respect to the state and the control. This kind of model can be applied to many experimental situations with a good accuracy. However, optimal control can also be applied to other configurations, like in non-linear systems~\\cite{lapert2008,ohtsuki2008,zhang2011time,chen2016,dorier2017,guerin2020,zhu2023}. The state of the system at time $t$ is described by the vector $|\\psi(t)\\rangle\\in\\mathcal{H}$ whose trajectory is the solution of the time-dependent Schr\\\"odinger equation (in units where $\\hbar=1$)\n\n\\begin{equation}\n\\frac{d \\ket{\\psi}}{dt} = -\\ii \\hat H(t) \\ket{\\psi},\n\\label{eq:schrodinger_eq}\n\\end{equation}\nwith a fixed initial state $|\\psi_0\\rangle$. The norm of $|\\psi\\rangle$ is a constant of motion equal to one at any time $t$, $\\langle\\psi|\\psi\\rangle=1$. Equation~\\eqref{eq:schrodinger_eq} is a complex-valued first-order differential equation, and a method must be found to define real-valued optimal control quantities. An idea is to define the adjoint state as an element of the Hilbert space, $\\ket{\\chi}\\in \\Mc H$. Then, the Lagrangian can be defined as:\n\n\\begin{equation}\n\\Mc L = F_0(\\ket{\\psi},u,t) + \\Re \\left( \\braket{\\chi}{\\dot \\psi} +\\ii \\bra{\\chi} \\hat H(t) \\ket{\\psi}\\right),\n\\end{equation}\nwhere $\\Re(\\cdot)$ and $\\Im(\\cdot)$ denote respectively the real and imaginary parts of a complex number. The conjugate variables $\\Re(\\langle\\chi|)$ and $\\Im(\\langle\\chi|)$ of respectively $\\Re(|\\psi\\rangle)$ and $\\Im(|\\psi\\rangle)$ satisfy the Euler-Lagrange equations\n$$\n\\Re(\\langle\\chi|)=\\frac{\\partial \\mathcal{L}}{\\partial \\Re(|\\dot{\\psi}\\rangle)},~\n\\Im(\\langle\\chi|)=\\frac{\\partial \\mathcal{L}}{\\partial \\Im(|\\dot{\\psi}\\rangle)},\n$$\nin which we use the convention that the derivative with respect to a ket gives a bra, and vice versa. We emphasize here that $\\ket{\\chi}$ is not necessary normalized to one. Using the Euler Lagrange equations for the variables $\\Re( \\langle\\chi|)$ and $\\Im( \\langle\\chi|)$, i.e. here\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\Re(\\langle\\chi |)}=0=\\frac{\\partial \\mathcal{L}}{\\partial \\Im(\\langle\\chi |)},\n$$\nwe arrive at the following equations of motion\n\n\\begin{equation}\n\\begin{split}\n\\frac{d \\Re (\\ket{\\psi})}{dt} &= \\Re \\left( - \\ii \\hat H(t) \\ket{\\psi} \\right), \\\\\n\\frac{d \\Im (\\ket{\\psi})}{dt} &= \\Im \\left( - \\ii \\hat H(t) \\ket{\\psi} \\right),\\nonumber\n\\end{split}\n\\end{equation}\nwhich correspond to the Schr\\\"odinger equation~\\eqref{eq:schrodinger_eq}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn optimal control problem for a quantum system can then be defined using the action:\n\n\\begin{equation}\nS = G(\\ket{\\psi(t_f)}) + \\int_0^{t_f}dt \\left( F_0 (\\ket{\\psi},u,t) + \\Re \\left( \\braket{\\chi}{\\dot \\psi} +\\ii \\bra{\\chi} \\hat H(t) \\ket{\\psi}\\right)\\right).\n\\label{eq:action_QOC_schrodinger}\n\\end{equation}\nThe Pontryagin Hamiltonian can be expressed as\n$$\nH_P=\\Re\\left(\\langle\\chi|\\dot\\psi\\rangle\\right)+\\chi_0 F_0(\\ket{\\psi},u,t),\n$$\nwhich can be transformed into\n\\begin{equation}\nH_P = \\Im \\left(\\bra{\\chi} \\hat H(t) \\ket{\\psi}\\right) + \\chi_0 F_0(\\ket{\\psi},u,t),\n\\label{eq:Hamilotnian_QOC_schrodinger}\n\\end{equation}\nwhere $\\chi_0$ is the abnormal multiplier, $\\chi_0\\leq 0$. The Hamilton equations can be written as\n\\begin{equation}\\label{eqcomplexadj}\n\\begin{split}\n|\\dot{\\psi}\\rangle &=2\\frac{\\partial H_P}{\\partial \\langle \\chi|}=-\\ii \\hat{H}|\\psi\\rangle ,\\\\\n\\langle\\dot{\\chi}| &=-2\\frac{\\partial H_P}{\\partial |\\psi\\rangle}=\\ii \\langle\\chi|\\hat{H}-2 \\chi_0\\frac{\\partial F_0}{\\partial |\\psi\\rangle},\\\\\n\\end{split}\n\\end{equation}\nwhere the factor 2 comes from the definition of the derivative with respect to $|\\psi\\rangle$ as $\\partial/\\partial |\\psi\\rangle=\\frac{1}{2}(\\partial/\\partial \\Re(|\\psi\\rangle)-\\ii \\partial/\\partial \\Im(|\\psi\\rangle)$. We observe that the dynamics of $\\ket{\\chi}$ are governed by the Schr\\\"odinger equation plus an additional term depending on the running cost. It reduces to the Schr\\\"odinger equation if $F_0$ does not depend on $|\\psi\\rangle$. In this case, since the pair $(|\\chi\\rangle,\\chi_0)$ is defined up to a multiplicative constant, it is always possible to assume that $\\langle\\chi|\\chi\\rangle(t)=1$ and to interpret $|\\chi\\rangle$ as an abstract wave function. In addition, the adjoint state satisfies a boundary condition at final time\n\\begin{equation}\\label{eqcomplexfinaladj}\n\\langle\\chi(t_f)| =2\\chi_0 \\frac{\\partial G}{\\partial |\\psi(t_f)\\rangle}.\n\\end{equation}\nWhen there is no constraint on the control, i.e. $U=\\mathbb{R}^m$, the condition $\\frac{\\partial H_P}{\\partial u_k}=0$ of the PMP yields for $k=1,\\cdots,m$:\n\n\\begin{equation}\\label{eqcomplexmax}\n\\frac{\\partial H_P}{\\partial u_k}=\\Im \\left(\\bra{\\chi} \\hat H_k(t) \\ket{\\psi}\\right) + \\chi_0 \\frac{\\partial F_0}{\\partial u_k}(\\ket{\\psi},u,t)\n=0.\n\\end{equation}\n\n\\begin{example}{}{ex9}\n\\label{ex9}\nWe consider several $G$ and $F_0$ functions that can be used in quantum control. The role of $G$ is often to measure the distance of the final state $|\\psi(t_f)\\rangle$ to the target state $|\\psi_f\\rangle$. Standard terminal costs are\n$$\nG_1(|\\psi(t_f)\\rangle)=1-|\\langle\\psi_f|\\psi(t_f)\\rangle|^2,~G_2(|\\psi(t_f)\\rangle=1-\\Re(\\langle\\psi_f|\\psi(t_f)\\rangle).\n$$\nThe function $G_2$ is related to the minimization of the square modulus of $|\\psi_f\\rangle-|\\psi(t_f)\\rangle$ since\n$$\n|| |\\psi(t_f)\\rangle-|\\psi_f\\rangle ||^2=2(1-\\Re(\\langle\\psi(t_f)|\\psi_f\\rangle).\n$$\nIn the case of the cost $G_1$, the target state is reached up to a global phase, whereas this phase is fixed to 0 in the second case. Different choices of the running cost $F_0$ are possible to penalize either the control parameter $u$ or the trajectory followed by the system. An example is given by\n$$\nF_0(|\\psi(t)\\rangle,u(t))=|\\langle\\psi_1|\\psi(t)\\rangle|^2+\\frac{u^2}{2},\n$$\nwhere the goal is to minimize both the energy of the pulse and the projection onto a forbidden state $|\\psi_1\\rangle$. Using Eq.~\\eqref{eqcomplexadj}, we deduce that the adjoint state is the solution of the differential equation\n$$\n|\\dot{\\chi}\\rangle =-\\ii \\hat{H}|\\chi\\rangle-2\\chi_0 \\langle \\psi_1|\\psi(t)\\rangle |\\psi_1\\rangle,\n$$\nwith the final condition given respectively by $|\\chi(t_f)\\rangle = -2\\chi_0 \\langle\\psi_f|\\psi(t_f)\\rangle |\\psi_f\\rangle$ and $|\\chi(t_f)\\rangle = -\\chi_0|\\psi_f\\rangle$ for $G_1$ and $G_2$. The maximization condition~\\eqref{eqcomplexmax} can be written as\n$$\n\\frac{\\partial H_P}{\\partial u_k}=\\Im (\\bra{\\chi} \\hat H_k(t) \\ket{\\psi}) + \\chi_0 u_k .\n$$\n\\end{example}\n\nThis formulation of the optimal control problem is of practical interest because it remains close to usual quantum mechanical equations. Nonetheless other procedures exist to transform complex-valued functions into real-valued ones. Alternatives are preferred when the system is described in terms of a density matrix or an evolution operator. As an illustrative example, we discuss here the case of a two-level quantum system whose state is given by the Bloch vector.\nFor a pure state, the density matrix $\\hat\\rho$ of a two-level quantum system can be written as\n\\begin{equation}\n\\hat \\rho = \\ket{\\psi}\\bra{\\psi} = \\frac{1}{2}\\left( \\hat \\Id_2 + x \\sigx + y \\sigy + z \\sigz \\right)=\\frac{1}{2}\\left( \\begin{array}{cc}\n1+z & x- \\ii y \\\\\nx+ \\ii y & 1-z\n\\end{array} \\right),\n\\label{eq:spin_density_matrix}\n\\end{equation}\n\nwhere $x,y,z$ are real numbers such that $x^2 + y^2 + z^2 = 1$ and $\\sigx, \\sigy,\\sigz$ are Pauli matrices. In the case of a mixed state, a similar result can be established, but with $x^2 + y^2 + z^2 \\leq 1$. The vector $q=(x,y,z)$ is called the Bloch vector. The interesting point is that $x= \\langle \\sigx \\rangle = \\textrm{Tr} (\\sigx \\hat \\rho)$, and similarly for $y$ and $z$. It is then straightforward to derive the equations of motion for the three real variables $x$, $y$, $z$ by computing $\\frac{d}{dt}\\langle\\sigx \\rangle$, $\\tfrac{d}{dt}\\langle \\sigy \\rangle$, and $\\tfrac{d}{dt}\\langle\\sigz \\rangle$. Assume, for instance, that the dynamics of $|\\psi\\rangle$ are governed by the following Schr\\\"odinger equation\n$$\ni|\\dot{\\psi}\\rangle = \\left[\\Delta\\frac{\\sigz}{2}+u_x\\frac{\\sigx}{2}+u_y\\frac{\\sigy}{2}\\right]|\\psi\\rangle,\n$$\nwhere $\\Delta$ is a fixed parameter. It can be shown that\n\\begin{equation}\\label{eqbloch}\n\\begin{split}\n\\dot{x}&=-\\Delta y+u_y z, \\\\\n\\dot{y}&=\\Delta x-u_x z, \\\\\n\\dot{z}&=u_x y-u_y x.\n\\end{split}\n\\end{equation}\nThe PMP can then be applied to this real differential system. The Pontryagin Hamiltonian can be written as\n$$\nH_P=\\Delta(p_y x-p_x y)+u_x(p_z y-p_y z)+u_y(p_xz-p_zx),\n$$\nwhere $(p_x,p_y,p_z)$ are the real coordinates of the adjoint state. This approach is particularly well adapted to study the optimal control of open quantum systems~\\cite{bonnard2009,lapert2010singular,Lapert_exploring_2012}, whose dynamics are governed by, e.g., the Lindblad-Kossakovski equation~\\cite{breuer2002theory,gardiner2004quantum}.\n\n\\subsection{Time-optimal control of a two-level quantum system}\n\\label{sec:OCT_spin}\n\n\nIn this section, we are interested in the design of time-optimal controls for state-to-state transfer of a two-level quantum system which is one of the most relevant and simple systems in quantum technologies~\\cite{raimond2006exploring,\nlevitt2013spin,cheng2023noisy}. We also discuss the results from the point of view of the QSL~\\cite{deffner2017quantum,frey2016,hegerfeldt2013driving}. In order to keep the discussion as simple as possible, we consider situations in which any interaction with the environment can be neglected so that the system is described by a pure state. We can leave aside the density matrix formalism.\n\nWe consider the time-optimal control of a two-level quantum system with respectively one and two independent control parameters. This issue has been considered in detail in references~\\cite{PRXQuantumsugny,d2001optimal,boscain2002,PhysRevA.63.032308,boscain2006time,PhysRevA.85.012317,sugny08,garon2013time,dionis2023time,evangelakos2023}. When the approximation of a closed quantum system is not satisfied, the optimal control problem has to be extended to account for interactions with the environment~\\cite{bonnard_optimal_2012,lapert2010singular,zhang2011time,rebentrost2009optimal,roloff2009optimal,lapert2011towards,PhysRevA.82.063418,\nfloether2012robust,mukherjee2013,lapert2013,riaz2019optimal,basilewitsch2019reservoir,fischer2019time,Ansel_2022}.\n\n\\subsubsection{The case of two controls.}\n\\label{sec:Time optimal control with two inputs and without drift}\n\nAs a first example, we consider the time-optimal control of a two-level quantum system whose Hamiltonian can be written as:\n\\begin{equation}\n\\label{eq:Ham_2_inputs_no_drift}\n\\hat H(t) = \\frac{ u_z (t)}{2}\\sigz + \\frac{ u_x(t)}{2} \\sigx.\n\\end{equation}\nThe control $u(t) =(u_x(t),u_z(t))$ is assimilated to a time-dependent electromagnetic field, with components in two different directions $x$ and $z$. In experiments, the strength of the field is limited such that $\\Vert u(t) \\Vert  \\in [0,u_0]$, where $u_0$ is the maximum available amplitude. The Hilbert space is spanned by the basis $\\{\\ket{\\uparrow},\\ket{\\downarrow}\\}$ with $\\ket{\\uparrow}=(1,0)$ and $\\ket{\\downarrow}=(0,1)$.\n\nWe focus on the control process that transforms the state $\\ket{\\uparrow}$ into $\\ket{\\psi_{f}}=e^{\\ii \\theta} \\ket{\\downarrow}$, where $\\theta$ is a phase factor which is not relevant for this quantum state. The cost functional to minimize is given by\n\n\\begin{equation}\n\\mathcal{C} = \\int_0^{t_f}dt.\n\\end{equation}\n\nWe are interested in control protocols reaching exactly the set of target states, so there is no terminal cost in this case. Moreover, control time is kept free in a time-optimal process. The Pontryagin's Hamiltonian reads\n\n\\begin{equation}\n\\begin{split}\nH_P & = \\Im \\left(\\bra{\\chi} \\hat H(t) \\ket{\\psi}\\right)+\\chi_0 \\\\\n& = \\frac{ u_z (t)}{2} \\Im \\left(\\bra{\\chi} \\sigz \\ket{\\psi}\\right) + \\frac{ u_x(t)}{2}  \\Im \\left(\\bra{\\chi} \\sigx \\ket{\\psi}\\right)+\\chi_0 \\\\\n& =  u_z (t) H_z(t) +   u_x(t) H_x(t)+\\chi_0,\n\\end{split}\n\\end{equation}\n\nwith\n\n\\begin{equation}\n\\label{eq:def_Hx}\nH_{x,y,z}(t) = \\frac{1}{2}\\Im\\left(\\bra{\\chi(t)} \\hat{\\sigma}_{x,y,z} \\ket{\\psi(t)}\\right).\n\\end{equation}\n\nThe next step is to compute Hamilton's equations which can be expressed as\n\n\\begin{align}\n\\label{eq:EOM_psi}\n& \\frac{d \\ket{\\psi(t)}}{dt} = -\\ii \\hat H(t) \\ket{\\psi(t)}, \\\\\n\\label{eq:EOM_chi}\n& \\frac{d \\ket{\\chi(t)}}{dt} = -\\ii \\hat H(t)\\ket{\\chi(t)}.\n\\end{align}\nThe time derivatives of the quantities $H_{x,y,z}$  satisfy the following differential system\n\\begin{equation}\\label{eq:dH}\n\\begin{split}\n\\dot{H}_x &= - u_z(t) H_y(t), \\\\\n\\dot{H}_y &= u_z(t) H_x(t)-  u_x(t) H_z(t), \\\\\n\\dot{H}_z &= u_x(t) H_y (t).\n\\end{split}\n\\end{equation}\nfrom which it can be shown that $H_x^2+H_y^2+H_z^2$ is a constant of motion.\n\nThe maximization condition on $H_P$ leads to both regular and singular trajectories. In the interior of $U$, we have $\\partial H_P/\\partial u_x=\\partial H_P/\\partial u_z=0$, which leads to $H_x(t)=H_z(t)=0$ on a non-zero time interval. This corresponds to a singular trajectory. Plugging these conditions into Eq.~\\eqref{eq:dH}, we obtain that $H_y(t)$ is constant. The case $H_y(t)=0$ is not relevant since it gives $|\\chi(t)\\rangle=0$ using \\eqref{eq:def_Hx}. When $H_y(t)\\neq 0$, we have $u_x(t) = u_z(t) =0$, which is obviously not optimal. Consequently, the optimal control is regular.\nThe regular trajectory corresponds to the boundary of $U$ for which $u_x^2+u_z^2=u_0^2$. Introducing the angle $\\theta$ such that $u_x=u_0\\cos\\theta$ and $u_z=u_0\\sin\\theta$, we get $H_P=u_0(\\cos\\theta H_x+\\sin\\theta H_z)+\\chi_0$. The maximization condition implies that $\\frac{\\partial H_P}{\\partial \\theta}=0$, i.e. $\\tan\\theta =\\frac{H_z}{H_x}$ and finally\n$$\nu_x=u_0\\frac{H_x}{\\sqrt{H_x^2+H_z^2}},~u_z=u_0\\frac{H_z}{\\sqrt{H_x^2+H_z^2}}.\n$$\nThe maximum value of $H_P$ is given by $H_P=u_0\\sqrt{H_x^2+H_z^2}+\\chi_0$ which is a constant of motion. The abnormal multiplier can be set to -1 which leads to\n$$\nH_x^2+H_z^2=\\frac{1}{u_0^2},\n$$\nusing $H_P=0$ for this time-optimal control. The optimal solution can then be described as follows. We emphasize that the control $u_z$ only produces a modification of the phase associated with each state $\\ket{\\uparrow}$ and $\\ket{\\downarrow}$ and cannot influence the population transfer which is only due to $u_x$.  We deduce that the control $u_x(t) = \\pm u_0$, $u_z(t)=0$ (for all $t$) allows us to reach the state with the maximum speed (if $u_z \\neq 0$, we have $u_x \\neq u_0$, and thus the velocity of rotation around the direction $x$ is not maximum). When $u_x(t)=+u_0$, it is straightforward to integrate the Schr\\\"odinger equation as $\\ket{\\psi(t)} = \\cos(u_0 t /2) \\ket{\\uparrow} + \\ii \\sin(u_0 t/2)\\ket{\\downarrow}$. Simultaneously, the adjoint state can be determined to be $\\ket{\\chi(t)}=-(2\\imath/u_0)\\hat{\\sigma}_x \\ket{\\psi(t)}$ from the constraints on $H_{x,y,z}$ (notice that $\\ket{\\chi}$ is not necessarily normalized here). For a given control time $t_f$, we deduce that $|\\langle\\psi(t_f)|\\downarrow\\rangle|^2=\\sin(u_0 t_f/2)^2$.  The minimum time $t^\\star$ to exactly reach the target state is therefore given by\n\\begin{equation}\nt^\\star= \\frac{\\pi}{u_0},\n\\end{equation}\nthe optimal trajectory corresponding to a $\\pi$- pulse, i.e. a pulse with a time-integrated area equal to $\\pi$ for any maximum pulse amplitude $u_0$.\n\\subsubsection{The case of one control with a drift.}\n\\label{sec:OCT_spin_time_1_input_and_offset}\n\nAs a second example of application, we consider the same control problem as in Sec.~\\ref{sec:Time optimal control with two inputs and without drift}, but with the following Hamiltonian:\n\n\\begin{equation}\\label{eqhamdrift}\n\\hat H = \\frac{\\Delta}{2}\\sigz + \\frac{u(t)}{2} \\sigx,\n\\end{equation}\nwhere $\\Delta \\in \\setR$ is a constant (a drift, also called frequency offset), and $u(t) \\in [-u_0,u_0]$ is a one-dimensional control parameter. The analysis is only slightly modified as follows. The Pontryagin Hamiltonian can be written as\n\n\\begin{equation}\nH_P = \\Delta H_z(t) + u(t) H_x(t),\n\\end{equation}\n\nand the Hamiltonian's equations are given by:\n\n\\begin{align}\n\\label{eq:EOM_psi_v2}\n& \\frac{d \\ket{\\psi(t)}}{dt} = -\\ii \\hat H(t) \\ket{\\psi(t)}, \\nonumber\\\\\n\n& \\frac{d \\ket{\\chi(t)}}{dt} = -\\ii \\hat H(t)\\ket{\\chi(t)}.\\nonumber\n\n\\end{align}\n\nThe maximization condition of the PMP consists in maximizing the only term of $H_P$ depending on $u(t)$, i.e. $u(t)H_x$ with the constraint $-u_0\\leq u(t)\\leq u_0$. When $H_x(t)\\neq 0$, it is straightforward to show that the control satisfies $u(t)=u_0~\\sgn [H_x(t)]$, while we cannot conclude if $H_x(t)=0$. We deduce that we have a regular trajectory when $u(t)=\\pm u_0$ and singular arcs when $H_x(t) =0$ on a non-zero time interval. We start the study with the description of regular controls.\n\n\\paragraph{Regular controls.}\n\nFrom $u(t) = u_0~\\sgn[H_x(t)]$, we observe that the control is a piecewise constant function and a change of sign  occurs when $H_x (t)= 0$ in an isolated point. In control theory, the function $H_x$ is called a switching function.  We obtain a bang-bang control that suddenly jumps from a control of maximum (or minimum) amplitude to its opposite. We can go one step further by calculating the time derivatives of $H_{x,y,z}$, which yields\n\n\\begin{equation}\\label{eqdH2}\n\\begin{split}\n\\dot{H}_x &= -\\Delta H_y(t), \\\\\n\\dot{H}_y &=\\Delta H_x(t)- u(t) H_z(t), \\\\\n\\dot{H}_z &= u(t) H_y .\n\\end{split}\n\\end{equation}\nWe observe that Eq.~\\eqref{eqdH2} gives a closed-form system of differential equations, similarly to Eq.~\\eqref{eq:dH}, that can be integrated on a bang (between two switchings). By denoting $t_i$ the switching time $i$, we have\n\n\\begin{equation}\n\\left( \\begin{array}{c}\nH_x(t) \\\\\nH_y(t) \\\\\nH_z(t)\n\\end{array} \\right) = \\exp \\left[ (t-t_i)\\left( \\begin{array}{ccc}\n0 & -\\Delta & 0 \\\\\n\\Delta & 0 & - u \\\\\n0 & u & 0\n\\end{array} \\right) \\right] . \\left( \\begin{array}{c}\n0 \\\\\nH_y(t_i) \\\\\nH_z(t_i)\n\\end{array} \\right),\n\\end{equation}\nwith $u=u_0~\\sgn[H_x(t)]$ a constant, $t \\in [t_i,t_{i+1}]$ and we use the fact that $H_x(t_i)=0$. The explicit calculation of the matrix exponential leads us to\n\n\\begin{equation}\\label{eqHx}\nH_x(t) = -\\frac{\\Delta}{\\Omega^2} \\left[\\Omega H_y(t_i) \\sin(\\Omega (t-t_i) )- u H_z(t_i) (1-\\cos(\\Omega (t-t_i)))  \\right],\n\\end{equation}\nwith $\\Omega = \\sqrt{u_0^2 + \\Delta^2}$. The duration of the bang, given by $t_{i+1}-t_i$ can be determined from Eq.~\\eqref{eqHx}. We arrive at\n\n\\begin{equation}\nt_{i+1}-t_i =\\left\\lbrace \\begin{array}{ll}\n\\frac{\\pi n}{\\Omega} & \\text{ if } \\frac{u H_z(t_i)}{\\Omega H_y(t_i)}=0, \\\\\n\\frac{2}{\\Omega}\\left[ \\pi n + \\arctan\\left( \\frac{\\Omega H_y(t_i)}{u H_z(t_i)}\\right) \\right] & \\text{ if } \\frac{u H_z(t_i)}{\\Omega H_y(t_i)} \\neq 0,\n\\end{array} \\right.\n\\label{eq:switching_times_bang_bang}\n\\end{equation}\n\nwhere $n$ is an integer chosen such that $t_{i+1} -t_i$ is the smallest positive non zero solution. We also observe that the function $H_{x}$ is $2\\pi/\\Omega$ periodic. We deduce that the maximum duration between two switchings is $2\\pi/\\Omega$.\nSimilar calculations as for $H_x(t)$ show that $H_z(t_{i+1}) = H_z(t_i)$ and $H_y(t_{i+1})=H_y(t_i)$. This means that at switching $i+1$, the system is in the same configuration as at switching $i$. All bangs have the same duration. For an arbitrary regular trajectory, we cannot start or end at a switching. Therefore, such a trajectory has the following structure: a first bang with control amplitude $u = \\pm u_0$ and duration $t_1$, followed by a series of bangs of amplitudes $\\pm u_0$ and duration $t_b$ and a final bang of duration $t_2$. Note that $t_1$ and $t_2$ are both less than or equal to $t_b$.\n\n\n\n\\paragraph{Singular controls.}\n\nThe control is singular when $H_x(t) = 0$ on a non-zero time interval, which gives $\\tfrac{d}{dt}H_x = \\tfrac{d^2}{dt^2}H_x = 0$. From Eq.~\\eqref{eqdH2}, we deduce that $H_y(t) = 0$ and $-u(t) H_z(t) = 0$. Therefore, the control is always equal to $0$, or the vector $(H_x,H_y,H_z)$ is zero. The latter condition is only satisfied if the adjoint state is always zero, which is a trivial solution.\n\n\\paragraph{Calculation of the optimal control.}\nThe final step is to determine the time-optimal trajectory. First, we argue that singular control (or concatenation of regular and singular controls) cannot be optimal. For a singular control equal to zero, the Hamiltonian operator can be simplified to $\\hat H = \\tfrac{\\omega}{2}\\sigz$, which cannot produce a population transfer between the initial and target states, and thus is not optimal. The optimal trajectory is therefore regular. To find the switching times of the optimal procedure, we analytically  compute the evolution operator for an increasing number of switchings, stopping when the corresponding control can steer the system to the target. If there are several solutions with the same number of switchings, we keep the one(s) with the shortest duration.\n\nWe integrate the Schr\\\"odinger equation on a single bang with a control amplitude $u=\\pm u_0$. The evolution operator associated with this bang is given by\n\n\\begin{equation}\n\\hat U_{\\text{bang}} = e^{-\\ii (\\Delta \\sigz + u \\sigx )t/2}= \\cos\\left(\\frac{\\Omega t}{2}\\right)\\hat \\Id - \\ii\\left( \\frac{\\Delta}{\\Omega}\\sigz+  \\frac{u}{\\Omega}\\sigx\\right)\\sin\\left(\\frac{\\Omega t}{2}\\right).\n\\label{eq:U_bang}\n\\end{equation}\nSince $\\Omega = \\sqrt{\\Delta^2 + u_0^2}$, we observe that the maximum for a population transfer with a single bang is $u^2/\\Omega^2<1$ unless $\\Delta =0$, hence, there is at least one switching. With two bangs we have:\n\\begin{equation}\n\\hat U_{\\text{2-bangs}} = e^{-\\ii (\\Delta \\sigz - u \\sigx )t_2/2} e^{-\\ii (\\Delta \\sigz + u \\sigx )t_1/2},\n\\end{equation}\n\nwhere $t_1$ and $t_2$ are the respective duration of the two bangs. The final distance to the target state can be expressed as\n\n\\begin{equation}\n\\left\\vert\\bra{\\downarrow}\\hat U_{\\text{2-bangs}} \\ket{\\uparrow}\\right\\vert^2 = \\frac{u^2}{\\Omega^4} \\left[ 4 \\Delta^2 \\sin^2\\left(\\frac{\\Omega t_1}{2}\\right) \\sin^2\\left(\\frac{\\Omega t_2}{2}\\right) + \\Omega^2 \\sin^2\\left(\\frac{\\Omega (t_1-t_2)}{2}\\right)\\right].\n\\end{equation}\n\nWe have to maximize this function such that $t_1+t_2$ is minimum. The solution to this problem is given by\n\n\\begin{align}\nt_1 &= \\frac{1}{\\Omega}(\\pi - \\arccos(\\Delta^2/u_0^2)),\\\\\nt_2 &= \\frac{1}{\\Omega}(\\pi + \\arccos(\\Delta^2/u_0^2)),\n\\end{align}\n\nwhen $|\\Delta|<u_0$, with a total control time given by\n\n\\begin{equation}\nt_f=t_1+t_2= \\frac{2\\pi}{\\Omega}.\n\\end{equation}\n\nNote that $t_1$ and $t_2$ can be permuted and the initial sign of the control plays no role. The optimal trajectories are represented in Fig.~\\ref{fig:blochsphere} using the Bloch vector representation. There are two equivalent solutions with a switching on the equator of the sphere. To show that this solution is the global optimal solution, it must also be compared with controls with a larger number of switchings. This is the case as described in~\\cite{PRXQuantumsugny,boscain2006time}. The number of bangs of the optimal solution is strictly larger than 2 when $|\\Delta|>u_0$~\\cite{boscain2006time,assemat2010,evangelakos2023}.\n\n\\begin{figure}[htbp]\n\\centering\n\\includegraphics[width=0.5\\textwidth]{fig5.pdf}\n\\caption{Plot of the time-optimal trajectories on the Bloch sphere in the case of a two-level quantum system with one control and a drift. The two optimal trajectories are represented in blue (solid and dashed lines) while the green curve depicts a geodesic going from the north to the south pole. The black solid line represents the equator of the sphere.}\n\\label{fig:blochsphere}\n\\end{figure}\n\n\n\\subsubsection{Quantum speed limit.}\n\\label{sec:QSL}\n\nIn this section, we present the results obtained from the point of view of QSL~\\cite{deffner2017quantum,hegerfeldt2013driving,PhysRevLett.130.010402,PhysRevA.92.062106}. First, we review the main idea on which QSL is based, and then we discuss its relation to the optimal control formalism.\n\nQSL can be described as a way to generate, as quickly as possible, a state orthogonal to the initial one. Since this concept does not take into account dynamical constraints, it provides only a lower bound on the minimum time to reach the orthogonal state. This statement can be formalized rigorously as follows. The speed in the Hilbert space is naturally given by the Schr\\\"odinger equation: $\\ket{\\dot \\psi} = -\\ii \\hat H \\ket{\\psi}$. Similarly to classical dynamical systems, the velocity vector can be decomposed into parallel and transverse contributions\n\n\\begin{align}\n\\ket{\\dot \\psi_\\parallel} &=\\ket\\psi\\bra\\psi \\dot{\\psi}\\rangle= -\\ii \\bra{\\psi}\\hat H \\ket{\\psi}~ \\ket{\\psi}, \\\\\n\\ket{\\dot \\psi_\\perp} &=\\ket{\\dot \\psi} - \\ket{\\dot \\psi_\\parallel}.\n\\end{align}\n\nBy construction, we have $\\braket{\\dot \\psi_\\perp}{\\psi}=0$. The transfer is maximized when the the norm of $\\ket{\\dot \\psi_\\parallel}$ is minimized and the one of $\\ket{\\dot \\psi_\\perp}$ maximized. The latter can be expressed as\n\\begin{equation}\n|\\dot \\psi_\\perp|^2 =  |\\dot \\psi|^2 - |\\dot \\psi_\\parallel|^2 = \\langle \\hat H ^2 \\rangle_{\\psi} - \\langle \\hat H \\rangle^2_\\psi,\n\\end{equation}\n\nwhich is the variance of $\\hat H$, denoted below $\\Delta H^2$. In the case of Hamiltonian~\\eqref{eq:Ham_2_inputs_no_drift} for two independent controls, the variance reads\n\n\\begin{equation}\n\\Delta H^2 = \\frac{u_0^2}{4} - \\frac{1}{4}\\left( u_z z + u_x x\\right)^2,\n\\label{eq:Delta E2}\n\\end{equation}\nin which the Bloch coordinates $(x,y,z)$ are used. The maximum of this variance, $u_0^2/4$, is obtained when $u_z z + u_x x =0$. The maximum value of $\\Delta H$ gives the QSL of Mandelstam-Tamm~\\cite{deffner2017quantum}. Note that other estimates have been proposed in the literature~\\cite{MARGOLUS1998188,PhysRevLett.130.010402} with similar properties as the Mandelstam-Tamm bound. The lower bound on the minimum time given by the QSL is\n$$\nt_{\\textrm{QSL}}=\\frac{\\pi/2}{\\sqrt{\\Delta H^2}}=\\frac{\\pi}{u_0}.\n$$\nWe observe that $t^\\star=t_{\\textrm{QSL}}$. This point can also be verified with the optimal solution derived from the PMP.\n\nFor the optimal control $u_x=u_0, u_z=0$, we find from the state evolution that the quantity $x=0$ at all times, therefore the condition $u_z z + u_x x =0$ is satisfied.\nThe corresponding trajectory on the Bloch sphere is the half of the great circle from the north to the south pole, in the $zOy$ plane. It is also a geodesic connecting the initial and the target states for which the speed of travel is maximum.\n\nHowever, the relation $t^\\star=t_{\\textrm{QSL}}$ is not generic, as shown in the case with one control of Sec.~\\ref{sec:OCT_spin_time_1_input_and_offset}. For the Hamiltonian~\\eqref{eqhamdrift}, the variance is given by\n$$\n\\Delta H^2=\\frac{\\Omega^2}{4}-\\frac{1}{4}(\\Delta z+u x)^2.\n$$\nThe maximum of $\\Delta H^2$ is $\\Omega^2/4$ which leads to $t_{\\textrm{QSL}}=\\pi/\\Omega$, whereas the minimum time is $t^\\star=2\\pi/\\Omega$. In this example, the lower bound cannot be saturated because the geodesic does not correspond to a dynamical trajectory of the system. In other words, the condition $\\Delta z+u x=0$ to reach the maximum speed cannot be satisfied by the control $u$. The geodesic corresponding to the trajectory used in the QSL approach is represented for this example in Fig.~\\ref{fig:blochsphere}.\n\\section{Numerical methods}\n\\label{sec:numerical_methods}\n\nIn this section, we present the state-of-the-art numerical methods that can be used to solve quantum optimal control problems. The literature in this field is important~\\cite{bonnans2006numerical,qin2008differential,ab2015comprehensive,\nfeng2021monarch,maros2012computational,davis1987genetic,goldberg2007genetic,pham2012intelligent} and we do not claim to be exhaustive. Our aim is to introduce the main ideas and technical details that can be found in the references mentioned. Numerical examples are described below with pseudo-codes, while Python codes are provided in the supplementary material.\n\nWe begin with a general presentation of optimization algorithms found in scientific computing software. These algorithms can be used as black boxes and are generally effective when the number of parameters to be optimized is relatively small. A list of software to solve quantum optimal control problems is given in a second step. Special emphasis is then placed on the description of two algorithms, namely the shooting techniques and the gradient-based algorithms which are two direct applications of the PMP.\n\n\n\\subsection{A general overview of standard optimization algorithms}\n\\label{sec:std_opt_algo}\n\nOptimization algorithms are generally implemented in most scientific computing softwares~\\cite{2020SciPy-NMeth,eaton2013gnu,MATLAB,Mathematica}. An optimization algorithm can be described as a systematic iterative method to find the minimum or  maximum of a function. The function must have a finite number of entries, and thus, continuous controls must be discretized in some way using e.g. piecewise constant functions, Taylor or Fourier expansions, etc. The optimization algorithms are usually presented in the following form\n\n\\begin{equation}\n\\texttt{OptAlgo}(\\texttt{Cost function},\\texttt{ Constraints},\\texttt{ Initial guess}, \\texttt{ Options})\n\\end{equation}\n\nwhere $\\texttt{Cost function}$ is the cost function or figure of merit $\\mathcal{C}$ to minimize or maximize, defined in Eq.~\\eqref{eq:def_cost_fun}, $\\texttt{ Constraints}$ are constraints that limit the domain of definition of the control, the set $U$, $\\texttt{Initial guess}$ is an initial guess of the control, used by the algorithm to find a better solution. $\\texttt{Options}$ are additional parameters that depend on the algorithm. They correspond either to a choice of a numerical method or to parameters that influence the convergence of the algorithm. We can roughly divide the algorithms into two different families, namely the gradient-free and the gradient-based algorithms~\\cite{kochroadmap}. As their name suggests, they are based either on the calculation of the gradient of the figure of merit or on some other procedure that allows the control protocol to be improved at each step of the iterative process. Both approaches have clear advantages depending on the system dimension and on the precision of the control process.\n\n\\begin{itemize}\n\\item[$-$]~\\textbf{Gradient-free algorithm.}\nThis set of methods includes different algorithms. As the number of variants is very large, only a few examples are  mentioned. In the \\textit{Simplex method}~\\cite{maros2012computational}, the space of control parameters $\\setR^n$ is explored with a $n$-simplex whose size decreases recursively around a minimum of the cost function. This approach has been applied in QOC with the algorithm CRAB~\\cite{calarco2011}, which can be used to control many body systems~\\cite{Calarco2022}. Another optimization procedure is based on \\textit{Evolutionary algorithms}~\\cite{davis1987genetic,goldberg2007genetic,pham2012intelligent,\nvenkata_rao_jaya:_2016} (differential evolution, genetic algorithms, simulated annealing, JAYA...). The basic idea of this approach is to iteratively explore the parameter space with $N$ particles, also called \"walkers\". The method used to update the position of the particles is specific to each method, but usually the particles are moved randomly with a probability law, designed  to favour a direction towards the global minimum of the cost function. An example of a particle path towards the minimum of a function using the JAYA algorithm~\\cite{venkata_rao_jaya:_2016} is shown in Fig.~\\ref{fig:JAYA}. Evolutionary algorithms are well-known in quantum control and are used experimentally to design a control protocol in a model-free approach~\\cite{rabitz1992}.\n\n\\item[$-$]~\\textbf{Gradient-based algorithm.} The gradient of the function with respect to the control parameters is calculated, and it allows to know in which direction the cost takes a lower value~\\cite{bonnans2006numerical,kelley1999iterative}. Based on the gradient, a new control is designed, and the minimum of the cost function is found iteratively. For the optimization of piecewise constant controls, two different procedures have been developed in QOC~\\cite{DYNAMO}, namely the Krotov-type methods~\\cite{koch2012,krotov1983iterative,palao2002,palao2003,goerz2020} and the GRAPE-type procedures~\\cite{khaneja_optimal_2005}. While GRAPE updates the control in all time slices concurrently, the Krotov procedure updates the control sequentially from one time slice to the next. A full comparison of the two methods is given in~\\cite{Jager2014,DYNAMO}. Such approaches can be improved by using the second order derivative of the cost function with respect to the control parameters~\\cite{bryson1975applied,grape2,tannor2011,sherson2020}. These improvements are not discussed in the main text but are included in some Python codes that can be found in the supplementary material. The GRAPE algorithm and its connection to the PMP is presented in Sec.~\\ref{sec:GRAPE_algo}.\n\\end{itemize}\n\n\\begin{figure}\n\\begin{center}\n\\includegraphics[width=0.85\\textwidth]{fig6.pdf}\n\\end{center}\n\\caption{Example of particle search of the global minimum of the function $f:(x,y)\\mapsto -(x^2 + x y - y^3/2 )e^{-(x^4 + y^4)/2}$ in the domain $x \\in [-2,2]$, $y\\in[-2,2]$ using the JAYA algorithm~\\cite{venkata_rao_jaya:_2016}. The position of the particles is given by black points while the global minimum is located by a gray star. The boxed numbers at the top right of each panel correspond to the number of iterations of the algorithm. Initially, the positions of all particles are generated randomly. They gradually move to regions of the control landscape where the function is likely to be a minimum. In the end, all the particles are located around the global minimum of the function.\n}\n\\label{fig:JAYA}\n\\end{figure}\n\n\n\nWe conclude this section by some general comments about the application of such algorithms to quantum control. First, we point out that machine learning techniques such as Reinforcement Learning can also be applied to find optimal control protocol~\\cite{carleoRMP2019,giannelli2022}. The coefficients of a neural network are optimized by trial and error in a learning step using in general gradient procedures. This network is then used to map each state of the system to the best action (or control) to take to reach the target state. From a more general point of view, Reinforcement Learning can be described as a kind of dynamic programming approach that can be mathematically justified in the continuous limit by the Hamilton-Jacobi-Bellman formulation of optimal control~\\cite{bertsekasbook}.\n\nFor quantum control problems, simplex methods are generally not the most effective, but they are able to design control procedures in very complex dynamics or when strong control constraints are considered. The accuracy achieved with evolutionary or gradient-based algorithms is generally better. Evolutionary algorithms are interesting when the number of parameters is small ($\\lesssim$ 20) or when the cost function cannot be differentiated. Among the different methods, we particularly recommend the simulated annealing method, which is a good compromise between the computation time and its ability to find the global minimum. Another interesting choice is the JAYA algorithm~\\cite{venkata_rao_jaya:_2016} which is very simple. Its main advantage lies in the absence of external parameters that influence the convergence (in most algorithms, one or more parameters have to be adjusted manually to obtain a good and fast convergence). Gradient-based algorithms are interesting when the number of parameters to be optimized is very large ($\\gtrsim 100$). They can be complemented by second-order formulations to improve convergence to a local extremum~\\cite{bryson1975applied,grape2}. Gradient-based algorithms are particularly efficient when the gradient can be computed from an analytic expression. Otherwise, it can be estimated using a finite difference approach, at the expense of computational time. Many factors can influence the choice of algorithm. The way the library is coded and the different options available are also important factors. It is generally advised to test a few different algorithms on a given QOC problem, as a preliminary step. This can also provide a good insight into the control problem since different algorithms may return different solutions (which can be only local extremums of the cost function). Many optimization algorithms are provided in scientific computing softwares. MATLAB~\\cite{MATLAB} proposes a global optimization toolbox with e.g. the function \\texttt{simulannealbnd} (simulated annealing with constraints) or the function \\texttt{fmincon} (gradient algorithm which takes into account the Hessian). In Python/ScyPy~\\cite{2020SciPy-NMeth}, different approaches can be found in the routine \\texttt{scipy.optimize} (mostly simplex and gradient-based methods). In Mathematica~\\cite{Mathematica}, the function \\texttt{NMinimize} can be used. It has built-in methods that cover all the families of algorithms listed here above. An advantage of this function is that it is not necessary to provide an initial guess control, the algorithm works simultaneously on several optimization processes in which the initial controls are chosen automatically.\n\n\\paragraph{Packages and Software for Quantum Optimal Control.} We provide below a list of freely available packages and softwares designed for quantum optimal control applications. The list is given in alphabetical order.\n\\begin{itemize}\n\\item[$-$]~Bocop~\\cite{Bocop}: Bocop is an open-source toolbox for solving optimal control problems. It is a general optimization software that can be used for any optimization problem. Its key features are global optimization for both deterministic and stochastic systems, computation of switching and stopping times, parallel execution with OpenMP, and Matlab/Python interfaces.\n\n\n\\item[$-$]~DYNAMO: Dynamic Framework for Quantum Optimal Control~\\cite{DYNAMO}. DYNAMO is a Matlab package with integrated GRAPE and Krotov methods.\n\n\\item[$-$]~HamPath~\\cite{caillau2012differential,HamPath}: HamPath is a general optimization package written in Fortran, and compatible with Matlab, Python, and GnuPlot. It is specifically designed to solve the equations of the PMP (shooting algorithm). Advanced higher-order optimal control methods are implemented, such as the computation of conjugated points~\\cite{bonnard_optimal_2012}.\n\n\\item[$-$]~Krotov~\\cite{goerz2020}: An open-source Python package based on the Krotov algorithm to solve problems of QOC extending from state-to-state transfer to quantum gate implementation.\n\n\\item[$-$]~OCTBEC: A Matlab toolbox for optimal quantum control of Bose–Einstein condensates~\\cite{HOHENESTER2014194}.\n\n\\item[$-$]~QEngine: A C++ library for quantum optimal control of ultracold atoms~\\cite{sorensen2019qengine}. QEngine is a C++ library for performing optimal control of Bose–Einstein condensates, Bose–Hubbard type models, and two interacting particles.\n\n\\item[$-$]~QOPT: An Experiment-Oriented Software Package for Qubit Simulation and Quantum Optimal Control~\\cite{teske2022qopt}. QOPT is a Python library with a scope similar to the optimization package of QuTiP, but it supports computations with stochastic noises.\n\n\\item[$-$]~Quandary: An open-source C++ package for high-performance optimal control of open quantum systems~\\cite{gunther2021quandary}. Quandary is fully compiled and supports parallel computation of system dynamics. It is therefore possible to optimize quantum controls for a system of large dimension. The optimization algorithm is a homemade gradient algorithm that has some similarities with GRAPE.\n\n\\item[$-$]~QuOCS: The quantum optimal control suite~\\cite{rossignolo2023quocs}. QuOCS is a Python library that combines several gradient-based optimization algorithms, such as GRAPE. The software performs both open- and closed-loop optimizations, and it can be connected to real-time quantum experiments.\n\n\\item[$-$]~Qocttols: A Python code that performs optimization calculations on quantum systems using  gradient-based algorithms~\\cite{castro}. It is an open and free software that works on closed and open systems. Floquet formalism with periodic perturbations can also be used.\n\n\\item[$-$]~QuTiP: Quantum Toolbox in Python~\\cite{johansson2012qutip}. QuTiP is an open-source Python package for simulating the dynamics of open quantum systems. It contains an optimization toolbox with integrated GRAPE and CRAB algorithms.\n\n\n\n\\item[$-$]~Spinach: Spinach is an open-source Matlab package~\\cite{spinach} for computations in Nuclear Magnetic Resonance (NMR), Electron Paramagnetic Resonance (EPR), Magnetic Resonance Imaging (MRI), Dynamic Nuclear Polarization (DNP) and Magic Angle Spinning (MAS). Optimal Control, and other topics of Magnetic Resonance spectroscopy are available.\n\n\n\\item[$-$]~SpinDrops~\\cite{Spindrops}: SpinDrops is an interactive quantum spin simulator. The software includes several state-of-the-art pulse sequences, but controls can also be created with an editor or with an optimizer based on GRAPE. This is a standalone software with android, iOs, Linux, macOS, and windows supports. An online version is also available.\n\n\\item[$-$]~Travolta: An open-source software for parallelized quantum optimal control computations in photo-excited systems using  gradient-based algorithms~\\cite{travolta}.\n\n\\item[$-$]~WavePacket: A Matlab package for numerical quantum dynamics~\\cite{schmidt2018wavepacket}. WavePacket is designed to solve a wide range of quantum dynamical and optimization problems. The optimization is based on the gradient-based algorithm described in~\\cite{Zhu1998,Zhu1998b,Ohtsuki2004}.\n\n\\end{itemize}\n\nThe majority of the packages incorporate GRAPE and sometimes, one or two other algorithms are also included. In this list, QuTiP and Spinach are the libraries with the largest number of functionalities. Their scope goes beyond control  optimization. \n\n\n\\subsection{Shooting techniques}\n\\label{sec:shooting_algo}\n\nBehind the name \"shooting techniques\" hides a large class of more or less sophisticated algorithms~\\cite{trelat2012optimal,bryson1996optimal,Bocop,bock1984multiple,\ntrelat2005controle,giftthaler2018family}. They are also called indirect methods in the literature~\\cite{trelat2012optimal}. The general idea is inspired by a shooting problem, where the goal is to place a bullet or an arrow on a target. The only parameters that can be modified are the orientation of the gun and the initial velocity of the bullet, i.e. the dynamics of the system can only be influenced by a precise choice of initial conditions. Random shots can be taken to get an estimate of these initial conditions, but more sophisticated strategies can be used for this purpose.\n\nUsing Hamiltonian's equations of the PMP, the optimal control problem can be cast into this form. The parameters to be optimized correspond to the initial value of the adjoint state (see Sec.~\\ref{sec:Lagrangian approach}). The estimation of this initial adjoint state may be improved by optimization algorithms such as those described in Sec.~\\ref{sec:std_opt_algo}.\nThe pseudo-code of the algorithm can be written as follows:\n\n\\begin{lstlisting}\nFunction $(X(t),u(t))$=Xtraj($\\Lambda_0$,$t_f$)\n(* Integrate from 0 to $t_f$ the equations of motion $\\dot \\Lambda = -\\partial _{X} H_P$ and\n $\\dot X = \\partial_{\\Lambda} H_P$, where the control is expressed using the maximization condition of\n the PMP in the form $u(t)=u(X(t),\\Lambda(t))$.\n The initial conditions for $\\Lambda$ are given by $\\Lambda_0$, and\n the ones for $X$ are fixed by the definition of the control problem. *)\n\nFunction $\\mathcal{C}$=Cost($X(t)$,$u(t)$,$t_f$)\n(* Return the value of the cost functional*)\n$\\Lambda_{\\textrm{opt}}$ = Minimize(Cost(Xtraj($\\Lambda_0$,$t_f$),$t_f$),$\\Lambda_0 \\in \\Mc D$)\n$u_{\\textrm{opt}}(t)$ = Xtraj($\\Lambda_{\\textrm{opt}}$,$t_f$)$[2]$\n\\end{lstlisting}\n\nHere above, \\texttt{Minimize} is an arbitrary optimization algorithm (see e.g. Sec.~\\ref{sec:std_opt_algo}), and $\\Mc D$ is the domain of definition of $\\Lambda_0$. The shooting technique is efficient when the solutions of the PMP are regular and without switching. The singular case can be difficult to solve numerically and switching times may not be detected by the algorithm~\\cite{bonnard2003singular,martinon2007using}. Efficient numerical codes have been developed to tackle these kinds of computation, such as Bocop~\\cite{Bocop} or Hampath~\\cite{caillau2012differential,HamPath}. \n\\begin{example}{}{ex10}\nWe illustrate the application of the shooting algorithm on the state-to-state transfer of a two-level quantum system with two controls and no drift. The Hamiltonian of the system is given by\n$$\n\\hat H = \\frac{u_x}{2} \\sigx+\\frac{u_y}{2} \\sigy.\n$$\nWe describe the state of the system in terms of Bloch coordinates $(x,y,z)$. Using Eq.~\\eqref{eqbloch}, we get:\n\\begin{equation}\n\\begin{split}\n\\dot{x}&=u_y z, \\\\\n\\dot{y}&=-u_x z, \\\\\n\\dot{z}&=-u_yx+u_xy.\\nonumber\n\\end{split}\n\\end{equation}\nThe goal of the control problem is to bring the system from $(1,0,0)$ to $(0,1,0)$ in minimum time with the constraint $u_x^2+u_y^2\\leq 1$. The cost functional is thus $\\mathcal{C}=t_f$. It can be shown that the optimal protocol consists of saturating the bound at any time, that is $u_x^2+u_y^2=1$~\\cite{PRXQuantumsugny}. The idea is that the fastest control protocol requires generally maximum control intensity. The Pontryagin Hamiltonian can be expressed as\n$$\nH_P=u_x(p_zy-p_yz)+u_y(p_xz-p_zx)+p_0,\n$$\nwhere $(p_x,p_y,p_z)$ is the adjoint state which also satisfies the Bloch equation, and $p_0$ the abnormal multiplier. Since the final time is free, we have $H_P=0$. The maximization condition of $H_P$ leads in the regular case to the following optimal controls\n\\begin{equation}\n\\begin{split}\nu_x &= (p_zy-p_yz)/H,\\\\\nu_y &= (p_xz-p_zx)/H, \\nonumber\\\\\n\\end{split}\n\\end{equation}\nwhere $H=\\sqrt{(p_zy-p_yz)^2+(p_xz-p_zx)^2}$.\n\nAs described in~\\cite{PRXQuantumsugny}, the control problem can be integrated exactly. The minimum time is $t_f=t^\\star=\\frac{\\pi\\sqrt{3}}{2}$ and the corresponding initial adjoint state at time $t=0$ has the coordinates $(p_x(0),\\frac{1}{\\sqrt{3}},\\pm 1)$. Note that the first component is not fixed but the same control process is obtained for any value of $p_x(0)$. The two possible values of $p_z(0)=\\pm 1$ correspond to the two symmetric trajectories with respect to the equator on the Bloch sphere. A global description of the control landscape can be made by representing the initial adjoint state on a sphere as $p_x=R_p\\sin\\Theta_p\\cos\\Theta_p$, $p_y=R_p\\sin\\Theta_p\\sin\\Phi_p$ and $p_z=R_p\\cos\\Theta_p$. We choose to normalize $R_p$ to one, i.e. the adjoint state $(p_x,p_y,p_z)$ belongs to the sphere of unit radius, while the abnormal multiplier is not known. For each couple of initial values $(\\Theta_p(0),\\Phi_p(0))$, we numerically compute the corresponding optimal trajectory and the euclidean distance $d$ to the target state at time $t^\\star$. By definition, the optimal solutions verify $d=0$. The corresponding control landscape is plotted in Fig.~\\ref{figcontour}.\n\nThe same problem can also be solved numerically by using an iterative shooting technique. In this case, the goal is not to calculate all the trajectories to obtain the control landscape, but to iteratively find the right solution that reaches the target state. We choose to normalize the abnormal multiplier to -1 which leads to $H_P=1$, but the modulus of $(p_x,p_y,p_z)$ is not fixed. A very good convergence of the algorithm is observed. We find the optimal solution and the corresponding initial values of the adjoint state with high numerical precision. The corresponding Python code \\emph{shooting.py} is provided in the supplementary material.\n\\end{example}\n\\begin{figure}[htbp]\n    \\begin{center}\n    \\includegraphics[width=0.75\\textwidth]{fig7.pdf}\n    \\end{center}\n  \\caption{Euclidean distance $d$ to the target state $(0,1,0)$ at time $t_f=t^\\star$ for the trajectories with different initial adjoint states parameterized by the angles $\\Theta_p$ and $\\Phi_p$. The minimum time solutions corresponding to the minimum of $d$ are represented by solid lines.}\n  \\label{figcontour}\n\\end{figure}\n\\subsection{Gradient-based algorithms designed for quantum control}\n\\label{sec:GRAPE_algo}\n\nIn this paragraph, we focus on a gradient-based algorithm called \\textbf{GRAPE}, for \\textbf{Gr}adient \\textbf{A}scent \\textbf{P}ulse \\textbf{E}ngineering~\\cite{rebentrost2009optimal,PhysRevA.82.063418,khaneja_optimal_2005,auckenthaler2010matrix,goodwin2016modified,PhysRevA.102.042612,ding2019robust}. This algorithm was used to solve a large amount of quantum control problems. The references~\\cite{kobzar2004exploring,kobzar2012exploring,Van_Damme_robust_2017,Ansel_2021,BEC2021,BEC2023,Lapert_exploring_2012,Ansel_2022,haidong2017,\nansel_2017,VANREETH201739,\nVan_Damme_time_optimal_2018,Ansel2023} is a very short list of studies in which GRAPE plays a key role. This list is by no means  exhaustive~\\cite{cat,kochroadmap}. GRAPE and its variants are one of the most famous gradient-based algorithms in the quantum control community.\n\n\\paragraph{A direct gradient algorithm.} We first present a direct derivation of a gradient-based algorithm. The explicit relation to the PMP is discussed in a second step. The basic assumption which may be experimentally relevant is that the control is a piecewise constant function with a finite number $N$ of time steps of duration $\\Delta t=t_f/N$. We denote by $u_n$ the amplitude of the control in the time interval $[(n-1)\\Delta t,n\\Delta t)$ with $n\\in \\{1,\\cdots, N\\}$. The control $u$ is now described by a set of $N$ real values $(u_1,u_2,\\cdots,u_N)$. The core of the algorithm is based on a standard gradient descent algorithm for finite-dimensional optimization~\\cite{bonnans2006numerical,kelley1999iterative}, which greatly simplifies its derivation.\n\nThe basic principle comes from the observation that if the cost function $\\mathcal{C}(u)$ is differentiable then $\\mathcal{C} ( u ) $ has the fastest decrease in the direction $-\\partial_{u_n} \\mathcal{C}(u)$ given by the gradient of the cost function with respect to the control parameters. An iterative algorithm to estimate the optimal control can then be designed. Starting from a given control $u$, a new control $u'$ such that $\\mathcal{C}(u')<\\mathcal{C}(u)$ can be computed from the relation\n\\begin{equation}\\label{eqnewcontrol}\nu'_n = u_n - \\epsilon \\frac{\\partial \\mathcal{C}(u) }{\\partial u_n},\n\\end{equation}\n\nwhere $\\epsilon>0$ is a parameter chosen small enough to guarantee the convergence of the algorithm, but large enough  to limit the number of iterations. The cost $\\mathcal{C}(u')$ can be estimated at first order in $\\epsilon$ as follows\n\n\\begin{equation}\n\\begin{split}\n\\mathcal{C}(u')&=\\mathcal{C}\\left(u-\\epsilon\\frac{\\partial \\mathcal{C}(u)}{\\partial u}\\right) \\\\\n&= \\mathcal{C}(u)-\\epsilon \\left(\\frac{\\partial \\mathcal{C}(u)}{\\partial u}\\right)^2 + O(\\epsilon^2) \\\\\n&\\lesssim \\mathcal{C}(u).\n\\end{split}\n\\end{equation}\nThe strict inequality is achieved for $\\epsilon$ small enough. The parameter $\\epsilon$ is a free parameter that must be set by hand. Its automatic adjustment can be performed using a line-search algorithm, which aims to find the optimal value of $\\epsilon$ at each iteration step. A line-search improves convergence but at the cost of increased computation time. Another way to speed up the convergence of the algorithm is to replace $\\epsilon$ by gradients of previous iterations as in conjugate gradient approaches or by the inverse of the Hessian in second-order methods. The latter is the core of Newton's type algorithms. The Hessian can be difficult to compute in practice, but it can be estimated simply by knowing  the gradient and the cost function. Such algorithms are called quasi-Newton~\\cite{gill1972quasi}. We will not discuss these questions about the choice of $\\epsilon$, and we assume in the following that this parameter is known. General ideas can be adapted to more advanced algorithms.\n\nTo summarize, the pseudo-code for a standard gradient algorithm is the following:\n\n\n\\begin{lstlisting}\nChoose an initial guess control $u_0$.\nFunction $(\\mathcal{C}_1,u)$ = GradientMinimize($u_0,\\epsilon,k_{max}$)\n\t$u=u_0$\n\t$\\mathcal{C}_1 = \\mathcal{C}(u)$\n\tFor [$1\\leq k\\leq k_{max}$,\n\t\t$u' = u - \\epsilon \\frac{\\partial \\mathcal{C}(u) }{\\partial u}$\n\t\t$\\mathcal{C}_2 = \\mathcal{C}(u')$\n\t\tIf[$\\mathcal{C}_2>\\mathcal{C}_1$,\n\t\t\tBreak,\n\t\t\t$\\mathcal{C}_1 = \\mathcal{C}_2$\n\t\t\t$u = u'$]]\n\n\\end{lstlisting}\n\nThe constant $k_{max}$ which corresponds to the maximum number of iterations is set large enough such that the convergence to a minimum of $\\mathcal{C}$ is reached.\n\n\\paragraph{Calculation of the gradient.} We have not yet clarified how $\\tfrac{\\partial \\mathcal{C}(u) }{\\partial u_n}$ is calculated, and this is the difficult part of the algorithm. GRAPE provides a clever calculation trick to save computational time and derive analytical formulas. For simplicity, we consider that the Hamiltonian of the quantum system is of the form:\n\n\\begin{equation}\n\\hat H(t) = \\hat H_0 + u(t) \\hat H_1,\n\\end{equation}\n\nwhere $\\hat H_0$ and $\\hat H_1$ are two time independent Hamiltonian operators which do not commute and $u$ is a one-dimensional piecewise constant unbounded function (i.e. $u(t) \\in \\setR$, and on the step interval $n$, $u(t) = u_n$). The generalization to several control parameters is straightforward. The evolution operator during a time step can be expressed as\n\\begin{equation}\n\\hat U_n=\\hat U(n \\Delta t,(n-1)\\Delta t) = e^{-\\ii  \\Delta t (\\hat H_0 + u_n \\hat H_1)},\n\\label{eq:U_evol_piecewise_constant}\n\\end{equation}\nand $|\\psi(t_f)\\rangle = \\hat U_N \\hat U_{N-1}\\cdots \\hat U_1 |\\psi_0\\rangle$. We denote by $|\\psi_n\\rangle =\\hat U_n\\cdots \\hat U_1|\\psi_0\\rangle$ the state at time $t=n\\Delta t$.\n\nTo simplify the description, we consider only the case of a terminal cost of the form $\\mathcal{C}=G(|\\psi(t_f)\\rangle) =G(\\hat U_N \\hat U_{N-1}\\cdots \\hat U_1 |\\psi_0\\rangle)$. Since $\\hat{U}_n$ is the only term depending on $u_n$, we deduce that the derivative of $G$ with respect to $u_n$ can be expressed as\n$$\n\\frac{\\partial{G}}{\\partial u_n}=\\frac{\\partial{G}}{\\partial |\\psi(t_f)\\rangle}\\frac{\\partial |\\psi(t_f)\\rangle}{\\partial u_n}+\\frac{\\partial \\langle\\psi(t_f)|}{\\partial u_n}\\frac{\\partial{G}}{\\partial \\langle\\psi(t_f)|},\n$$\nwhich leads to\n\\begin{equation}\n\\begin{split}\n\\frac{\\partial{G}}{\\partial u_n}&=\\frac{\\partial{G}}{\\partial |\\psi(t_f)\\rangle}\n\\hat U_N \\cdots \\frac{\\partial \\hat U_n}{\\partial u_n}\\cdots \\hat U_1 |\\psi_0\\rangle \\\\\n&+ \\langle\\psi_0|\n\\hat U_1^\\dagger \\cdots \\frac{\\partial \\hat U_n^\\dagger}{\\partial u_n}\\cdots \\hat U_N^\\dagger \\frac{\\partial{G}}{\\partial \\langle\\psi(t_f)|}.\n\\end{split}\n\\end{equation}\nIt remains to compute $\\frac{\\partial \\hat U_n}{\\partial u_n}$ which is not trivial because $[\\hat H_0,\\hat H_1]\\neq 0$. A formal expression can be obtained from the Wilcox formula~\\cite{wilcox1967exponential,wilcox} which gives the derivative of the exponential of a matrix $A(\\theta)$ with respect to a parameter $\\theta$\n$$\n\\frac{\\partial e^{tA}}{\\partial \\theta}=e^{tA}\\int_0^t e^{-t' A}\\frac{\\partial A}{\\partial \\theta}e^{t'A}dt'.\n$$\nWe deduce that\n$$\n\\frac{\\partial \\hat U_n}{\\partial u_n}=-\\ii\\Delta t \\hat U_n\\overline{\\hat H}_1,\n$$\nwhere $\\overline{\\hat H}_1=\\frac{1}{\\Delta t}\\int_0^{\\Delta t}e^{\\ii t'(\\hat H_0+u_n\\hat H_1)}dt'\\hat H_1 e^{-\\ii t'(\\hat H_0+u_n\\hat H_1)}dt'$ is the average of $\\hat H_1$ in the Heisenberg representation over the duration $\\Delta t$. For a sufficiently small time step $\\Delta t$, $\\overline{\\hat H}_1$ can be identified to $\\hat H_1$ and we arrive at\n$$\n\\frac{\\partial \\hat U_n}{\\partial u_n}\\simeq -\\ii \\Delta t \\hat H_1\\hat U_n.\n$$\nIntroducing an adjoint state $|\\chi\\rangle$ defined by a backward propagation in time from the target state such that $\\langle\\chi (t_f)|=\\langle\\chi_N|=\\frac{\\partial G}{\\partial |\\psi(t_f)\\rangle}$ and $\\langle\\chi_n|=\\langle \\chi_N|\\hat U_N\\cdots \\hat U_n$, the gradient can be expressed as\n$$\n\\frac{\\partial G}{\\partial u_n}=-\\ii\\Delta t(\\langle \\chi_n|\\hat H_1|\\psi_n\\rangle-\\langle\\psi_n|\\hat H_1|\\chi_n\\rangle),\n$$\nwhich can be simplified into\n\\begin{equation}\\label{eqgradientfinal}\n\\frac{\\partial G}{\\partial u_n}=2\\Delta t \\Im(\\langle\\chi_n|\\hat H_1|\\psi_n\\rangle).\n\\end{equation}\nWe finally arrive at\n\\begin{equation}\\label{eqfinalgrape}\n\\delta u_n=u_n'-u_n=-\\epsilon \\Im(\\langle\\chi_n|\\hat{H}_1|\\psi_n\\rangle ).\n\\end{equation}\n\n\\begin{example}{}{ex11}\\label{ex11}\nWe consider the same terminal costs $G_1$ and $G_2$ as in Example~\\ref{example:ex9}. We recall that $G_1=1-|\\langle\\psi_f|\\psi(t_f)\\rangle|^2$ and $G_2=1-\\Re\\left(\\langle\\psi_f|\\psi(t_f)\\rangle\\right)$. We deduce that the final condition of the adjoint state is given respectively by $|\\chi_N\\rangle=-\\langle\\psi_f|\\psi(t_f)\\rangle |\\psi_f\\rangle$ and $|\\chi_N\\rangle=-\\frac{|\\psi_f\\rangle}{2}$ for $G_1$ and $G_2$.\n\\end{example}\nUsing Eq.~\\eqref{eqgradientfinal}, we observe that the gradient can be computed in a very efficient way with a forward and a backward propagation in time of the dynamics from the initial and the target states, respectively. The pseudo-code for this computation can be written as follows.\n\n\\begin{lstlisting}\nFunction $\\ket{\\psi(t)}$ = PropforwardSchrodingerEq($u$,$\\ket{\\psi_{0}}$)\n(*Propagate forward the Schrodinger equation using the control $u$ and the\ninitial state $\\ket{\\psi_{0}}$. Compute and store the quantum state at each time step, *)\n\nFunction $\\ket{\\chi(t)}$ = PropbackwardSchrodingerEq($u$,$\\ket{\\chi(t_f)}$)\n(*Propagate backward the Schrodinger equation using the control $u$ and the\nfinal state $\\ket{\\chi(t_f)}=2\\chi_0\\frac{\\partial G}{\\partial \\langle\\psi(t_f)|}$. Compute and store $\\ket{\\chi(t)}$ at each time step.*)\n\nFunction $\\delta u$=GradientGRAPE($u,\\epsilon$)\n\t$\\ket{\\psi(t)}$ = PropforwardSchrodingerEq($u(t)$,$\\ket{\\psi_{0}}$)\n\t$\\ket{\\chi(t)}$ = PropbackwardSchrodingerEq($u(t)$,$\\ket{\\chi(t_f)}$)\n\tFor[$1\\leq n\\leq N$, $\\delta u_n= -\\epsilon\\Im (\\langle\\chi_n|  \\hat H_1  |\\psi_n\\rangle)$]\n\t\n\n\\end{lstlisting}\n\nUsing this method, the time evolution of a quantum state is computed twice at each evaluation of the gradient, while $N + 1 \\gg 2$ evaluations of the dynamics are necessary when the gradient is estimated from a finite difference (i.e. with a formula $\\partial_{u_n}G \\approx (G(u_n + \\Delta u_n) - G(u_n))/\\Delta(u_n)$). When the number of time steps is large (hundreds or thousands of time steps), this approach can significantly reduce the computational time.\n\n\\paragraph{GRAPE versus PMP} The relation between GRAPE and the PMP can be established in the limit when the time step becomes infinitesimally small. In this case, the standard derivative is replaced by a functional one. Starting from Eq.~\\eqref{eqSPont}, we see that the choice $\\delta u=\\epsilon \\frac{\\partial H_P}{\\partial u}$ leads to\n$$\n\\delta S=-\\epsilon \\int_0^{t_f} \\left(\\frac{\\partial H_P}{\\partial u}\\right)^2dt,\n$$\nwhich is negative to first order in $\\epsilon$. Using\n$H_P=\\Im(\\langle \\chi|\\hat H|\\psi\\rangle)$, we deduce that\n$$\n\\delta u=-\\epsilon \\Im(\\langle \\chi|\\hat{H}_1|\\psi\\rangle),\n$$\nwhich is the continuous version of the time-discretized gradient derived in Eq.~\\eqref{eqfinalgrape}. In other words, a gradient-based algorithm can be viewed as a time digitalization of an equivalent algorithm derived from the PMP. Note that this equivalence is only valid when $U$ is open, i.e. in the weak PMP.\n\nOver the years, several versions of GRAPE have been developed. Some possible modifications are presented below.\n\n\n\n\n\\paragraph{Parameterized continuous functions.}\nAn interesting modification of the algorithm is to replace a piecewise constant control by parameterized functions~\\cite{skinner_optimal_2010}. The control $u$ is then expanded over a set of functions $\\{f_k(t)\\}_{k=1,\\cdots,k_{\\textrm{max}}}$ such as $u=v(\\{a_kf_k(t)\\})$ where $v$ is a smooth function. The goal is to find the real coefficients $(a_k)$ to minimize the cost functional. In the case of  Fourier series, this would be:\n\n\\begin{equation}\nu(t) = a_0+\\sum_{k=1}^{k_{max}} a_k \\cos(k\\omega   t)+b_k \\sin(k\\omega   t),~\\omega = 2\\pi/t_f.\n\\label{eq:param_control_fourier}\n\\end{equation}\nAt each step of the iterative algorithm, the values of the parameters are corrected from the gradients $\\frac{\\partial G}{\\partial a_k}$, $\\frac{\\partial G}{\\partial b_k}$. Using the chain rule derivation, the gradient $\\frac{\\partial G}{\\partial a_k}$ for example can be expressed as\n$$\n\\frac{\\partial G}{\\partial a_k}=\\sum_{n=1}^N\\frac{\\partial G}{\\partial u_n}\\frac{\\partial u_n}{\\partial a_k}=\\sum_{n=1}^N \\frac{\\partial G}{\\partial u_n}\\frac{\\partial v}{\\partial a_k}f_k(t_n),\n$$\nwhere the functions $f_k$ are approximated by piecewise constant functions with the same time step as $u$. The following pseudo-code can be used to implement this variant of GRAPE.\n\n\n\\begin{lstlisting}\nFunction $u$ = Control($\\{a_k,b_k\\}$)\n(* Compute $u_k = u(t_k)$ for each $t_k$ using the parameters $\\{a_k,b_k\\}_{k=0,\\cdots,k_{\\textrm{max}}}$  *)\n\nFunction $\\delta a, \\delta b$ = GradientGRAPE2($\\{a_k,b_k\\}$)\n\t$u$ = Control($\\{a_k,b_k\\}$)\n\t$\\ket{\\psi(t)}$ = PropforwardSchrodingerEq($u(t)$,$\\ket{\\psi_{0}}$)\n\t$\\ket{\\chi(t)}$ = PropbackwardSchrodingerEq($u(t)$,$\\ket{\\chi(t_f)}$)\n\tFor[$1\\leq n\\leq N$,\n\t\t$\\delta u_n= -\\epsilon\\Im \\bra{\\chi(t_{n})}  \\hat H_1  \\ket{\\psi(t_n)}$]\n\tFor[$1\\leq k\\leq k_{\\textrm{max}}$,\n\t\t$\\delta a_k=\\sum_{n=1}^N \\delta u_n\\frac{\\partial u}{\\partial a_k}(t_n)$\n                $\\delta b_k=\\sum_{n=1}^N \\delta u_n\\frac{\\partial u}{\\partial b_k}(t_n) $]\n\t\t\n\\end{lstlisting}\n\n\n\\paragraph{Exact gradient.}\nA slight modification of the system can be used to compute the gradient exactly. The idea is to consider a larger system given by the Hamiltonian $\\tilde{H}$\n\\begin{equation}\n\\tilde{H}=\\begin{pmatrix}\n\\hat{H} & 0 \\\\\n\\partial_u\\hat{H} & \\hat{H}\n\\end{pmatrix} .\n\\end{equation}\nIn the standard case, we have $\\partial_u\\hat{H}=\\hat{H}_1$. The matrix exponential of $\\tilde{H}$ has interesting properties such as\n$$\ne^{\\alpha\\tilde{H}}=\n\\begin{pmatrix}\ne^{\\alpha \\hat H} & 0\\\\\n\\partial_u e^{\\alpha \\hat{H}} & e^{\\alpha\\hat{H}}\n\\end{pmatrix},\n$$\nwhich can be shown by using a Taylor series expansion of the exponential function. The parameter $\\alpha$ is a complex constant. We deduce that the evolution operator $\\tilde{U}$ corresponding to the Hamiltonian $\\tilde{H}$ can be expressed as\n$$\n\\tilde{U}=\n\\begin{pmatrix}\n\\hat{U} & 0\\\\\n\\partial_u \\hat{U} & \\hat{U}\n\\end{pmatrix}.\n$$\nThis result suggests that the propagator $\\hat{U}$ and its gradient can be computed simultaneously by solving the Schr\\\"odinger equation associated to the Hamiltonian $\\tilde{H}$ with the initial condition $(\\hat I, 0)$. The gradient can then be used directly in GRAPE to correct the control at each step of the algorithm. This variant is known in the literature as the auxiliary matrix approach of GRAPE.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{example}{}{ex12}\nTo illustrate the method, we study the control of a state-to-state transfer in a two-level quantum system with minimal energy. The goal is to steer the system from the initial state $\\ket{\\uparrow}$ to the target $\\ket{\\downarrow}$ at time $t_f$ fixed  with the Hamiltonian $\\hat H(t) = \\frac{ \\Delta}{2}\\sigz + \\frac{ u}{2} \\sigx$. The cost functional to minimize can be expressed as\n$$\n\\mathcal{C}=1-|\\langle \\uparrow|\\psi(t_f)\\rangle |^2+\\frac{p_0}{2}\\int_0^{t_f}u(t)^2dt.\n$$\nwhere $p_0$ is a factor allowing to adjust the relative weight of the two terms in the cost. The control time $t_f$ is set to $2\\pi/\\sqrt{1+\\Delta^2}$. The Hamiltonian's equations of the PMP show that the dynamics of the state $|\\psi(t)\\rangle$ and adjoint state $|\\chi(t)\\rangle$ are governed by the Schr\\\"odinger equation with respectively the initial condition $|\\psi(0)\\rangle =|\\uparrow\\rangle$ and the final condition $|\\chi(t_f)\\rangle = \\langle \\uparrow |\\psi(t_f)\\rangle |\\uparrow\\rangle$ with $\\chi_0=-1/2$ (see Example~\\ref{example:ex9}). In the GRAPE algorithm, the correction $\\delta u$ at each step can be written as\n$$\n\\delta u =-\\epsilon\\left(\\Im(\\langle\\chi(t)|\\frac{\\sigx}{2}|\\psi(t)\\rangle +\\frac{p_0}{2}u(t)\\right).\n$$\nThe convergence of the algorithm is very good in this example. An example is plotted in Fig.~\\ref{figgrape}. The corresponding Python code \\emph{GRAPE.py} is provided in the supplementary material. The Python code \\emph{GRAPE2.py} solves the same optimal control problem but with a polynomial parameterization of the control pulse.\n\\end{example}\n\\begin{figure}[htbp]\n    \\centering\n    \\includegraphics[width=8cm]{fig8.pdf}\n  \\caption{Optimal control $u$ (black line) designed by GRAPE for Example~\\ref{example:ex12}. Numerical parameters are set to $u_0=1$ and $\\Delta=\\frac{u_0}{2}$. The red line represents the time-optimal solution with the constraint $|u(t)|\\leq u_0$. The blue line depicts the guess control used in the algorithm. The parameter $p_0$ is set to $0.1/t_f$.}\\label{figgrape}\n\\end{figure}\n\n\n\\section{From theory to experiment: Optimal control of a Bose-Einstein condensate in an optical lattice}\\label{sectheoexp}\nWe propose to demonstrate the different steps of the application of QOC to a specific experimental system, namely the control of a Bose-Einstein Condensate (BEC) in an optical lattice. We refer the reader to the references~\\cite{BEC2021,BEC2023,BEC2024} for additional theoretical and experimental details on these results.\n\n\n\n\n\n\\subsection{The model system}\n\\label{BEC_model}\n\nDilute Bose-Einstein condensates are routinely produced in cold atom experiments: a gas of identical bosons, cooled to a temperature close to absolute zero condenses into a single quantum state, and is therefore described by a single wave function~\\cite{RMP2008}. On typical experimental timescales (see Sec.~\\ref{sec:Experimental_optimal_control}), we can neglect both the interactions between the atoms of the gas and the parabolic magnetic potential used to confine the system. Such conditions allow us to simplify the description of the dynamics of the system which are governed by the  Schr\\\"odinger equation with a periodic sinusoidal potential~\\cite{Eckardt2017}. This trapping potential originates from a one-dimensional optical lattice produced by two interfering laser beams of wavelength $\\lambda$ propagating in opposite directions, whose relative phase and amplitude can be modulated in time with good precision. The goal of the control is then to manipulate the motional state of the BEC in this potential.\n\n\nThe wave function $\\ket{\\psi(t)}$ which belongs to the Hilbert space $\\mathcal{H}= \\mathcal{L}^2(\\mathbb{R}, dx)$, evolves in time according to the Schrödinger equation,\n       \\begin{align}\n  \\imath \\hbar \\frac{d\\ket{\\psi(t)}}{dt} = \\left( \\frac{\\hat{p}^2}{2m} - \\frac{s(t) E_L}{2}\\cos\\left( k_L \\hat{x} + \\varphi(t) \\right) \\right) \\ket{\\psi(t)},\\label{BEC_SE}\n\\end{align}\nwhere $\\hat{p}=-\\imath\\hbar\\frac{\\partial}{\\partial x}$ and $\\hat{x}=x$ are respectively the momentum operator and the position operator in the position representation with $x$ the spatial coordinate along the optical lattice axis. We denote by $m$ the mass of the atom, $k_L=2\\pi/(\\lambda/2)$ the wave vector and $E_L=\\frac{\\hbar^2 k_L^2}{2m}=4E_R$ (with $E_R$ the recoil energy) the energy associated with the lattice. \nThe control parameters are the dimensionless depth $s(t)$ and the phase $\\varphi(t)$. In this section, we focus on using $\\varphi(t)$ as a single control parameter, while the dimensionless lattice depth $s$ is kept constant. Varying $\\varphi$ as a function of time corresponds to moving or shaking the lattice position along the $x$- axis. We consider the following change of variables to obtain dimensionless coordinates:\n    \\begin{align}\n      t &\\rightarrow \\frac{E_L}{\\hbar}t, \\nonumber\\\\\n      x &\\rightarrow k_L x.\\nonumber\n    \\end{align}\nThis yields,\n       \\begin{align}\n  \n  \\imath \\frac{d\\ket{\\psi(t)}}{dt} \\equiv \\hat{H}(t)\\ket{\\psi(t)}=\\left(\\hat{p}^2 - \\frac{s}{2}\\cos\\left( \\hat{x} + \\varphi(t) \\right)\\right)\\ket{\\psi(t)},\n\\end{align}\nwith $\\hat{p}=-\\imath\\frac{\\partial}{\\partial x}$ and $\\hat{x}=x$  in the position representation.\n\nWe denote by $\\ket{\\phi_{\\alpha}}$ the eigenvectors of the momentum operator with eigenvalue $\\alpha$ and wavefunction $\\phi_{\\alpha}(x)=\\frac{1}{\\sqrt{2\\pi}}e^{\\imath \\alpha x}$. Since the potential is periodic in $x$, the Bloch theorem states that the parameter $\\alpha$ can be expressed as $\\alpha = n + q$, where $n \\in \\mathbb{Z}$ and $q\\in [-0.5,0.5]$ is the quasimomentum. The quasimomentum can formally take any real value, but due to the periodicity of the potential, two quasimomenta separated by an integer are equivalent. \nFurthermore, this periodicity implies that the quasimomentum $q$ is conserved during the  control process. In the subspace of a given quasimomentum $q$, we can expand a generic state on the plane wave basis as,\n       \\begin{align}\n  \\ket{\\psi} = \\sum_{n\\in\\mathbb{Z}}c_{q,n} \\ket{\\phi_{q+n}}.\\nonumber\n\\end{align}\nUsing this decomposition, the dynamic is given in terms of the coefficients $c_{q,n}$ as\n    \\begin{align}\n   \\label{BEC_coef} \\imath\\Dot{c}_{q,n} &= \\left( n+q \\right)^2c_{q,n} - \\frac{s}{4}\\left(e^{\\imath\\varphi(t)}c_{q,n-1} + e^{-\\imath\\varphi(t)}c_{q,n+1} \\right).\\nonumber\n\\end{align}\n\nThe Schr\\\"odinger equation can be written in matrix form as follows,\n       \\begin{align}\n   \\imath\\frac{d\\ket{\\psi(t)}}{dt} = \\left( \\hat{H}_0 + \\cos\\left( \\varphi(t)\\right)\\hat{H}_1 + \\sin\\left( \\varphi(t)\\right)\\hat{H}_2\\right) \\ket{\\psi(t)},\\nonumber\n\\end{align}\nwhere\n       \\begin{align}\n   \\ket{\\psi(t)} = \\begin{pmatrix}\n  \\vdots \\\\\n  c_{q,n-1} \\\\\n  c_{q,n} \\\\\n  c_{q,n+1} \\\\\n  \\vdots\n  \\end{pmatrix},\n\\end{align}\n       \\begin{align}\n       \\centering\n  \\label{BEC_vector} \\hat{H}_0 =  \\begin{pmatrix}\n  & \\ddots &  &  &  \\\\\n  \\dots & 0 & \\left((n-1)+q\\right)^2 & 0 & 0 & 0 & \\dots \\\\\n  \\dots & 0 & 0 & \\left(n+q\\right)^2 & 0 & 0 & \\dots \\\\\n  \\dots & 0 & 0 & 0 & \\left((n+1)+q\\right)^2 & 0 & \\dots \\\\\n   &  &  &  &  & \\ddots &\n  \\end{pmatrix}, \n\\end{align}\nand\n       \\begin{align}\n       \\centering\n  \\hat{H}_1 = \\begin{pmatrix}\n  \\ddots &  & \\ddots &  &  \\\\\n  \\dots & -\\frac{s}{4} & 0 & -\\frac{s}{4} & 0 & 0 & \\dots \\\\\n  \\dots & 0 & -\\frac{s}{4} & 0 & -\\frac{s}{4} & 0 & \\dots \\\\\n  \\dots & 0 & 0 & -\\frac{s}{4} & 0 & -\\frac{s}{4} & \\dots \\\\\n   &  &  &  & \\ddots &  & \\ddots\n  \\end{pmatrix}, \\\n  \\hat{H}_2 = \\begin{pmatrix}\n  \\ddots &  & \\ddots &  &  \\\\\n  \\dots & -\\imath\\frac{s}{4} & 0 & \\imath\\frac{s}{4} & 0 & 0 & \\dots \\\\\n  \\dots & 0 & -\\imath\\frac{s}{4} & 0 & \\imath\\frac{s}{4} & 0 & \\dots \\\\\n  \\dots & 0 & 0 & -\\imath\\frac{s}{4} & 0 & \\imath\\frac{s}{4} & \\dots \\\\\n   &  &  &  & \\ddots &  & \\ddots\n  \\end{pmatrix}.\\nonumber\n  \\end{align}\n\\paragraph{Landau-Zener-type Hamiltonian.} Interestingly, a two-level approximation can be derived from the BEC system when $s\\ll 1$.\n\nCalculating the eigenvalues of $\\hat{H}$ as a function of the quasimomentum yields the lattice band structure $E_m(q)$ ($m\\in\\mathbb{N}$), as shown in Fig.~\\ref{levels_figure}. We denote the corresponding Bloch eigenfunctions $\\ket{\\Psi_m(q)}$. For a small value of $s$ (typically $s<0.5$) and $q$ close to $0.5$, the first two energy bands $E_0(q)$ and $E_1(q)$ are well-separated in energy from the others, and may be considered as an effective two-level system. If the BEC is initially prepared in the subspace formed by these first two bands, it will remain in this subspace.\n\\begin{figure}\n         \\centering\n    \\includegraphics[width=0.5\\textwidth]{fig9.pdf}\n    \\caption{Eigenvalues $E(q)$ of $\\hat{H}$ (colored solid lines) and $\\hat{p}^2$ (black dashed lines) as a function of $q$.}\n   \\label{levels_figure}\n\\end{figure}\n\n\nExperimentally, while the quasimomentum can be tuned from its initial value by the application of a force (inducing a Bloch oscillation), it is most often equal to zero, corresponding to a homogeneous BEC, or to the ground state of the lattice $\\ket{\\Psi_0(0)}$ (prepared adiabatically). We therefore do not consider the quasimomentum as a control parameter here, and assume $q=0$ for the rest of this section. Using the unitary transformation,\n    \\begin{align}\n      \\ket{\\psi} &\\rightarrow \\hat{U}\\ket{\\psi}, \\nonumber \\\\\n      \\hat{H} &\\rightarrow \\hat{U}\\hat{H}\\hat{U}^{\\dagger} + \\imath \\dot{\\hat{U}} \\hat{U}^{\\dagger},\\nonumber\n    \\end{align}\nwhere $\\hat{U}=e^{-\\imath \\hat{p}\\varphi(t)}$, we obtain the following Hamiltonian,\n    \\begin{align}\n      \\hat{H} = \\left(\\hat{p} + \\frac{\\dot{\\varphi}(t)}{2} \\right)^2 - \\frac{s}{2} \\cos \\left( \\hat{x} \\right) ,\\nonumber\n    \\end{align}\nwhere $\\frac{\\dot{\\varphi}(t)}{2}$ plays the role of a controlled quasi momentum (effectively, $\\frac{\\dot{\\varphi}(t)}{2}$ is the quasimomentum of the BEC in the reference frame of the moving lattice). If we set $\\frac{\\dot{\\varphi}(t)}{2} = \\frac{1}{2} + \\frac{\\dot{\\tilde{\\varphi}}(t)}{2}$, with $\\dot{\\tilde{\\varphi}}(t)$ a control parameter close to $0$, we can isolate the first two energy levels and write the Hamiltonian in the basis $\\left( \\ket{\\phi_{0-1}}, \\ket{\\phi_{0+0}} \\right)$ such that,\n       \\begin{align}\n   \\hat{H} = \\begin{pmatrix}\n  \\left( \\frac{1}{2} - \\frac{\\tilde{\\dot{\\varphi}}(t)}{2} \\right)^2 & -\\frac{s}{4}\\\\\n   -\\frac{s}{4} & \\left( \\frac{1}{2} + \\frac{\\tilde{\\dot{\\varphi}}(t)}{2} \\right)^2  \\\\\n  \\end{pmatrix}.\n\\end{align}\nUp to a term proportional to the identity, we obtain a two-level quantum system whose Hamiltonian can be expressed as\n    \\begin{align}\n      \\hat{H} = \\frac{\\Delta(t)}{2}\\hat{\\sigma}_z + \\frac{\\omega}{2}\\hat{\\sigma}_x ,\n    \\end{align}\nwhere $\\omega=-\\frac{s}{2}$ and $\\Delta(t) = - \\dot{\\tilde{\\varphi}}(t)$.\nThis expression is similar to the Hamiltonian of a two-level quantum system with an offset term and a single control, for which time optimal control strategies are studied in Sec.~\\ref{sec:OCT_spin_time_1_input_and_offset}. With the following basis change,\n\\begin{align}\n\\ket{\\phi_{0-1}}\\rightarrow\\frac{1}{\\sqrt{2}}(\\ket{\\phi_{0-1}}+\\ket{\\phi_{0-0}})\\nonumber \\\\\n\\ket{\\phi_{0-0}}\\rightarrow\\frac{1}{\\sqrt{2}}(\\ket{\\phi_{0-1}}-\\ket{\\phi_{0-0}})\\nonumber\n\\end{align}\nthe correspondence is exact and allows to implement optimal solutions in this system as shown in Sec.~\\ref{sec:Experimental_optimal_control}.\n\n\n\\subsection{Numerical optimal control}\n\\label{sec:Numerical-optimal-control}\n\nWe consider the application of GRAPE to the control of a BEC in an optical lattice. The BEC system has a Hamiltonian given by\n       \\begin{align}\n\\hat{H} = \\hat{H}_0 + \\cos\\left(u(t)\\right) \\hat{H}_1 + \\sin\\left(u(t)\\right)\\hat{H}_2,\n\\end{align}\nwhere we set $u(t)=\\varphi(t)$ to use the same notation as in the general description of GRAPE. The goal is to find a control that minimizes the cost function $\\mathcal{C}$ at a fixed final time $t_f$\n       \\begin{align}\n  \\mathcal{C} = 1 - \\lvert \\braket{\\psi_{f}}{\\psi(t_f)} \\rvert^2,\n  \\label{eq:cost_bec}\n\\end{align}\nwhere $\\ket{\\psi(t_f)}$ is the state at final time, and $\\ket{\\psi_{f}}$, the target state. The application of the PMP yields a Pontryagin Hamiltonian of the form \\eqref{eq:Hamilotnian_QOC_schrodinger} with $F_0=0$. The adjoint state $\\ket{\\chi(t)}$ whose time evolution is also governed by the Schr\\\"odinger equation, has the final condition\n       \\begin{align}\n  \\ket{\\chi(t_f)} = -2\\chi_0\\braket{\\psi_{f}}{\\psi(t_f)}\\ket{\\psi_{f}}.\n\t\\end{align}\nThe abnormal multiplier is set to $\\chi_0=-1/2$ in the numerical simulation. Using the maximization condition of the PMP, the control is iteratively updated such that\n       \\begin{align}\n u'_n = u_n - \\epsilon \\Im \\left(\\bra{\\chi(t_{n})}{\\left(-\\sin\\left(u_n\\right)\\hat H_1 + \\cos\\left(u_n\\right)\\hat{H}_2\\right)}\\ket{\\psi(t_n)}\\right)\n\\end{align}\nThe control time is set to a multiple duration characteristic of the dynamical timescale of the system (usually the inverse spacing between the two lowest energy levels), and discretized into several hundred steps, so that the step duration is small with respect to this dynamical timescale, and the control is therefore quasi-continuous. The infinite dimensional Hilbert space is truncated so that $|n|\\leq n_{\\textrm{max}}$, where $n_{\\textrm{max}}$ is chosen with respect to the initial and target states to avoid boundary effects. In the numerical simulations, the control usually involves $400$ steps and $n_{\\textrm{max}}=10$. Thus the truncated space has a dimension of $2\\times n_\\textrm{max}+1=21$. Under these conditions, one iteration of the algorithm takes $0.17$ seconds, and it takes about $100$ iterations, i.e. $17$~seconds to obtain a cost function of order $10^{-4}$. The numerical simulations, written in Python, were conducted on a standard laptop computer.\n\n\n\n\n\n\\paragraph{State-to-state transfer.}\n\n\n\nWe first illustrate the optimal control of state-to-state transfer. The initial state of the BEC is represented by the state $\\ket{\\phi_{0+0}}$ and several target states are considered, which can be expressed in the canonical basis $\\ket{\\phi_{0+n}}$ with $q=0$. Attainable states range from individual momentum basis vectors to superposition of states~\\cite{BEC2021}. Alternatively, they can also be Gaussian states, corresponding to a localized probability density in position and momentum within a lattice site~\\cite{BEC2023}.\nHere we define as Gaussian a state whose $x$ and $p$ probability density functions are normal distributions with standard deviations $\\sigma_{x_0}$ and $\\sigma_{p_0}$ equal to those of the ground state $\\ket{\\Psi_0(0)}$ of the lattice Hamiltonian. The Gaussian state tends to the exact ground state for $s \\gg 1$, with $\\sigma_{x_0}=s^{-1/4}$, and $\\sigma_{p_0}=s^{1/4}/2$. A displaced Gaussian state $\\ket{g(x_c,p_c)}$ has non-zero position and momentum averages within a lattice site, $\\langle x\\rangle =x_c$ and $\\langle p\\rangle=p_c$.\nWe can further define a squeezed Gaussian state $\\ket{g(x_c,p_c,\\xi)}$, for which the standard deviations are modified as $\\sigma_x=\\xi \\sigma_{x_0}$ and $\\sigma_p=\\frac{\\sigma_{p_0}}{\\xi}$, where $\\xi$ is the $x$- squeezing parameter ($\\xi=1$ corresponding to a Gaussian state). The smaller $\\xi$ becomes, the more the standard deviation $\\sigma_x$ decreases and $\\sigma_p$ increases.\n\nGaussian and squeezed states can be projected on the basis $\\left( \\ket{\\phi_{0,+n}} \\right)$ with the coefficients~\\cite{BEC2023},\n    \\begin{align}\n    c_{0,n}\\left( x_c, p_c, \\xi \\right) = \\left( \\frac{2 \\xi^2}{\\pi \\sqrt{s}} \\right)^{1/4} e^{\\imath x_c p_c /2} e^{-\\imath n x_c} e^{-\\xi^2 \\left( n - p_c \\right)^2 / \\sqrt{s}}.\n\\end{align}\n\n\nThree numerical examples of optimal controls for state to state transfer are considered in Fig.~\\ref{oct_BEC}. In each case, the initial state is $\\ket{\\phi_{0+0}}$, and we set $q=0$, $s=5$, $n_{max}=10$ and $t_f=7.6$, which corresponds to a duration of 150~$\\mu$s. The target states are chosen to be $\\ket{\\phi_{0+2}}$, the centered Gaussian state $\\ket{g(x_c=0,p_c=0,\\xi=1)}$ and the centered squeezed Gaussian state $\\ket{g(x_c=0,p_c=0,\\xi=1/3)}$. The numerical results can be obtained from a code provided in the supplementary material.\n\n\n\n\\begin{figure}\n    \\includegraphics[width=\\textwidth]{fig10.pdf}\n\n    \\caption{Examples of control for the state-to-state transfer of a BEC: In red the transfer from $\\ket{\\phi_{0+0}}$ to $\\ket{\\phi_{0+2}}$, in dotted-blue to the Gaussian state $\\ket{g(0,0,1)}$ and in dashed-yellow to the squeezed state $\\ket{g(0,0,1/3)}$. The transfers are computed for $q=0$ and $s=5$. (a) Control phase. (b) Bar diagram of the momentum distribution reached at time $t_f$. (c) Probability density in position within a lattice cell at final time.}\n   \\label{oct_BEC}\n\\end{figure}\n\n\n\\subsection{Experimental optimal control}\n\\label{sec:Experimental_optimal_control}\n\nIn this section, we present some illustrative optimal control results applied to a BEC manipulated in an optical lattice, following the previous formalism. Other examples can be found in~\\cite{BEC2021,BEC2023,BEC2024}. We focus here on the experimental implementation of the control in a real system, namely the experimental setup at LCAR in Toulouse, the manipulation of the full quantum state of the BEC in the lattice, and a realization of an optimal control in an effective two-level quantum system.\n\n\n\n\\subsubsection{Experimental setup.}\n\nIn a typical implementation~\\cite{BEC2021}, a BEC of $^{87}$Rb atoms is trapped in a one-dimensional optical lattice, created by two counter-propagating laser beams. The beams' wavelength, $\\lambda=1063.9\\,$nm, is chosen far from the main optical resonances of the atom, in order to minimize light scattering and heating of the condensate. This also sets the lattice spacing $d=\\lambda/2\\simeq 532\\,$nm, and the characteristic energy scale $E_L=h^2/(2md^2)=h\\cdot8.111\\,$kHz, which also sets a characteristic timescale for the dynamics.\n\nIn the Schr\\\"odinger equation~\\eqref{BEC_SE} governing the dynamics, both the dimensionless lattice depth $s(t)$ and its position $\\varphi(t)$ can be varied arbitrarily in time, and therefore act as control parameters. This is achieved by using  Acousto-Optic-Modulators (AOMs) placed in the path of the lattice beams, which can adjust the amplitude and phase of the outgoing beam by varying the amplitude and phase of the RF signal applied to the AOM crystal. A common modulator varies the amplitude of a first laser beam which is then split to form the two arms of the lattice, after which one of the arms goes through an AOM which modulates its phase, thus varying the relative phase between the lattice beams and controlling $\\varphi(t)$.\n\nTo ensure that optimal control solutions can be successfully applied in this system, it is important to consider the timescales involved. On the one hand, the dynamical timescale is determined by both the atoms' inertia and the depth of the sine potential: for a typical depth $s=5$, the energy spacing between the two lowest bands at $q=0$ is $1.975\\,E_L$ (close to the level spacing in the harmonic approximation $\\hbar \\omega\\simeq 2.236\\,E_L$), corresponding to a characteristic duration of $T_0=62.4\\,\\mu$s.\n    On the other hand, through a combination of bandwidth limitations from the driving electronics and the AOM itself, changes in the amplitude and phase of a beam exiting the AOM occur on a typical duration of $100\\,$ns for sudden changes, corresponding to a spectral bandwidth of about 3 MHz. This sets the main limit on the speed with which the controls can be varied: the $3\\,$MHz bandwidth nonetheless allows changes to be made almost instantaneously with respect to the dynamical timescale.\n\n\nThis leads to the typical choice for the control ramps applied experimentally: a duration of $100\\,\\mu\\mathrm{s}\\simeq 1.6T_0$, discretized in 400 intervals of $250\\,$ns with a constant phase. Any change in the value of the phase occurs much faster than the inertial response of the atoms: the control is therefore quasi-continuous.\n\n\\medskip\n\nIn addition to these considerations on the lattice control, it is also crucial to consider other experimental effects that are not included in the modeling of the experiment:\n\\begin{itemize}\n\\item[$-$]~The laser wavelength $\\lambda$ must be known precisely as it effectively enters in the dimensionless timescale $E_Lt/\\hbar=\\alpha t$. For commercially available fiber lasers, the wavelength is typically known to an accuracy of $10^{-4}$.\n\\item[$-$]~The $^{87}$Rb atoms used here experience repulsive interactions within the BEC, characterised by a scattering length $a=104 a_0$~\\cite{DGObook}. These interactions can be described in the mean-field approximation by an additional, non-linear potential term in the Schrödinger equation \\eqref{BEC_SE}, $V_\\mathrm{int}=\\beta|\\psi(x,t)|^2$, yielding the Gross-Pitaevskii equation. The constant $\\beta$ characterizes the non-linearity for the 1-D dynamics in the lattice. In the experiments presented here, it is typically small ($\\beta<0.5$) due to the dilute nature of the BEC.\n\\item[$-$]~In a realistic experiment, the BEC loaded in the optical lattice has a finite size, which may also be affected by interactions. This corresponds to the occupation of a finite interval of quasimomenta around a central value (here $q=0$). In experiments shown here, about 100 lattice sites are populated, leading to an estimate of the quasi-momentum width $\\Delta q\\sim0.02$~\\cite{Dubertrand2016}.\n\\item[$-$]~Last but not least, the lattice depth $s$ is a fixed parameter for the optimal control using $\\varphi(t)$, but it must be known with a good precision to derive efficient controls. This means that it is crucial to have a precise calibration of the lattice depth~\\cite{Cabrera2018} before optimizing the control, as well as excellent stability of the experimental setup.\n\\end{itemize}\n\n\\begin{figure}\n         \\centering\n    \\includegraphics[width=0.8\\textwidth]{fig11.pdf}\n\n    \\caption{Examples of the influence of various parameters on the performance of the optimal control ramps for the state-to-state transfer of a BEC. (a) Fidelity of the preparation for a control ramp computed at an expected depth $s_0=5$, when the ramp is applied in a lattice of actual constant depth $s$, as the value of $s$ is varied. The full orange line (resp. blue dotted line and yellow dashed line) corresponds to the transfer from $\\ket{\\phi_{0+0}}$ to $\\ket{\\phi_{0+2}}$ (resp. to the Gaussian state $\\ket{g(0,0,1)}$ and to the squeezed state $\\ket{g(0,0,1/3)}$), here and throughout the figure. Likewise, throughout the figure, the control ramps are calculated for fixed values of the parameters: $s=5$, $q=0$, $\\beta=0$ and $\\alpha=1$, and are the ramps obtained in Fig.~\\ref{oct_BEC}. (b) Similarly, fidelity of the preparation against a variation of the quasi-momentum: the control ramp is applied to the initial state $\\ket{\\phi_{q+0}}$, with $q$ varied. (c) Similarly, fidelity of the preparation against a variation of the timescale factor $\\alpha$ (see text). (d) Fidelity of the preparation against a variation of the interaction parameter $\\beta$ (see text). In each graph, the gray shaded area denotes typical experimental uncertainty intervals for the varied parameter.}\n   \\label{oct_BEC_parameters}\n\\end{figure}\n\nTo illustrate the role of these parameters, we show in Fig.~\\ref{oct_BEC_parameters} how the fidelity of the state preparations studied in Sec.~\\ref{sec:Numerical-optimal-control} are affected by variations around the value for which the optimal control is calculated, with relevant uncertainty ranges highlighted. For realistic values of the parameters, Fig.~\\ref{oct_BEC_parameters} demonstrates that the timescale factor is very well characterized and that the effect of interaction is mostly negligible. It also highlights the importance of a well-calibrated lattice depth, and a small quasimomentum distribution width for the success of the state transfer. The quasimomentum effect is all the more important the more squeezed the target state is (\\emph{i.e.} extended in momentum).\n\nThere are other important constraints on the controls available experimentally, namely on the maximum depth $s$ that can be applied to the atoms, and on the maximum available control time. The former is limited by the maximum laser intensity available, to $s\\lesssim40$ for the experimental setup described here. The latter is constrained, when using $^{87}$Rb, by interaction-induced dynamical instabilities that may occur on a timescale of several milliseconds~\\cite{DupontPNAS2023}.\n\nFinally, we emphasize that in an experimental situation, it is not possible to directly measure the complex coefficients $c_{q,n}$ characterizing the quantum state. A single measurement of the BEC consists in imaging the absorption from a resonant infrared laser beam by the atoms, which is imaged on a CCD camera. This imaging is performed after a time-of-flight, during which the various momentum components of the BEC will separate spatially. Such a measurement, when good care is taken to remove any parasitic signal on the camera, will only provide a measurement of the probabilities $|c_{q,n}|^2$. A full characterization of the prepared state may therefore require multiple measurements, in order to extract relative phases~\\cite{BEC2021}, or to perform a full state reconstruction~\\cite{BEC2023}.\n\n\\subsubsection{Full quantum state control.}\n\nA clear demonstration of optimal transfer between quantum states with control of probability amplitudes is provided by the preparation of energy eigenstates. The Bloch eigenstates corresponding to the energy spectrum for the lattice potential,  as shown in Fig.~\\ref{levels_figure}, are defined by their coefficients $c_{q,n}^{(m)}$, stationary solutions of Eq.~\\eqref{BEC_coef}, with specific amplitudes and signs. When such a state is prepared in the lattice, its stationary nature means that the momentum distribution (the measured probabilities $|c_{q,n}|^2$) do not evolve at subsequent times.\n\n\n\\begin{figure}\n         \\centering\n    \\includegraphics[width=0.8\\textwidth]{fig12.pdf}\n    \\caption{Preparation of lattice eigenstates. (a) Lattice band structure for $s=8.2$ (colored lines) with labels for the eigenstates prepared in (b) and (c). The shaded area denotes the sine potential depth. (b) Preparation of the $P$ band eigenstate at quasi-momentum $q = 0$ for a depth $s = 8.15 \\pm 0.30$. $\\mathrm{(b_1)}$ Experimental evolution of momentum distribution of the prepared state in the static lattice. $\\mathrm{(b_2)}$ Corresponding theoretical evolution. (c) Same as (b) for the preparation of the $D$ band eigenstate  at quasi-momentum $q= 0.25 k_L$ for a depth  $s = 8.26 \\pm 0.10$.}\n   \\label{oct_BEC_eigenstates}\n\\end{figure}\n\nThis control process is illustrated in Fig.~\\ref{oct_BEC_eigenstates}. We first apply an optimal control ramp to transfer the lattice ground state $\\ket{\\Psi_0(0)}$ to the $P$ band Bloch state at $q=0$, as denoted in Fig~\\ref{oct_BEC_eigenstates} \\textbf{a}, in a lattice of depth $s=8.2$. After the preparation, we measure the momentum distribution obtained for increasing hold times, up to $110~\\mu\\mathrm{s}\\simeq1.5T_0$. The result of this measurement is shown in Fig.~\\ref{oct_BEC_eigenstates} \\textbf{b$_1$}: the momentum distribution shows no significant evolution, as expected for an eigenstate. For comparison, Fig.~\\ref{oct_BEC_eigenstates} \\textbf{b$_2$} shows the numerical evolution of the momentum distribution, as expected from the theoretical final state of the optimal control ramp (which has a numerical fidelity of 99\\% to the theoretical $P$ band eingenstate in good agreement with the experimental results).\n\nIt is also possible to prepare eigenstates in a subspace with non-zero quasi-momentum $q_0$, by taking advantage of a change of reference frame. We first prepare the plane wave superposition with coefficients $c_{q_0,n}^{(m)}$, starting from the lattice ground state. At the end of the preparation ramp $\\varphi(t)$, instead of returning the phase to $\\varphi=0$ (in which case we remain in the $q=0$ subspace where the prepared state is not an eigenstate), we set the lattice in linear motion with a ramp\n\n$\\varphi(t>t_f)=2q_0(t-t_f)$ (in reduced units). This effectively translates the state into the $q=q_0$ subspace in the reference frame of the lattice, and the prepared superposition is then an eigenstate. This is illustrated in Fig~\\ref{oct_BEC_eigenstates} \\textbf{c$_1$}, which shows the evolution of the momentum distribution after preparation of the $D$ band eigenstate at $q_0=0.25k_L$ (as denoted in Fig~\\ref{oct_BEC_eigenstates} \\textbf{a}) in the moving lattice. Again the distribution shows very little evolution, in good agreement with the numerically expected result shown in Fig~\\ref{oct_BEC_eigenstates} \\textbf{c$_2$}.\nFurther examples of state preparation and state characterization can be found in~\\cite{BEC2021,BEC2023,BEC2024}.\n\n\\subsubsection{Two-level optimal control.}\n\nFinally, the BEC system lends itself to the emulation of a two-level quantum system, as introduced in Sec.~\\ref{BEC_model}.  The control protocol derived in Sec.~\\ref{sec:OCT_spin_time_1_input_and_offset} can be used to perform a time-optimal transfer between the states,\n\\begin{align}\n\\ket{+}=\\frac{1}{\\sqrt{2}}(\\ket{\\phi_{0-1}}+\\ket{\\phi_{0-0}}),\\nonumber \\\\\n\\ket{-}=\\frac{1}{\\sqrt{2}}(\\ket{\\phi_{0-1}}-\\ket{\\phi_{0-0}}).\\nonumber\n\\end{align}\nIn this context, the lattice depth plays the role of the constant offset $\\Delta=s/2$, while the phase variation $\\dot{\\varphi}=1+u(t)$ provides the variable control. To perform the bang-bang protocol of Sec.~\\ref{sec:OCT_spin_time_1_input_and_offset}, we choose a lattice depth of $s\\simeq0.5$, and a maximum control $u_0=0.5$. This yields typical control times for the experiment of $t_1\\simeq 64\\,\\mu\\mathrm{s}$ and $t_2\\simeq 156\\,\\mu\\mathrm{s}$.\n\nIn order to assess the result of the transfer, we need to characterize the initial and final states. However both $\\ket{+}$ and $\\ket{-}$ are approximate eigenstates of the Hamiltonian at a small depth, and they cannot be distinguished from a simple measurement of the (equally weighted) populations in $\\ket{\\phi_{0-1}}$ and $\\ket{\\phi_{0-0}}$. To circumvent this issue, we use a quench of these states into a deeper lattice $s_\\mathrm{meas}\\simeq6$, in which they are not eigenstates. The population dynamics in the deeper lattice then allow us to clearly distinguish $\\ket{+}$ and $\\ket{-}$. These state superpositions can be prepared using an optimal transfer ramp,  in the deeper lattice of depth $s_\\mathrm{meas}$, both as initial states for the bang-bang protocol, and to characterize their evolution. We use a preparation ramp with numerical fidelity $F>99.5\\%$, and a duration of $1.5T_0$ (about $84\\,\\mu\\mathrm{s}$). The state is then characterized by recording the momentum distribution dynamics for $200\\,\\mu\\mathrm{s}$ at $10\\,\\mu\\mathrm{s}$ intervals.\n\n\\begin{figure}\n         \\centering\n    \\includegraphics[width=0.8\\textwidth]{fig13.pdf}\n    \\caption{Time-optimal control of an effective two-level quantum system on the BEC platform. (a) Theoretical $\\mathrm{(a_1)}$ and experimental $\\mathrm{(a_2)}$ dynamics of the momentum distribution after preparation of the state $\\ket{+}$ in the lattice of depth $s_\\mathrm{meas}=6.07\\pm0.05$. The numerical fidelity after preparation is $F>0.995$. (b) Same as (a) for the preparation and characterization of state $\\ket{-}$. (c) Experimental evolution of the momentum distribution for a bang-bang transfer between $\\ket{-}$ and $\\ket{+}$ in a lattice of depth $s=0.57\\pm0.03$ $\\mathrm{(c_1)}$, followed by a quench to a deep lattice $s_\\mathrm{meas}=5.90\\pm0.09$ $\\mathrm{(c_2)}$. The optimal control parameters are $u_0=0.5$, $t_1=63.66\\,\\mu\\mathrm{s}$, and $t_2=159.5\\,\\mu\\mathrm{s}$. (d) Same as (c) for a bang-bang transfer between $\\ket{+}$ and $\\ket{-}$. The color map for probabilities on all graphs extends from 0 to 1. \n    }\n   \\label{oct_BEC_LZexp}\n\\end{figure}\n\nThe realization of such a procedure is shown in Fig.~\\ref{oct_BEC_LZexp}. Panels Fig.\\ref{oct_BEC_LZexp} \\textbf{a} and Fig.\\ref{oct_BEC_LZexp} \\textbf{b} present both the numerical and experimental evolution of the states $\\ket{+}$ and $\\ket{-}$ in the quenched lattice of depth $s_\\mathrm{meas}$. This demonstrates that the change in relative phase can be clearly identified through the non-equilibrium dynamics in the deeper lattice, and shows at the same time that the two superposition states can be prepared efficiently as initial states for the bang-bang control.\nFigure~\\ref{oct_BEC_LZexp} \\textbf{c} (resp. \\textbf{d}) shows the experimental results from the time-optimal control for the transfer from $\\ket{-}$ to $\\ket{+}$ (resp. $\\ket{+}$ to $\\ket{-}$), followed by a quench to the depth $s_\\mathrm{meas}$. The control is realized by applying two successive constant phase drifts $\\dot{\\varphi}=1\\pm 0.5$ for times $t_1$ and $t_2$ and is identical for both transfers (only the initial state is changed). After the transfer, the quenched dynamics confirm that the states have been exchanged with good accuracy, the dynamics being almost identical to those of panels  Fig.~\\ref{oct_BEC_LZexp} \\textbf{a} and Fig.~\\ref{oct_BEC_LZexp} \\textbf{b}.\n\nNote that a more thorough characterization of the initially prepared and final states can be achieved by state reconstruction techniques using the lattice dynamics data~\\cite{BEC2023}. This goes beyond our purpose here, which has been to illustrate how the BEC platform can be used to emulate optimal control of a two-level quantum system.\n\n\n\\section{Conclusion}\n\\label{sec:conclusion}\n\nIn this introduction to the toolbox of quantum optimal control, we present both analytical and numerical methods based on the PMP. The key elements of this mathematical theory are described from an analogy with classical Lagrangian and Hamiltonian mechanics. It is then shown how these results can be used to design optimal control strategies for quantum systems. A comprehensive description of existing optimization algorithms is provided with a discussion of their advantages and areas of applicability. Particular attention is paid to shooting techniques and gradient-based algorithms that are directly derived from the PMP. Several problems of state-to-state transfer in a two-level quantum system have been analyzed in detail. The link between the optimal solution and the quantum speed limit is also explored in this case. The experimental implementation in the case of BEC in a one-dimensional optical lattice is described in a final section. Starting from the modeling of the quantum dynamics, we show step by step how the optimal control is computed and then implemented experimentally, to realize both two-level and many-level controls. Experimental constraints and limitations are also discussed.\n\nThe goal of this tutorial paper is to provide an overview of the toolbox of QOC that we hope is accessible to physicists with a background in quantum dynamics. Simple and concrete physical examples have been used throughout this paper to illustrate the different mathematical concepts. In particular, we have successively reused and adapted the same examples to illustrate different important points. Therefore, the examples presented in this introduction are not representative of the systems encountered in the literature. For instance, a key aspect which has not been discussed concerns the control of open quantum systems. Most experimental configurations must be modeled by taking into account the interaction of the system with its environment, and such features must be integrated into the optimization process~\\cite{Koch2016}.\nThe degree to which OCT techniques have been applied to that end depends on the characteristics of the open quantum system considered.\n\nIn the Markovian regime in which the memory effect is neglected, optimal control procedures are now quite well understood. The main difficulty lies in the loss of complete controlability and therefore in the fact that certain target states are not reachable~\\cite{dirr2009}. The situation is not at the same stage of maturity for non-Markovian dynamics. Although this aspect has been explored in a few examples, the usefulness of the memory effect as a resource for optimal control remains to be clarified.\n\n\n\n\nA wide range of problems have been already solved in quantum optimal control, but with the advent of quantum computers and the progress of quantum technologies, new objectives are emerging. A first one concerns optimization performed on a quantum computer~\\cite{hogg2000quantum,moll2018quantum,blekos2023review}. Such optimizations are based on quantum algorithms and the realization of quantum circuits in order to solve optimal control problems. The optimization can focus on the control design for the computer itself, but it can also be a totally independent control problem. Different quantum optimization algorithms can be distinguished extending from purely quantum algorithms, such as Grover's type algorithms~\\cite{baritompa2005grover} and quantum annealing methods~\\cite{das2005quantum,yarkoni2022quantum} or to hybrid algorithms based on a gradient descent~\\cite{yang2017optimizing,cerezo2021variational,bonet2023performance}. In the latter case, the core of the algorithm is classical and it is the same as the one presented in Sec.~\\ref{sec:GRAPE_algo}. The difference resides in the evaluation of the gradient, which is computed using quantum algorithms. This is particularly well adapted to quantum optimal control, but it does not offer any significant advantage, since the core of the algorithm remains classical. Breakthroughs are expected with pure quantum algorithms. Recently a few properties of the PMP have been combined with a Quantum Approximate Optimization Algorithm (QAOA)~\\cite{PhysRevLett.126.070505,PhysRevApplied.16.054023}, but a general and versatile quantum algorithm based on the PMP remains to be found. Ideally, a PMP based quantum optimization algorithm should be designed for any type of cost function (not only restricted to bang-bang controls, like in~\\cite{PhysRevLett.126.070505}), and it would solve the shooting problem using the advantages offered by quantum computations.\n\nA second family of open questions concerns the inclusion of energy efficiency and sustainable development criteria in quantum technologies~\\cite{PRXQuantum.3.020101}. The latter are not exempt from the challenge of global warming. Despite the fact that we are working with a very small number of quantum quantities, energy consumption is far from ideal. With the most powerful devices, we are beginning to achieve quantum supremacy, but we are far from having a quantum energy advantage. Part of the huge energy consumption comes from the cooling system needed to keep noise levels low, while a second source is due to controls. So far, we have mostly focused on time-optimal strategies, to avoid noise or dissipation effects. However, energy-optimal strategies can achieve a similar result with a drastic reduction  in energy consumption.  Optimizing energy consumption at all stages of quantum computing (and other quantum technologies) will be one of the major problems to be solved in the near future.\n\n\\noindent \\textbf{Acknowledgments.} We gratefully acknowledge useful discussions with Pr. H. R. Jauslin. We thank the support from the Erasmus Mundus Master QuanTeem (Project number: 101050730), the project QuanTEdu-France (ANR-22-CMAS-0001) on quantum technologies  and the ANR project QuCoBEC (ANR-22-CE47-0008-02).\n\n\\appendix\n\n\\section{List of mathematical symbols and acronyms}\\label{secappA}\n\\begin{itemize}\n    \\item[$ $] $X$ is the state of a dynamical system and $X_a$ is a vector component of $X$.\n    \\item[$ $] $\\delta X$ is the infinitesimal difference between two states of a dynamical system.\n    \\item[$ $] $S$ is an action functional.\n    \\item[$ $] $\\delta S$ is the functional derivative of the action $S$ for two trajectories close to each other.\n    \\item[$ $] $\\Mc L$ is a Lagrangian.\n    \\item[$ $] $F$ is a vector function defining the system dynamics.\n    \\item[$ $] $u$ is a control parameter, $u_a$ is a vector component of $u$, and $u_n$ is the value at step $n$ of a piecewise constant control.\n    \\item[$ $] $U$ is the domain of definition of $u(t)$ (a subset of $\\setR^m$).\n    \\item[$ $] $G$ is a terminal cost function.\n    \\item[$ $] $F_0$ is a running cost function.\n    \\item[$ $] $\\mathcal{C}$ is the cost functional to minimize in an optimal control problem.\n     \\item[$ $] $\\Lambda$ is the adjoint state, and $\\Lambda_a$ is a vector component.\n    \\item[$ $] $\\Lambda_0$ is the adnormal multiplier.\n    \\item[$ $] $H_p$ is the Pontryagin's Hamiltonian\n    \\item[$ $] $\\Re$ and $\\Im$ are respectively the real and imaginary parts of a complex number.\n    \\item[$ $] $\\ket{\\psi}$ is a quantum state.\n    \\item[$ $] $\\hat \\rho$ is a density matrix.\n    \\item[$ $] $\\hat H$ is the Hamiltonian operator of a quantum system.\n    \\item[$ $] $\\sigx, \\sigy$ and $\\sigz$ are the Pauli matrices.\n    \\item[$ $] $(x,y,z)$ are the coordinates of the Bloch vector for a two-level quantum system.\n     \\item[$ $] $\\hat U(t_f,t_i)$ is the evolution operator from $t=t_i$ to $t=t_f$.\n     \\item[$ $] $\\ket{\\chi}$ is the adjoint state of $\\ket{\\psi}$.\n\\end{itemize}\nThe following acronyms are used in this paper:\n\\begin{itemize}\n\\item[$ $] OCT: Optimal Control Theory\n\\item[$ $] QOC: Quantum Optimal Control Theory\n\\item[$ $] PMP: Pontryagin Maximum Principle\n\\item[$ $] QSL: Quantum Speed Limit\n\\end{itemize}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Lagrange Multiplier}\\label{app_lagrange}\nWe recall in this section basic results on Lagrange multiplier in the finite-dimensional case. We are particularly interested in abnormal multipliers which are less described in the literature.\n\nThe method of Lagrange multiplier is a standard technique in finite-dimensional optimization problem that transforms a constrained optimization into an unconstrained one at the cost of an increase in dimension. Consider for instance the maximization (or minimization) of a smooth function $F_0$ on $\\mathbb{R}^2$ with variables $(x,y)$. In absence of constraints, a necessary condition to fulfill for the extrema of the function is given by\n$$\n\\nabla F_0 =\\begin{pmatrix}\n\\frac{\\partial F_0}{\\partial x} \\\\\n\\frac{\\partial F_0}{\\partial y}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix}.\n$$\nA slightly more difficult task is to find the maximum of $F_0$ under a constraint of the form $F(x,y)=0$ where $F$ is also a smooth function on $\\mathbb{R}^2$. The extrema of $F_0$ are to be found on the level curve $F(x,y)=0$. In the generic case, the extremum is located at the point of $\\mathbb{R}^2$ where the level curves  of $F$ and $F_0$ intersect at one point. At this point, the two curves have a common tangent and their gradient vectors are parallel. This gives the following condition\n$$\n\\nabla F_0=-\\lambda \\nabla F,\n$$\nwhere $\\lambda$ is a non-zero real parameter called a Lagrange multiplier. Note that the exact value of $\\lambda$ is not important for finding the extrema. A systematic way to solve this problem is to introduce a new function $L$ on $\\mathbb{R}^3$ as\n$$\nL(x,y,\\lambda)=F_0(x,y)+\\lambda F(x,y).\n$$\nThe extrema are characterized by $\\nabla L=0$, which leads to the same equations as those established previously. The extremum point is denoted $(x_0,y_0)$.\n\nA singular behavior occurs when $\\nabla F(x_0,y_0)=0$ and in this case it is necessary to adapt the procedure. A typical situation corresponds to the problem where the condition $F(x,y)=0$ is satisfied at only one isolated point. It is then clear that the value of $F_0$ at this point cannot be compared to its neighboring points. Consider for instance the function $F_0$ to be maximized $F_0(x,y)=x+y$ under the constraint $F(x,y)=x^2+y^2=0$. The point $(0,0)$ is the only point that satisfies the constraint. It therefore corresponds by construction to the point maximizing $F_0$. Note that we also have $\\nabla F(0,0)=0$. This solution can be obtained by the general approach by modifying the definition of $L$ which is now a function on $\\mathbb{R}^4$ such that\n$$\nL(x,y,\\lambda,\\lambda_0)=\\lambda_0F_0(x,y)+\\lambda F(x,y),\n$$\nwhere the real parameter $\\lambda_0$ is called the abnormal multiplier. Note that $(\\lambda,\\lambda_0)$ is defined up to a real factor and that the two multipliers cannot be simultaneously equal to 0. The extrema are given by the conditions $\\partial L/\\partial x=\\partial L/\\partial y=\\partial L/\\partial \\lambda=0$. When $\\lambda_0\\neq 0$, we recover the previous formulation of the optimization problem. New solutions appear when $\\lambda_0=0$ and $\\nabla F=0$. They have the peculiarity of not depending on $F_0$, i.e. the function to maximize. They are called abnormal extremal solutions. In the previous example, the point $(0,0)$ corresponds to such a solution for which $\\lambda_0=0$.\n\n\\section{Pontryagin Maximum Principle}\\label{appendixPMP}\nWe describe in a heuristic way the origin of the maximization condition of the PMP described in Thm.~\\ref{thm:Pontryagin Maximum Principle}~\\cite{bertsekasbook}. This also leads to a valuable geometric interpretation of the adjoint state and of the optimal control problem.\n\nWe first consider a time-optimal control process where the goal is to steer the system from $X_0$ to $X_f$ in minimum time. We denote respectively by $u^\\star$ and $X^\\star$ a smooth optimal control and the corresponding trajectory such that $\\dot{X}^\\star=F(X^\\star,u^\\star)$. We consider another admissible control $u$ close to $u^\\star$ which generates the dynamic $X(t)$ with $\\dot{X}=F(X,u)$. The two trajectories are very close to each other and the small difference between the two is $\\delta X=X-X^\\star$. Starting from $\\dot{X}^\\star+\\delta \\dot{X}=F(X^\\star+\\delta X,u)$, a linearized equation of motion is used to calculate the dynamics around the reference trajectory  $X^\\star$ (up to terms of order two in $\\delta X$)\n\\begin{equation}\\label{eqappdelta}\n\\delta \\dot{X}=A\\delta X+F(X^\\star,u)-F(X^\\star,u^\\star),\n\\end{equation}\nwhere the elements of the $n\\times n$- matrix $A$ are given by $A_{ij}=\\partial_{X_j} F_i|_{(X^\\star,u^\\star)}$. The rigorous derivation of Eq.~\\eqref{eqappdelta} can be done under the assumption of regularity of $F$ and convexity of the set $F(X,u)$ for $u\\in U$~\\cite{bertsekasbook}. The solution of Eq.~\\eqref{eqappdelta} can be expressed as\n\\begin{equation}\\label{eqappdelta2}\n\\delta X(t)=\\int_{0}^t V(t,t')[F(X^\\star,u)-F(X^\\star,u^\\star)]dt',\n\\end{equation}\nwhere $\\delta X(0)=0$ and $V$ is the propagator from times 0 to $t$ associated to the differential equation $\\dot{V}(t,0)=A(t) V(t,0)$, with $V(0,0)=I_n$~\\cite{bryson1975applied}. The different elements of the optimal control problem are schematically represented in Fig.~\\ref{fig10}.\n\\begin{figure}[htbp]\n\\begin{center}\n\\includegraphics[width=10cm]{figC1.pdf}\n\\end{center}\n\\caption{Schematic description of a time-optimal control problem from $X_0$ to $X_f$. The vector space is $\\mathbb{R}^2$ and the coordinates of $X$ are $(X_1,X_2)$. The set $\\mathcal{A}(t_f,X_0)$ is the reachable set at time $t_f$ from $X_0$ (different reachable sets are plotted in shades of gray at different times). Two trajectories are plotted in black reaching respectively the points $X_f$ (optimal trajectory) and $X(t_f)$ (non-optimal trajectory). The blue line corresponds to the tangent to the reachable set in $X_f$. The adjoint state $\\Lambda(t_f)$ is orthogonal to this line, while $\\dot{X}$ is tangent to the trajectory.}\n\\label{fig10}\n\\end{figure}\nWe then introduce the reachable set $\\mathcal{A}(t_f,X_0)$ at time $t_f$ from the state $X_0$ as the set of all the states $X(t_f)$ that can be reached by a trajectory starting from $X_0$ at time $t=0$, the trajectory being associated to an admissible control $u$. If the target state $X_f$ is attained exactly by the optimal solution at $t=t_f$ then $X_f$ is on the boundary of $\\mathcal{A}(t_f,X_0)$. It is clear that if $X_f$ belongs to the interior of $\\mathcal{A}(t_f,X_0)$ then a smaller time $t'<t_f$ can be found such that $X_f\\in \\mathcal{A}(t',X_0)$ and $t_f$ is not the minimum time to reach the target (see the reachable sets at different times in Fig.~\\ref{fig10} to be convinced of this point). Assuming that the reachable set is convex in a neighborhood of $X_f$, we consider the plane tangent to $\\mathcal{A}(t_f,X_0)$ in $X_f$ and we define the adjoint state $\\Lambda(t_f)$ at time $t_f$ as a vector orthogonal to this plane and pointing outwards. For any final state $X(t_f)$ close to $X_f$ and associated to a non-optimal trajectory, we have $\\delta X(t_f)\\cdot \\Lambda(t_f)\\leq 0$ where $\\delta X(t_f)=X(t_f)-X_f$.\n\nWe introduce a time-dependent vector $\\Lambda(t)\\in\\mathbb{R}^n$ such that $\\Lambda(t)^\\intercal=\\Lambda(t_f)^\\intercal V(t_f,t)$. Using the relation $\\dot{V}(t_f,t)=-V(t_f,t)A$, we arrive at $\\dot{\\Lambda}^\\intercal=-\\Lambda(t)^\\intercal A$ or\n\\begin{equation}\\label{eqappdelta3}\n\\dot{\\Lambda}(t)=-A^\\intercal \\Lambda(t).\n\\end{equation}\nAs expected, $\\Lambda(t)$ which is defined by a backward propagation of the dynamic can be identified with the adjoint state of the PMP. The Pontryagin Hamiltonian $H_P$ reads $H_P=\\Lambda^\\intercal F$. The corresponding Hamiltonian equation for the adjoint state can be written as\n$$\n\\dot{\\Lambda}_i(t)=-\\frac{\\partial H_P}{\\partial X_i}=-\\Lambda^\\intercal \\partial_{X_i}F(X,u)=-\\sum_j\\partial_{X_i}F_j(X,u)\\Lambda_j,\n$$\nwhich is equivalent to Eq.~\\eqref{eqappdelta3}. Starting from Eq.~\\eqref{eqappdelta2} and the condition $\\delta X(t_f)\\cdot \\Lambda(t_f)\\leq 0$, we obtain\n$$\n\\Lambda(t_f)^\\intercal \\int_0^{t_f}V(t_f,t')[F(X^\\star,u)-F(X^\\star,u^\\star)]dt'\\leq 0,\n$$\nwhich leads to\n$$\n\\int_0^{t_f}\\Lambda(t')^\\intercal[F(X^\\star,u)-F(X^\\star,u^\\star)]dt'\\leq 0.\n$$\nThis inequality can be transformed into\n\\begin{equation}\\label{eqappmax}\n\\int_0^{t_f}[H_P(t')-H_P^\\star(t')]dt'\\leq 0,\n\\end{equation}\nwhere $H_P=\\Lambda^\\intercal F(X^\\star,u)$ and $H_P^\\star=\\Lambda^\\intercal F(X^\\star,u^\\star)$.\nConsider now for $u$ a control equal to $u^\\star$ except on a very short time interval $[t,t+dt]$, we deduce that the condition \\eqref{eqappmax} is satisfied if and only if\n$$\nH_P(t)\\leq H_P^\\star(t),\n$$\nfor $t\\in [0,t_f]$, i.e. $u^\\star$ maximizes the function $H_P$ along the optimal trajectory.\n\nWe consider in a second step the relative position of $\\Lambda(t_f)$ and $\\dot{X}$ as represented in Fig.~\\ref{fig10}. For the optimal solution, we deduce by construction that $H_P(t_f)=\\Lambda(t_f)\\cdot \\dot{X}(t_f)\\geq 0$. In this case, $X_f$ does not belong to $\\mathcal{A}(X_0,t_f-dt)$ for any sufficiently small time step $dt>0$. Note that if $\\dot{X}$ points inwards the reachable set, the trajectory will be time-maximal. When $H_P$ does not depend explicitly on time, the Pontryagin Hamiltonian is a constant of motion. It is straightforward to show this property if $U$ is an open set. In this case, we have $\\frac{\\partial H_P}{\\partial u}=\\Lambda\\cdot \\frac{\\partial F}{\\partial u}=0$ at any time $t$. We deduce that $\\dot{H}_P=\\dot{\\Lambda}F+\\Lambda\\dot{F}$ can be expressed as\n$$\n\\frac{d}{dt}H_P=-\\Lambda \\frac{\\partial F}{\\partial X}F+\\Lambda\\frac{\\partial F}{\\partial X}F=0.\n$$\nWe denote by $-\\Lambda_0$ the positive constant equal to $\\Lambda\\cdot \\dot{X}$ at any time $t\\in [0,t_f]$. A new Pontryagin Hamiltonian can then be defined as\n$$\nH_P=\\Lambda\\cdot \\dot{X}+\\Lambda_0,\n$$\nwhere $H_P=0$ along the optimal trajectory. The abnormal extremals for which $\\Lambda_0=0$ can be identified here to the trajectory tangent to the boundary of the reachable set since in this case $\\Lambda(t_f)\\cdot \\dot{X}(t_f)=0$.\n\n\nThis argument can be extended to an optimal control problem with a fixed control time $t_f$ where the goal is to minimize the cost functional $\\mathcal{C}=G(X(t_f))$. This cost can be for instance the distance from the target $X_f$ to $X(t_f)$. A geometric description of this case is given in Fig.~\\ref{fig11}. We consider the level sets of the function $G$ as the set of points where $G(X)$ is a constant. The optimal situation corresponds to the case where the boundary of the reachable set at $t_f$ and the level set $G(X(t_f))$ are tangent in $X(t_f)$. We introduce the plane tangent to the two sets in $X(t_f)$. The adjoint state $\\Lambda(t_f)$ is then defined up to a factor as the opposite of the gradient of the level set in $X(t_f)$, such that the two vectors point outwards of their respective sets. We denote by $\\Lambda_0$ this negative constant and we finally have\n$$\n\\Lambda(t_f)=\\Lambda_0\\frac{\\partial G(X(t_f)}{\\partial X(t_f)}.\n$$\nBy definition of the optimal solution, we have\n$$\nG(X^\\star(t_f))\\leq G(X(t_f)).\n$$\nAt first order in $\\delta X$, we deduce that\n\\begin{equation}\\label{eqappdelta4}\n\\partial_XG^\\intercal \\delta X(t_f)\\geq 0.\n\\end{equation}\nIt is then straightforward to show that\n$$\n\\Lambda(t_f)^\\intercal \\int_0^{t_f}V(t_f,t')[F(X^\\star,u)-F(X^\\star,u^\\star)]dt'\\leq 0,\n$$\nwhich gives\n$$\n\\int_0^{t_f}\\Lambda(t')^\\intercal[F(X^\\star,u)-F(X^\\star,u^\\star)]dt'\\leq 0.\n$$\nWe obtain the same inequality \\eqref{eqappmax} by introducing the Pontryagin Hamiltonian.\n\n\n\n\\begin{figure}[htbp]\n\\begin{center}\n\\includegraphics[width=10cm]{figC2.pdf}\n\\end{center}\n\\caption{Same as Fig.~\\ref{fig10} but for the case of a fixed final time, the goal of the control problem being to minimize the distance to the target defined by the function $G$. The different reachable sets and the level surfaces of $G$ are plotted respectively in shades of gray and red. The blue line represents the common tangent plane to the boundary of the reachable set $\\mathcal{A}(t_f,X_0)$ and the level curves of the distance $G(X(t_f))$. $\\partial G$ denotes the gradient vector pointing outwards of the level surface at $X(t_f)$.}\n\\label{fig11}\n\\end{figure}\n\n\n\n\n\n\n\n\\section{Description of the numerical optimization codes}\\label{app_code}\nThis section aims to briefly present the different codes provided in the supplementary material. All codes are in Python and are executable independently of each other. The two GRAPE codes require only the use of the standard Python scientific libraries Numpy and SciPy. \n\n\\paragraph{Shooting algorithm.} The code  \\textit{shooting.py} implements the shooting method, an algorithm used in Sec.~\\ref{sec:shooting_algo}. The integration of the equations of motion given by the PMP is performed using the Python package Nutopy~\\cite{nutopy} and the function \\textit{ivp.exp}. The optimization uses the function \\textit{nle.solve}. The final part of the code returns two graphs, one with the Bloch coordinates $(x,y,z)$ of the two-level quantum system, and the other with the optimized control.\n\n\n\\paragraph{GRAPE for a two-level quantum system.}\n\\textit{GRAPE.py} provides a minimal working code for the optimization of a control with GRAPE. It allows to derive the results of Sec.~\\ref{sec:GRAPE_algo}, i.e. state-to-state transfer from the ground to the excited states. Several build-in functions are defined in the code, to perform the forward and backward propagation of the Schr\\\"odinger equation using the split-operator method. The optimization uses the Scipy function \\textit{minimize} with the BFGS method. Here, the exact gradient is provided to the solver (with the function \\textit{GradientGRAPE}), and the Hessian is estimated numerically from \\textit{minimize}. An initial control must be provided to the algorithm. Many different choices would lead to the same result. Here, a cosine wave function is used. It is chosen different from zero, and smooth in order to obtain a good starting point for the algorithm. \\textit{GRAPE2.py} considers the same control problem but with a polynomial parameterization of the control pulse. The computation of the gradient is modified accordingly.\n\n\n\n\n\n\n\n\\paragraph{GRAPE for a BEC system} The code \\textit{GRAPE\\_BEC.py} implements the algorithm GRAPE for state-to-state transfer in the case of BEC in a one-dimensional optical lattice. The results are those presented in Sec.~\\ref{sec:Numerical-optimal-control}. The code is composed of several sections. The first is a class (``BEC'') which generates from the size of the system $N_k$, the value of the quasimomentum  $q$ and the depth of the lattice $s$, the Hamiltonian of the system (the matrices $H_0$, $H_1$ and $H_2$). The second section (``propagation''), also a class, defines the functions which return the fidelity and the correction to be made to the control given by the PMP. The third section groups together the functions of the code. The fourth section describes the system parameters, the constants, the control time, the values of $q$ and $s$, the initial state and the initial guess for the control. The fifth section aims to calculate the optimal control. The \\textit{scipy.optimize.minimize} algorithm is used to iteratively find the solution to the state-to-state transfer problem, where the function to be minimized is ``Cost'' and the gradient ``dCost'' (based on class ``propagation''). The algorithm uses a L-BFGS-B method, it is therefore a second order algorithm since the Hessian is iteratively approximated at each step. The user can define a maximum number of iterations, a tolerance, and a bound for the control. The final sixth section displays the results for the three transfers, namely the controls, the population $\\left\\lvert c_{q,n} \\right\\rvert^2$, and the probability density.\n\nAt each iteration, we can also perform a line search approach to find the best value of the parameter $\\epsilon$. This is done by computing a step that satisfies the Wolfe condition with \\textit{scipy.optimize.line\\_search}.\n\n\n\\section*{References}\n\\bibliographystyle{vancouver}\n\n\n\n\n\n\n\\end{document}\n"}
{"paper_id": "2403-00532", "version": "2403-00532v2", "root_file_path": "d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\data_raw\\2403-00532\\tex\\2403-00532v2\\file_arxiv2.tex", "metadata": {"total_length": 180119, "merged_count": 1, "merged_files": ["file_arxiv2.tex"], "missing_files": []}, "content": "\n\n\\documentclass[english,a4paper]{article}\n\n\n\\expandafter\\let\\csname equation*\\endcsname\\relax\n\n\\expandafter\\let\\csname endequation*\\endcsname\\relax\n\n\\usepackage{bm}\n\\usepackage{graphicx}\n\\usepackage{amsmath}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsthm}\n\\usepackage{amstext}\n\\usepackage{amsbsy}\n\\usepackage{amsopn}\n\\usepackage{amscd}\n\\usepackage{amsxtra}\n\\usepackage[colorlinks=true,allcolors=blue]{hyperref}\n\\usepackage{color}\n\\usepackage{soul}\n\\usepackage[dvipsnames]{xcolor}\n\\usepackage{listings}\n\n\\usepackage{cite}\n\n\n\n\n\n\n\n\n\\usepackage{booktabs} \n\\usepackage{enumitem}\n\\usepackage{makeidx}\n\n\n\\usepackage[most]{tcolorbox}\n\t\\tcbuselibrary{theorems}\n\t\\tcbuselibrary{breakable}\n\\usepackage[left=1cm,right=1cm,top=2cm,bottom=2cm]{geometry}\n\n\n\n\n\n\n\n\n\n\n\\newcommand{\\setim}{\\mathrm{Ran}}\n\n\n\\newcommand{\\Texp}{\\mathbb{T}\\mathrm{exp}}\n\\newcommand{\\Te}{\\mathbb{T}e}\n\\newcommand{\\T}{\\mathbb{T}}\n\\newcommand{\\Pexp}{\\mathbb{P}\\mathrm{exp}}\n\\newcommand{\\Pe}{\\mathbb{P}e}\n\\newcommand{\\slim}[1]{\\text{\\textnormal{s--}} \\hspace{-.40em} \\lim_{#1}}\n\\newcommand{\\wlim}[1]{\\text{\\textnormal{w--}} \\hspace{-.40em} \\lim_{#1}}\n\\newcommand{\\siglim}[1]{\\sigma \\hspace{-.15em} \\text{--} \\hspace{-.20em} \\lim_{#1}}\n\\newcommand{\\pdirac}{\\sqcup \\hspace{-0.42em} \\sqcup}\n\\newcommand{\\ad}{\\mathrm{Ad}}\n\\newcommand{\\addiff}{\\mathrm{ad}}\n\\newcommand{\\oiint}{\\bigcirc \\hspace{-1.15em} {\\int \\hspace{-0.9em} \\int}}\n\\newcommand{\\oiiint}{\\bigcirc \\hspace{-1.35em} {\\int \\hspace{-0.95em} \\int \\hspace{-0.95em} \\int}}\n\\newcommand{\\loiint}{o \\hspace{-0.6em} {\\int \\hspace{-0.55em} \\int}}\n\\newcommand{\\bivec}{\\overset{\\rotatebox{180}{$\\curvearrowleft$}}}\n\n\\newcommand{\\cvec}{\\bivec}\n\\renewcommand{\\div}{\\mathrm{div} }\n\\newcommand{\\grad}{\\overrightarrow{\\mathrm{grad}} }\n\\newcommand{\\rot}{\\cvec {\\mathrm{rot}} }\n\n\\newcommand{\\ihbar}{\\imath \\hbar}\n\\newcommand{\\norme}[1]{\\begin{array}{||c||} #1 \\end{array}}\n\\newcommand{\\dslash}{\\partial \\hspace{-0.8em} \\diagdown}\n\\newcommand{\\sumint}{\\Sigma \\hspace{-0.85em} \\int}\n\\newcommand{\\fig}[4]{\\begin{figure}[!h]\n                     \\begin{center}\n                     \\includegraphics[width=#4]{#2}\n                     \\end{center}\n                     \\caption{\\label{#1} \\textit{#3}}\n                     \\end{figure}}\n\\newcommand{\\figdbl}[5]{\\begin{figure}[!h]\n                       \\begin{center}\n\t\t       \\includegraphics[width=#5]{#2}\n\t\t       \\includegraphics[width=#5]{#3}\n\t\t       \\end{center}\n\t\t       \\caption{\\label{#1} \\textit{#4}}\n\t\t       \\end{figure}}\n\\newcommand{\\ket}[1]{| #1 \\rangle}\n\\newcommand{\\bra}[1]{\\langle #1 |}\n\\newcommand{\\braket}[2]{\\langle #1 | #2 \\rangle}\n\\newcommand{\\ketbra}[2]{| #1 \\rangle \\langle #2 |}\n\\newcommand{\\card}{\\mathrm{card}\\ }\n\\newcommand{\\lbrakint}{\\left[ \\hspace{-0.1em} [}\n\\newcommand{\\rbrakint}{] \\hspace{-0.1em} \\right]}\n\\newcommand{\\sgn}{\\mathrm{sgn}}\n\\newcommand{\\dg}{\\delta_{(g)}}\n\\newcommand{\\cupg}{\\cup_{(g)}}\n\\newcommand{\\bigveebar}{\\underline{\\bigvee}}\n\\newcommand{\\ppint}{\\smallsetminus \\hspace{-.90em} \\int}\n\\newcommand{\\dgpe}{\\mathring{\\delta}}\n\\newcommand{\\ssi}{si et seulement si }\n\\newcommand{\\dom}{\\mathrm{Dom}}\n\\newcommand{\\Sp}{\\mathrm{Sp}}\n\\newcommand{\\core}{\\mathrm{Cor}}\n\\newcommand{\\Res}{\\mathrm{Res}}\n\\newcommand{\\Ric}{\\mathrm{Ric}}\n\\newcommand{\\argsh}{\\mathrm{argsh}\\ }\n\\newcommand{\\llangle}{\\langle \\hspace{-0.2em} \\langle}\n\\newcommand{\\rrangle}{\\rangle \\hspace{-0.2em} \\rangle}\n\\newcommand{\\RE}{\\Re \\mathrm{e}}\n\\newcommand{\\IM}{\\Im \\mathrm{m}}\n\\newcommand{\\cotan}{\\mathrm{cotan}\\ }\n\\newcommand{\\sinc}{\\mathrm{sinc}}\n\\newcommand{\\lllangle}{\\langle \\hspace{-0.2em} \\langle \\hspace{-0.2em} \\langle}\n\\newcommand{\\rrrangle}{\\rangle \\hspace{-0.2em} \\rangle \\hspace{-0.2em} \\rangle}\n\\newcommand{\\id}{\\mathrm{id}}\n\\newcommand{\\au}{\\ \\mathrm{a.u.}}\n\n\\newcommand{\\Mc}[1]{\\mathcal{#1}}\n\\newcommand{\\setZ}{\\mathbb{Z}}\n\\newcommand{\\setN}{\\mathbb{N}}\n\\newcommand{\\setR}{\\mathbb{R}}\n\\newcommand{\\setQ}{\\mathbb{Q}}\n\\newcommand{\\setC}{\\mathbb{C}}\n\\newcommand{\\Id}{\\mathbb{I}}\n\n\\newcommand{\\ii}{\\imath}\n\\newcommand{\\A}{\\hat a}\n\\newcommand{\\Ad}{\\hat a^\\dagger}\n\\newcommand{\\sigp}{\\hat{\\sigma}_+}\n\\newcommand{\\sigm}{\\hat{\\sigma}_-}\n\\newcommand{\\sigz}{\\hat{\\sigma}_z}\n\\newcommand{\\sigx}{\\hat{\\sigma}_x}\n\\newcommand{\\sigy}{\\hat{\\sigma}_y}\n\n\n\n\\newcommand{\\red}[1]{{\\color{red} #1}}\n\n\n\n\n\n\n\n\n\n\\begin{document}\n\n\n\\newtcbtheorem[auto counter]{definition}\n  {Definition}{breakable,theorem style=plain,fonttitle=\\bfseries\\upshape, fontupper=\\itshape ,arc=0mm, boxrule=0.5mm,coltitle=black, colback=gray!10!white,colframe=gray!25!black, left=1mm,right=1mm,top=1mm,bottom=1mm}{def}\n\n\\newtcbtheorem[auto counter]{example}\n  {Example}{breakable,theorem style=plain,fonttitle=\\bfseries\\upshape ,\n  arc=0mm, boxrule=0.5mm,coltitle=black, colback=gray!10!white,colframe=white, left=1mm,right=1mm,top=1mm,bottom=1mm}{example}\n\n\n\n\n  \\newtcbtheorem[auto counter]{property}\n  {Property}{breakable,theorem style=plain,fonttitle=\\bfseries\\upshape, fontupper=\\itshape ,arc=0mm, boxrule=0.5mm,coltitle=black, colback=gray!10!white,colframe=gray!25!white, left=1mm,right=1mm,top=1mm,bottom=1mm}{property}\n\n  \\newtcbtheorem[auto counter]{proposition}\n  {Proposition}{breakable,theorem style=plain,fonttitle=\\bfseries\\upshape, fontupper=\\itshape ,arc=0mm, boxrule=0.5mm,coltitle=black, colback=gray!10!white,colframe=gray!50!white, left=1mm,right=1mm,top=1mm,bottom=1mm}{prop}\n\n\\newtcbtheorem[auto counter]{theorem}\n  {Theorem}{breakable,theorem style=plain,fonttitle=\\bfseries\\upshape, fontupper=\\itshape ,arc=0mm, boxrule=0.5mm,coltitle=black, colback=gray!10!white,colframe=gray!75!black, left=1mm,right=1mm,top=1mm,bottom=1mm}{thm}\n\n\n\\tcolorboxenvironment{proof}{\nblanker,breakable,left=5mm,\nbefore skip=10pt,after skip=10pt,\nborderline west={1mm}{0pt}{gray}}\n\n\n\\lstset{language=Mathematica,numbers=left, numberstyle=\\tiny, stepnumber=1, mathescape=true,frame=single}\n\n\\title{Introduction to Theoretical and Experimental aspects of Quantum Optimal Control}\n\n\n\\author{Q. Ansel\\footnote{Institut UTINAM,  CNRS UMR 6213, Universit\\'{e} de Franche-Comt\\'{e}, Observatoire des Sciences de l’Univers THETA, 41 bis avenue de l’Observatoire, F-25010, Besançon, France}, E. Dionis, F. Arrouas, B. Peaudecerf\\footnote{Laboratoire Collisions Agr\\'egats R\\'eactivit\\'e, UMR 5589, FeRMI, UT3, Universit\\'e de Toulouse, CNRS, 118 Route de Narbonne, 31062 Toulouse CEDEX 09, France}, S. Gu\\'erin\\footnote{Laboratoire Interdisciplinaire Carnot de Bourgogne, CNRS UMR 6303, Universit\\'{e} de Bourgogne, BP 47870, F-21078 Dijon, France}, D. Gu\\'ery-Odelin\\footnote{Laboratoire Collisions Agr\\'egats R\\'eactivit\\'e, UMR 5589, FeRMI, UT3, Universit\\'e de Toulouse, CNRS, 118 Route de Narbonne, 31062 Toulouse CEDEX 09, France}, D. Sugny\\footnote{Laboratoire Interdisciplinaire Carnot de Bourgogne, CNRS UMR 6303, Universit\\'{e} de Bourgogne, BP 47870, F-21078 Dijon, France, dominique.sugny@u-bourgogne.fr}}\n\n\\maketitle\n\n\\begin{abstract}\nQuantum optimal control is a set of methods for designing time-varying electromagnetic fields to perform operations in quantum technologies. This tutorial paper introduces the basic elements of this theory based on the Pontryagin maximum principle, in a physicist-friendly way. An analogy with classical Lagrangian and Hamiltonian mechanics is proposed to present the main results used in this field. Emphasis is placed on the different numerical algorithms to solve a quantum optimal control problem. Several examples ranging from the control of two-level quantum systems to that of Bose-Einstein Condensates (BEC) in a one-dimensional optical lattice are studied in detail, using both analytical and numerical methods. Codes based on shooting method and gradient-based algorithms are provided. The connection between optimal processes and the quantum speed limit is also discussed in two-level quantum systems. In the case of BEC, the experimental implementation of optimal control protocols is described, both for two-level and many-level cases, with the current constraints and limitations of such platforms. This presentation is illustrated by the corresponding experimental results.\n\\end{abstract}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Introduction to Quantum Control}\n\nThe design and development of quantum technologies requires the use of many advanced techniques in order to tackle the fragility of quantum information, and the difficulty of isolating and manipulating quantum entities~\\cite{raimond2006exploring,kurizki2015quantum,acin2018quantum,becher20232023,shore-book,ricebook,RMPsugny}. These challenges have led in particular to the development of Quantum Optimal Control (QOC)~\\cite{guerin2003,werschnik2007,brif2010,bonnard_optimal_2012,altafini2012,cat,\nkochroadmap,rembold2020introduction,PRXQuantumsugny,kuprov2023spin,dalessandro-book,stefanatos2021}, a branch of optimal control theory whose aim is to adapt and apply the tools of optimal control to quantum systems. As its name suggests, optimal control~\\cite{pontryaginbook,leemarkusbook,bryson1975applied,kirk2004optimal,trelat2012optimal,agrachev-book,bressan-piccoli,schaettler-book,liberzon-book,boscain-book,jurdjevic-book} is a mathematical theory that refers to the design of time-varying controls to manipulate dynamical systems in order to ideally achieve specific goals (encapsulated in a figure of merit). QOC was initiated in the eighties, and has since then seen an exponential development. It nowadays comprises a very powerful toolbox of analytical and numerical methods for designing control protocols under various experimental constraints and limitations. QOC is therefore not only a mature theoretical field, but also a very efficient approach from an experimental point of view on many quantum platforms. A key aspect of QOC is that it is an open-loop procedure that provides the optimal control process without any feedback from the experiment. Successful applications therefore require a very accurate modeling of the dynamical system and careful consideration of the controls that are experimentally available. Such QOC techniques have recently been successfully applied in many different areas of quantum technologies, ranging from quantum computing and simulation to sensing~\\cite{kochroadmap,rembold2020introduction}, making QOC a key tool of growing importance in the development of quantum technologies. This can be seen by its growing presence in the literature, as illustrated in Fig.~\\ref{fig:number_of_publications}. QOC is not the only control method that is available to manipulate quantum systems in an optimized fashion. Among others, we can mention adiabatic passage techniques~\\cite{vitanov2001,RMPstirap,guerin2011}, shortcut to adiabaticity approaches~\\cite{RMPSTA,STA,campo2013,whitty2020,torosov2021} or composite pulses~\\cite{vitanovCP,dridi2020,Ivanov2022,torosov2011}. While quantum optimal control in most cases deals\nwith time-dependent pulses, it can be applied to other issues such as the control of Hamiltonian structure~\\cite{rabitz2001,rabitz2011}. Optimal control can also be performed in the frequency domain as, e.g., shown in~\\cite{rabitz2016}. Furthermore, the framework of the quantum speed limit (QSL)~\\cite{deffner2017quantum,frey2016,bukov2019,oconnor2021,poggi2019,campo2013b}, which seeks to establish lower bounds on the minimum time required to steer a system from a given initial state to a target state, is closely linked to QOC~\\cite{deffner2017quantum,calarco2009,diaz2020,hegerfeldt2013driving}. The corresponding time is expressed as a ratio between the distance to the target state and the dynamical speed of evolution. This approach, like QOC, has been the subject of intense development in recent years.\n\n\\begin{figure}[htbp]\n    \\centering\n    \\includegraphics[width=15cm]{fig1.pdf}\n    \\caption{Growth in the number of scientific articles $N_{QOC}$ (red curve) with the keywords ``quantum\" and ``optimal control\" published each year on the internet, during the period 1990-2022, and ratio between $N_{QOC}$ and the number of scientific articles $N_Q$ with the keywords ``quantum\" (blue curve). Despite an increasing number of annual publications, the ratio decreased during the  period 2000-2015 (the number of publications in quantum science grew very rapidly during these years), but since 2018 the trend has reversed. At the beginning of 2023 (not shown in the graph), the trend is confirmed with almost 1\\% of publications concerning QOC. Data were collected using Google Scholar.}\n    \\label{fig:number_of_publications}\n\\end{figure}\n\nSeveral textbooks and reviews have introduced and described the basic elements and provided a fairly complete picture of the field of QOC~\\cite{bonnard_optimal_2012,rembold2020introduction,\nPRXQuantumsugny,dalessandro-book,wilhelmreview}. Others have focused on its practical numerical implementation or on the description of various optimization algorithms~\\cite{werschnik2007,bonnans2006numerical,koch2012,borzi-book}. However, either their starting points are at a high mathematical level, or they only consider a specific aspect of the QOC toolbox, so that it can be difficult for a newcomer to get a complete overview of the field and to apply such techniques to their own system. This tutorial paper aims to fill that gap. This introduction covers a wide range of aspects, from the description of the Pontryagin Maximum Principle (PMP) which can be seen as the central mathematical result of the theory~\\cite{pontryaginbook,leemarkusbook,liberzon-book}, to the analytical or numerical computations of optimal control and their experimental implementation. Mathematical issues are treated with a minimum of rigor, but with several reminders of basic notions. The various numerical algorithms available in the literature are described. Special emphasis is given to two numerical methods, namely the shooting and the gradient-based algorithms, which allow solving low- and high-dimensional quantum optimal problems respectively. Numerical codes are provided in the supplementary material. We point out that the efficiency of such algorithms is due to the absence of traps in most of quantum control landscapes, as originally proposed in~\\cite{rabitz2004,rabitz2006} and then discussed and extended in~\\cite{pechen2011there,moore2012exploring,chakrabarti2007quantum,pechen2012quantum,larocca2020}. Due to the large number and diversity of optimization methods, some interesting topics are not covered in this tutorial. Among others, we can mention machine learning techniques~\\cite{HushScience2017,bukovPRX2018,dayPRL2019,mehtaPhysRep2019,carleoRMP2019,giannelli2022,khalid2023}, the Hamilton-Jacobi-Bellman approach~\\cite{bertsekasbook}, second-order optimization algorithms~\\cite{grape2,tannor2011, sherson2020,goodwin2023}, closed-loop control~\\cite{naturerouchon,egger2014,porotti2023}, controllability and accessibility of quantum systems~\\cite{infinity-1,sachkov_controllability_2000,schirmer_complete_2001,schirmer2003controllability,Albertini_notions_of_controllability_2003}, quantum control landscapes~\\cite{pechen2011there,moore2012exploring,chakrabarti2007quantum,pechen2012quantum,larocca2020}, robust optimal control~\\cite{kobzar2004exploring,k1,k2,kobzar2012exploring,daems:2013,chen2014sampling,Van_Damme_robust_2017,zeng2018,wu2019,dridi2020b,dong2021,Ansel_2021,propson2022,guerin2022,guerin2022b,harutyunyan2022robust,nelson2023,schirmer2023,carolan2023}, optimal control of linear systems~\\cite{liberzon-book,vardan2020,vardan2019,li2017,li2011,evangelakos2023b},\noptimal control and quantum sensing~\\cite{Degen_review_2017,poggiali2018,wittler2021,Lin_optimal_2021,ansel2023optimal,Liu_optimal_QM_review_2022} and the control of open quantum systems~\\cite{sugny07,dirr2009,bonnard2009,addis2016,Koch2016,fux2021,pechen2021,pechen2023}.\n\nThe success of QOC has been demonstrated theoretically in a large number of quantum systems~\\cite{kochroadmap}. Although such control protocols are interesting from a theoretical point of view, e.g. to know the physical limits of a dynamical system in terms of control time or fidelity, they are generally not the final answer to a control problem: the ultimate goal is to implement the calculated control process experimentally. Several difficulties have to be overcome in order to transfer the theoretical result to the experiment, which can be divided into two categories. The first problem comes from the model system which must be sufficiently precise in an open-loop framework to describe the physical system, but relatively simple to apply the optimization algorithms numerically. A second obstacle is related to the family of control pulses that can be implemented experimentally. Depending on the experimental setup, different constraints may arise, ranging from the control time and the maximum intensity of the control available, to its Fourier bandwidth or its time discretization. It is now possible to take these constraints into account in optimization algorithms. This development makes QOC more useful in\nterms of experimental applications and helps to bridge the gap between control theory and control\nexperiments. Such a project has been successfully carried out on various platforms such as superconducting circuits, NV centers, magnetic resonance and trapped atoms, ions and molecules~\\cite{kochroadmap}. The experimental implementation of optimal control protocols is highly dependent on the system under study, and we cannot cover all possible situations here. In this paper, we focus on a specific example for which we provide numerical solutions, namely the control of a Bose-Einstein Condensate (BEC) in a one-dimensional optical lattice~\\cite{RMP1999,RMP2008,gross2017}. This system has attracted a lot of interest in recent years from a control point of view~\\cite{Hohenester2007,Jager2014,Sorensen2018,bason2012,zhou2018,Weidner2018,BEC2021,BEC2023,BEC2024,frank2016}, especially for quantum simulation applications~\\cite{altman,noriRMP,cirac2012}, for which QOC can be  used to efficiently prepare the initial state of the system~\\cite{dupont2023}. It allows us to illustrate the implementation of optimal control on an infinite-size system, as well as to emulate a two-level system. We discuss the different steps of the experimental implementation from the modeling of the system dynamics, to the experimental constraints on the control design and the measurement of the final state of the system. The impact of various experimental limitations is highlighted and discussed. Experimental results based on optimal control protocols are presented.\n\nThis tutorial  paper is organized as follows. In Sec.~\\ref{chap:optimal control theory}, we introduce the optimal control theory based on the PMP. The analogy with classical Lagrangian and Hamiltonian systems is used to describe the basic concepts of this theory. In Sec.~\\ref{sec:QOC}, we show how such results can be adapted to quantum systems. The time-optimal control of a two-level quantum system is used as an illustrative example. The connection between QOC and QSL in this system is discussed. In Sec.~\\ref{sec:numerical_methods}, a detailed introduction to numerical methods is given with particular emphasis on two different  approaches, namely the shooting method and gradient-based optimization algorithms which are also illustrated in a two-level quantum system. Pseudo-codes describing the structure of the algorithms are provided in the main text, and Python codes for specific control processes can be found in the supplementary material. These numerical approaches are used in Sec.~\\ref{sectheoexp} to manipulate a BEC in an optical lattice. A complete description of an experimental implementation is given as well as the constraints and limitations of the experimental apparatus used. A number of examples in classical and quantum physics are given throughout the text. The simplest ones are placed in a gray box and can be ignored by a reader already experienced in optimal control. A conclusion is given and prospective views are suggested in Sec.~\\ref{sec:conclusion}.~\\ref{secappA} gives a list of mathematical symbols and acronyms used in the article. \\ref{app_lagrange} and~\\ref{appendixPMP} contain mathematical results used in the main text. A description of the numerical codes of the supplementary material is also provided in~\\ref{app_code}.\n\n\n\n\n\n\n\n\n\n\\section{Optimal Control Theory and Pontryagin Maximum Principle}\n\\label{chap:optimal control theory}\n\n\\subsection{Introduction}\n\nOptimal Control Theory (OCT) has its roots in the calculus of variations which is over 300 years old~\\cite{Courant_Hilber_vol_1,Courant_Hilber_vol_2,gelfand2000calculus}.\nInterest in this branch of mathematics grew rapidly with the advent of computer science in the early 1960s. A rigorous mathematical framework for OCT was given by L. Pontryagin and his co-workers in 1960 with the introduction of the Pontryagin Maximum Principle (PMP)~\\cite{pontryaginbook,leemarkusbook,bryson1975applied,kirk2004optimal,liberzon-book}, which then led to a variety of applications~\\cite{bryson1996optimal}. In particular, OCT was at the origin of optimal trajectory prediction in aeronautics~\\cite{bonnard_optimal_2012,trelat2012optimal}. Today, OCT is used in a wide range of fields ranging from economics, to physics and electronics, to name but a few. The PMP transforms the optimal control problem into a\ngeneralized Hamiltonian system subject to a maximization condition and some boundary conditions. In this framework,\nthe goal is to find the Hamiltonian trajectory that reaches the target state, while minimizing the cost functional which\ndefines the optimization procedure. A key advantage of the PMP is that it reduces the initial infinite-dimensional control\nlandscape to a finite low-dimensional space. This brief description also shows that optimal control is closely related  to classical mechanics and its Lagrangian or Hamiltonian formalism.\n\nBefore we present the PMP at the end of this section, let us take a step back to basics. As the above brief description of optimal control shows, a first observation is that optimal control problems are very similar to those in classical mechanics~\\cite{goldsteinbook,arnoldbook}, where the goal is to find the trajectory of a classical system that minimizes a certain quantity, the action.\nThe same direction is followed in OCT, but instead of the usual action, other quantities relevant to the control procedure are minimized. Another important modification is the presence of a time-dependent parameter in the dynamical equations which can be shaped to some extent by an external operator. In the case of an aeroplane, for example, the control parameters include all possible actions that can be performed on the aircraft, such as modifying the engine thrust or changing the orientation of elevators and ailerons. In quantum physics, the control agent is generally a shaped electromagnetic field. The formal concepts are illustrated in this section using a simple example, namely a point particle controlled by a time-dependent force. We begin by recalling some key elements of the calculus of variations applied to a point particle.\n\\begin{example}{}{ex1}\n A basic idea of the calculus of variation and of the principle of least action is to derive the equation of motion of a physical system from a single quantity, the action~\\cite{goldsteinbook}. Mathematically, the latter takes the form of a functional~\\cite{gelfand2000calculus} i.e. a function of functions. In the case of a free one-dimensional particle, the action $S$ can be expressed as\n\n\\begin{equation}\nS[x]= \\int_0^{t_f} \\frac{1}{2} m \\dot x^2(t)~dt,\n\\end{equation}\n\nwhere $x(t)\\in \\setR$ is the position of the particle at time $t$ with $t\\in [0,t_f]$, $t_f$ the duration of the dynamics, $m$ its mass, and the dot symbolizes the time derivative. The physical motion is associated with the least action $S$. From the condition $\\delta S=0$ where $\\delta S$ is the functional derivative of $S$ taken for two trajectories close to each other~\\cite{goldsteinbook}, it can be shown that the physical trajectory satisfies the Euler-Lagrange equation\n\n\\begin{equation}\n\\frac{\\partial \\Mc L}{\\partial x} -\\frac{d}{dt} \\frac{\\partial \\Mc L}{\\partial \\dot x} = 0,\n\\label{eq:Euler_Lagrange_point_particle}\n\\end{equation}\n\nwhere $\\Mc L = m\\dot x^2/2$ is the Lagrangian, here equal to the kinetic energy of the particle. An explicit calculation leads to the expected result $m \\Ddot x(t) = 0$.\n\nThe model system can be extended by introducing a control $f(t)\\in\\setR$, which takes the form of a  force applied to the particle. With this modification, the action becomes\n\n\\begin{equation}\nS[x]= \\int_0^{t_f} \\left( \\frac{1}{2} m \\dot x^2 (t) + f(t) x(t) \\right)~dt,\n\\end{equation}\nwhere the extra term $-f(t)x(t)$ is a potential energy. The Lagrangian is then defined as the difference between the kinetic energy and the potential energy. The equation of motion, calculated using Eq.~\\eqref{eq:Euler_Lagrange_point_particle} with the new Lagrangian is\n$m \\Ddot x(t) = f(t)$. The latter can be determined from Newton's law, but also from the Hamiltonian formalism, in which the Lagrangian is replaced by the Hamiltonian $H$. The Hamiltonian is a function on the phase space i.e. position and momentum ($\\Gamma = \\setR^2$ for a point particle), rather than a function of position and velocity for the Lagrangian. The phase space can be constructed by defining a conjugate variable, the momentum, as $p = \\tfrac{\\partial \\Mc L}{\\partial \\dot x}$, and $H$ can be expressed as $H= p \\dot x - \\Mc L$~\\cite{goldsteinbook}. This change of variables is called a Legendre transformation. The equations of motion are given by Hamilton's first-order differential equations\n\n\\begin{equation}\n\\dot x=\\frac{\\partial H}{\\partial p},~\\dot p=-\\frac{\\partial H}{\\partial x},\n\\label{eq:Hamilton_equation_point_particle}\n\\end{equation}\nwhich allow us to recover the system dynamics. In the case of a point particle, we have $p = m \\dot x$ and the Hamiltonian is $H = \\tfrac{p^2}{2m} - f x$, which leads from Eq.~\\eqref{eq:Hamilton_equation_point_particle} to $\\dot p = f$ and $m\\dot x = p$, and finally to $m\\Ddot x = f$ as expected.\n\nNow suppose that a particular time-dependent force can be generated. It is clear that the dynamics of the system are modified, the corresponding trajectory being solution of the equation $m\\Ddot x(t)=f(t)$. The particle is then steered from the point $x(0)$ to $x(t_f)$. This example can be reformulated in terms of optimal control. The idea is to reverse the procedure by fixing the initial and final states of the system a priori and by designing the corresponding control. Since this problem can have a very large number of solutions, a specific control is selected based on an additional criterion as described below.\n\\end{example}\n\nAn optimal control problem is usually defined as follows. The first step is to introduce the system to be controlled, whose state $X(t) = \\{ X_a(t)\\}_{a=1,\\cdots,n}$ is a real vector, $X(t)\\in \\setR^n$ with coordinates $X_a$. We assume that the system dynamics are described by a real first-order differential equation of the form\n\n\\begin{equation}\\label{eqcontrol}\n\\dot X(t) = F (X(t),u(t),t),\n\\end{equation}\n\nas provided e.g. by Eq.~\\eqref{eq:Hamilton_equation_point_particle}, where $u(t) \\in U \\subset \\setR^m$ is the control with $m$ real components and $F = \\{ F_a\\}_{a=1,\\cdots,n}$ is a vector function that should be smooth enough. The set $U$ corresponds to the admissible values of the control and is determined by the operator. This choice is usually dictated by the experimental limitations of the device or by the hypotheses used to derive the model system. Note that $U=\\mathbb{R}^m$ if there is no specific constraint. A standard example for a one-dimensional control parameter is $U=[u_{\\min},u_{\\max}]$ where $u_{\\min}$ and $u_{\\max}$ are respectively the minimum and maximum of allowed control values.\nA solution to Eq.~\\eqref{eqcontrol} is well-defined from a mathematical point of view if the control $u$ belongs to a particular set of functions called the \\textbf{\\textit{admissible}} controls. In many cases, continuous or piecewise continuous functions are sufficient to guarantee the existence of a solution, but not of the optimal control, as discussed in~\\cite{PRXQuantumsugny}. We also emphasize that there are problems for which the optimal control is not a piecewise continuous function but presents for instance a chattering process, characterized by an infinite number of switchings between two extreme values in a finite time interval~\\cite{schaettler-book,fuller}. Note that this phenomenon can also be observed in QOC as recently shown in~\\cite{Robin2022}.\n\n\n\nThe optimal control protocol can be found by introducing a cost functional $\\mathcal{C}$ that should be minimized. Note that a maximization can also be considered by minimizing $-\\mathcal{C}$. The functional $\\mathcal{C}$ can be expressed as the sum of a terminal cost $G$ and a running cost $F_0$ depending respectively on the final state and the trajectory followed by the system:\n\n\\begin{equation}\n\\mathcal{C} = G(X(t_f),t_f) + \\int_0^{t_f} F_0 (X(t),u(t),t)dt ,\n\\label{eq:def_cost_fun}\n\\end{equation}\nwhere $t_f$ is the control time, which can be fixed or free. In standard applications, the terminal cost $G$ can describe the distance to the target state, while the second term can penalize either the control time or the energy of the control. Such running costs correspond to $F_0=1$ and $F_0=\\frac{1}{2}u^2(t)$, respectively, up to a constant factor.\n\\begin{example}{}{ex2}\nWe consider the problem of steering a one-dimensional point particle with minimum energy consumption between the points $x=0$ and $x=1$ in a given time $t_f$, such that the initial and final velocities are zero. The equations of motion in the phase space $\\mathbb{R}^2$ are\n\\begin{equation}\n\\frac{d}{dt} \\left(\\begin{array}{c}\nx \\\\\np\n\\end{array} \\right) = \\left( \\begin{array}{c}\np/m \\\\\nf\n\\end{array} \\right),\n\\end{equation}\nwith initial $(0,0)$ and final $(1,0)$ states respectively. The cost functional associated with this optimal control problem is chosen to be\n\n\\begin{equation}\n\\mathcal{C} = (x(t_f)-1)^2+p(t_f)^2+ \\int_0^{t_f} \\frac{1}{2}\\alpha f^2(t) ~dt,\n\\end{equation}\nwhere $\\alpha$ is a constant factor such that the second term has an energy dimension. Note that $\\alpha$ can also be used to weight the relative importance of the two terms in the cost functional. The minimum of $\\mathcal{C}$ corresponds to a compromise between the distance of the final state to the target and the energy consumed along the trajectory to reach the final state.\n\\end{example}\n\n\n\nIn summary, the  task in an optimal control problem is to find the control $u^*$ that minimizes the cost functional $\\mathcal{C}$ under the constraint that $\\dot X=F(X,u,t)$. It can be mathematically described as an infinite dimensional constrained optimization problem since all amplitudes $u(t)$ in a continuous time interval are optimized. As in a finite dimensional constrained optimization problem, the main difficulty comes from the condition~\\eqref{eqcontrol} to be satisfied at any time $t$. The functional $\\mathcal{C}$ cannot be minimized directly due to this additional constraint. The idea is then to increase the dimension of the state of the system to obtain an unconstrained optimization problem~\\cite{bryson1975applied,ito2008lagrange,contreras_dynamic_2017}. An extended space is defined by doubling the number of variables of the system state. The adjoint state $\\Lambda (t)\\in\\setR^n$ is introduced and the new state can be written as $(X,\\Lambda,u)\\in\\setR^{2n+m}$. The state $\\Lambda(t)$ plays the same role as Lagrange multipliers in a finite dimensional problem, except that the static constraints and a finite number of Lagrange multipliers are respectively replaced by a dynamical constraint~\\eqref{eqcontrol} and a time-dependent function. We refer the reader to~\\ref{app_lagrange} for details on this approach.\n\nWe then consider the action associated with the optimal control problem:\n\n\\begin{definition}{Action of the optimal control problem}{Action of the optimal control problem}\nLet $X(t) \\in \\setR^n$ be the state of a physical system at time $t$, $\\Lambda(t)\\in \\setR^n$ its adjoint state, and $u(t) \\in U \\subset \\setR^m$ a control. Let $G$ be a terminal cost function, $F_0$ a running cost, and $F$ a vector function describing the system dynamics. The optimal control action is defined as\n\\begin{equation}\\label{eq_def_action}\nS = G(X(t_f),t_f) + \\int_0^{t_f} dt \\underbrace{\\left[ F_0 (X(t),u(t),t) +\\Lambda(t)\\cdot \\left(\\dot X(t)-F(X(t),u(t),t)\\right)\\right]}_{\\Mc L (X,\\dot{X},u,t,\\Lambda)}.\n\\end{equation}\nThe function in the integral is the Lagrangian $\\Mc L$ of the optimal control problem.\n\\end{definition}\nFrom this definition, it is straightforward to deduce that $\\Lambda$ is the conjugate coordinate of $X$ using\n$$\n\\frac{\\partial \\Mc L}{\\partial \\dot X} = \\Lambda.\n$$\n\n\n\\begin{example}{}{ex3}\nA direct application of Def.~\\ref{def:Action of the optimal control problem} to the system defined in Example~\\ref{example:ex2} leads us to the following Lagrangian:\n\n\\begin{equation}\n\\Mc L = \\frac{1}{2}\\alpha f^2 + \\Lambda_x( \\dot x - p/m) + \\Lambda_p(\\dot p -f),\n\\end{equation}\n\nwhere $\\Lambda_x$ and $\\Lambda_p$ are respectively the adjoint states of $x$ and $p$, $\\Lambda=(\\Lambda_x,\\Lambda_p)$.\n\\end{example}\n\n\n\n\n\\subsection{First-order variation and Pontryagin Maximum Principle}\n\\label{sec:first order variation}\n\n\\subsubsection{Lagrangian formulation.}\n\\label{sec:Lagrangian approach}\n\nThe action introduced in Def.~\\ref{def:Action of the optimal control problem} is very similar to the action of a usual classical system and it contains all the information needed to find the solutions of the optimal control problem. We use the same approach as in classical mechanics by calculating the conditions that the extremals of $S$ must fulfill when considering a small variation of the control~\\cite{Courant_Hilber_vol_1,Courant_Hilber_vol_2,gelfand2000calculus}. The analysis is more demanding than in classical mechanics because the set $U$ can be closed as in the case $U=[u_{\\min},u_{\\max}]$. One must be careful at the boundary of $U$, because it leads to extremals that are not specified by functional derivatives of $S$. This is similar to the case of a function $f$ defined on a closed interval which has extrema on the boundary of the interval that are not given by a zero of its derivative.\n\nHere, we examine a system with unconstrained controls. The case of a closed set is discussed below with the Hamiltonian formalism. Extremals are characterized by the condition $\\delta S=0$ for small variations $\\delta X$, $\\delta \\Lambda$, and $\\delta u$ which are assumed to be independent. These variations are chosen to be as general as possible, and they are not a priori restricted to satisty $\\dot X = F$.\n\nThe functional derivative of $S$ with respect to the three variables, $X$, $\\Lambda$ and $u$ is\n$$\n\\delta S=\\frac{\\partial G}{\\partial X(t_f)}\\delta X(t_f)+\\int_0^{t_f}\\left[\\frac{\\partial F_0}{\\partial X}\\delta X+\\frac{\\partial F_0}{\\partial u}\\delta u\n+\\delta \\Lambda\\cdot (\\dot{X}-F)+\\Lambda\\cdot\\left(\\delta \\dot{X}-\\frac{\\partial F}{\\partial X}\\delta X-\\frac{\\partial F}{\\partial u}\\delta u\\right)\\right].\n$$\nIntegrating by part the term $\\Lambda\\cdot\\delta \\dot{X}$, we obtain\n\\begin{align}\n&\\delta S=\\left(\\frac{\\partial G}{\\partial X(t_f)}+\\Lambda(t_f)\\right)\\delta X(t_f)+\\Lambda(0)\\delta X(0)\\nonumber \\\\\n& +\\int_0^{t_f}dt\\left(\\left[\\frac{\\partial F_0}{\\partial X}-\\dot{\\Lambda}-\\Lambda\\frac{\\partial F}{\\partial X}\\right]\\delta X+\\left[\\dot{X}-F\\right]\\delta \\Lambda+\n\\left[\\frac{\\partial F_0}{\\partial u}-\\Lambda\\frac{\\partial F}{\\partial u}\\right]\\delta u \\right).\\nonumber\n\\end{align}\nNext, we deduce that the extremals of $S$ fulfill the following conditions\n\\begin{align}\n& \\dot{\\Lambda}=\\frac{\\partial F_0}{\\partial X}-\\Lambda\\frac{\\partial F}{\\partial X},~\\Lambda(t_f)=-\\frac{\\partial G}{\\partial X(t_f)},\\nonumber \\\\\n& \\dot{X}=F,~X(0)=X_0,\\nonumber \\\\\n& \\Lambda\\frac{\\partial F}{\\partial u}-\\frac{\\partial F_0}{\\partial u}=0.\\nonumber\n\\end{align}\nThe initial condition on $X$ comes from the fact that $\\delta X(0)=0$, i.e. we consider trajectories with the same initial state.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\noindent The results are summarized in the following theorem.\n\n\\begin{theorem}{Extremals of the action}{Extremals of the action}\n\\label{th:extremal}\nExtremals of the action of the optimal control problem  at a fixed final time with the initial state $X(0)=X_0$ satisfy the following conditions:\n\n\\begin{align}\n\\label{eq_S_p}\n& \\dot X_a (t)  = F_a (t) ~\\textrm{(Euler-Lagrange equation for $\\Lambda$)},\\\\\n\\label{eq_S_x}\n& \\dot \\Lambda_a (t)  = \\frac{\\partial F_0}{\\partial X_a(t)} - \\Lambda(t)\\cdot \\frac{\\partial F}{\\partial X_a(t)}~\\textrm{(Euler-Lagrange equation for $X$)},\\\\\n& \\Lambda_a (t_f) = -\\frac{\\partial G}{\\partial X_a(t_f)}~\\textrm{(Boundary condition for $X$)},\\\\\n\\label{eq_S_u}\n& \\frac{\\partial F_0}{\\partial u_a}  = \\Lambda(t)\\cdot \\frac{\\partial F}{\\partial u_a(t)}~\\textrm{(Euler-Lagrange equation for $u$)}.\n\\end{align}\n\\end{theorem}\n\nSeveral comments on these results can be made. Equation~\\eqref{eq_S_p} is the equation of motion, and thus, any solution of $\\delta S =0$ must correspond to a physical trajectory of the system. Equation~\\eqref{eq_S_x} is derived from the Euler-Lagrange equation $\\frac{d}{dt}\\frac{\\partial \\mathcal{L}}{\\partial \\dot{X}}-\\frac{\\partial \\mathcal{L}}{\\partial X}=0$ for $X$. It returns a differential equation for the dynamics of $\\Lambda(t)$, whose value at final time is given by the gradient of the terminal cost $G$ over the final state $X(t_f)$. Equation~\\eqref{eq_S_u} leads to a strong condition for the control $u$. As can be seen in the examples, Eq.~\\eqref{eq_S_u} can often be used to express the control as a function of $X$ and $\\Lambda$, i.e. $u(t) = u(X(t),\\Lambda(t))$. The control is then completely determined by the dynamics of the state and its adjoint state. Using the $n$ initial conditions of $X$ and the $n$ final conditions of $\\Lambda$ parameterizing $X(t)$ and $\\Lambda(t)$, we obtain that the optimal control is a function of a finite number of parameters, transforming thus an infinite-dimensional optimization problem into a finite one.\n\nWe emphasize that identities of Th.~\\ref{thm:Extremals of the action} are satisfied by all the extremals of the action. It is therefore only a necessary optimality condition that selects trajectory candidates to be optimal.\nIn practice, it is straightforward to establish equations of Th.~\\ref{thm:Extremals of the action} which can be expressed as a set of non-linear coupled differential equations with two-side boundary conditions. The difficult task is to find the solutions of such equations. This can be done analytically in the simplest cases, otherwise numerically. Some examples of applications to quantum systems are given in Sec.~\\ref{sec:QOC}.\n\n\\begin{example}{}{ex4}\n\nTheorem~\\ref{thm:Extremals of the action} can be applied directly  to the optimal control Lagrangian $\\Mc L =\\frac{1}{2}\\alpha f^2 + \\Lambda_x( \\dot x - p/m) + \\Lambda_p(\\dot p -f)$ of Example~\\ref{example:ex3}. The derivatives of $\\Mc L$ with respect to the states $(x,p)$ and the adjoint states $(\\Lambda_x,\\Lambda_p)$ give the equations of motion\n\n\\begin{equation}\n\\begin{split}\n& \\dot x = p/m, \\\\\n& \\dot  p = f, \\\\\n& \\dot \\Lambda_x = 0, \\\\\n& \\dot \\Lambda_p = -\\frac{\\Lambda_x}{m},\n\\end{split}\n\\end{equation}\nwhile control can be obtained from the derivative of $\\Mc L$ with respect to the control $f$\n$$\nf=\\frac{\\Lambda_p}{\\alpha}.\n$$\nBoundary conditions can also be deduced using the terminal cost $G=(x(t_f)-1)^2+p(t_f)^2$, introduced in Example~\\ref{example:ex2}. We obtain:\n\\begin{align*}\n    \\Lambda_x(t_f)& = - 2(x(t_f)-1),\\\\\n    \\Lambda_p(t_f) &= - 2p(t_f).\n\\end{align*}\nFor an optimal trajectory reaching exactly the target, we have $x(t_f)=1$ and $p(t_f)=0$, and thus $\\Lambda_x(t_f) = \\Lambda_p(t_f)=0$.\n\n\\end{example}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsubsection{Hamiltonian formulation.}\n\nAs in classical mechanics~\\cite{goldsteinbook}, we can adopt either a Lagrangian or a Hamiltonian formalism to derive the equations of motion. The Hamiltonian structure can be derived from the optimal control action introduced in Def.~\\ref{def:Action of the optimal control problem}. Since $\\Lambda$ is the conjugate momentum of $X$, the Hamiltonian $H_P$, called the Pontryagin Hamiltonian, can be defined as $H_P = \\Lambda\\cdot \\dot X - \\Mc L = \\Lambda\\cdot  F - F_0$. When $X$ and $\\Lambda$ satisfy the extremal equations, the functional derivative of $S$ given by\n$$\n\\delta S=\\int_0^{t_f}\\left(\\frac{\\partial F_0}{\\partial u}-\\Lambda\\cdot \\frac{\\partial F}{\\partial u}\\right)\\cdot \\delta u(t) dt\n$$\ncan be then written as\n\\begin{equation}\\label{eqSPont}\n\\delta S=-\\int_0^{t_f}\\frac{\\partial H_P}{\\partial u}\\cdot \\delta u(t) dt .\n\\end{equation}\nEquation~\\eqref{eqSPont} allows us, for an open set $U$, to find the extremal condition for the control, $\\frac{\\partial H_P}{\\partial u}=0$. Here only the \\textbf{\\textit{normal}} extremals are obtained. Under certain conditions, control processes that do not depend on the running cost $F_0$ can also be solutions of the optimal control problem. Such extremals are called \\textbf{\\textit{abnormal}}. To take them into account, a negative constant $\\Lambda_0\\leq 0$ is added to the definition of the Hamiltonian $H_P$ which is given in its final form as follows~\\cite{bonnard_optimal_2012,PRXQuantumsugny,kirk2004optimal}.\n\n\\begin{definition}{Pontryagin Hamiltonian}{Pontryagin Hamiltonian}\nThe Pontryagin Hamiltonian $H_P$ is given by:\n\\begin{equation}\nH_P = \\Lambda\\cdot F+ \\Lambda_0 F_0.\n\\label{eq:def_Hamiltonian_OC}\n\\end{equation}\n\\end{definition}\nFor $\\Lambda_0<0$, we find the previous definition of $H_P$ by noting that $\\Lambda_0$ can be normalized to -1 without loss of generality. The Pontraygin Hamiltonian does not depend on $F_0$ when $\\Lambda_0=0$, which leads to the family of abnormal solutions. We stress that abnormal solutions also occur in finite-dimensional optimization problems as a special case of Lagrange multipliers. This point is discussed in~\\ref{app_lagrange} which shows that abnormal extremals are not restricted to optimal control problems. An example of abnormal control is described in Example~\\ref{example:ex8}.\n\\begin{example}{}{ex5}\nThe Pontryagin Hamiltonian for the optimal control Lagrangian $\\Mc L =\\frac{1}{2}\\alpha f^2 + \\Lambda_x( \\dot x - p/m) + \\Lambda_p(\\dot p -f)$ can be written as\n\n\\begin{equation}\nH_P = \\Lambda_x\\frac{p}{m} + \\Lambda_p f +\\frac{\\Lambda_0}{2}\\alpha f^2,\n\\end{equation}\nwhere $\\Lambda_0$ is a negative constant. In the abnormal case, the Pontryagin Hamiltonian is given by\n$$\nH_P=\\Lambda_x\\frac{p}{m} + \\Lambda_p f.\n$$\n\\end{example}\nOptimal trajectories are given by Hamilton's equations, once again highlighting the link between classical mechanics and optimal control. They can be written as:\n\n\\begin{equation}\n\\begin{split}\n& \\dot \\Lambda_a = -\\frac{\\partial H_P}{\\partial X_a},  \\\\\n& \\dot X_a = \\frac{\\partial H_P}{\\partial \\Lambda_a},  \\\\\n& \\frac{\\partial H_P}{\\partial u_a} =0,\n\\end{split}\n\\label{eq:weak_PMP}\n\\end{equation}\n\nA straightforward calculation reveals that these equations are equivalent to dynamical equations of Th.~\\ref{thm:Extremals of the action}. Equations~\\eqref{eq:weak_PMP} that can be used with unconstrained control ($U$ is an open set) correspond to \\textit{\"the weak Pontryagin principle\"}. The theory can be extended to consider the general case. This leads to the \\textit{Pontryagin Maximum Principle} (PMP) which can be stated as follows~\\cite{bonnard_optimal_2012,PRXQuantumsugny,pontryaginbook,bryson1975applied,\nkirk2004optimal,trelat2012optimal,liberzon-book}.\n\n\\begin{theorem}{Pontryagin Maximum Principle}{Pontryagin Maximum Principle}\nWe consider the dynamical system defined by\n$$\n\\dot X(t) = F(X(t),u(t)) ,\n$$\nwhere $F$ is a smooth vector function and  $u:[0,t_f]\\rightarrow U \\subset \\setR^m$ the control. The goal of the control protocol is to steer the system from $X_0$ to $X_f$ at time $t_f$ which is fixed or free. The optimal control problem $u^\\star$ is defined from the cost functional $\\mathcal{C}$ to minimize\n$$\n\\mathcal{C}= G(X(t_f)) + \\int_0^{t_f} F_0 (X(t'),u(t'))dt',\n$$\nwhere $F_0$ and $G$ are two smooth functions. The Pontryagin Hamiltonian $H_P$ is defined as\n$$\nH_P(X,\\Lambda,\\Lambda_0,u) = \\Lambda\\cdot F(X,u)+ \\Lambda_0F_0(X,u),\n$$\nwhere $\\Lambda(t) \\in \\setR^n$ is the adjoint state, and the abnormal multiplier $\\Lambda_0\\leq 0$ a constant. The pair $(X,u^\\star)$ is optimal if there exists a non zero continuous pair $(\\Lambda,\\Lambda_0)$ such that the trajectories of the extended system are given by Hamilton's equations $\\dot \\Lambda = -\\partial_{X} H_P$ and $\\dot X = \\partial_{\\Lambda}H_P$, and the maximization condition can be written almost everywhere on $[0,t_f]$  as:\n\\begin{equation}\\label{eqmaxPMP}\nH_P (X,\\Lambda,\\Lambda_0,u^\\star) = \\max_{u \\in U} H_P (X,\\Lambda,\\Lambda_0,u).\n\\end{equation}\nThe state and adjoint state satisfy respectively the initial and final conditions\n$$\nX(0)=X_0,~\\Lambda(t_f)=\\Lambda_0\\frac{\\partial G(X(t_f)}{\\partial X(t_f)}.\n$$\n\\end{theorem}\nSome mathematical details and a geometric interpretation of the PMP can be found in~\\ref{appendixPMP}. We observe that the first-order condition $\\frac{\\partial H_P}{\\partial u}=0$ is replaced by a stronger maximization condition~\\eqref{eqmaxPMP} of the Pontryagin Hamiltonian along the optimal trajectory. This modification is crucial to treat the case of a closed set $U$ such as $U=[u_{\\textrm{min}},u_{\\textrm{max}}]$ since the maximum of $H_P$ can be defined on an open or a closed set. A schematic description of this maximization is given in Fig.~\\ref{fignew}.\n\\begin{figure}[htp]\n\\begin{center}\n\\includegraphics[width=7.5cm]{fig2.pdf}\n\\end{center}\n\\caption{Schematic plot of the Pontryagin Hamiltonian $H_P$ as a function of the control $u$ in the interval $U=[u_{\\textrm{min}},u_{\\textrm{max}}]$ represented by the vertical dashed lines. The dots indicate the position of the extremal values of $H_P$. In this example, the global maximum of $H_P$ lies on the edge of $U$, while a local maximum lies inside $U$.}\n\\label{fignew}\n\\end{figure}\n\nSome additional comments and extensions can be made on Theorem~\\ref{thm:Pontryagin Maximum Principle}. The Hamiltonian $H_P$ is generally called a \\textbf{\\textit{pseudo-Hamiltonian}} because it depends on the control $u$. It can be shown that the Hamiltonian $H_P$ is constant in time and it is constantly zero if $t_f$ is free. A brief description of this condition is given in~\\ref{appendixPMP}. A solution $X$ of this optimal control problem is called an extremal trajectory and is candidate to be optimal. This means that the PMP is only a necessary condition for optimality and several extremal trajectories may exist. Additional work is required to select the optimal solution among such trajectories. As in a finite-dimensional optimization problem (see~\\ref{app_lagrange} for details), two different sets of trajectories can be extremals, namely the \\textbf{\\textit{normal}} and the \\textbf{\\textit{abnormal}}, respectively for $\\Lambda_0< 0$ and $\\Lambda_0=0$. The pair $(\\Lambda,\\Lambda_0)$ is defined up to a constant factor, but cannot simultaneously be equal to 0. This degree of freedom comes from the fact that $H_P$ and the adjoint states are abstract quantities which have no physical meaning. Multiplying $(\\Lambda,\\Lambda_0)$ by a constant factor amounts to multiplying $H_P$ by the same factor without changing the optimal control problem. This allows in the normal case to normalize the constant $\\Lambda_0$ to a specific value such as $-1/2$ or $-1$. The target state here is only a point, but a set of points or a subset of $\\mathbb{R}^n$ can be chosen as target. The final condition for the adjoint state must then be adapted. It is also possible to consider the trajectories that reach exactly the state $X_f$ at time $t_f$. In this case, the final conditions are satisfied by the state as $X(t_f)=X_f$ and not by the final adjoint state $\\Lambda(t_f)$. In a general situation, the optimal solutions of the Hamiltonian's equations of the PMP are defined from $2n$ conditions given at initial or final times on the state or the adjoint state.\n\nBased on Th.~\\ref{thm:Pontryagin Maximum Principle}, a systematic way to solve the optimal equations given by the PMP can be formulated. The different steps must be followed for both normal and abnormal extremals.\nThe first objective is to use the maximization condition to express the control in terms of the state and adjoint state as $u=v(X,\\Lambda)$. If this is possible, the control is said to be \\textbf{\\textit{regular}}, otherwise it is \\textbf{\\textit{singular}}. Note that the two situations can be mixed for a given trajectory in the sense that the control can be regular at some times and singular at others. Different quantum optimal control problems in which singular extremals are optimal have been found in the literature~\\cite{PRXQuantumsugny,lapert2010singular,Lapert_exploring_2012}. In the regular situation, the second step is to insert the expression of the control into the Hamiltonian's equation. If the function $v$ is smooth, we get a well-defined Hamiltonian system as in classical mechanics except that the desired trajectory is defined by two-side boundary conditions on $X(0)$ and $\\Lambda(t_f)$. A straightforward way to solve this problem is to use a shooting technique which consists of finding the initial value $\\Lambda(0)$ such that the final condition on the adjoint state at time $t_f$ is satisfied. This approach faces two main difficulties due to the possible complexity of the system dynamics. The first is related to the non uniqueness of the solution and the second to the potentially strong sensitivity to initial conditions in the case of chaotic dynamics. The latter is found especially in high-dimensional systems. This observation justifies the use of shooting techniques only for simple control problems of low dimension. Other numerical optimization methods have been developed to solve more complicated issues. Such optimization algorithms are described in Sec.~\\ref{sec:numerical_methods}.\n\nThe following examples illustrate different situations that can be encountered with the PMP.\n\n\\begin{example}{}{ex6}\n\nWe consider the control of a point particle in the energy minimum case from state $(0,0)$ to $(1,0)$ in a fixed time $t_f$. The cost functional to minimize is given by\n$$\n\\mathcal{C}=\\frac{1}{2}(x(t_f)-1)^2+\\frac{1}{2}p(t_f)^2+\\frac{\\alpha}{2}\\int_0^{t_f}f^2(t)dt.\n$$\nThe Pontryagin Hamiltonian can be expressed as\n$$\nH_P=\\Lambda_x\\frac{p}{m}+\\Lambda_p f-\\frac{\\alpha f^2}{2},\n$$\nwhere $\\Lambda_0$ has been set to -1 (the abnormal case plays no role in this problem). Hamilton's equations can then be written as $\\dot x = p/m$, $\\dot p= f$, $\\dot \\Lambda_x =0$, $\\dot \\Lambda_p = - \\Lambda_x/m$, and the maximization condition reads $f=\\Lambda_p/\\alpha$. The extremal trajectories are therefore regular. Note that we find the same expressions as in Example~\\ref{example:ex4}.\n\nPlugging the expression of $f$ into $H_P$, a true Hamiltonian $H$ is obtained\n$$\nH=\\Lambda_x\\frac{p}{m}+\\frac{\\Lambda_p^2}{2\\alpha},\n$$\nand the optimal trajectories are given by the Hamiltonian's equations derived from $H$. Since $\\Lambda_x$ is a constant of motion, it is straightforward to show that\n$$\nf(t)=\\frac{\\Lambda_p(t)}{\\alpha}=-\\frac{\\Lambda_x}{\\alpha m}t+\\frac{\\Lambda_p(0)}{\\alpha},\n$$\nand the system dynamics read\n\\begin{equation}\n\\begin{split}\np(t)&=-\\frac{\\Lambda_x}{2\\alpha m}t^2 +  \\frac{\\Lambda_p(0)}{\\alpha} t, \\\\\nx(t)&=-\\frac{\\Lambda_x}{6\\alpha m^2}t^3+\\frac{\\Lambda_p(0)}{2\\alpha m}t^2,\n\\end{split}\n\\end{equation}\nwhere we use the initial state $(x(0),p(0))=(0,0)$. The optimal trajectory is derived from the final condition $\\Lambda_x(t_f)=1-x(t_f)$ and $\\Lambda_p(t_f)=-p(t_f)$. A linear system of equations then allows to find $\\Lambda_x$ and $\\Lambda_p(0)$\n\\begin{equation}\n\\begin{split}\n& \\left(1-\\frac{t_f^3}{6\\alpha m^2}\\right)\\Lambda_x+\\frac{t_f^2}{2\\alpha m}\\Lambda_p(0)=1, \\\\\n& \\left(-\\frac{t_f^2}{2\\alpha m}-\\frac{t_f}{m}\\right)\\Lambda_x+\\left(\\frac{t_f}{\\alpha}+1\\right)\\Lambda_p(0)=0.\n\\end{split}\n\\end{equation}\nOptimal trajectories are represented in Fig.~\\ref{fig3} for different values of the parameters.\n\\end{example}\n\n\\begin{figure}[htbp]\n\\begin{center}\n\\includegraphics[width=7.5cm]{fig3a.pdf}\n\\includegraphics[width=7.5cm]{fig3b.pdf}\n\\end{center}\n\\caption{Plot of the optimal trajectories in the space $(x,p)$ and of the corresponding control $f(t)$ for $\\alpha=0.25$ (black) and $\\alpha=1$ (red). Numerical parameters are set to $t_f=10$ and $m=1$.}\n\\label{fig3}\n\\end{figure}\n\n\n\n\n\\begin{example}{}{ex7}\nWe consider the same control problem but in minimum time. To simplify the description of the optimal solution, we exchange the roles of the initial and target states which in this case are respectively $(1,0)$ and $(0,0)$. There is no constraint on the control energy, only on its amplitude, $f(t)\\in [-f_0,f_0]$ where $f_0$ is the maximum force allowed. For time-optimal problems, it is often preferable to reach the target exactly, the cost functional is defined from a running cost as $\\mathcal{C}=\\int_0^{t_f}dt=t_f$. Again, the abnormal extremals are not relevant and the parameter $\\Lambda_0$ is normalized to -1. The Pontryagin Hamiltonian can be written as\n$$\nH_P=\\Lambda_x\\frac{p}{m}+\\Lambda_pf-1,\n$$\nand Hamilton's equations are the same as in the preceding example. Since the final time is free, the Pontryagin Hamiltonian is zero at any time $t$. We deduce that $\\Lambda_x\\frac{p}{m}+\\Lambda_pf=1$ is a constant along the optimal trajectory. In the open interval, the maximization condition $\\frac{\\partial H_P}{\\partial f}=0$ leads to $\\Lambda_p(t)=0$ on a non-zero time interval and thus to $\\dot{\\Lambda}_p(t)=0=-\\Lambda_x/m$. We deduce that the condition $\\Lambda_x\\frac{p}{m}+\\Lambda_pf=1$ cannot be satisfied and that the optimal control takes values on the boundary of the interval, i.e. $f(t)=\\pm f_0$. The sign of $f$ has to be chosen to maximize $H_P$ at any time $t$. The only Hamiltonian term depending on $f$ being $\\Lambda_p f$, we deduce that the optimal control can be expressed as $f(t)=\\textrm{sign}[\\Lambda_p]f_0$. The latter is a square wave  with switchings at times for which $\\Lambda_p(t)=0$. Such a solution is referred to as \\emph{bang-bang} in the control literature. The Hamiltonian's equations can be directly integrated which leads for a trajectory in the interval $[t_i,t]$ to\n\\begin{equation}\n\\begin{split}\nx(t)&=f\\frac{(t-t_i)^2}{2m}+\\frac{p(t_i)}{m}(t-t_i)+x(t_i), \\\\\np(t)&=f(t-t_i)+p(t_i),\\nonumber\n\\end{split}\n\\end{equation}\nwhere $f$ is a constant equal to $+f_0$ or $-f_0$. Note that $p$ is an increasing (resp. decreasing) function of time when $f=+f_0$ (resp. $f=-f_0$).\nWe also deduce that the optimal trajectories in the space $(x,p)$ are parabolas. For the adjoint state, we arrive at\n\\begin{equation}\n\\begin{split}\n\\Lambda_x(t)&=\\Lambda_x, \\\\\n\\Lambda_p(t)&=-\\frac{\\Lambda_x}{m}(t-t_i)+\\Lambda_p(t_i),\\nonumber\n\\end{split}\n\\end{equation}\nwhere $\\Lambda_x$ is a constant. $\\Lambda_p$ is a linear function of time with at most one zero. The optimal control therefore has at most one switching and the candidates to optimality are of the form:\\\\\n- $f(t)=+f_0$ for $t\\in[0,t_f]$,\\\\\n- $f(t)=-f_0$ for $t\\in[0,t_f]$,\\\\\n- $f(t)=+f_0$ for $t\\in[0,t_s[$ and $f(t)=-f_0$  for $t\\in ]t_s,t_f]$,\\\\\n- $f(t)=-f_0$ for $t\\in[0,t_s[$ and $f(t)=+f_0$  for $t\\in ]t_s,t_f]$,\\\\\nwhere $t_s$ is the switching time to be determined. The two bang solutions, i.e. the trajectories without switching, correspond to the arcs of parabolas passing through the target state $(0,0)$ as shown in Fig.~\\ref{fig4}. We denote this set of points by $\\mathcal{P}$. If the initial state belongs to $\\mathcal{P}$ then the time-optimal solution is a constant control equal to $+f_0$ when $x(0)>0$ and $-f_0$ otherwise. In the general case, two arcs must be concatenated. This is the situation of the example for which $(x(0),p(0))=(1,0)$. A first bang with $f=-f_0$ is used to reach $\\mathcal{P}$ and then $f=+f_0$ to attain the target state, as depicted in Fig.~\\ref{fig4}.\n\\end{example}\n\\begin{figure}[htbp]\n\\begin{center}\n\\includegraphics[width=7.5cm]{fig4.pdf}\n\\end{center}\n\\caption{\nPlot of the time-optimal trajectory to go from the state $(1,0)$ to $(0,0)$ represented as black dots. The optimal control is bang-bang with first a control equal to $-f_0$ and then to $+f_0$. The red curve depicts the set $\\mathcal{P}$.\n}\\label{fig4}\n\\end{figure}\n\n\n\n\\begin{example}{}{ex8}\nWe now give an example of abnormal control. We follow the example presented in~\\cite{Arutyunov}. The control problem is almost the same as in Example~7 where the minimum time $t^\\star$ to reach the origin is derived. The unique optimal control switches at time $t_s$ from $-f_0$ to $+f_0$. The time $t_f$ is set to $t^\\star$ and we consider a running cost $\\mathcal{C}$ to minimize\n$$\n\\mathcal{C}=\\int_0^{t^\\star}f(t)\\sqrt{|t-t_s|}dt.\n$$\nSince the control time is equal to the minimum time, the constraints on the control being the same, it is clear that the only solution to the optimal control problem is the one previously derived and that this process does not depend on the chosen cost functional.\n\nUsing the PMP in this case, we arrive at the following Pontryagin Hamiltonian\n$$\nH_P=\\Lambda_x\\frac{p}{m}+\\Lambda_pf(t)+\\Lambda_0f(t)\\sqrt{|t-t_s|}.\n$$\nThe Hamiltonian equations are the same as before, but the maximization condition is modified due to the new cost functional. Note that $\\Lambda_p(t)$ is a linear function of time. The control $f$ is chosen to maximize the expression $f(t)(\\Lambda_p+\\Lambda_0\\sqrt{|t-t_s|})$. Since the optimal solution has a switching at $t=t_s$, we deduce that $\\Lambda_p(t_s)=0$. If $\\Lambda_0\\neq 0$ then there is a small interval around $t=t_s$ such that $\\Lambda_p(t)+\\Lambda_0\\sqrt{|t-t_s|})\\neq 0$ because a square root function grows faster than a linear function at the origin. There is a contradiction because $t_s$ is a switching time. We conclude that $\\Lambda_0=0$ and that the optimal solution is an abnormal extremal of the control problem.\n\\end{example}\n\n\\section{Quantum Optimal Control}\n\\label{sec:QOC}\n\n\\subsection{From the Schr\\\"odinger equation to the Pontryagin Hamiltonian}\n\\label{sec:from_QM_to_PMP}\n\nWe are now interested in applying OCT to a quantum system. Let $\\Mc H$ be the Hilbert space of the system, and assume that the quantum Hamiltonian operator of the system can be written as\n\n\\begin{equation}\n\\hat H(t) = \\hat H_0 + \\sum_{k = 1}^m u_k(t) \\hat H_k ,\n\\end{equation}\nwith real control parameters $u_k$. The quantum operators are denoted by a hat in the rest of the paper. We restrict the discussion to bilinear systems whose dynamics are linear with respect to the state and the control. This kind of model can be applied to many experimental situations with a good accuracy. However, optimal control can also be applied to other configurations, when the control enters non-linearly in the Hamiltonian~\\cite{lapert2008,ohtsuki2008,rabitz2011b} or in non-linear systems~\\cite{zhang2011time,chen2016,dorier2017,guerin2020,zhu2023}. The state of the system at time $t$ is described by the vector $|\\psi(t)\\rangle\\in\\mathcal{H}$ whose trajectory is the solution of the time-dependent Schr\\\"odinger equation (in units where $\\hbar=1$)\n\n\\begin{equation}\n\\frac{d \\ket{\\psi}}{dt} = -\\ii \\hat H(t) \\ket{\\psi},\n\\label{eq:schrodinger_eq}\n\\end{equation}\nwith a fixed initial state $|\\psi_0\\rangle$. The norm of $|\\psi\\rangle$ is a constant of motion equal to one at any time $t$, $\\langle\\psi|\\psi\\rangle=1$. Equation~\\eqref{eq:schrodinger_eq} is a complex-valued first-order differential equation, and a method must be found to define real-valued optimal control quantities. An idea is to define the adjoint state as an element of the Hilbert space, $\\ket{\\chi}\\in \\Mc H$. Then, the Lagrangian can be defined as:\n\n\\begin{equation}\n\\Mc L = F_0(\\ket{\\psi},u,t) + \\Re \\left( \\braket{\\chi}{\\dot \\psi} +\\ii \\bra{\\chi} \\hat H(t) \\ket{\\psi}\\right),\n\\end{equation}\nwhere $\\Re(\\cdot)$ and $\\Im(\\cdot)$ denote respectively the real and imaginary parts of a complex number. The conjugate variables $\\Re(\\langle\\chi|)$ and $\\Im(\\langle\\chi|)$ of respectively $\\Re(|\\psi\\rangle)$ and $\\Im(|\\psi\\rangle)$ satisfy the Euler-Lagrange equations\n$$\n\\Re(\\langle\\chi|)=\\frac{\\partial \\mathcal{L}}{\\partial \\Re(|\\dot{\\psi}\\rangle)},~\n\\Im(\\langle\\chi|)=\\frac{\\partial \\mathcal{L}}{\\partial \\Im(|\\dot{\\psi}\\rangle)},\n$$\nin which we use the convention that the derivative with respect to a ket gives a bra, and vice versa. We emphasize here that $\\ket{\\chi}$ is not necessary normalized to one. Using the Euler Lagrange equations for the variables $\\Re( \\langle\\chi|)$ and $\\Im( \\langle\\chi|)$, i.e. here\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\Re(\\langle\\chi |)}=0=\\frac{\\partial \\mathcal{L}}{\\partial \\Im(\\langle\\chi |)},\n$$\nwe arrive at the following equations of motion\n\n\\begin{equation}\n\\begin{split}\n\\frac{d \\Re (\\ket{\\psi})}{dt} &= \\Re \\left( - \\ii \\hat H(t) \\ket{\\psi} \\right), \\\\\n\\frac{d \\Im (\\ket{\\psi})}{dt} &= \\Im \\left( - \\ii \\hat H(t) \\ket{\\psi} \\right),\\nonumber\n\\end{split}\n\\end{equation}\nwhich correspond to the Schr\\\"odinger equation~\\eqref{eq:schrodinger_eq}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn optimal control problem for a quantum system can then be defined using the action:\n\n\\begin{equation}\nS = G(\\ket{\\psi(t_f)}) + \\int_0^{t_f}dt \\left( F_0 (\\ket{\\psi},u,t) + \\Re \\left( \\braket{\\chi}{\\dot \\psi} +\\ii \\bra{\\chi} \\hat H(t) \\ket{\\psi}\\right)\\right).\n\\label{eq:action_QOC_schrodinger}\n\\end{equation}\nThe Pontryagin Hamiltonian can be expressed as\n$$\nH_P=\\Re\\left(\\langle\\chi|\\dot\\psi\\rangle\\right)+\\chi_0 F_0(\\ket{\\psi},u,t),\n$$\nwhich can be transformed into\n\\begin{equation}\nH_P = \\Im \\left(\\bra{\\chi} \\hat H(t) \\ket{\\psi}\\right) + \\chi_0 F_0(\\ket{\\psi},u,t),\n\\label{eq:Hamilotnian_QOC_schrodinger}\n\\end{equation}\nwhere $\\chi_0$ is the abnormal multiplier, $\\chi_0\\leq 0$. The Hamilton equations can be written as\n\\begin{equation}\\label{eqcomplexadj}\n\\begin{split}\n|\\dot{\\psi}\\rangle &=2\\frac{\\partial H_P}{\\partial \\langle \\chi|}=-\\ii \\hat{H}|\\psi\\rangle ,\\\\\n\\langle\\dot{\\chi}| &=-2\\frac{\\partial H_P}{\\partial |\\psi\\rangle}=\\ii \\langle\\chi|\\hat{H}-2 \\chi_0\\frac{\\partial F_0}{\\partial |\\psi\\rangle},\\\\\n\\end{split}\n\\end{equation}\nwhere the factor 2 comes from the definition of the derivative with respect to $|\\psi\\rangle$ as $\\partial/\\partial |\\psi\\rangle=\\frac{1}{2}(\\partial/\\partial \\Re(|\\psi\\rangle)-\\ii \\partial/\\partial \\Im(|\\psi\\rangle)$. We observe that the dynamics of $\\ket{\\chi}$ are governed by the Schr\\\"odinger equation plus an additional term depending on the running cost. It reduces to the Schr\\\"odinger equation if $F_0$ does not depend on $|\\psi\\rangle$. In this case, since the pair $(|\\chi\\rangle,\\chi_0)$ is defined up to a multiplicative constant, it is always possible to assume that $\\langle\\chi|\\chi\\rangle(t)=1$ and to interpret $|\\chi\\rangle$ as an abstract wave function. In addition, the adjoint state satisfies a boundary condition at final time\n\\begin{equation}\\label{eqcomplexfinaladj}\n\\langle\\chi(t_f)| =2\\chi_0 \\frac{\\partial G}{\\partial |\\psi(t_f)\\rangle}.\n\\end{equation}\nWhen there is no constraint on the control, i.e. $U=\\mathbb{R}^m$, the condition $\\frac{\\partial H_P}{\\partial u_k}=0$ of the PMP yields for $k=1,\\cdots,m$:\n\n\\begin{equation}\\label{eqcomplexmax}\n\\frac{\\partial H_P}{\\partial u_k}=\\Im \\left(\\bra{\\chi} \\hat H_k(t) \\ket{\\psi}\\right) + \\chi_0 \\frac{\\partial F_0}{\\partial u_k}(\\ket{\\psi},u,t)\n=0.\n\\end{equation}\n\n\\begin{example}{}{ex9}\n\\label{ex9}\nWe consider several $G$ and $F_0$ functions that can be used in quantum control. The role of $G$ is often to measure the distance of the final state $|\\psi(t_f)\\rangle$ to the target state $|\\psi_f\\rangle$. Standard terminal costs are\n$$\nG_1(|\\psi(t_f)\\rangle)=1-|\\langle\\psi_f|\\psi(t_f)\\rangle|^2,~G_2(|\\psi(t_f)\\rangle=1-\\Re(\\langle\\psi_f|\\psi(t_f)\\rangle).\n$$\nThe function $G_2$ is related to the minimization of the square modulus of $|\\psi_f\\rangle-|\\psi(t_f)\\rangle$ since\n$$\n|| |\\psi(t_f)\\rangle-|\\psi_f\\rangle ||^2=2(1-\\Re(\\langle\\psi(t_f)|\\psi_f\\rangle).\n$$\nIn the case of the cost $G_1$, the target state is reached up to a global phase, whereas this phase is fixed to 0 in the second case. Different choices of the running cost $F_0$ are possible to penalize either the control parameter $u$ or the trajectory followed by the system. An example is given by\n$$\nF_0(|\\psi(t)\\rangle,u(t))=|\\langle\\psi_1|\\psi(t)\\rangle|^2+\\frac{u^2}{2},\n$$\nwhere the goal is to minimize both the energy of the pulse and the projection onto a forbidden state $|\\psi_1\\rangle$. Using Eq.~\\eqref{eqcomplexadj}, we deduce that the adjoint state is the solution of the differential equation\n$$\n|\\dot{\\chi}\\rangle =-\\ii \\hat{H}|\\chi\\rangle-2\\chi_0 \\langle \\psi_1|\\psi(t)\\rangle |\\psi_1\\rangle,\n$$\nwith the final condition given respectively by $|\\chi(t_f)\\rangle = -2\\chi_0 \\langle\\psi_f|\\psi(t_f)\\rangle |\\psi_f\\rangle$ and $|\\chi(t_f)\\rangle = -\\chi_0|\\psi_f\\rangle$ for $G_1$ and $G_2$. The maximization condition~\\eqref{eqcomplexmax} can be written as\n$$\n\\frac{\\partial H_P}{\\partial u_k}=\\Im (\\bra{\\chi} \\hat H_k(t) \\ket{\\psi}) + \\chi_0 u_k .\n$$\n\\end{example}\n\nThis formulation of the optimal control problem is of practical interest because it remains close to usual quantum mechanical equations. Nonetheless other procedures exist to transform complex-valued functions into real-valued ones. Alternatives are preferred when the system is described in terms of a density matrix or an evolution operator. As an illustrative example, we discuss here the case of a two-level quantum system whose state is given by the Bloch vector.\nFor a pure state, the density matrix $\\hat\\rho$ of a two-level quantum system can be written as\n\\begin{equation}\n\\hat \\rho = \\ket{\\psi}\\bra{\\psi} = \\frac{1}{2}\\left( \\hat \\Id_2 + x \\sigx + y \\sigy + z \\sigz \\right)=\\frac{1}{2}\\left( \\begin{array}{cc}\n1+z & x- \\ii y \\\\\nx+ \\ii y & 1-z\n\\end{array} \\right),\n\\label{eq:spin_density_matrix}\n\\end{equation}\n\nwhere $x,y,z$ are real numbers such that $x^2 + y^2 + z^2 = 1$ and $\\sigx, \\sigy,\\sigz$ are Pauli matrices. In the case of a mixed state, a similar result can be established, but with $x^2 + y^2 + z^2 \\leq 1$. The vector $q=(x,y,z)$ is called the Bloch vector. The interesting point is that $x= \\langle \\sigx \\rangle = \\textrm{Tr} (\\sigx \\hat \\rho)$, and similarly for $y$ and $z$. It is then straightforward to derive the equations of motion for the three real variables $x$, $y$, $z$ by computing $\\frac{d}{dt}\\langle\\sigx \\rangle$, $\\tfrac{d}{dt}\\langle \\sigy \\rangle$, and $\\tfrac{d}{dt}\\langle\\sigz \\rangle$. Assume, for instance, that the dynamics of $|\\psi\\rangle$ are governed by the following Schr\\\"odinger equation\n$$\ni|\\dot{\\psi}\\rangle = \\left[\\Delta\\frac{\\sigz}{2}+u_x\\frac{\\sigx}{2}+u_y\\frac{\\sigy}{2}\\right]|\\psi\\rangle,\n$$\nwhere $\\Delta$ is a fixed parameter. It can be shown that\n\\begin{equation}\\label{eqbloch}\n\\begin{split}\n\\dot{x}&=-\\Delta y+u_y z, \\\\\n\\dot{y}&=\\Delta x-u_x z, \\\\\n\\dot{z}&=u_x y-u_y x.\n\\end{split}\n\\end{equation}\nThe PMP can then be applied to this real differential system. The Pontryagin Hamiltonian can be written as\n$$\nH_P=\\Delta(p_y x-p_x y)+u_x(p_z y-p_y z)+u_y(p_xz-p_zx),\n$$\nwhere $(p_x,p_y,p_z)$ are the real coordinates of the adjoint state. This approach is particularly well adapted to study the optimal control of open quantum systems~\\cite{bonnard2009,lapert2010singular,Lapert_exploring_2012}, whose dynamics are governed by, e.g., the Lindblad-Kossakovski equation~\\cite{breuer2002theory,gardiner2004quantum}.\n\n\\subsection{Time-optimal control of a two-level quantum system}\n\\label{sec:OCT_spin}\n\n\nIn this section, we are interested in the design of time-optimal controls for state-to-state transfer of a two-level quantum system which is one of the most relevant and simple systems in quantum technologies~\\cite{raimond2006exploring,\nlevitt2013spin,cheng2023noisy}. We also discuss the results from the point of view of the QSL~\\cite{deffner2017quantum,frey2016,hegerfeldt2013driving}. In order to keep the discussion as simple as possible, we consider situations in which any interaction with the environment can be neglected so that the system is described by a pure state. We can leave aside the density matrix formalism.\n\nWe consider the time-optimal control of a two-level quantum system with respectively one and two independent control parameters. This issue has been considered in detail in references~\\cite{PRXQuantumsugny,d2001optimal,boscain2002,PhysRevA.63.032308,boscain2006time,PhysRevA.85.012317,sugny08,garon2013time,dionis2023time,evangelakos2023}. When the approximation of a closed quantum system is not satisfied, the optimal control problem has to be extended to account for interactions with the environment~\\cite{bonnard_optimal_2012,lapert2010singular,zhang2011time,rebentrost2009optimal,roloff2009optimal,lapert2011towards,PhysRevA.82.063418,\nfloether2012robust,mukherjee2013,lapert2013,riaz2019optimal,basilewitsch2019reservoir,fischer2019time,Ansel_2022}.\n\n\\subsubsection{The case of two controls.}\n\\label{sec:Time optimal control with two inputs and without drift}\n\nAs a first example, we consider the time-optimal control of a two-level quantum system whose Hamiltonian can be written as:\n\\begin{equation}\n\\label{eq:Ham_2_inputs_no_drift}\n\\hat H(t) = \\frac{ u_z (t)}{2}\\sigz + \\frac{ u_x(t)}{2} \\sigx.\n\\end{equation}\nThe control $u(t) =(u_x(t),u_z(t))$ is assimilated to a time-dependent electromagnetic field, with components in two different directions $x$ and $z$. In experiments, the strength of the field is limited such that $\\Vert u(t) \\Vert  \\in [0,u_0]$, where $u_0$ is the maximum available amplitude. The Hilbert space is spanned by the basis $\\{\\ket{\\uparrow},\\ket{\\downarrow}\\}$ with $\\ket{\\uparrow}=(1,0)$ and $\\ket{\\downarrow}=(0,1)$.\n\nWe focus on the control process that transforms the state $\\ket{\\uparrow}$ into $\\ket{\\psi_{f}}=e^{\\ii \\theta} \\ket{\\downarrow}$, where $\\theta$ is a phase factor which is not relevant for this quantum state. The cost functional to minimize is given by\n\n\\begin{equation}\n\\mathcal{C} = \\int_0^{t_f}dt.\n\\end{equation}\n\nWe are interested in control protocols reaching exactly the set of target states, so there is no terminal cost in this case. Moreover, control time is kept free in a time-optimal process. The Pontryagin's Hamiltonian reads\n\n\\begin{equation}\n\\begin{split}\nH_P & = \\Im \\left(\\bra{\\chi} \\hat H(t) \\ket{\\psi}\\right)+\\chi_0 \\\\\n& = \\frac{ u_z (t)}{2} \\Im \\left(\\bra{\\chi} \\sigz \\ket{\\psi}\\right) + \\frac{ u_x(t)}{2}  \\Im \\left(\\bra{\\chi} \\sigx \\ket{\\psi}\\right)+\\chi_0 \\\\\n& =  u_z (t) H_z(t) +   u_x(t) H_x(t)+\\chi_0,\n\\end{split}\n\\end{equation}\n\nwith\n\n\\begin{equation}\n\\label{eq:def_Hx}\nH_{x,y,z}(t) = \\frac{1}{2}\\Im\\left(\\bra{\\chi(t)} \\hat{\\sigma}_{x,y,z} \\ket{\\psi(t)}\\right).\n\\end{equation}\n\nThe next step is to compute Hamilton's equations which can be expressed as\n\n\\begin{align}\n\\label{eq:EOM_psi}\n& \\frac{d \\ket{\\psi(t)}}{dt} = -\\ii \\hat H(t) \\ket{\\psi(t)}, \\\\\n\\label{eq:EOM_chi}\n& \\frac{d \\ket{\\chi(t)}}{dt} = -\\ii \\hat H(t)\\ket{\\chi(t)}.\n\\end{align}\nThe time derivatives of the quantities $H_{x,y,z}$  satisfy the following differential system\n\\begin{equation}\\label{eq:dH}\n\\begin{split}\n\\dot{H}_x &= - u_z(t) H_y(t), \\\\\n\\dot{H}_y &= u_z(t) H_x(t)-  u_x(t) H_z(t), \\\\\n\\dot{H}_z &= u_x(t) H_y (t).\n\\end{split}\n\\end{equation}\nfrom which it can be shown that $H_x^2+H_y^2+H_z^2$ is a constant of motion.\n\nThe maximization condition on $H_P$ leads to both regular and singular trajectories. In the interior of $U$, we have $\\partial H_P/\\partial u_x=\\partial H_P/\\partial u_z=0$, which leads to $H_x(t)=H_z(t)=0$ on a non-zero time interval. This corresponds to a singular trajectory. Plugging these conditions into Eq.~\\eqref{eq:dH}, we obtain that $H_y(t)$ is constant. The case $H_y(t)=0$ is not relevant since it gives $|\\chi(t)\\rangle=0$ using \\eqref{eq:def_Hx}. When $H_y(t)\\neq 0$, we have $u_x(t) = u_z(t) =0$, which is obviously not optimal. Consequently, the optimal control is regular.\nThe regular trajectory corresponds to the boundary of $U$ for which $u_x^2+u_z^2=u_0^2$. Introducing the angle $\\theta$ such that $u_x=u_0\\cos\\theta$ and $u_z=u_0\\sin\\theta$, we get $H_P=u_0(\\cos\\theta H_x+\\sin\\theta H_z)+\\chi_0$. The maximization condition implies that $\\frac{\\partial H_P}{\\partial \\theta}=0$, i.e. $\\tan\\theta =\\frac{H_z}{H_x}$ and finally\n$$\nu_x=u_0\\frac{H_x}{\\sqrt{H_x^2+H_z^2}},~u_z=u_0\\frac{H_z}{\\sqrt{H_x^2+H_z^2}}.\n$$\nThe maximum value of $H_P$ is given by $H_P=u_0\\sqrt{H_x^2+H_z^2}+\\chi_0$ which is a constant of motion. The abnormal multiplier can be set to -1 which leads to\n$$\nH_x^2+H_z^2=\\frac{1}{u_0^2},\n$$\nusing $H_P=0$ for this time-optimal control. The optimal solution can then be described as follows. We emphasize that the control $u_z$ only produces a modification of the phase associated with each state $\\ket{\\uparrow}$ and $\\ket{\\downarrow}$ and cannot influence the population transfer which is only due to $u_x$.  We deduce that the control $u_x(t) = \\pm u_0$, $u_z(t)=0$ (for all $t$) allows us to reach the state with the maximum speed (if $u_z \\neq 0$, we have $u_x \\neq u_0$, and thus the velocity of rotation around the direction $x$ is not maximum). When $u_x(t)=+u_0$, it is straightforward to integrate the Schr\\\"odinger equation as $\\ket{\\psi(t)} = \\cos(u_0 t /2) \\ket{\\uparrow} + \\ii \\sin(u_0 t/2)\\ket{\\downarrow}$. Simultaneously, the adjoint state can be determined to be $\\ket{\\chi(t)}=-(2\\imath/u_0)\\hat{\\sigma}_x \\ket{\\psi(t)}$ from the constraints on $H_{x,y,z}$ (notice that $\\ket{\\chi}$ is not necessarily normalized here). For a given control time $t_f$, we deduce that $|\\langle\\psi(t_f)|\\downarrow\\rangle|^2=\\sin(u_0 t_f/2)^2$.  The minimum time $t^\\star$ to exactly reach the target state is therefore given by\n\\begin{equation}\nt^\\star= \\frac{\\pi}{u_0},\n\\end{equation}\nthe optimal trajectory corresponding to a $\\pi$- pulse, i.e. a pulse with a time-integrated area equal to $\\pi$ for any maximum pulse amplitude $u_0$.\n\\subsubsection{The case of one control with a drift.}\n\\label{sec:OCT_spin_time_1_input_and_offset}\n\nAs a second example of application, we consider the same control problem as in Sec.~\\ref{sec:Time optimal control with two inputs and without drift}, but with the following Hamiltonian:\n\n\\begin{equation}\\label{eqhamdrift}\n\\hat H = \\frac{\\Delta}{2}\\sigz + \\frac{u(t)}{2} \\sigx,\n\\end{equation}\nwhere $\\Delta \\in \\setR$ is a constant (a drift, also called frequency offset), and $u(t) \\in [-u_0,u_0]$ is a one-dimensional control parameter. The analysis is only slightly modified as follows. The Pontryagin Hamiltonian can be written as\n\n\\begin{equation}\nH_P = \\Delta H_z(t) + u(t) H_x(t),\n\\end{equation}\n\nand the Hamiltonian's equations are given by:\n\n\\begin{align}\n\\label{eq:EOM_psi_v2}\n& \\frac{d \\ket{\\psi(t)}}{dt} = -\\ii \\hat H(t) \\ket{\\psi(t)}, \\nonumber\\\\\n\n& \\frac{d \\ket{\\chi(t)}}{dt} = -\\ii \\hat H(t)\\ket{\\chi(t)}.\\nonumber\n\n\\end{align}\n\nThe maximization condition of the PMP consists in maximizing the only term of $H_P$ depending on $u(t)$, i.e. $u(t)H_x$ with the constraint $-u_0\\leq u(t)\\leq u_0$. When $H_x(t)\\neq 0$, it is straightforward to show that the control satisfies $u(t)=u_0~\\sgn [H_x(t)]$, while we cannot conclude if $H_x(t)=0$. We deduce that we have a regular trajectory when $u(t)=\\pm u_0$ and singular arcs when $H_x(t) =0$ on a non-zero time interval. We start the study with the description of regular controls.\n\n\\paragraph{Regular controls.}\n\nFrom $u(t) = u_0~\\sgn[H_x(t)]$, we observe that the control is a piecewise constant function and a change of sign  occurs when $H_x (t)= 0$ in an isolated point. In control theory, the function $H_x$ is called a switching function.  We obtain a bang-bang control that suddenly jumps from a control of maximum (or minimum) amplitude to its opposite. We can go one step further by calculating the time derivatives of $H_{x,y,z}$, which yields\n\n\\begin{equation}\\label{eqdH2}\n\\begin{split}\n\\dot{H}_x &= -\\Delta H_y(t), \\\\\n\\dot{H}_y &=\\Delta H_x(t)- u(t) H_z(t), \\\\\n\\dot{H}_z &= u(t) H_y .\n\\end{split}\n\\end{equation}\nWe observe that Eq.~\\eqref{eqdH2} gives a closed-form system of differential equations, similarly to Eq.~\\eqref{eq:dH}, that can be integrated on a bang (between two switchings). By denoting $t_i$ the switching time $i$, we have\n\n\\begin{equation}\n\\left( \\begin{array}{c}\nH_x(t) \\\\\nH_y(t) \\\\\nH_z(t)\n\\end{array} \\right) = \\exp \\left[ (t-t_i)\\left( \\begin{array}{ccc}\n0 & -\\Delta & 0 \\\\\n\\Delta & 0 & - u \\\\\n0 & u & 0\n\\end{array} \\right) \\right] . \\left( \\begin{array}{c}\n0 \\\\\nH_y(t_i) \\\\\nH_z(t_i)\n\\end{array} \\right),\n\\end{equation}\nwith $u=u_0~\\sgn[H_x(t)]$ a constant, $t \\in [t_i,t_{i+1}]$ and we use the fact that $H_x(t_i)=0$. The explicit calculation of the matrix exponential leads us to\n\n\\begin{equation}\\label{eqHx}\nH_x(t) = -\\frac{\\Delta}{\\Omega^2} \\left[\\Omega H_y(t_i) \\sin(\\Omega (t-t_i) )- u H_z(t_i) (1-\\cos(\\Omega (t-t_i)))  \\right],\n\\end{equation}\nwith $\\Omega = \\sqrt{u_0^2 + \\Delta^2}$. The duration of the bang, given by $t_{i+1}-t_i$ can be determined from Eq.~\\eqref{eqHx}. We arrive at\n\n\\begin{equation}\nt_{i+1}-t_i =\\left\\lbrace \\begin{array}{ll}\n\\frac{\\pi n}{\\Omega} & \\text{ if } \\frac{u H_z(t_i)}{\\Omega H_y(t_i)}=0, \\\\\n\\frac{2}{\\Omega}\\left[ \\pi n + \\arctan\\left( \\frac{\\Omega H_y(t_i)}{u H_z(t_i)}\\right) \\right] & \\text{ if } \\frac{u H_z(t_i)}{\\Omega H_y(t_i)} \\neq 0,\n\\end{array} \\right.\n\\label{eq:switching_times_bang_bang}\n\\end{equation}\n\nwhere $n$ is an integer chosen such that $t_{i+1} -t_i$ is the smallest positive non zero solution. We also observe that the function $H_{x}$ is $2\\pi/\\Omega$ periodic. We deduce that the maximum duration between two switchings is $2\\pi/\\Omega$.\nSimilar calculations as for $H_x(t)$ show that $H_z(t_{i+1}) = H_z(t_i)$ and $H_y(t_{i+1})=H_y(t_i)$. This means that at switching $i+1$, the system is in the same configuration as at switching $i$. All bangs have the same duration. For an arbitrary regular trajectory, we cannot start or end at a switching. Therefore, such a trajectory has the following structure: a first bang with control amplitude $u = \\pm u_0$ and duration $t_1$, followed by a series of bangs of amplitudes $\\pm u_0$ and duration $t_b$ and a final bang of duration $t_2$. Note that $t_1$ and $t_2$ are both less than or equal to $t_b$.\n\n\n\n\\paragraph{Singular controls.}\n\nThe control is singular when $H_x(t) = 0$ on a non-zero time interval, which gives $\\tfrac{d}{dt}H_x = \\tfrac{d^2}{dt^2}H_x = 0$. From Eq.~\\eqref{eqdH2}, we deduce that $H_y(t) = 0$ and $-u(t) H_z(t) = 0$. Therefore, the control is always equal to $0$, or the vector $(H_x,H_y,H_z)$ is zero. The latter condition is only satisfied if the adjoint state is always zero, which is a trivial solution.\n\n\\paragraph{Calculation of the optimal control.}\nThe final step is to determine the time-optimal trajectory. First, we argue that singular control (or concatenation of regular and singular controls) cannot be optimal. For a singular control equal to zero, the Hamiltonian operator can be simplified to $\\hat H = \\tfrac{\\omega}{2}\\sigz$, which cannot produce a population transfer between the initial and target states, and thus is not optimal. The optimal trajectory is therefore regular. To find the switching times of the optimal procedure, we analytically  compute the evolution operator for an increasing number of switchings, stopping when the corresponding control can steer the system to the target. If there are several solutions with the same number of switchings, we keep the one(s) with the shortest duration.\n\nWe integrate the Schr\\\"odinger equation on a single bang with a control amplitude $u=\\pm u_0$. The evolution operator associated with this bang is given by\n\n\\begin{equation}\n\\hat U_{\\text{bang}} = e^{-\\ii (\\Delta \\sigz + u \\sigx )t/2}= \\cos\\left(\\frac{\\Omega t}{2}\\right)\\hat \\Id - \\ii\\left( \\frac{\\Delta}{\\Omega}\\sigz+  \\frac{u}{\\Omega}\\sigx\\right)\\sin\\left(\\frac{\\Omega t}{2}\\right).\n\\label{eq:U_bang}\n\\end{equation}\nSince $\\Omega = \\sqrt{\\Delta^2 + u_0^2}$, we observe that the maximum for a population transfer with a single bang is $u^2/\\Omega^2<1$ unless $\\Delta =0$, hence, there is at least one switching. With two bangs we have:\n\\begin{equation}\n\\hat U_{\\text{2-bangs}} = e^{-\\ii (\\Delta \\sigz - u \\sigx )t_2/2} e^{-\\ii (\\Delta \\sigz + u \\sigx )t_1/2},\n\\end{equation}\n\nwhere $t_1$ and $t_2$ are the respective duration of the two bangs. The final distance to the target state can be expressed as\n\n\\begin{equation}\n\\left\\vert\\bra{\\downarrow}\\hat U_{\\text{2-bangs}} \\ket{\\uparrow}\\right\\vert^2 = \\frac{u^2}{\\Omega^4} \\left[ 4 \\Delta^2 \\sin^2\\left(\\frac{\\Omega t_1}{2}\\right) \\sin^2\\left(\\frac{\\Omega t_2}{2}\\right) + \\Omega^2 \\sin^2\\left(\\frac{\\Omega (t_1-t_2)}{2}\\right)\\right].\n\\end{equation}\n\nWe have to maximize this function such that $t_1+t_2$ is minimum. The solution to this problem is given by\n\n\\begin{align}\nt_1 &= \\frac{1}{\\Omega}(\\pi - \\arccos(\\Delta^2/u_0^2)),\\\\\nt_2 &= \\frac{1}{\\Omega}(\\pi + \\arccos(\\Delta^2/u_0^2)),\n\\end{align}\n\nwhen $|\\Delta|<u_0$, with a total control time given by\n\n\\begin{equation}\nt_f=t_1+t_2= \\frac{2\\pi}{\\Omega}.\n\\end{equation}\n\nNote that $t_1$ and $t_2$ can be permuted and the initial sign of the control plays no role. The optimal trajectories are represented in Fig.~\\ref{fig:blochsphere} using the Bloch vector representation. There are two equivalent solutions with a switching on the equator of the sphere. To show that this solution is the global optimal solution, it must also be compared with controls with a larger number of switchings. This is the case as described in~\\cite{PRXQuantumsugny,boscain2006time}. The number of bangs of the optimal solution is strictly larger than 2 when $|\\Delta|>u_0$~\\cite{boscain2006time,assemat2010,evangelakos2023}.\n\n\\begin{figure}[htbp]\n\\centering\n\\includegraphics[width=0.5\\textwidth]{fig5.pdf}\n\\caption{Plot of the time-optimal trajectories on the Bloch sphere in the case of a two-level quantum system with one control and a drift. The two optimal trajectories are represented in blue (solid and dashed lines) while the green curve depicts a geodesic going from the north to the south pole. The black solid line represents the equator of the sphere.}\n\\label{fig:blochsphere}\n\\end{figure}\n\n\n\\subsubsection{Quantum speed limit.}\n\\label{sec:QSL}\n\nIn this section, we present the results obtained from the point of view of QSL~\\cite{deffner2017quantum,hegerfeldt2013driving,PhysRevLett.130.010402,PhysRevA.92.062106}. First, we review the main idea on which QSL is based, and then we discuss its relation to the optimal control formalism.\n\nQSL can be described as a way to generate, as quickly as possible, a state orthogonal to the initial one. Since this concept does not take into account dynamical constraints, it provides only a lower bound on the minimum time to reach the orthogonal state. This statement can be formalized rigorously as follows. The speed in the Hilbert space is naturally given by the Schr\\\"odinger equation: $\\ket{\\dot \\psi} = -\\ii \\hat H \\ket{\\psi}$. Similarly to classical dynamical systems, the velocity vector can be decomposed into parallel and transverse contributions\n\n\\begin{align}\n\\ket{\\dot \\psi_\\parallel} &=\\ket\\psi\\bra\\psi \\dot{\\psi}\\rangle= -\\ii \\bra{\\psi}\\hat H \\ket{\\psi}~ \\ket{\\psi}, \\\\\n\\ket{\\dot \\psi_\\perp} &=\\ket{\\dot \\psi} - \\ket{\\dot \\psi_\\parallel}.\n\\end{align}\n\nBy construction, we have $\\braket{\\dot \\psi_\\perp}{\\psi}=0$. The transfer is maximized when the the norm of $\\ket{\\dot \\psi_\\parallel}$ is minimized and the one of $\\ket{\\dot \\psi_\\perp}$ maximized. The latter can be expressed as\n\\begin{equation}\n|\\dot \\psi_\\perp|^2 =  |\\dot \\psi|^2 - |\\dot \\psi_\\parallel|^2 = \\langle \\hat H ^2 \\rangle_{\\psi} - \\langle \\hat H \\rangle^2_\\psi,\n\\end{equation}\n\nwhich is the variance of $\\hat H$, denoted below $\\Delta H^2$. In the case of Hamiltonian~\\eqref{eq:Ham_2_inputs_no_drift} for two independent controls, the variance reads\n\n\\begin{equation}\n\\Delta H^2 = \\frac{u_0^2}{4} - \\frac{1}{4}\\left( u_z z + u_x x\\right)^2,\n\\label{eq:Delta E2}\n\\end{equation}\nin which the Bloch coordinates $(x,y,z)$ are used. The maximum of this variance, $u_0^2/4$, is obtained when $u_z z + u_x x =0$. The maximum value of $\\Delta H$ gives the QSL of Mandelstam-Tamm~\\cite{deffner2017quantum}. Note that other estimates have been proposed in the literature~\\cite{MARGOLUS1998188,PhysRevLett.130.010402} with similar properties as the Mandelstam-Tamm bound. The lower bound on the minimum time given by the QSL is\n$$\nt_{\\textrm{QSL}}=\\frac{\\pi/2}{\\sqrt{\\Delta H^2}}=\\frac{\\pi}{u_0}.\n$$\nWe observe that $t^\\star=t_{\\textrm{QSL}}$. This point can also be verified with the optimal solution derived from the PMP.\n\nFor the optimal control $u_x=u_0, u_z=0$, we find from the state evolution that the quantity $x=0$ at all times, therefore the condition $u_z z + u_x x =0$ is satisfied.\nThe corresponding trajectory on the Bloch sphere is the half of the great circle from the north to the south pole, in the $zOy$ plane. It is also a geodesic connecting the initial and the target states for which the speed of travel is maximum.\n\nHowever, the relation $t^\\star=t_{\\textrm{QSL}}$ is not generic, as shown in the case with one control of Sec.~\\ref{sec:OCT_spin_time_1_input_and_offset}. For the Hamiltonian~\\eqref{eqhamdrift}, the variance is given by\n$$\n\\Delta H^2=\\frac{\\Omega^2}{4}-\\frac{1}{4}(\\Delta z+u x)^2.\n$$\nThe maximum of $\\Delta H^2$ is $\\Omega^2/4$ which leads to $t_{\\textrm{QSL}}=\\pi/\\Omega$, whereas the minimum time is $t^\\star=2\\pi/\\Omega$. In this example, the lower bound cannot be saturated because the geodesic does not correspond to a dynamical trajectory of the system. In other words, the condition $\\Delta z+u x=0$ to reach the maximum speed cannot be satisfied by the control $u$. The geodesic corresponding to the trajectory used in the QSL approach is represented for this example in Fig.~\\ref{fig:blochsphere}.\n\\section{Numerical methods}\n\\label{sec:numerical_methods}\n\nIn this section, we present the state-of-the-art numerical methods that can be used to solve quantum optimal control problems. The literature in this field is important~\\cite{bonnans2006numerical,qin2008differential,ab2015comprehensive,\nfeng2021monarch,maros2012computational,davis1987genetic,goldberg2007genetic,pham2012intelligent} and we do not claim to be exhaustive. Our aim is to introduce the main ideas and technical details that can be found in the references mentioned. Numerical examples are described below with pseudo-codes, while Python codes are provided in the supplementary material.\n\nWe begin with a general presentation of optimization algorithms found in scientific computing software. These algorithms can be used as black boxes and are generally effective when the number of parameters to be optimized is relatively small. A list of software to solve quantum optimal control problems is given in a second step. Special emphasis is then placed on the description of two algorithms, namely the shooting techniques and the gradient-based algorithms which are two direct applications of the PMP.\n\n\n\\subsection{A general overview of standard optimization algorithms}\n\\label{sec:std_opt_algo}\n\nOptimization algorithms are generally implemented in most scientific computing softwares~\\cite{2020SciPy-NMeth,eaton2013gnu,MATLAB,Mathematica}. An optimization algorithm can be described as a systematic iterative method to find the minimum or  maximum of a function. The function must have a finite number of entries, and thus, continuous controls must be discretized in some way using e.g. piecewise constant functions, Taylor or Fourier expansions, etc. The optimization algorithms are usually presented in the following form\n\n\\begin{equation}\n\\texttt{OptAlgo}(\\texttt{Cost function},\\texttt{ Constraints},\\texttt{ Initial guess}, \\texttt{ Options})\n\\end{equation}\n\nwhere $\\texttt{Cost function}$ is the cost function or figure of merit $\\mathcal{C}$ to minimize or maximize, defined in Eq.~\\eqref{eq:def_cost_fun}, $\\texttt{ Constraints}$ are constraints that limit the domain of definition of the control, the set $U$, $\\texttt{Initial guess}$ is an initial guess of the control, used by the algorithm to find a better solution. $\\texttt{Options}$ are additional parameters that depend on the algorithm. They correspond either to a choice of a numerical method or to parameters that influence the convergence of the algorithm. We can roughly divide the algorithms into two different families, namely the gradient-free and the gradient-based algorithms~\\cite{kochroadmap}. As their name suggests, they are based either on the calculation of the gradient of the figure of merit or on some other procedure that allows the control protocol to be improved at each step of the iterative process. Both approaches have clear advantages depending on the system dimension and on the precision of the control process.\n\n\\begin{itemize}\n\\item[$-$]~\\textbf{Gradient-free algorithm.}\nThis set of methods includes different algorithms. As the number of variants is very large, only a few examples are  mentioned. In the \\textit{Simplex method}~\\cite{maros2012computational}, the space of control parameters $\\setR^n$ is explored with a $n$-simplex whose size decreases recursively around a minimum of the cost function. This approach has been applied in QOC with the algorithm CRAB~\\cite{calarco2011}, which can be used to control many body systems~\\cite{Calarco2022}. Another optimization procedure is based on \\textit{Evolutionary algorithms}~\\cite{davis1987genetic,goldberg2007genetic,pham2012intelligent,\nvenkata_rao_jaya:_2016} (differential evolution, genetic algorithms, simulated annealing, JAYA...). The basic idea of this approach is to iteratively explore the parameter space with $N$ particles, also called \"walkers\". The method used to update the position of the particles is specific to each method, but usually the particles are moved randomly with a probability law, designed  to favour a direction towards the global minimum of the cost function. An example of a particle path towards the minimum of a function using the JAYA algorithm~\\cite{venkata_rao_jaya:_2016} is shown in Fig.~\\ref{fig:JAYA}. Evolutionary algorithms are well-known in quantum control and are used experimentally to design a control protocol in a model-free approach~\\cite{rabitz1992}.\n\n\\item[$-$]~\\textbf{Gradient-based algorithm.} The gradient of the function with respect to the control parameters is calculated, and it allows to know in which direction the cost takes a lower value~\\cite{bonnans2006numerical,kelley1999iterative}. Based on the gradient, a new control is designed, and the minimum of the cost function is found iteratively. For the optimization of piecewise constant controls, two different procedures have been developed in QOC~\\cite{DYNAMO}, namely the Krotov-type methods~\\cite{koch2012,krotov1983iterative,palao2002,palao2003,goerz2020} and the GRAPE-type procedures~\\cite{khaneja_optimal_2005}. While GRAPE updates the control in all time slices concurrently, the Krotov procedure updates the control sequentially from one time slice to the next. A full comparison of the two methods is given in~\\cite{Jager2014,DYNAMO}. Such approaches can be improved by using the second order derivative of the cost function with respect to the control parameters~\\cite{bryson1975applied,grape2,tannor2011,sherson2020}. These improvements are not discussed in the main text but are included in some Python codes that can be found in the supplementary material. The GRAPE algorithm and its connection to the PMP is presented in Sec.~\\ref{sec:GRAPE_algo}.\n\\end{itemize}\n\n\\begin{figure}\n\\begin{center}\n\\includegraphics[width=0.85\\textwidth]{fig6.pdf}\n\\end{center}\n\\caption{Example of particle search of the global minimum of the function $f:(x,y)\\mapsto -(x^2 + x y - y^3/2 )e^{-(x^4 + y^4)/2}$ in the domain $x \\in [-2,2]$, $y\\in[-2,2]$ using the JAYA algorithm~\\cite{venkata_rao_jaya:_2016}. The position of the particles is given by black points while the global minimum is located by a gray star. The boxed numbers at the top right of each panel correspond to the number of iterations of the algorithm. Initially, the positions of all particles are generated randomly. They gradually move to regions of the control landscape where the function is likely to be a minimum. In the end, all the particles are located around the global minimum of the function.\n}\n\\label{fig:JAYA}\n\\end{figure}\n\n\n\nWe conclude this section by some general comments about the application of such algorithms to quantum control. First, we point out that machine learning techniques such as Reinforcement Learning can also be applied to find optimal control protocol~\\cite{carleoRMP2019,giannelli2022}. The coefficients of a neural network are optimized by trial and error in a learning step using in general gradient procedures. This network is then used to map each state of the system to the best action (or control) to take to reach the target state. From a more general point of view, Reinforcement Learning can be described as a kind of dynamic programming approach that can be mathematically justified in the continuous limit by the Hamilton-Jacobi-Bellman formulation of optimal control~\\cite{bertsekasbook}.\n\nFor quantum control problems, simplex methods are generally not the most effective, but they are able to design control procedures in very complex dynamics or when strong control constraints are considered. The accuracy achieved with evolutionary or gradient-based algorithms is generally better. Evolutionary algorithms are interesting when the number of parameters is small ($\\lesssim$ 20) or when the cost function cannot be differentiated. Among the different methods, we particularly recommend the simulated annealing method, which is a good compromise between the computation time and its ability to find the global minimum. Another interesting choice is the JAYA algorithm~\\cite{venkata_rao_jaya:_2016} which is very simple. Its main advantage lies in the absence of external parameters that influence the convergence (in most algorithms, one or more parameters have to be adjusted manually to obtain a good and fast convergence). Gradient-based algorithms are interesting when the number of parameters to be optimized is very large ($\\gtrsim 100$). They can be complemented by second-order formulations to improve convergence to a local extremum~\\cite{bryson1975applied,grape2}. Gradient-based algorithms are particularly efficient when the gradient can be computed from an analytic expression. Otherwise, it can be estimated using a finite difference approach, at the expense of computational time. Many factors can influence the choice of algorithm. The way the library is coded and the different options available are also important factors. It is generally advised to test a few different algorithms on a given QOC problem, as a preliminary step. This can also provide a good insight into the control problem since different algorithms may return different solutions (which can be only local extremums of the cost function). Many optimization algorithms are provided in scientific computing softwares. MATLAB~\\cite{MATLAB} proposes a global optimization toolbox with e.g. the function \\texttt{simulannealbnd} (simulated annealing with constraints) or the function \\texttt{fmincon} (gradient algorithm which takes into account the Hessian). In Python/ScyPy~\\cite{2020SciPy-NMeth}, different approaches can be found in the routine \\texttt{scipy.optimize} (mostly simplex and gradient-based methods). In Mathematica~\\cite{Mathematica}, the function \\texttt{NMinimize} can be used. It has built-in methods that cover all the families of algorithms listed here above. An advantage of this function is that it is not necessary to provide an initial guess control, the algorithm works simultaneously on several optimization processes in which the initial controls are chosen automatically.\n\n\\paragraph{Packages and Software for Quantum Optimal Control.} We provide below a list of freely available packages and softwares designed for quantum optimal control applications. The list is given in alphabetical order.\n\\begin{itemize}\n\\item[$-$]~Bocop~\\cite{Bocop}: Bocop is an open-source toolbox for solving optimal control problems. It is a general optimization software that can be used for any optimization problem. Its key features are global optimization for both deterministic and stochastic systems, computation of switching and stopping times, parallel execution with OpenMP, and Matlab/Python interfaces.\n\n\n\\item[$-$]~DYNAMO: Dynamic Framework for Quantum Optimal Control~\\cite{DYNAMO}. DYNAMO is a Matlab package with integrated GRAPE and Krotov methods.\n\n\\item[$-$]~HamPath~\\cite{caillau2012differential,HamPath}: HamPath is a general optimization package written in Fortran, and compatible with Matlab, Python, and GnuPlot. It is specifically designed to solve the equations of the PMP (shooting algorithm). Advanced higher-order optimal control methods are implemented, such as the computation of conjugated points~\\cite{bonnard_optimal_2012}.\n\n\\item[$-$]~Krotov~\\cite{goerz2020}: An open-source Python package based on the Krotov algorithm to solve problems of QOC extending from state-to-state transfer to quantum gate implementation.\n\n\\item[$-$]~OCTBEC: A Matlab toolbox for optimal quantum control of Bose–Einstein condensates~\\cite{HOHENESTER2014194}.\n\n\\item[$-$]~QEngine: A C++ library for quantum optimal control of ultracold atoms~\\cite{sorensen2019qengine}. QEngine is a C++ library for performing optimal control of Bose–Einstein condensates, Bose–Hubbard type models, and two interacting particles.\n\n\\item[$-$]~QOPT: An Experiment-Oriented Software Package for Qubit Simulation and Quantum Optimal Control~\\cite{teske2022qopt}. QOPT is a Python library with a scope similar to the optimization package of QuTiP, but it supports computations with stochastic noises.\n\n\\item[$-$]~Quandary: An open-source C++ package for high-performance optimal control of open quantum systems~\\cite{gunther2021quandary}. Quandary is fully compiled and supports parallel computation of system dynamics. It is therefore possible to optimize quantum controls for a system of large dimension. The optimization algorithm is a homemade gradient algorithm that has some similarities with GRAPE.\n\n\\item[$-$]~QuOCS: The quantum optimal control suite~\\cite{rossignolo2023quocs}. QuOCS is a Python library that combines several gradient-based optimization algorithms, such as GRAPE and AD-GRAPE. The QuOCS is initially developed and mainly used as part of the gradient-free dCRAB method. The software performs both open- and closed-loop optimizations, and it can be connected to real-time quantum experiments.\n\n\\item[$-$]~Qocttols: A Python code that performs optimization calculations on quantum systems using  gradient-based algorithms~\\cite{castro}. It is an open and free software that works on closed and open systems. Floquet formalism with periodic perturbations can also be used.\n\n\\item[$-$]~QuTiP: Quantum Toolbox in Python~\\cite{johansson2012qutip}. QuTiP is an open-source Python package for simulating the dynamics of open quantum systems. It contains an optimization toolbox with integrated GRAPE and CRAB algorithms.\n\n\n\n\\item[$-$]~Spinach: Spinach is an open-source Matlab package~\\cite{spinach} for computations in Nuclear Magnetic Resonance (NMR), Electron Paramagnetic Resonance (EPR), Magnetic Resonance Imaging (MRI), Dynamic Nuclear Polarization (DNP) and Magic Angle Spinning (MAS). Optimal Control, and other topics of Magnetic Resonance spectroscopy are available.\n\n\n\\item[$-$]~SpinDrops~\\cite{Spindrops}: SpinDrops is an interactive quantum spin simulator. The software includes several state-of-the-art pulse sequences, but controls can also be created with an editor or with an optimizer based on GRAPE. This is a standalone software with android, iOs, Linux, macOS, and windows supports. An online version is also available.\n\n\\item[$-$]~Travolta: An open-source software for parallelized quantum optimal control computations in photo-excited systems using  gradient-based algorithms~\\cite{travolta}.\n\n\\item[$-$]~WavePacket: A Matlab package for numerical quantum dynamics~\\cite{schmidt2018wavepacket}. WavePacket is designed to solve a wide range of quantum dynamical and optimization problems. The optimization is based on the gradient-based algorithm described in~\\cite{Zhu1998,Zhu1998b,Ohtsuki2004}.\n\n\\end{itemize}\n\nThe majority of the packages incorporate GRAPE and sometimes, one or two other algorithms are also included. In this list, QuTiP and Spinach are the libraries with the largest number of functionalities. Their scope goes beyond control  optimization. \n\n\n\\subsection{Shooting techniques}\n\\label{sec:shooting_algo}\n\nBehind the name \"shooting techniques\" hides a large class of more or less sophisticated algorithms~\\cite{trelat2012optimal,bryson1996optimal,Bocop,bock1984multiple,\ntrelat2005controle,giftthaler2018family}. They are also called indirect methods in the literature~\\cite{trelat2012optimal}. The general idea is inspired by a shooting problem, where the goal is to place a bullet or an arrow on a target. The only parameters that can be modified are the orientation of the gun and the initial velocity of the bullet, i.e. the dynamics of the system can only be influenced by a precise choice of initial conditions. Random shots can be taken to get an estimate of these initial conditions, but more sophisticated strategies can be used for this purpose.\n\nUsing Hamiltonian's equations of the PMP, the optimal control problem can be cast into this form. The parameters to be optimized correspond to the initial value of the adjoint state (see Sec.~\\ref{sec:Lagrangian approach}). The estimation of this initial adjoint state may be improved by optimization algorithms such as those described in Sec.~\\ref{sec:std_opt_algo}.\nThe pseudo-code of the algorithm can be written as follows:\n\n\\begin{lstlisting}\nFunction $(X(t),u(t))$=Xtraj($\\Lambda_0$,$t_f$)\n(* Integrate from 0 to $t_f$ the equations of motion $\\dot \\Lambda = -\\partial _{X} H_P$ and\n $\\dot X = \\partial_{\\Lambda} H_P$, where the control is expressed using the maximization condition of\n the PMP in the form $u(t)=u(X(t),\\Lambda(t))$.\n The initial conditions for $\\Lambda$ are given by $\\Lambda_0$, and\n the ones for $X$ are fixed by the definition of the control problem. *)\n\nFunction $\\mathcal{C}$=Cost($X(t)$,$u(t)$,$t_f$)\n(* Return the value of the cost functional*)\n$\\Lambda_{\\textrm{opt}}$ = Minimize(Cost(Xtraj($\\Lambda_0$,$t_f$),$t_f$),$\\Lambda_0 \\in \\Mc D$)\n$u_{\\textrm{opt}}(t)$ = Xtraj($\\Lambda_{\\textrm{opt}}$,$t_f$)$[2]$\n\\end{lstlisting}\n\nHere above, \\texttt{Minimize} is an arbitrary optimization algorithm (see e.g. Sec.~\\ref{sec:std_opt_algo}), and $\\Mc D$ is the domain of definition of $\\Lambda_0$. The shooting technique is efficient when the solutions of the PMP are regular and without switching. The singular case can be difficult to solve numerically and switching times may not be detected by the algorithm~\\cite{bonnard2003singular,martinon2007using}. Efficient numerical codes have been developed to tackle these kinds of computation, such as Bocop~\\cite{Bocop} or Hampath~\\cite{caillau2012differential,HamPath}. \n\\begin{example}{}{ex10}\nWe illustrate the application of the shooting algorithm on the state-to-state transfer of a two-level quantum system with two controls and no drift. The Hamiltonian of the system is given by\n$$\n\\hat H = \\frac{u_x}{2} \\sigx+\\frac{u_y}{2} \\sigy.\n$$\nWe describe the state of the system in terms of Bloch coordinates $(x,y,z)$. Using Eq.~\\eqref{eqbloch}, we get:\n\\begin{equation}\n\\begin{split}\n\\dot{x}&=u_y z, \\\\\n\\dot{y}&=-u_x z, \\\\\n\\dot{z}&=-u_yx+u_xy.\\nonumber\n\\end{split}\n\\end{equation}\nThe goal of the control problem is to bring the system from $(1,0,0)$ to $(0,1,0)$ in minimum time with the constraint $u_x^2+u_y^2\\leq 1$. The cost functional is thus $\\mathcal{C}=t_f$. It can be shown that the optimal protocol consists of saturating the bound at any time, that is $u_x^2+u_y^2=1$~\\cite{PRXQuantumsugny}. The idea is that the fastest control protocol requires generally maximum control intensity. The Pontryagin Hamiltonian can be expressed as\n$$\nH_P=u_x(p_zy-p_yz)+u_y(p_xz-p_zx)+p_0,\n$$\nwhere $(p_x,p_y,p_z)$ is the adjoint state which also satisfies the Bloch equation, and $p_0$ the abnormal multiplier. Since the final time is free, we have $H_P=0$. The maximization condition of $H_P$ leads in the regular case to the following optimal controls\n\\begin{equation}\n\\begin{split}\nu_x &= (p_zy-p_yz)/H,\\\\\nu_y &= (p_xz-p_zx)/H, \\nonumber\\\\\n\\end{split}\n\\end{equation}\nwhere $H=\\sqrt{(p_zy-p_yz)^2+(p_xz-p_zx)^2}$.\n\nAs described in~\\cite{PRXQuantumsugny}, the control problem can be integrated exactly. The minimum time is $t_f=t^\\star=\\frac{\\pi\\sqrt{3}}{2}$ and the corresponding initial adjoint state at time $t=0$ has the coordinates $(p_x(0),\\frac{1}{\\sqrt{3}},\\pm 1)$. Note that the first component is not fixed but the same control process is obtained for any value of $p_x(0)$. The two possible values of $p_z(0)=\\pm 1$ correspond to the two symmetric trajectories with respect to the equator on the Bloch sphere. A global description of the control landscape can be made by representing the initial adjoint state on a sphere as $p_x=R_p\\sin\\Theta_p\\cos\\Theta_p$, $p_y=R_p\\sin\\Theta_p\\sin\\Phi_p$ and $p_z=R_p\\cos\\Theta_p$. We choose to normalize $R_p$ to one, i.e. the adjoint state $(p_x,p_y,p_z)$ belongs to the sphere of unit radius, while the abnormal multiplier is not known. For each couple of initial values $(\\Theta_p(0),\\Phi_p(0))$, we numerically compute the corresponding optimal trajectory and the euclidean distance $d$ to the target state at time $t^\\star$. By definition, the optimal solutions verify $d=0$. The corresponding control landscape is plotted in Fig.~\\ref{figcontour}.\n\nThe same problem can also be solved numerically by using an iterative shooting technique. In this case, the goal is not to calculate all the trajectories to obtain the control landscape, but to iteratively find the right solution that reaches the target state. We choose to normalize the abnormal multiplier to -1 which leads to $H_P=1$, but the modulus of $(p_x,p_y,p_z)$ is not fixed. A very good convergence of the algorithm is observed. We find the optimal solution and the corresponding initial values of the adjoint state with high numerical precision. The corresponding Python code \\emph{shooting.py} is provided in the supplementary material.\n\\end{example}\n\\begin{figure}[htbp]\n    \\begin{center}\n    \\includegraphics[width=0.75\\textwidth]{fig7.pdf}\n    \\end{center}\n  \\caption{Euclidean distance $d$ to the target state $(0,1,0)$ at time $t_f=t^\\star$ for the trajectories with different initial adjoint states parameterized by the angles $\\Theta_p$ and $\\Phi_p$. The minimum time solutions corresponding to the minimum of $d$ are represented by solid lines.}\n  \\label{figcontour}\n\\end{figure}\n\\subsection{Gradient-based algorithms designed for quantum control}\n\\label{sec:GRAPE_algo}\n\nIn this paragraph, we focus on a gradient-based algorithm called \\textbf{GRAPE}, for \\textbf{Gr}adient \\textbf{A}scent \\textbf{P}ulse \\textbf{E}ngineering~\\cite{rebentrost2009optimal,PhysRevA.82.063418,khaneja_optimal_2005,auckenthaler2010matrix,goodwin2016modified,PhysRevA.102.042612,ding2019robust}. This algorithm was used to solve a large amount of quantum control problems. The references~\\cite{kobzar2004exploring,kobzar2012exploring,Van_Damme_robust_2017,Ansel_2021,BEC2021,BEC2023,Lapert_exploring_2012,Ansel_2022,haidong2017,\nansel_2017,VANREETH201739,\nVan_Damme_time_optimal_2018,Ansel2023} is a very short list of studies in which GRAPE plays a key role. This list is by no means  exhaustive~\\cite{cat,kochroadmap}. GRAPE and its variants are one of the most famous gradient-based algorithms in the quantum control community.\n\n\\paragraph{A direct gradient algorithm.} We first present a direct derivation of a gradient-based algorithm. The explicit relation to the PMP is discussed in a second step. The basic assumption which may be experimentally relevant is that the control is a piecewise constant function with a finite number $N$ of time steps of duration $\\Delta t=t_f/N$. We denote by $u_n$ the amplitude of the control in the time interval $[(n-1)\\Delta t,n\\Delta t)$ with $n\\in \\{1,\\cdots, N\\}$. The control $u$ is now described by a set of $N$ real values $(u_1,u_2,\\cdots,u_N)$. The core of the algorithm is based on a standard gradient descent algorithm for finite-dimensional optimization~\\cite{bonnans2006numerical,kelley1999iterative}, which greatly simplifies its derivation.\n\nThe basic principle comes from the observation that if the cost function $\\mathcal{C}(u)$ is differentiable then $\\mathcal{C} ( u ) $ has the fastest decrease in the direction $-\\partial_{u_n} \\mathcal{C}(u)$ given by the gradient of the cost function with respect to the control parameters. An iterative algorithm to estimate the optimal control can then be designed. Starting from a given control $u$, a new control $u'$ such that $\\mathcal{C}(u')<\\mathcal{C}(u)$ can be computed from the relation\n\\begin{equation}\\label{eqnewcontrol}\nu'_n = u_n - \\epsilon \\frac{\\partial \\mathcal{C}(u) }{\\partial u_n},\n\\end{equation}\n\nwhere $\\epsilon>0$ is a parameter chosen small enough to guarantee the convergence of the algorithm, but large enough  to limit the number of iterations. The cost $\\mathcal{C}(u')$ can be estimated at first order in $\\epsilon$ as follows\n\n\\begin{equation}\n\\begin{split}\n\\mathcal{C}(u')&=\\mathcal{C}\\left(u-\\epsilon\\frac{\\partial \\mathcal{C}(u)}{\\partial u}\\right) \\\\\n&= \\mathcal{C}(u)-\\epsilon \\left(\\frac{\\partial \\mathcal{C}(u)}{\\partial u}\\right)^2 + O(\\epsilon^2) \\\\\n&\\lesssim \\mathcal{C}(u).\n\\end{split}\n\\end{equation}\nThe strict inequality is achieved for $\\epsilon$ small enough. The parameter $\\epsilon$ is a free parameter that must be set by hand. Its automatic adjustment can be performed using a line-search algorithm, which aims to find the optimal value of $\\epsilon$ at each iteration step. A line-search improves convergence but at the cost of increased computation time. Another way to speed up the convergence of the algorithm is to replace $\\epsilon$ by gradients of previous iterations as in conjugate gradient approaches or by the inverse of the Hessian in second-order methods. The latter is the core of Newton's type algorithms. The Hessian can be difficult to compute in practice, but it can be estimated simply by knowing  the gradient and the cost function. Such algorithms are called quasi-Newton~\\cite{gill1972quasi}. We will not discuss these questions about the choice of $\\epsilon$, and we assume in the following that this parameter is known. General ideas can be adapted to more advanced algorithms.\n\nTo summarize, the pseudo-code for a standard gradient algorithm is the following:\n\n\n\\begin{lstlisting}\nChoose an initial guess control $u_0$.\nFunction $(\\mathcal{C}_1,u)$ = GradientMinimize($u_0,\\epsilon,k_{max}$)\n\t$u=u_0$\n\t$\\mathcal{C}_1 = \\mathcal{C}(u)$\n\tFor [$1\\leq k\\leq k_{max}$,\n\t\t$u' = u - \\epsilon \\frac{\\partial \\mathcal{C}(u) }{\\partial u}$\n\t\t$\\mathcal{C}_2 = \\mathcal{C}(u')$\n\t\tIf[$\\mathcal{C}_2>\\mathcal{C}_1$,\n\t\t\tBreak,\n\t\t\t$\\mathcal{C}_1 = \\mathcal{C}_2$\n\t\t\t$u = u'$]]\n\n\\end{lstlisting}\n\nThe constant $k_{max}$ which corresponds to the maximum number of iterations is set large enough such that the convergence to a minimum of $\\mathcal{C}$ is reached.\n\n\\paragraph{Calculation of the gradient.} We have not yet clarified how $\\tfrac{\\partial \\mathcal{C}(u) }{\\partial u_n}$ is calculated, and this is the difficult part of the algorithm. GRAPE provides a clever calculation trick to save computational time and derive analytical formulas. For simplicity, we consider that the Hamiltonian of the quantum system is of the form:\n\n\\begin{equation}\n\\hat H(t) = \\hat H_0 + u(t) \\hat H_1,\n\\end{equation}\n\nwhere $\\hat H_0$ and $\\hat H_1$ are two time independent Hamiltonian operators which do not commute and $u$ is a one-dimensional piecewise constant unbounded function (i.e. $u(t) \\in \\setR$, and on the step interval $n$, $u(t) = u_n$). The generalization to several control parameters is straightforward. The evolution operator during a time step can be expressed as\n\\begin{equation}\n\\hat U_n=\\hat U(n \\Delta t,(n-1)\\Delta t) = e^{-\\ii  \\Delta t (\\hat H_0 + u_n \\hat H_1)},\n\\label{eq:U_evol_piecewise_constant}\n\\end{equation}\nand $|\\psi(t_f)\\rangle = \\hat U_N \\hat U_{N-1}\\cdots \\hat U_1 |\\psi_0\\rangle$. We denote by $|\\psi_n\\rangle =\\hat U_n\\cdots \\hat U_1|\\psi_0\\rangle$ the state at time $t=n\\Delta t$.\n\nTo simplify the description, we consider only the case of a terminal cost of the form $\\mathcal{C}=G(|\\psi(t_f)\\rangle) =G(\\hat U_N \\hat U_{N-1}\\cdots \\hat U_1 |\\psi_0\\rangle)$. Since $\\hat{U}_n$ is the only term depending on $u_n$, we deduce that the derivative of $G$ with respect to $u_n$ can be expressed as\n$$\n\\frac{\\partial{G}}{\\partial u_n}=\\frac{\\partial{G}}{\\partial |\\psi(t_f)\\rangle}\\frac{\\partial |\\psi(t_f)\\rangle}{\\partial u_n}+\\frac{\\partial \\langle\\psi(t_f)|}{\\partial u_n}\\frac{\\partial{G}}{\\partial \\langle\\psi(t_f)|},\n$$\nwhich leads to\n\\begin{equation}\n\\begin{split}\n\\frac{\\partial{G}}{\\partial u_n}&=\\frac{\\partial{G}}{\\partial |\\psi(t_f)\\rangle}\n\\hat U_N \\cdots \\frac{\\partial \\hat U_n}{\\partial u_n}\\cdots \\hat U_1 |\\psi_0\\rangle \\\\\n&+ \\langle\\psi_0|\n\\hat U_1^\\dagger \\cdots \\frac{\\partial \\hat U_n^\\dagger}{\\partial u_n}\\cdots \\hat U_N^\\dagger \\frac{\\partial{G}}{\\partial \\langle\\psi(t_f)|}.\n\\end{split}\n\\end{equation}\nIt remains to compute $\\frac{\\partial \\hat U_n}{\\partial u_n}$ which is not trivial because $[\\hat H_0,\\hat H_1]\\neq 0$. A formal expression can be obtained from the Wilcox formula~\\cite{wilcox1967exponential,wilcox} which gives the derivative of the exponential of a matrix $A(\\theta)$ with respect to a parameter $\\theta$\n$$\n\\frac{\\partial e^{tA}}{\\partial \\theta}=e^{tA}\\int_0^t e^{-t' A}\\frac{\\partial A}{\\partial \\theta}e^{t'A}dt'.\n$$\nWe deduce that\n$$\n\\frac{\\partial \\hat U_n}{\\partial u_n}=-\\ii\\Delta t \\hat U_n\\overline{\\hat H}_1,\n$$\nwhere $\\overline{\\hat H}_1=\\frac{1}{\\Delta t}\\int_0^{\\Delta t}e^{\\ii t'(\\hat H_0+u_n\\hat H_1)}dt'\\hat H_1 e^{-\\ii t'(\\hat H_0+u_n\\hat H_1)}dt'$ is the average of $\\hat H_1$ in the Heisenberg representation over the duration $\\Delta t$. For a sufficiently small time step $\\Delta t$, $\\overline{\\hat H}_1$ can be identified to $\\hat H_1$ and we arrive at\n$$\n\\frac{\\partial \\hat U_n}{\\partial u_n}\\simeq -\\ii \\Delta t \\hat H_1\\hat U_n.\n$$\nIntroducing an adjoint state $|\\chi\\rangle$ defined by a backward propagation in time from the target state such that $\\langle\\chi (t_f)|=\\langle\\chi_N|=\\frac{\\partial G}{\\partial |\\psi(t_f)\\rangle}$ and $\\langle\\chi_n|=\\langle \\chi_N|\\hat U_N\\cdots \\hat U_n$, the gradient can be expressed as\n$$\n\\frac{\\partial G}{\\partial u_n}=-\\ii\\Delta t(\\langle \\chi_n|\\hat H_1|\\psi_n\\rangle-\\langle\\psi_n|\\hat H_1|\\chi_n\\rangle),\n$$\nwhich can be simplified into\n\\begin{equation}\\label{eqgradientfinal}\n\\frac{\\partial G}{\\partial u_n}=2\\Delta t \\Im(\\langle\\chi_n|\\hat H_1|\\psi_n\\rangle).\n\\end{equation}\nWe finally arrive at\n\\begin{equation}\\label{eqfinalgrape}\n\\delta u_n=u_n'-u_n=-\\epsilon \\Im(\\langle\\chi_n|\\hat{H}_1|\\psi_n\\rangle ).\n\\end{equation}\n\n\\begin{example}{}{ex11}\\label{ex11}\nWe consider the same terminal costs $G_1$ and $G_2$ as in Example~\\ref{example:ex9}. We recall that $G_1=1-|\\langle\\psi_f|\\psi(t_f)\\rangle|^2$ and $G_2=1-\\Re\\left(\\langle\\psi_f|\\psi(t_f)\\rangle\\right)$. We deduce that the final condition of the adjoint state is given respectively by $|\\chi_N\\rangle=-\\langle\\psi_f|\\psi(t_f)\\rangle |\\psi_f\\rangle$ and $|\\chi_N\\rangle=-\\frac{|\\psi_f\\rangle}{2}$ for $G_1$ and $G_2$.\n\\end{example}\nUsing Eq.~\\eqref{eqgradientfinal}, we observe that the gradient can be computed in a very efficient way with a forward and a backward propagation in time of the dynamics from the initial and the target states, respectively. The pseudo-code for this computation can be written as follows.\n\n\\begin{lstlisting}\nFunction $\\ket{\\psi(t)}$ = PropforwardSchrodingerEq($u$,$\\ket{\\psi_{0}}$)\n(*Propagate forward the Schrodinger equation using the control $u$ and the\ninitial state $\\ket{\\psi_{0}}$. Compute and store the quantum state at each time step, *)\n\nFunction $\\ket{\\chi(t)}$ = PropbackwardSchrodingerEq($u$,$\\ket{\\chi(t_f)}$)\n(*Propagate backward the Schrodinger equation using the control $u$ and the\nfinal state $\\ket{\\chi(t_f)}=2\\chi_0\\frac{\\partial G}{\\partial \\langle\\psi(t_f)|}$. Compute and store $\\ket{\\chi(t)}$ at each time step.*)\n\nFunction $\\delta u$=GradientGRAPE($u,\\epsilon$)\n\t$\\ket{\\psi(t)}$ = PropforwardSchrodingerEq($u(t)$,$\\ket{\\psi_{0}}$)\n\t$\\ket{\\chi(t)}$ = PropbackwardSchrodingerEq($u(t)$,$\\ket{\\chi(t_f)}$)\n\tFor[$1\\leq n\\leq N$, $\\delta u_n= -\\epsilon\\Im (\\langle\\chi_n|  \\hat H_1  |\\psi_n\\rangle)$]\n\t\n\n\\end{lstlisting}\n\nUsing this method, the time evolution of a quantum state is computed twice at each evaluation of the gradient, while $N + 1 \\gg 2$ evaluations of the dynamics are necessary when the gradient is estimated from a finite difference (i.e. with a formula $\\partial_{u_n}G \\approx (G(u_n + \\Delta u_n) - G(u_n))/\\Delta(u_n)$). When the number of time steps is large (hundreds or thousands of time steps), this approach can significantly reduce the computational time.\n\n\\paragraph{GRAPE versus PMP} The relation between GRAPE and the PMP can be established in the limit when the time step becomes infinitesimally small. In this case, the standard derivative is replaced by a functional one. Starting from Eq.~\\eqref{eqSPont}, we see that the choice $\\delta u=\\epsilon \\frac{\\partial H_P}{\\partial u}$ leads to\n$$\n\\delta S=-\\epsilon \\int_0^{t_f} \\left(\\frac{\\partial H_P}{\\partial u}\\right)^2dt,\n$$\nwhich is negative to first order in $\\epsilon$. Using\n$H_P=\\Im(\\langle \\chi|\\hat H|\\psi\\rangle)$, we deduce that\n$$\n\\delta u=-\\epsilon \\Im(\\langle \\chi|\\hat{H}_1|\\psi\\rangle),\n$$\nwhich is the continuous version of the time-discretized gradient derived in Eq.~\\eqref{eqfinalgrape}. In other words, a gradient-based algorithm can be viewed as a time digitalization of an equivalent algorithm derived from the PMP. Note that this equivalence is only valid when $U$ is open, i.e. in the weak PMP.\n\nOver the years, several versions of GRAPE have been developed. Some possible modifications are presented below.\n\n\n\n\n\\paragraph{Parameterized continuous functions.}\nAn interesting modification of the algorithm is to replace a piecewise constant control by parameterized functions~\\cite{skinner_optimal_2010}. The control $u$ is then expanded over a set of functions $\\{f_k(t)\\}_{k=1,\\cdots,k_{\\textrm{max}}}$ such as $u=v(\\{a_kf_k(t)\\})$ where $v$ is a smooth function. The goal is to find the real coefficients $(a_k)$ to minimize the cost functional. In the case of  Fourier series, this would be:\n\n\\begin{equation}\nu(t) = a_0+\\sum_{k=1}^{k_{max}} a_k \\cos(k\\omega   t)+b_k \\sin(k\\omega   t),~\\omega = 2\\pi/t_f.\n\\label{eq:param_control_fourier}\n\\end{equation}\nAt each step of the iterative algorithm, the values of the parameters are corrected from the gradients $\\frac{\\partial G}{\\partial a_k}$, $\\frac{\\partial G}{\\partial b_k}$. Using the chain rule derivation, the gradient $\\frac{\\partial G}{\\partial a_k}$ for example can be expressed as\n$$\n\\frac{\\partial G}{\\partial a_k}=\\sum_{n=1}^N\\frac{\\partial G}{\\partial u_n}\\frac{\\partial u_n}{\\partial a_k}=\\sum_{n=1}^N \\frac{\\partial G}{\\partial u_n}\\frac{\\partial v}{\\partial a_k}f_k(t_n),\n$$\nwhere the functions $f_k$ are approximated by piecewise constant functions with the same time step as $u$. The following pseudo-code can be used to implement this variant of GRAPE.\n\n\n\\begin{lstlisting}\nFunction $u$ = Control($\\{a_k,b_k\\}$)\n(* Compute $u_k = u(t_k)$ for each $t_k$ using the parameters $\\{a_k,b_k\\}_{k=0,\\cdots,k_{\\textrm{max}}}$  *)\n\nFunction $\\delta a, \\delta b$ = GradientGRAPE2($\\{a_k,b_k\\}$)\n\t$u$ = Control($\\{a_k,b_k\\}$)\n\t$\\ket{\\psi(t)}$ = PropforwardSchrodingerEq($u(t)$,$\\ket{\\psi_{0}}$)\n\t$\\ket{\\chi(t)}$ = PropbackwardSchrodingerEq($u(t)$,$\\ket{\\chi(t_f)}$)\n\tFor[$1\\leq n\\leq N$,\n\t\t$\\delta u_n= -\\epsilon\\Im \\bra{\\chi(t_{n})}  \\hat H_1  \\ket{\\psi(t_n)}$]\n\tFor[$1\\leq k\\leq k_{\\textrm{max}}$,\n\t\t$\\delta a_k=\\sum_{n=1}^N \\delta u_n\\frac{\\partial u}{\\partial a_k}(t_n)$\n                $\\delta b_k=\\sum_{n=1}^N \\delta u_n\\frac{\\partial u}{\\partial b_k}(t_n) $]\n\t\t\n\\end{lstlisting}\n\n\n\\paragraph{Exact gradient.}\nA slight modification of the system can be used to compute the gradient exactly. The idea is to consider a larger system given by the Hamiltonian $\\tilde{H}$\n\\begin{equation}\n\\tilde{H}=\\begin{pmatrix}\n\\hat{H} & 0 \\\\\n\\partial_u\\hat{H} & \\hat{H}\n\\end{pmatrix} .\n\\end{equation}\nIn the standard case, we have $\\partial_u\\hat{H}=\\hat{H}_1$. The matrix exponential of $\\tilde{H}$ has interesting properties such as\n$$\ne^{\\alpha\\tilde{H}}=\n\\begin{pmatrix}\ne^{\\alpha \\hat H} & 0\\\\\n\\partial_u e^{\\alpha \\hat{H}} & e^{\\alpha\\hat{H}}\n\\end{pmatrix},\n$$\nwhich can be shown by using a Taylor series expansion of the exponential function. The parameter $\\alpha$ is a complex constant. We deduce that the evolution operator $\\tilde{U}$ corresponding to the Hamiltonian $\\tilde{H}$ can be expressed as\n$$\n\\tilde{U}=\n\\begin{pmatrix}\n\\hat{U} & 0\\\\\n\\partial_u \\hat{U} & \\hat{U}\n\\end{pmatrix}.\n$$\nThis result suggests that the propagator $\\hat{U}$ and its gradient can be computed simultaneously by solving the Schr\\\"odinger equation associated to the Hamiltonian $\\tilde{H}$ with the initial condition $(\\hat I, 0)$. The gradient can then be used directly in GRAPE to correct the control at each step of the algorithm. This variant is known in the literature as the auxiliary matrix approach of GRAPE.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{example}{}{ex12}\nTo illustrate the method, we study the control of a state-to-state transfer in a two-level quantum system with minimal energy. The goal is to steer the system from the initial state $\\ket{\\uparrow}$ to the target $\\ket{\\downarrow}$ at time $t_f$ fixed  with the Hamiltonian $\\hat H(t) = \\frac{ \\Delta}{2}\\sigz + \\frac{ u}{2} \\sigx$. The cost functional to minimize can be expressed as\n$$\n\\mathcal{C}=1-|\\langle \\uparrow|\\psi(t_f)\\rangle |^2+\\frac{p_0}{2}\\int_0^{t_f}u(t)^2dt.\n$$\nwhere $p_0$ is a factor allowing to adjust the relative weight of the two terms in the cost. The control time $t_f$ is set to $2\\pi/\\sqrt{1+\\Delta^2}$. The Hamiltonian's equations of the PMP show that the dynamics of the state $|\\psi(t)\\rangle$ and adjoint state $|\\chi(t)\\rangle$ are governed by the Schr\\\"odinger equation with respectively the initial condition $|\\psi(0)\\rangle =|\\uparrow\\rangle$ and the final condition $|\\chi(t_f)\\rangle = \\langle \\uparrow |\\psi(t_f)\\rangle |\\uparrow\\rangle$ with $\\chi_0=-1/2$ (see Example~\\ref{example:ex9}). In the GRAPE algorithm, the correction $\\delta u$ at each step can be written as\n$$\n\\delta u =-\\epsilon\\left(\\Im(\\langle\\chi(t)|\\frac{\\sigx}{2}|\\psi(t)\\rangle +\\frac{p_0}{2}u(t)\\right).\n$$\nThe convergence of the algorithm is very good in this example. An example is plotted in Fig.~\\ref{figgrape}. The corresponding Python code \\emph{GRAPE.py} is provided in the supplementary material. The Python code \\emph{GRAPE2.py} solves the same optimal control problem but with a polynomial parameterization of the control pulse.\n\\end{example}\n\\begin{figure}[htbp]\n    \\centering\n    \\includegraphics[width=8cm]{fig8.pdf}\n  \\caption{Optimal control $u$ (black line) designed by GRAPE for Example~\\ref{example:ex12}. Numerical parameters are set to $u_0=1$ and $\\Delta=\\frac{u_0}{2}$. The red line represents the time-optimal solution with the constraint $|u(t)|\\leq u_0$. The blue line depicts the guess control used in the algorithm. The parameter $p_0$ is set to $0.1/t_f$.}\\label{figgrape}\n\\end{figure}\n\n\n\\section{From theory to experiment: Optimal control of a Bose-Einstein condensate in an optical lattice}\\label{sectheoexp}\nWe propose to demonstrate the different steps of the application of QOC to a specific experimental system, namely the control of a Bose-Einstein Condensate (BEC) in an optical lattice. We refer the reader to the references~\\cite{BEC2021,BEC2023,BEC2024} for additional theoretical and experimental details on these results.\n\n\n\n\n\n\\subsection{The model system}\n\\label{BEC_model}\n\nDilute Bose-Einstein condensates are routinely produced in cold atom experiments: a gas of identical bosons, cooled to a temperature close to absolute zero condenses into a single quantum state, and is therefore described by a single wave function~\\cite{RMP2008}. On typical experimental timescales (see Sec.~\\ref{sec:Experimental_optimal_control}), we can neglect both the interactions between the atoms of the gas and the parabolic magnetic potential used to confine the system. Such conditions allow us to simplify the description of the dynamics of the system which are governed by the  Schr\\\"odinger equation with a periodic sinusoidal potential~\\cite{Eckardt2017}. This trapping potential originates from a one-dimensional optical lattice produced by two interfering laser beams of wavelength $\\lambda$ propagating in opposite directions, whose relative phase and amplitude can be modulated in time with good precision. The goal of the control is then to manipulate the motional state of the BEC in this potential.\n\n\nThe wave function $\\ket{\\psi(t)}$ which belongs to the Hilbert space $\\mathcal{H}= \\mathcal{L}^2(\\mathbb{R}, dx)$, evolves in time according to the Schrödinger equation,\n       \\begin{align}\n  \\imath \\hbar \\frac{d\\ket{\\psi(t)}}{dt} = \\left( \\frac{\\hat{p}^2}{2m} - \\frac{s(t) E_L}{2}\\cos\\left( k_L \\hat{x} + \\varphi(t) \\right) \\right) \\ket{\\psi(t)},\\label{BEC_SE}\n\\end{align}\nwhere $\\hat{p}=-\\imath\\hbar\\frac{\\partial}{\\partial x}$ and $\\hat{x}=x$ are respectively the momentum operator and the position operator in the position representation with $x$ the spatial coordinate along the optical lattice axis. We denote by $m$ the mass of the atom, $k_L=2\\pi/(\\lambda/2)$ the wave vector and $E_L=\\frac{\\hbar^2 k_L^2}{2m}=4E_R$ (with $E_R$ the recoil energy) the energy associated with the lattice. \nThe control parameters are the dimensionless depth $s(t)$ and the phase $\\varphi(t)$. In this section, we focus on using $\\varphi(t)$ as a single control parameter, while the dimensionless lattice depth $s$ is kept constant. Varying $\\varphi$ as a function of time corresponds to moving or shaking the lattice position along the $x$- axis. We consider the following change of variables to obtain dimensionless coordinates:\n    \\begin{align}\n      t &\\rightarrow \\frac{E_L}{\\hbar}t, \\nonumber\\\\\n      x &\\rightarrow k_L x.\\nonumber\n    \\end{align}\nThis yields,\n       \\begin{align}\n  \n  \\imath \\frac{d\\ket{\\psi(t)}}{dt} \\equiv \\hat{H}(t)\\ket{\\psi(t)}=\\left(\\hat{p}^2 - \\frac{s}{2}\\cos\\left( \\hat{x} + \\varphi(t) \\right)\\right)\\ket{\\psi(t)},\n\\end{align}\nwith $\\hat{p}=-\\imath\\frac{\\partial}{\\partial x}$ and $\\hat{x}=x$  in the position representation.\n\nWe denote by $\\ket{\\phi_{\\alpha}}$ the eigenvectors of the momentum operator with eigenvalue $\\alpha$ and wavefunction $\\phi_{\\alpha}(x)=\\frac{1}{\\sqrt{2\\pi}}e^{\\imath \\alpha x}$. Since the potential is periodic in $x$, the Bloch theorem states that the parameter $\\alpha$ can be expressed as $\\alpha = n + q$, where $n \\in \\mathbb{Z}$ and $q\\in [-0.5,0.5]$ is the quasimomentum. The quasimomentum can formally take any real value, but due to the periodicity of the potential, two quasimomenta separated by an integer are equivalent. \nFurthermore, this periodicity implies that the quasimomentum $q$ is conserved during the  control process. In the subspace of a given quasimomentum $q$, we can expand a generic state on the plane wave basis as,\n       \\begin{align}\n  \\ket{\\psi} = \\sum_{n\\in\\mathbb{Z}}c_{q,n} \\ket{\\phi_{q+n}}.\\nonumber\n\\end{align}\nUsing this decomposition, the dynamic is given in terms of the coefficients $c_{q,n}$ as\n    \\begin{align}\n   \\label{BEC_coef} \\imath\\Dot{c}_{q,n} &= \\left( n+q \\right)^2c_{q,n} - \\frac{s}{4}\\left(e^{\\imath\\varphi(t)}c_{q,n-1} + e^{-\\imath\\varphi(t)}c_{q,n+1} \\right).\\nonumber\n\\end{align}\n\nThe Schr\\\"odinger equation can be written in matrix form as follows,\n       \\begin{align}\n   \\imath\\frac{d\\ket{\\psi(t)}}{dt} = \\left( \\hat{H}_0 + \\cos\\left( \\varphi(t)\\right)\\hat{H}_1 + \\sin\\left( \\varphi(t)\\right)\\hat{H}_2\\right) \\ket{\\psi(t)},\\nonumber\n\\end{align}\nwhere\n       \\begin{align}\n   \\ket{\\psi(t)} = \\begin{pmatrix}\n  \\vdots \\\\\n  c_{q,n-1} \\\\\n  c_{q,n} \\\\\n  c_{q,n+1} \\\\\n  \\vdots\n  \\end{pmatrix},\n\\end{align}\n       \\begin{align}\n       \\centering\n  \\label{BEC_vector} \\hat{H}_0 =  \\begin{pmatrix}\n  & \\ddots &  &  &  \\\\\n  \\dots & 0 & \\left((n-1)+q\\right)^2 & 0 & 0 & 0 & \\dots \\\\\n  \\dots & 0 & 0 & \\left(n+q\\right)^2 & 0 & 0 & \\dots \\\\\n  \\dots & 0 & 0 & 0 & \\left((n+1)+q\\right)^2 & 0 & \\dots \\\\\n   &  &  &  &  & \\ddots &\n  \\end{pmatrix}, \n\\end{align}\nand\n       \\begin{align}\n       \\centering\n  \\hat{H}_1 = \\begin{pmatrix}\n  \\ddots &  & \\ddots &  &  \\\\\n  \\dots & -\\frac{s}{4} & 0 & -\\frac{s}{4} & 0 & 0 & \\dots \\\\\n  \\dots & 0 & -\\frac{s}{4} & 0 & -\\frac{s}{4} & 0 & \\dots \\\\\n  \\dots & 0 & 0 & -\\frac{s}{4} & 0 & -\\frac{s}{4} & \\dots \\\\\n   &  &  &  & \\ddots &  & \\ddots\n  \\end{pmatrix}, \\\n  \\hat{H}_2 = \\begin{pmatrix}\n  \\ddots &  & \\ddots &  &  \\\\\n  \\dots & -\\imath\\frac{s}{4} & 0 & \\imath\\frac{s}{4} & 0 & 0 & \\dots \\\\\n  \\dots & 0 & -\\imath\\frac{s}{4} & 0 & \\imath\\frac{s}{4} & 0 & \\dots \\\\\n  \\dots & 0 & 0 & -\\imath\\frac{s}{4} & 0 & \\imath\\frac{s}{4} & \\dots \\\\\n   &  &  &  & \\ddots &  & \\ddots\n  \\end{pmatrix}.\\nonumber\n  \\end{align}\n\\paragraph{Landau-Zener-type Hamiltonian.} Interestingly, a two-level approximation can be derived from the BEC system when $s\\ll 1$.\n\nCalculating the eigenvalues of $\\hat{H}$ as a function of the quasimomentum yields the lattice band structure $E_m(q)$ ($m\\in\\mathbb{N}$), as shown in Fig.~\\ref{levels_figure}. We denote the corresponding Bloch eigenfunctions $\\ket{\\Psi_m(q)}$. For a small value of $s$ (typically $s<0.5$) and $q$ close to $0.5$, the first two energy bands $E_0(q)$ and $E_1(q)$ are well-separated in energy from the others, and may be considered as an effective two-level system. If the BEC is initially prepared in the subspace formed by these first two bands, it will remain in this subspace.\n\\begin{figure}\n         \\centering\n    \\includegraphics[width=0.5\\textwidth]{fig9.pdf}\n    \\caption{Eigenvalues $E(q)$ of $\\hat{H}$ (colored solid lines) and $\\hat{p}^2$ (black dashed lines) as a function of $q$.}\n   \\label{levels_figure}\n\\end{figure}\n\n\nExperimentally, while the quasimomentum can be tuned from its initial value by the application of a force (inducing a Bloch oscillation), it is most often equal to zero, corresponding to a homogeneous BEC, or to the ground state of the lattice $\\ket{\\Psi_0(0)}$ (prepared adiabatically). We therefore do not consider the quasimomentum as a control parameter here, and assume $q=0$ for the rest of this section. Using the unitary transformation,\n    \\begin{align}\n      \\ket{\\psi} &\\rightarrow \\hat{U}\\ket{\\psi}, \\nonumber \\\\\n      \\hat{H} &\\rightarrow \\hat{U}\\hat{H}\\hat{U}^{\\dagger} + \\imath \\dot{\\hat{U}} \\hat{U}^{\\dagger},\\nonumber\n    \\end{align}\nwhere $\\hat{U}=e^{-\\imath \\hat{p}\\varphi(t)}$, we obtain the following Hamiltonian,\n    \\begin{align}\n      \\hat{H} = \\left(\\hat{p} + \\frac{\\dot{\\varphi}(t)}{2} \\right)^2 - \\frac{s}{2} \\cos \\left( \\hat{x} \\right) ,\\nonumber\n    \\end{align}\nwhere $\\frac{\\dot{\\varphi}(t)}{2}$ plays the role of a controlled quasi momentum (effectively, $\\frac{\\dot{\\varphi}(t)}{2}$ is the quasimomentum of the BEC in the reference frame of the moving lattice). If we set $\\frac{\\dot{\\varphi}(t)}{2} = \\frac{1}{2} + \\frac{\\dot{\\tilde{\\varphi}}(t)}{2}$, with $\\dot{\\tilde{\\varphi}}(t)$ a control parameter close to $0$, we can isolate the first two energy levels and write the Hamiltonian in the basis $\\left( \\ket{\\phi_{0-1}}, \\ket{\\phi_{0+0}} \\right)$ such that,\n       \\begin{align}\n   \\hat{H} = \\begin{pmatrix}\n  \\left( \\frac{1}{2} - \\frac{\\tilde{\\dot{\\varphi}}(t)}{2} \\right)^2 & -\\frac{s}{4}\\\\\n   -\\frac{s}{4} & \\left( \\frac{1}{2} + \\frac{\\tilde{\\dot{\\varphi}}(t)}{2} \\right)^2  \\\\\n  \\end{pmatrix}.\n\\end{align}\nUp to a term proportional to the identity, we obtain a two-level quantum system whose Hamiltonian can be expressed as\n    \\begin{align}\n      \\hat{H} = \\frac{\\Delta(t)}{2}\\hat{\\sigma}_z + \\frac{\\omega}{2}\\hat{\\sigma}_x ,\n    \\end{align}\nwhere $\\omega=-\\frac{s}{2}$ and $\\Delta(t) = - \\dot{\\tilde{\\varphi}}(t)$.\nThis expression is similar to the Hamiltonian of a two-level quantum system with an offset term and a single control, for which time optimal control strategies are studied in Sec.~\\ref{sec:OCT_spin_time_1_input_and_offset}. With the following basis change,\n\\begin{align}\n\\ket{\\phi_{0-1}}\\rightarrow\\frac{1}{\\sqrt{2}}(\\ket{\\phi_{0-1}}+\\ket{\\phi_{0-0}})\\nonumber \\\\\n\\ket{\\phi_{0-0}}\\rightarrow\\frac{1}{\\sqrt{2}}(\\ket{\\phi_{0-1}}-\\ket{\\phi_{0-0}})\\nonumber\n\\end{align}\nthe correspondence is exact and allows to implement optimal solutions in this system as shown in Sec.~\\ref{sec:Experimental_optimal_control}.\n\n\n\\subsection{Numerical optimal control}\n\\label{sec:Numerical-optimal-control}\n\nWe consider the application of GRAPE to the control of a BEC in an optical lattice. The BEC system has a Hamiltonian given by\n       \\begin{align}\n\\hat{H} = \\hat{H}_0 + \\cos\\left(u(t)\\right) \\hat{H}_1 + \\sin\\left(u(t)\\right)\\hat{H}_2,\n\\end{align}\nwhere we set $u(t)=\\varphi(t)$ to use the same notation as in the general description of GRAPE. The goal is to find a control that minimizes the cost function $\\mathcal{C}$ at a fixed final time $t_f$\n       \\begin{align}\n  \\mathcal{C} = 1 - \\lvert \\braket{\\psi_{f}}{\\psi(t_f)} \\rvert^2,\n  \\label{eq:cost_bec}\n\\end{align}\nwhere $\\ket{\\psi(t_f)}$ is the state at final time, and $\\ket{\\psi_{f}}$, the target state. The application of the PMP yields a Pontryagin Hamiltonian of the form \\eqref{eq:Hamilotnian_QOC_schrodinger} with $F_0=0$. The adjoint state $\\ket{\\chi(t)}$ whose time evolution is also governed by the Schr\\\"odinger equation, has the final condition\n       \\begin{align}\n  \\ket{\\chi(t_f)} = -2\\chi_0\\braket{\\psi_{f}}{\\psi(t_f)}\\ket{\\psi_{f}}.\n\t\\end{align}\nThe abnormal multiplier is set to $\\chi_0=-1/2$ in the numerical simulation. Using the maximization condition of the PMP, the control is iteratively updated such that\n       \\begin{align}\n u'_n = u_n - \\epsilon \\Im \\left(\\bra{\\chi(t_{n})}{\\left(-\\sin\\left(u_n\\right)\\hat H_1 + \\cos\\left(u_n\\right)\\hat{H}_2\\right)}\\ket{\\psi(t_n)}\\right)\n\\end{align}\nThe control time is set to a multiple duration characteristic of the dynamical timescale of the system (usually the inverse spacing between the two lowest energy levels), and discretized into several hundred steps, so that the step duration is small with respect to this dynamical timescale, and the control is therefore quasi-continuous. The infinite dimensional Hilbert space is truncated so that $|n|\\leq n_{\\textrm{max}}$, where $n_{\\textrm{max}}$ is chosen with respect to the initial and target states to avoid boundary effects. In the numerical simulations, the control usually involves $400$ steps and $n_{\\textrm{max}}=10$. Thus the truncated space has a dimension of $2\\times n_\\textrm{max}+1=21$. Under these conditions, one iteration of the algorithm takes $0.17$ seconds, and it takes about $100$ iterations, i.e. $17$~seconds to obtain a cost function of order $10^{-4}$. The numerical simulations, written in Python, were conducted on a standard laptop computer.\n\n\n\n\n\n\\paragraph{State-to-state transfer.}\n\n\n\nWe first illustrate the optimal control of state-to-state transfer. The initial state of the BEC is represented by the state $\\ket{\\phi_{0+0}}$ and several target states are considered, which can be expressed in the canonical basis $\\ket{\\phi_{0+n}}$ with $q=0$. Attainable states range from individual momentum basis vectors to superposition of states~\\cite{BEC2021}. Alternatively, they can also be Gaussian states, corresponding to a localized probability density in position and momentum within a lattice site~\\cite{BEC2023}.\nHere we define as Gaussian a state whose $x$ and $p$ probability density functions are normal distributions with standard deviations $\\sigma_{x_0}$ and $\\sigma_{p_0}$ equal to those of the ground state $\\ket{\\Psi_0(0)}$ of the lattice Hamiltonian. The Gaussian state tends to the exact ground state for $s \\gg 1$, with $\\sigma_{x_0}=s^{-1/4}$, and $\\sigma_{p_0}=s^{1/4}/2$. A displaced Gaussian state $\\ket{g(x_c,p_c)}$ has non-zero position and momentum averages within a lattice site, $\\langle x\\rangle =x_c$ and $\\langle p\\rangle=p_c$.\nWe can further define a squeezed Gaussian state $\\ket{g(x_c,p_c,\\xi)}$, for which the standard deviations are modified as $\\sigma_x=\\xi \\sigma_{x_0}$ and $\\sigma_p=\\frac{\\sigma_{p_0}}{\\xi}$, where $\\xi$ is the $x$- squeezing parameter ($\\xi=1$ corresponding to a Gaussian state). The smaller $\\xi$ becomes, the more the standard deviation $\\sigma_x$ decreases and $\\sigma_p$ increases.\n\nGaussian and squeezed states can be projected on the basis $\\left( \\ket{\\phi_{0,+n}} \\right)$ with the coefficients~\\cite{BEC2023},\n    \\begin{align}\n    c_{0,n}\\left( x_c, p_c, \\xi \\right) = \\left( \\frac{2 \\xi^2}{\\pi \\sqrt{s}} \\right)^{1/4} e^{\\imath x_c p_c /2} e^{-\\imath n x_c} e^{-\\xi^2 \\left( n - p_c \\right)^2 / \\sqrt{s}}.\n\\end{align}\n\n\nThree numerical examples of optimal controls for state to state transfer are considered in Fig.~\\ref{oct_BEC}. In each case, the initial state is $\\ket{\\phi_{0+0}}$, and we set $q=0$, $s=5$, $n_{max}=10$ and $t_f=7.6$, which corresponds to a duration of 150~$\\mu$s. The target states are chosen to be $\\ket{\\phi_{0+2}}$, the centered Gaussian state $\\ket{g(x_c=0,p_c=0,\\xi=1)}$ and the centered squeezed Gaussian state $\\ket{g(x_c=0,p_c=0,\\xi=1/3)}$. The numerical results can be obtained from a code provided in the supplementary material.\n\n\n\n\\begin{figure}\n    \\includegraphics[width=\\textwidth]{fig10.pdf}\n\n    \\caption{Examples of control for the state-to-state transfer of a BEC: In red the transfer from $\\ket{\\phi_{0+0}}$ to $\\ket{\\phi_{0+2}}$, in dotted-blue to the Gaussian state $\\ket{g(0,0,1)}$ and in dashed-yellow to the squeezed state $\\ket{g(0,0,1/3)}$. The transfers are computed for $q=0$ and $s=5$. (a) Control phase. (b) Bar diagram of the momentum distribution reached at time $t_f$. (c) Probability density in position within a lattice cell at final time.}\n   \\label{oct_BEC}\n\\end{figure}\n\n\n\\subsection{Experimental optimal control}\n\\label{sec:Experimental_optimal_control}\n\nIn this section, we present some illustrative optimal control results applied to a BEC manipulated in an optical lattice, following the previous formalism. Other examples can be found in~\\cite{BEC2021,BEC2023,BEC2024}. We focus here on the experimental implementation of the control in a real system, namely the experimental setup at LCAR in Toulouse, the manipulation of the full quantum state of the BEC in the lattice, and a realization of an optimal control in an effective two-level quantum system.\n\n\n\n\\subsubsection{Experimental setup.}\n\nIn a typical implementation~\\cite{BEC2021}, a BEC of $^{87}$Rb atoms is trapped in a one-dimensional optical lattice, created by two counter-propagating laser beams. The beams' wavelength, $\\lambda=1063.9\\,$nm, is chosen far from the main optical resonances of the atom, in order to minimize light scattering and heating of the condensate. This also sets the lattice spacing $d=\\lambda/2\\simeq 532\\,$nm, and the characteristic energy scale $E_L=h^2/(2md^2)=h\\cdot8.111\\,$kHz, which also sets a characteristic timescale for the dynamics.\n\nIn the Schr\\\"odinger equation~\\eqref{BEC_SE} governing the dynamics, both the dimensionless lattice depth $s(t)$ and its position $\\varphi(t)$ can be varied arbitrarily in time, and therefore act as control parameters. This is achieved by using  Acousto-Optic-Modulators (AOMs) placed in the path of the lattice beams, which can adjust the amplitude and phase of the outgoing beam by varying the amplitude and phase of the RF signal applied to the AOM crystal. A common modulator varies the amplitude of a first laser beam which is then split to form the two arms of the lattice, after which one of the arms goes through an AOM which modulates its phase, thus varying the relative phase between the lattice beams and controlling $\\varphi(t)$.\n\nTo ensure that optimal control solutions can be successfully applied in this system, it is important to consider the timescales involved. On the one hand, the dynamical timescale is determined by both the atoms' inertia and the depth of the sine potential: for a typical depth $s=5$, the energy spacing between the two lowest bands at $q=0$ is $1.975\\,E_L$ (close to the level spacing in the harmonic approximation $\\hbar \\omega\\simeq 2.236\\,E_L$), corresponding to a characteristic duration of $T_0=62.4\\,\\mu$s.\n    On the other hand, through a combination of bandwidth limitations from the driving electronics and the AOM itself, changes in the amplitude and phase of a beam exiting the AOM occur on a typical duration of $100\\,$ns for sudden changes, corresponding to a spectral bandwidth of about 3 MHz. This sets the main limit on the speed with which the controls can be varied: the $3\\,$MHz bandwidth nonetheless allows changes to be made almost instantaneously with respect to the dynamical timescale.\n\n\nThis leads to the typical choice for the control ramps applied experimentally: a duration of $100\\,\\mu\\mathrm{s}\\simeq 1.6T_0$, discretized in 400 intervals of $250\\,$ns with a constant phase. Any change in the value of the phase occurs much faster than the inertial response of the atoms: the control is therefore quasi-continuous.\n\n\\medskip\n\nIn addition to these considerations on the lattice control, it is also crucial to consider other experimental effects that are not included in the modeling of the experiment:\n\\begin{itemize}\n\\item[$-$]~The laser wavelength $\\lambda$ must be known precisely as it effectively enters in the dimensionless timescale $E_Lt/\\hbar=\\alpha t$. For commercially available fiber lasers, the wavelength is typically known to an accuracy of $10^{-4}$.\n\\item[$-$]~The $^{87}$Rb atoms used here experience repulsive interactions within the BEC, characterised by a scattering length $a=104 a_0$~\\cite{DGObook}. These interactions can be described in the mean-field approximation by an additional, non-linear potential term in the Schrödinger equation \\eqref{BEC_SE}, $V_\\mathrm{int}=\\beta|\\psi(x,t)|^2$, yielding the Gross-Pitaevskii equation. The constant $\\beta$ characterizes the non-linearity for the 1-D dynamics in the lattice. In the experiments presented here, it is typically small ($\\beta<0.5$) due to the dilute nature of the BEC.\n\\item[$-$]~In a realistic experiment, the BEC loaded in the optical lattice has a finite size, which may also be affected by interactions. This corresponds to the occupation of a finite interval of quasimomenta around a central value (here $q=0$). In experiments shown here, about 100 lattice sites are populated, leading to an estimate of the quasi-momentum width $\\Delta q\\sim0.02$~\\cite{Dubertrand2016}.\n\\item[$-$]~Last but not least, the lattice depth $s$ is a fixed parameter for the optimal control using $\\varphi(t)$, but it must be known with a good precision to derive efficient controls. This means that it is crucial to have a precise calibration of the lattice depth~\\cite{Cabrera2018} before optimizing the control, as well as excellent stability of the experimental setup.\n\\end{itemize}\n\n\\begin{figure}\n         \\centering\n    \\includegraphics[width=0.8\\textwidth]{fig11.pdf}\n\n    \\caption{Examples of the influence of various parameters on the performance of the optimal control ramps for the state-to-state transfer of a BEC. (a) Fidelity of the preparation for a control ramp computed at an expected depth $s_0=5$, when the ramp is applied in a lattice of actual constant depth $s$, as the value of $s$ is varied. The full orange line (resp. blue dotted line and yellow dashed line) corresponds to the transfer from $\\ket{\\phi_{0+0}}$ to $\\ket{\\phi_{0+2}}$ (resp. to the Gaussian state $\\ket{g(0,0,1)}$ and to the squeezed state $\\ket{g(0,0,1/3)}$), here and throughout the figure. Likewise, throughout the figure, the control ramps are calculated for fixed values of the parameters: $s=5$, $q=0$, $\\beta=0$ and $\\alpha=1$, and are the ramps obtained in Fig.~\\ref{oct_BEC}. (b) Similarly, fidelity of the preparation against a variation of the quasi-momentum: the control ramp is applied to the initial state $\\ket{\\phi_{q+0}}$, with $q$ varied. (c) Similarly, fidelity of the preparation against a variation of the timescale factor $\\alpha$ (see text). (d) Fidelity of the preparation against a variation of the interaction parameter $\\beta$ (see text). In each graph, the gray shaded area denotes typical experimental uncertainty intervals for the varied parameter.}\n   \\label{oct_BEC_parameters}\n\\end{figure}\n\nTo illustrate the role of these parameters, we show in Fig.~\\ref{oct_BEC_parameters} how the fidelity of the state preparations studied in Sec.~\\ref{sec:Numerical-optimal-control} are affected by variations around the value for which the optimal control is calculated, with relevant uncertainty ranges highlighted. For realistic values of the parameters, Fig.~\\ref{oct_BEC_parameters} demonstrates that the timescale factor is very well characterized and that the effect of interaction is mostly negligible. It also highlights the importance of a well-calibrated lattice depth, and a small quasimomentum distribution width for the success of the state transfer. The quasimomentum effect is all the more important the more squeezed the target state is (\\emph{i.e.} extended in momentum).\n\nThere are other important constraints on the controls available experimentally, namely on the maximum depth $s$ that can be applied to the atoms, and on the maximum available control time. The former is limited by the maximum laser intensity available, to $s\\lesssim40$ for the experimental setup described here. The latter is constrained, when using $^{87}$Rb, by interaction-induced dynamical instabilities that may occur on a timescale of several milliseconds~\\cite{DupontPNAS2023}.\n\nFinally, we emphasize that in an experimental situation, it is not possible to directly measure the complex coefficients $c_{q,n}$ characterizing the quantum state. A single measurement of the BEC consists in imaging the absorption from a resonant infrared laser beam by the atoms, which is imaged on a CCD camera. This imaging is performed after a time-of-flight, during which the various momentum components of the BEC will separate spatially. Such a measurement, when good care is taken to remove any parasitic signal on the camera, will only provide a measurement of the probabilities $|c_{q,n}|^2$. A full characterization of the prepared state may therefore require multiple measurements, in order to extract relative phases~\\cite{BEC2021}, or to perform a full state reconstruction~\\cite{BEC2023}.\n\n\\subsubsection{Full quantum state control.}\n\nA clear demonstration of optimal transfer between quantum states with control of probability amplitudes is provided by the preparation of energy eigenstates. The Bloch eigenstates corresponding to the energy spectrum for the lattice potential,  as shown in Fig.~\\ref{levels_figure}, are defined by their coefficients $c_{q,n}^{(m)}$, stationary solutions of Eq.~\\eqref{BEC_coef}, with specific amplitudes and signs. When such a state is prepared in the lattice, its stationary nature means that the momentum distribution (the measured probabilities $|c_{q,n}|^2$) do not evolve at subsequent times.\n\n\n\\begin{figure}\n         \\centering\n    \\includegraphics[width=0.8\\textwidth]{fig12.pdf}\n    \\caption{Preparation of lattice eigenstates. (a) Lattice band structure for $s=8.2$ (colored lines) with labels for the eigenstates prepared in (b) and (c). The shaded area denotes the sine potential depth. (b) Preparation of the $P$ band eigenstate at quasi-momentum $q = 0$ for a depth $s = 8.15 \\pm 0.30$. $\\mathrm{(b_1)}$ Experimental evolution of momentum distribution of the prepared state in the static lattice. $\\mathrm{(b_2)}$ Corresponding theoretical evolution. (c) Same as (b) for the preparation of the $D$ band eigenstate  at quasi-momentum $q= 0.25 k_L$ for a depth  $s = 8.26 \\pm 0.10$.}\n   \\label{oct_BEC_eigenstates}\n\\end{figure}\n\nThis control process is illustrated in Fig.~\\ref{oct_BEC_eigenstates}. We first apply an optimal control ramp to transfer the lattice ground state $\\ket{\\Psi_0(0)}$ to the $P$ band Bloch state at $q=0$, as denoted in Fig~\\ref{oct_BEC_eigenstates} \\textbf{a}, in a lattice of depth $s=8.2$. After the preparation, we measure the momentum distribution obtained for increasing hold times, up to $110~\\mu\\mathrm{s}\\simeq1.5T_0$. The result of this measurement is shown in Fig.~\\ref{oct_BEC_eigenstates} \\textbf{b$_1$}: the momentum distribution shows no significant evolution, as expected for an eigenstate. For comparison, Fig.~\\ref{oct_BEC_eigenstates} \\textbf{b$_2$} shows the numerical evolution of the momentum distribution, as expected from the theoretical final state of the optimal control ramp (which has a numerical fidelity of 99\\% to the theoretical $P$ band eingenstate in good agreement with the experimental results).\n\nIt is also possible to prepare eigenstates in a subspace with non-zero quasi-momentum $q_0$, by taking advantage of a change of reference frame. We first prepare the plane wave superposition with coefficients $c_{q_0,n}^{(m)}$, starting from the lattice ground state. At the end of the preparation ramp $\\varphi(t)$, instead of returning the phase to $\\varphi=0$ (in which case we remain in the $q=0$ subspace where the prepared state is not an eigenstate), we set the lattice in linear motion with a ramp\n\n$\\varphi(t>t_f)=2q_0(t-t_f)$ (in reduced units). This effectively translates the state into the $q=q_0$ subspace in the reference frame of the lattice, and the prepared superposition is then an eigenstate. This is illustrated in Fig~\\ref{oct_BEC_eigenstates} \\textbf{c$_1$}, which shows the evolution of the momentum distribution after preparation of the $D$ band eigenstate at $q_0=0.25k_L$ (as denoted in Fig~\\ref{oct_BEC_eigenstates} \\textbf{a}) in the moving lattice. Again the distribution shows very little evolution, in good agreement with the numerically expected result shown in Fig~\\ref{oct_BEC_eigenstates} \\textbf{c$_2$}.\nFurther examples of state preparation and state characterization can be found in~\\cite{BEC2021,BEC2023,BEC2024}.\n\n\\subsubsection{Two-level optimal control.}\n\nFinally, the BEC system lends itself to the emulation of a two-level quantum system, as introduced in Sec.~\\ref{BEC_model}.  The control protocol derived in Sec.~\\ref{sec:OCT_spin_time_1_input_and_offset} can be used to perform a time-optimal transfer between the states,\n\\begin{align}\n\\ket{+}=\\frac{1}{\\sqrt{2}}(\\ket{\\phi_{0-1}}+\\ket{\\phi_{0-0}}),\\nonumber \\\\\n\\ket{-}=\\frac{1}{\\sqrt{2}}(\\ket{\\phi_{0-1}}-\\ket{\\phi_{0-0}}).\\nonumber\n\\end{align}\nIn this context, the lattice depth plays the role of the constant offset $\\Delta=s/2$, while the phase variation $\\dot{\\varphi}=1+u(t)$ provides the variable control. To perform the bang-bang protocol of Sec.~\\ref{sec:OCT_spin_time_1_input_and_offset}, we choose a lattice depth of $s\\simeq0.5$, and a maximum control $u_0=0.5$. This yields typical control times for the experiment of $t_1\\simeq 64\\,\\mu\\mathrm{s}$ and $t_2\\simeq 156\\,\\mu\\mathrm{s}$.\n\nIn order to assess the result of the transfer, we need to characterize the initial and final states. However both $\\ket{+}$ and $\\ket{-}$ are approximate eigenstates of the Hamiltonian at a small depth, and they cannot be distinguished from a simple measurement of the (equally weighted) populations in $\\ket{\\phi_{0-1}}$ and $\\ket{\\phi_{0-0}}$. To circumvent this issue, we use a quench of these states into a deeper lattice $s_\\mathrm{meas}\\simeq6$, in which they are not eigenstates. The population dynamics in the deeper lattice then allow us to clearly distinguish $\\ket{+}$ and $\\ket{-}$. These state superpositions can be prepared using an optimal transfer ramp,  in the deeper lattice of depth $s_\\mathrm{meas}$, both as initial states for the bang-bang protocol, and to characterize their evolution. We use a preparation ramp with numerical fidelity $F>99.5\\%$, and a duration of $1.5T_0$ (about $84\\,\\mu\\mathrm{s}$). The state is then characterized by recording the momentum distribution dynamics for $200\\,\\mu\\mathrm{s}$ at $10\\,\\mu\\mathrm{s}$ intervals.\n\n\\begin{figure}\n         \\centering\n    \\includegraphics[width=0.8\\textwidth]{fig13.pdf}\n    \\caption{Time-optimal control of an effective two-level quantum system on the BEC platform. (a) Theoretical $\\mathrm{(a_1)}$ and experimental $\\mathrm{(a_2)}$ dynamics of the momentum distribution after preparation of the state $\\ket{+}$ in the lattice of depth $s_\\mathrm{meas}=6.07\\pm0.05$. The numerical fidelity after preparation is $F>0.995$. (b) Same as (a) for the preparation and characterization of state $\\ket{-}$. (c) Experimental evolution of the momentum distribution for a bang-bang transfer between $\\ket{-}$ and $\\ket{+}$ in a lattice of depth $s=0.57\\pm0.03$ $\\mathrm{(c_1)}$, followed by a quench to a deep lattice $s_\\mathrm{meas}=5.90\\pm0.09$ $\\mathrm{(c_2)}$. The optimal control parameters are $u_0=0.5$, $t_1=63.66\\,\\mu\\mathrm{s}$, and $t_2=159.5\\,\\mu\\mathrm{s}$. (d) Same as (c) for a bang-bang transfer between $\\ket{+}$ and $\\ket{-}$. The color map for probabilities on all graphs extends from 0 to 1. \n    }\n   \\label{oct_BEC_LZexp}\n\\end{figure}\n\nThe realization of such a procedure is shown in Fig.~\\ref{oct_BEC_LZexp}. Panels Fig.\\ref{oct_BEC_LZexp} \\textbf{a} and Fig.\\ref{oct_BEC_LZexp} \\textbf{b} present both the numerical and experimental evolution of the states $\\ket{+}$ and $\\ket{-}$ in the quenched lattice of depth $s_\\mathrm{meas}$. This demonstrates that the change in relative phase can be clearly identified through the non-equilibrium dynamics in the deeper lattice, and shows at the same time that the two superposition states can be prepared efficiently as initial states for the bang-bang control.\nFigure~\\ref{oct_BEC_LZexp} \\textbf{c} (resp. \\textbf{d}) shows the experimental results from the time-optimal control for the transfer from $\\ket{-}$ to $\\ket{+}$ (resp. $\\ket{+}$ to $\\ket{-}$), followed by a quench to the depth $s_\\mathrm{meas}$. The control is realized by applying two successive constant phase drifts $\\dot{\\varphi}=1\\pm 0.5$ for times $t_1$ and $t_2$ and is identical for both transfers (only the initial state is changed). After the transfer, the quenched dynamics confirm that the states have been exchanged with good accuracy, the dynamics being almost identical to those of panels  Fig.~\\ref{oct_BEC_LZexp} \\textbf{a} and Fig.~\\ref{oct_BEC_LZexp} \\textbf{b}.\n\nNote that a more thorough characterization of the initially prepared and final states can be achieved by state reconstruction techniques using the lattice dynamics data~\\cite{BEC2023}. This goes beyond our purpose here, which has been to illustrate how the BEC platform can be used to emulate optimal control of a two-level quantum system.\n\n\n\\section{Conclusion}\n\\label{sec:conclusion}\n\nIn this introduction to the toolbox of quantum optimal control, we present both analytical and numerical methods based on the PMP. The key elements of this mathematical theory are described from an analogy with classical Lagrangian and Hamiltonian mechanics. It is then shown how these results can be used to design optimal control strategies for quantum systems. A comprehensive description of existing optimization algorithms is provided with a discussion of their advantages and areas of applicability. Particular attention is paid to shooting techniques and gradient-based algorithms that are directly derived from the PMP. Several problems of state-to-state transfer in a two-level quantum system have been analyzed in detail. The link between the optimal solution and the quantum speed limit is also explored in this case. The experimental implementation in the case of BEC in a one-dimensional optical lattice is described in a final section. Starting from the modeling of the quantum dynamics, we show step by step how the optimal control is computed and then implemented experimentally, to realize both two-level and many-level controls. Experimental constraints and limitations are also discussed.\n\nThe goal of this tutorial paper is to provide an overview of the toolbox of QOC that we hope is accessible to physicists with a background in quantum dynamics. Simple and concrete physical examples have been used throughout this paper to illustrate the different mathematical concepts. In particular, we have successively reused and adapted the same examples to illustrate different important points. Therefore, the examples presented in this introduction are not representative of the systems encountered in the literature. For instance, a key aspect which has not been discussed concerns the control of open quantum systems. Most experimental configurations must be modeled by taking into account the interaction of the system with its environment, and such features must be integrated into the optimization process~\\cite{Koch2016}.\nThe degree to which OCT techniques have been applied to that end depends on the characteristics of the open quantum system considered.\n\nIn the Markovian regime in which the memory effect is neglected, optimal control procedures are now quite well understood. The main difficulty lies in the loss of complete controlability and therefore in the fact that certain target states are not reachable~\\cite{dirr2009}. The situation is not at the same stage of maturity for non-Markovian dynamics. Although this aspect has been explored in a few examples, the usefulness of the memory effect as a resource for optimal control remains to be clarified.\n\n\n\n\nA wide range of problems have been already solved in quantum optimal control, but with the advent of quantum computers and the progress of quantum technologies, new objectives are emerging. A first one concerns optimization performed on a quantum computer~\\cite{hogg2000quantum,moll2018quantum,blekos2023review}. Such optimizations are based on quantum algorithms and the realization of quantum circuits in order to solve optimal control problems. The optimization can focus on the control design for the computer itself, but it can also be a totally independent control problem. Different quantum optimization algorithms can be distinguished extending from purely quantum algorithms, such as Grover's type algorithms~\\cite{baritompa2005grover} and quantum annealing methods~\\cite{das2005quantum,yarkoni2022quantum} or to hybrid algorithms based on a gradient descent~\\cite{yang2017optimizing,cerezo2021variational,bonet2023performance}. In the latter case, the core of the algorithm is classical and it is the same as the one presented in Sec.~\\ref{sec:GRAPE_algo}. The difference resides in the evaluation of the gradient, which is computed using quantum algorithms. This is particularly well adapted to quantum optimal control, but it does not offer any significant advantage, since the core of the algorithm remains classical. Breakthroughs are expected with pure quantum algorithms. Recently a few properties of the PMP have been combined with a Quantum Approximate Optimization Algorithm (QAOA)~\\cite{PhysRevLett.126.070505,PhysRevApplied.16.054023}, but a general and versatile quantum algorithm based on the PMP remains to be found. Ideally, a PMP based quantum optimization algorithm should be designed for any type of cost function (not only restricted to bang-bang controls, like in~\\cite{PhysRevLett.126.070505}), and it would solve the shooting problem using the advantages offered by quantum computations.\n\nA second family of open questions concerns the inclusion of energy efficiency and sustainable development criteria in quantum technologies~\\cite{PRXQuantum.3.020101}. The latter are not exempt from the challenge of global warming. Despite the fact that we are working with a very small number of quantum quantities, energy consumption is far from ideal. With the most powerful devices, we are beginning to achieve quantum supremacy, but we are far from having a quantum energy advantage. Part of the huge energy consumption comes from the cooling system needed to keep noise levels low, while a second source is due to controls. So far, we have mostly focused on time-optimal strategies, to avoid noise or dissipation effects. However, energy-optimal strategies can achieve a similar result with a drastic reduction  in energy consumption.  Optimizing energy consumption at all stages of quantum computing (and other quantum technologies) will be one of the major problems to be solved in the near future.\n\n\\noindent \\textbf{Acknowledgments.} We gratefully acknowledge useful discussions with Pr. H. R. Jauslin. We thank the support from the Erasmus Mundus Master QuanTeem (Project number: 101050730), the project QuanTEdu-France (ANR-22-CMAS-0001) on quantum technologies  and the ANR project QuCoBEC (ANR-22-CE47-0008-02).\n\n\\appendix\n\n\\section{List of mathematical symbols and acronyms}\\label{secappA}\n\\begin{itemize}\n    \\item[$ $] $X$ is the state of a dynamical system and $X_a$ is a vector component of $X$.\n    \\item[$ $] $\\delta X$ is the infinitesimal difference between two states of a dynamical system.\n    \\item[$ $] $S$ is an action functional.\n    \\item[$ $] $\\delta S$ is the functional derivative of the action $S$ for two trajectories close to each other.\n    \\item[$ $] $\\Mc L$ is a Lagrangian.\n    \\item[$ $] $F$ is a vector function defining the system dynamics.\n    \\item[$ $] $u$ is a control parameter, $u_a$ is a vector component of $u$, and $u_n$ is the value at step $n$ of a piecewise constant control.\n    \\item[$ $] $U$ is the domain of definition of $u(t)$ (a subset of $\\setR^m$).\n    \\item[$ $] $G$ is a terminal cost function.\n    \\item[$ $] $F_0$ is a running cost function.\n    \\item[$ $] $\\mathcal{C}$ is the cost functional to minimize in an optimal control problem.\n     \\item[$ $] $\\Lambda$ is the adjoint state, and $\\Lambda_a$ is a vector component.\n    \\item[$ $] $\\Lambda_0$ is the adnormal multiplier.\n    \\item[$ $] $H_p$ is the Pontryagin's Hamiltonian\n    \\item[$ $] $\\Re$ and $\\Im$ are respectively the real and imaginary parts of a complex number.\n    \\item[$ $] $\\ket{\\psi}$ is a quantum state.\n    \\item[$ $] $\\hat \\rho$ is a density matrix.\n    \\item[$ $] $\\hat H$ is the Hamiltonian operator of a quantum system.\n    \\item[$ $] $\\sigx, \\sigy$ and $\\sigz$ are the Pauli matrices.\n    \\item[$ $] $(x,y,z)$ are the coordinates of the Bloch vector for a two-level quantum system.\n     \\item[$ $] $\\hat U(t_f,t_i)$ is the evolution operator from $t=t_i$ to $t=t_f$.\n     \\item[$ $] $\\ket{\\chi}$ is the adjoint state of $\\ket{\\psi}$.\n\\end{itemize}\nThe following acronyms are used in this paper:\n\\begin{itemize}\n\\item[$ $] OCT: Optimal Control Theory\n\\item[$ $] QOC: Quantum Optimal Control Theory\n\\item[$ $] PMP: Pontryagin Maximum Principle\n\\item[$ $] QSL: Quantum Speed Limit\n\\end{itemize}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Lagrange Multiplier}\\label{app_lagrange}\nWe recall in this section basic results on Lagrange multiplier in the finite-dimensional case. We are particularly interested in abnormal multipliers which are less described in the literature.\n\nThe method of Lagrange multiplier is a standard technique in finite-dimensional optimization problem that transforms a constrained optimization into an unconstrained one at the cost of an increase in dimension. Consider for instance the maximization (or minimization) of a smooth function $F_0$ on $\\mathbb{R}^2$ with variables $(x,y)$. In absence of constraints, a necessary condition to fulfill for the extrema of the function is given by\n$$\n\\nabla F_0 =\\begin{pmatrix}\n\\frac{\\partial F_0}{\\partial x} \\\\\n\\frac{\\partial F_0}{\\partial y}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix}.\n$$\nA slightly more difficult task is to find the maximum of $F_0$ under a constraint of the form $F(x,y)=0$ where $F$ is also a smooth function on $\\mathbb{R}^2$. The extrema of $F_0$ are to be found on the level curve $F(x,y)=0$. In the generic case, the extremum is located at the point of $\\mathbb{R}^2$ where the level curves  of $F$ and $F_0$ intersect at one point. At this point, the two curves have a common tangent and their gradient vectors are parallel. This gives the following condition\n$$\n\\nabla F_0=-\\lambda \\nabla F,\n$$\nwhere $\\lambda$ is a non-zero real parameter called a Lagrange multiplier. Note that the exact value of $\\lambda$ is not important for finding the extrema. A systematic way to solve this problem is to introduce a new function $L$ on $\\mathbb{R}^3$ as\n$$\nL(x,y,\\lambda)=F_0(x,y)+\\lambda F(x,y).\n$$\nThe extrema are characterized by $\\nabla L=0$, which leads to the same equations as those established previously. The extremum point is denoted $(x_0,y_0)$.\n\nA singular behavior occurs when $\\nabla F(x_0,y_0)=0$ and in this case it is necessary to adapt the procedure. A typical situation corresponds to the problem where the condition $F(x,y)=0$ is satisfied at only one isolated point. It is then clear that the value of $F_0$ at this point cannot be compared to its neighboring points. Consider for instance the function $F_0$ to be maximized $F_0(x,y)=x+y$ under the constraint $F(x,y)=x^2+y^2=0$. The point $(0,0)$ is the only point that satisfies the constraint. It therefore corresponds by construction to the point maximizing $F_0$. Note that we also have $\\nabla F(0,0)=0$. This solution can be obtained by the general approach by modifying the definition of $L$ which is now a function on $\\mathbb{R}^4$ such that\n$$\nL(x,y,\\lambda,\\lambda_0)=\\lambda_0F_0(x,y)+\\lambda F(x,y),\n$$\nwhere the real parameter $\\lambda_0$ is called the abnormal multiplier. Note that $(\\lambda,\\lambda_0)$ is defined up to a real factor and that the two multipliers cannot be simultaneously equal to 0. The extrema are given by the conditions $\\partial L/\\partial x=\\partial L/\\partial y=\\partial L/\\partial \\lambda=0$. When $\\lambda_0\\neq 0$, we recover the previous formulation of the optimization problem. New solutions appear when $\\lambda_0=0$ and $\\nabla F=0$. They have the peculiarity of not depending on $F_0$, i.e. the function to maximize. They are called abnormal extremal solutions. In the previous example, the point $(0,0)$ corresponds to such a solution for which $\\lambda_0=0$.\n\n\\section{Pontryagin Maximum Principle}\\label{appendixPMP}\nWe describe in a heuristic way the origin of the maximization condition of the PMP described in Thm.~\\ref{thm:Pontryagin Maximum Principle}~\\cite{bertsekasbook}. This also leads to a valuable geometric interpretation of the adjoint state and of the optimal control problem.\n\nWe first consider a time-optimal control process where the goal is to steer the system from $X_0$ to $X_f$ in minimum time. We denote respectively by $u^\\star$ and $X^\\star$ a smooth optimal control and the corresponding trajectory such that $\\dot{X}^\\star=F(X^\\star,u^\\star)$. We consider another admissible control $u$ close to $u^\\star$ which generates the dynamic $X(t)$ with $\\dot{X}=F(X,u)$. The two trajectories are very close to each other and the small difference between the two is $\\delta X=X-X^\\star$. Starting from $\\dot{X}^\\star+\\delta \\dot{X}=F(X^\\star+\\delta X,u)$, a linearized equation of motion is used to calculate the dynamics around the reference trajectory  $X^\\star$ (up to terms of order two in $\\delta X$)\n\\begin{equation}\\label{eqappdelta}\n\\delta \\dot{X}=A\\delta X+F(X^\\star,u)-F(X^\\star,u^\\star),\n\\end{equation}\nwhere the elements of the $n\\times n$- matrix $A$ are given by $A_{ij}=\\partial_{X_j} F_i|_{(X^\\star,u^\\star)}$. The rigorous derivation of Eq.~\\eqref{eqappdelta} can be done under the assumption of regularity of $F$ and convexity of the set $F(X,u)$ for $u\\in U$~\\cite{bertsekasbook}. The solution of Eq.~\\eqref{eqappdelta} can be expressed as\n\\begin{equation}\\label{eqappdelta2}\n\\delta X(t)=\\int_{0}^t V(t,t')[F(X^\\star,u)-F(X^\\star,u^\\star)]dt',\n\\end{equation}\nwhere $\\delta X(0)=0$ and $V$ is the propagator from times 0 to $t$ associated to the differential equation $\\dot{V}(t,0)=A(t) V(t,0)$, with $V(0,0)=I_n$~\\cite{bryson1975applied}. The different elements of the optimal control problem are schematically represented in Fig.~\\ref{fig10}.\n\\begin{figure}[htbp]\n\\begin{center}\n\\includegraphics[width=10cm]{figC1.pdf}\n\\end{center}\n\\caption{Schematic description of a time-optimal control problem from $X_0$ to $X_f$. The vector space is $\\mathbb{R}^2$ and the coordinates of $X$ are $(X_1,X_2)$. The set $\\mathcal{A}(t_f,X_0)$ is the reachable set at time $t_f$ from $X_0$ (different reachable sets are plotted in shades of gray at different times). Two trajectories are plotted in black reaching respectively the points $X_f$ (optimal trajectory) and $X(t_f)$ (non-optimal trajectory). The blue line corresponds to the tangent to the reachable set in $X_f$. The adjoint state $\\Lambda(t_f)$ is orthogonal to this line, while $\\dot{X}$ is tangent to the trajectory.}\n\\label{fig10}\n\\end{figure}\nWe then introduce the reachable set $\\mathcal{A}(t_f,X_0)$ at time $t_f$ from the state $X_0$ as the set of all the states $X(t_f)$ that can be reached by a trajectory starting from $X_0$ at time $t=0$, the trajectory being associated to an admissible control $u$. If the target state $X_f$ is attained exactly by the optimal solution at $t=t_f$ then $X_f$ is on the boundary of $\\mathcal{A}(t_f,X_0)$. It is clear that if $X_f$ belongs to the interior of $\\mathcal{A}(t_f,X_0)$ then a smaller time $t'<t_f$ can be found such that $X_f\\in \\mathcal{A}(t',X_0)$ and $t_f$ is not the minimum time to reach the target (see the reachable sets at different times in Fig.~\\ref{fig10} to be convinced of this point). Assuming that the reachable set is convex in a neighborhood of $X_f$, we consider the plane tangent to $\\mathcal{A}(t_f,X_0)$ in $X_f$ and we define the adjoint state $\\Lambda(t_f)$ at time $t_f$ as a vector orthogonal to this plane and pointing outwards. For any final state $X(t_f)$ close to $X_f$ and associated to a non-optimal trajectory, we have $\\delta X(t_f)\\cdot \\Lambda(t_f)\\leq 0$ where $\\delta X(t_f)=X(t_f)-X_f$.\n\nWe introduce a time-dependent vector $\\Lambda(t)\\in\\mathbb{R}^n$ such that $\\Lambda(t)^\\intercal=\\Lambda(t_f)^\\intercal V(t_f,t)$. Using the relation $\\dot{V}(t_f,t)=-V(t_f,t)A$, we arrive at $\\dot{\\Lambda}^\\intercal=-\\Lambda(t)^\\intercal A$ or\n\\begin{equation}\\label{eqappdelta3}\n\\dot{\\Lambda}(t)=-A^\\intercal \\Lambda(t).\n\\end{equation}\nAs expected, $\\Lambda(t)$ which is defined by a backward propagation of the dynamic can be identified with the adjoint state of the PMP. The Pontryagin Hamiltonian $H_P$ reads $H_P=\\Lambda^\\intercal F$. The corresponding Hamiltonian equation for the adjoint state can be written as\n$$\n\\dot{\\Lambda}_i(t)=-\\frac{\\partial H_P}{\\partial X_i}=-\\Lambda^\\intercal \\partial_{X_i}F(X,u)=-\\sum_j\\partial_{X_i}F_j(X,u)\\Lambda_j,\n$$\nwhich is equivalent to Eq.~\\eqref{eqappdelta3}. Starting from Eq.~\\eqref{eqappdelta2} and the condition $\\delta X(t_f)\\cdot \\Lambda(t_f)\\leq 0$, we obtain\n$$\n\\Lambda(t_f)^\\intercal \\int_0^{t_f}V(t_f,t')[F(X^\\star,u)-F(X^\\star,u^\\star)]dt'\\leq 0,\n$$\nwhich leads to\n$$\n\\int_0^{t_f}\\Lambda(t')^\\intercal[F(X^\\star,u)-F(X^\\star,u^\\star)]dt'\\leq 0.\n$$\nThis inequality can be transformed into\n\\begin{equation}\\label{eqappmax}\n\\int_0^{t_f}[H_P(t')-H_P^\\star(t')]dt'\\leq 0,\n\\end{equation}\nwhere $H_P=\\Lambda^\\intercal F(X^\\star,u)$ and $H_P^\\star=\\Lambda^\\intercal F(X^\\star,u^\\star)$.\nConsider now for $u$ a control equal to $u^\\star$ except on a very short time interval $[t,t+dt]$, we deduce that the condition \\eqref{eqappmax} is satisfied if and only if\n$$\nH_P(t)\\leq H_P^\\star(t),\n$$\nfor $t\\in [0,t_f]$, i.e. $u^\\star$ maximizes the function $H_P$ along the optimal trajectory.\n\nWe consider in a second step the relative position of $\\Lambda(t_f)$ and $\\dot{X}$ as represented in Fig.~\\ref{fig10}. For the optimal solution, we deduce by construction that $H_P(t_f)=\\Lambda(t_f)\\cdot \\dot{X}(t_f)\\geq 0$. In this case, $X_f$ does not belong to $\\mathcal{A}(X_0,t_f-dt)$ for any sufficiently small time step $dt>0$. Note that if $\\dot{X}$ points inwards the reachable set, the trajectory will be time-maximal. When $H_P$ does not depend explicitly on time, the Pontryagin Hamiltonian is a constant of motion. It is straightforward to show this property if $U$ is an open set. In this case, we have $\\frac{\\partial H_P}{\\partial u}=\\Lambda\\cdot \\frac{\\partial F}{\\partial u}=0$ at any time $t$. We deduce that $\\dot{H}_P=\\dot{\\Lambda}F+\\Lambda\\dot{F}$ can be expressed as\n$$\n\\frac{d}{dt}H_P=-\\Lambda \\frac{\\partial F}{\\partial X}F+\\Lambda\\frac{\\partial F}{\\partial X}F=0.\n$$\nWe denote by $-\\Lambda_0$ the positive constant equal to $\\Lambda\\cdot \\dot{X}$ at any time $t\\in [0,t_f]$. A new Pontryagin Hamiltonian can then be defined as\n$$\nH_P=\\Lambda\\cdot \\dot{X}+\\Lambda_0,\n$$\nwhere $H_P=0$ along the optimal trajectory. The abnormal extremals for which $\\Lambda_0=0$ can be identified here to the trajectory tangent to the boundary of the reachable set since in this case $\\Lambda(t_f)\\cdot \\dot{X}(t_f)=0$.\n\n\nThis argument can be extended to an optimal control problem with a fixed control time $t_f$ where the goal is to minimize the cost functional $\\mathcal{C}=G(X(t_f))$. This cost can be for instance the distance from the target $X_f$ to $X(t_f)$. A geometric description of this case is given in Fig.~\\ref{fig11}. We consider the level sets of the function $G$ as the set of points where $G(X)$ is a constant. The optimal situation corresponds to the case where the boundary of the reachable set at $t_f$ and the level set $G(X(t_f))$ are tangent in $X(t_f)$. We introduce the plane tangent to the two sets in $X(t_f)$. The adjoint state $\\Lambda(t_f)$ is then defined up to a factor as the opposite of the gradient of the level set in $X(t_f)$, such that the two vectors point outwards of their respective sets. We denote by $\\Lambda_0$ this negative constant and we finally have\n$$\n\\Lambda(t_f)=\\Lambda_0\\frac{\\partial G(X(t_f)}{\\partial X(t_f)}.\n$$\nBy definition of the optimal solution, we have\n$$\nG(X^\\star(t_f))\\leq G(X(t_f)).\n$$\nAt first order in $\\delta X$, we deduce that\n\\begin{equation}\\label{eqappdelta4}\n\\partial_XG^\\intercal \\delta X(t_f)\\geq 0.\n\\end{equation}\nIt is then straightforward to show that\n$$\n\\Lambda(t_f)^\\intercal \\int_0^{t_f}V(t_f,t')[F(X^\\star,u)-F(X^\\star,u^\\star)]dt'\\leq 0,\n$$\nwhich gives\n$$\n\\int_0^{t_f}\\Lambda(t')^\\intercal[F(X^\\star,u)-F(X^\\star,u^\\star)]dt'\\leq 0.\n$$\nWe obtain the same inequality \\eqref{eqappmax} by introducing the Pontryagin Hamiltonian.\n\n\n\n\\begin{figure}[htbp]\n\\begin{center}\n\\includegraphics[width=10cm]{figC2.pdf}\n\\end{center}\n\\caption{Same as Fig.~\\ref{fig10} but for the case of a fixed final time, the goal of the control problem being to minimize the distance to the target defined by the function $G$. The different reachable sets and the level surfaces of $G$ are plotted respectively in shades of gray and red. The blue line represents the common tangent plane to the boundary of the reachable set $\\mathcal{A}(t_f,X_0)$ and the level curves of the distance $G(X(t_f))$. $\\partial G$ denotes the gradient vector pointing outwards of the level surface at $X(t_f)$.}\n\\label{fig11}\n\\end{figure}\n\n\n\n\n\n\n\n\\section{Description of the numerical optimization codes}\\label{app_code}\nThis section aims to briefly present the different codes provided in the supplementary material. All codes are in Python and are executable independently of each other. The two GRAPE codes require only the use of the standard Python scientific libraries Numpy and SciPy. \n\n\\paragraph{Shooting algorithm.} The code  \\textit{shooting.py} implements the shooting method, an algorithm used in Sec.~\\ref{sec:shooting_algo}. The integration of the equations of motion given by the PMP is performed using the Python package Nutopy~\\cite{nutopy} and the function \\textit{ivp.exp}. The optimization uses the function \\textit{nle.solve}. The final part of the code returns two graphs, one with the Bloch coordinates $(x,y,z)$ of the two-level quantum system, and the other with the optimized control. We also provide a second code \\textit{shooting2.py} without the package Nutopy, but with less precision.\n\n\n\\paragraph{GRAPE for a two-level quantum system.}\n\\textit{GRAPE.py} provides a minimal working code for the optimization of a control with GRAPE. It allows to derive the results of Sec.~\\ref{sec:GRAPE_algo}, i.e. state-to-state transfer from the ground to the excited states. Several build-in functions are defined in the code, to perform the forward and backward propagation of the Schr\\\"odinger equation using the split-operator method. The optimization uses the Scipy function \\textit{minimize} with the BFGS method. Here, the exact gradient is provided to the solver (with the function \\textit{GradientGRAPE}), and the Hessian is estimated numerically from \\textit{minimize}. An initial control must be provided to the algorithm. Many different choices would lead to the same result. Here, a cosine wave function is used. It is chosen different from zero, and smooth in order to obtain a good starting point for the algorithm. \\textit{GRAPE2.py} considers the same control problem but with a polynomial parameterization of the control pulse. The computation of the gradient is modified accordingly.\n\n\n\n\n\n\n\n\\paragraph{GRAPE for a BEC system} The code \\textit{GRAPE\\_BEC.py} implements the algorithm GRAPE for state-to-state transfer in the case of BEC in a one-dimensional optical lattice. The results are those presented in Sec.~\\ref{sec:Numerical-optimal-control}. The code is composed of several sections. The first is a class (``BEC'') which generates from the size of the system $N_k$, the value of the quasimomentum  $q$ and the depth of the lattice $s$, the Hamiltonian of the system (the matrices $H_0$, $H_1$ and $H_2$). The second section (``propagation''), also a class, defines the functions which return the fidelity and the correction to be made to the control given by the PMP. The third section groups together the functions of the code. The fourth section describes the system parameters, the constants, the control time, the values of $q$ and $s$, the initial state and the initial guess for the control. The fifth section aims to calculate the optimal control. The \\textit{scipy.optimize.minimize} algorithm is used to iteratively find the solution to the state-to-state transfer problem, where the function to be minimized is ``Cost'' and the gradient ``dCost'' (based on class ``propagation''). The algorithm uses a L-BFGS-B method, it is therefore a second order algorithm since the Hessian is iteratively approximated at each step. The user can define a maximum number of iterations, a tolerance, and a bound for the control. The final sixth section displays the results for the three transfers, namely the controls, the population $\\left\\lvert c_{q,n} \\right\\rvert^2$, and the probability density.\n\nAt each iteration, we can also perform a line search approach to find the best value of the parameter $\\epsilon$. This is done by computing a step that satisfies the Wolfe condition with \\textit{scipy.optimize.line\\_search}.\n\n\n\\section*{References}\n\\bibliographystyle{vancouver}\n\n\n\n\n\n\n\n\\end{document}\n"}
{"paper_id": "2403-00533", "version": "2403-00533v1", "root_file_path": "d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\data_raw\\2403-00533\\tex\\2403-00533v1\\main.tex", "metadata": {"total_length": 64852, "merged_count": 1, "merged_files": ["main.tex"], "missing_files": []}, "content": "\\documentclass[superscriptaddress,onecolumn,showpacs,a4paper,\namssymb,amsmath,nobibnotes,aps,prd,\nshowkeys,\nnofootinbib,notitlepage]{revtex4-1}\n\\usepackage{tensor}\n\\usepackage{amsmath}\n\\usepackage{amssymb} \n\\usepackage[cm]{fullpage}\n\\usepackage{graphicx} \n\\usepackage{siunitx}\n\\usepackage{lipsum}\n\\usepackage{tabularx} \n\\usepackage{comment}\n\\usepackage[colorlinks,linkcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}\n\\usepackage[nameinlink]{cleveref}\n\\usepackage[table,xcdraw]{xcolor}\n\\usepackage[utf8]{inputenc}\n\\usepackage{tikz}\n\\def\\checkmark{\\tikz\\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} \n\\usepackage{fontawesome} \n\\newcommand{\\pmark}{\\faWarningSign}\n\\DeclareSIUnit\\parsec{pc}\n\\newcommand{\\eqtxt}[1]{\\mathrel{\\overset{\\makebox[0pt]{\\mbox{\\normalfont\\tiny\\sffamily #1}}}{=}}}\n\n\\newcommand{\\mary}[1]{{ \\textbf{\\textcolor{purple}{  [Mary: #1] } }}}\n\\newcommand{\\WG}[1]{{\\textcolor{blue}{\\bf[WG: #1]}}}\n\\newcommand{\\Adam}[1]{{\\textcolor{red}{\\bf[Adam: #1]}} }\n\n\\begin{document}\n\n\\title{Gravitational waves in a cyclic Universe: resilience through cycles and vacuum state}\n\n\\author{Mariaveronica De Angelis}\n\\email{mdeangelis1@sheffield.ac.uk}\n\n\\author{Adam Smith}\n\\email{asmith69@sheffield.ac.uk}\n\n\\author{William Giar\\`e}\n\\email{w.giare@sheffield.ac.uk}\n\n\\author{Carsten van de Bruck}\n\\email{c.vandebruck@sheffield.ac.uk}\n\n\\affiliation{School of Mathematics and Statistics, The University of Sheffield, Hounsfield Road, S3 7RH Sheffield, United Kingdom}\n\n\n\n\\begin{abstract}\nWe present a generalised calculation for the spectrum of primordial tensor perturbations in a cyclic Universe, making no assumptions about the vacuum state of the theory and accounting for the contribution of tensor modes produced in the dark energy phase of the previous cycle. We show that these modes have minimal impact on the spectrum observed in the current cycle, except for corrections on scales as large as the comoving Hubble radius today. These corrections are due to sub-horizon modes produced towards the end of the dark energy phase, persisting into the \\textit{ekpyrotic} phase of the next cycle as additional quanta. In relation to the vacuum state, we argue that non-Bunch-Davies quanta can easily overwhelm the energy density driving the dark energy phase, potentially compromising the model. Therefore, avoiding backreaction effects sets restrictive constraints on deviations away from the Bunch-Davies vacuum during this phase, limiting the overall freedom to consider alternative vacua in the cyclic Universe.\n\\end{abstract}\n\n\\maketitle\n\n\n\\section{Introduction}\\label{intro}\n\nThe most compelling observational evidence supporting cosmological inflation~\\cite{Guth:1980zm,Linde:1981mu,Albrecht:1982wi,Vilenkin:1983xq} as the leading theory of the early Universe is currently provided by the Planck satellite measurement of the spectral index of scalar perturbations, $n_s = 0.9649\\pm0.0042$~\\cite{Planck:2018jri}. In the simplest single-field slow-roll inflationary models, the spectrum of scalar modes is expected to be almost but not exactly flat~\\cite{Mukhanov:1981xt, Bardeen:1983qw,Hawking:1982cz,Guth:1982ec}, with deviations from flatness are quantified in terms of how much $n_s$ deviates from 1~\\cite{Lidsey:1995np,Lyth:1998xn,Baumann:2009ds,Martin:2013tda}. As a result, the Planck data seem to be in excellent agreement with the theoretical predictions of inflationary models~\\cite{Planck:2018jri,Planck:2018vyg}, ruling out a Harrison-Zeldovich scale-invariant spectrum~\\cite{Harrison:1969fb,Zeldovich:1972zz,Peebles:1970ag} (corresponding to $n_s=1$) at a statistical level exceeding $8.5$ standard deviations and lending weight to the inflationary paradigm.\n\nThat being said, with no aim to downplay the significance of this result or its interpretation, it is crucial to emphasise that, on its own, it does not provide conclusive evidence for cosmological inflation. Even hinging on a certain level of optimism and setting aside the uncertainty surrounding constraints on $n_s$ from CMB experiments other than Planck\\footnote{Over the years, constraints on the spectral index have been released by a multitude of Planck-independent CMB experiments such as WMAP~\\cite{WMAP:2012fli,WMAP:2012nax}, the Atacama Cosmology Telescope (ACT)~\\cite{ACT:2020frw, ACT:2020gnv}, and the South Pole Telescope (SPT)~\\cite{SPT-3G:2014dbx, SPT-3G:2021eoc}. When considering these data at face value, Planck is currently the only experiment excluding $n_s=1$ at a statistical significance much larger than $3\\sigma$. Conversely, ACT shows a preference for $n_s=1$~\\cite{ACT:2020gnv,Giare:2022rvg}. Different combinations of these data overall support the result $n_s\\ne1$, although sometimes they lead to discordant results in terms of the other inflationary parameters or the preferred inflationary models~\\cite{Forconi:2021que,Giare:2023wzl}.} -- or the potential implications arising from the well-known tensions~\\cite{Bernal:2016gxb,Verde:2019ivm,DiValentino:2020zio,DiValentino:2021izs,Abdalla:2022yfr} characterising the recent debate\\footnote{For studies suggesting potential implications of cosmological tensions for inflation, see, e.g., Refs.~\\cite{DiValentino:2018zjj,Ye:2021nej,Ye:2022efx,Jiang:2022uyg,Jiang:2022qlj,Takahashi:2021bti,Lin:2022gbl,Hazra:2022rdl,Braglia:2021sun,Keeley:2020rmo,Jiang:2023bsz}}\n--  alternative theoretical mechanisms have been put forth, yielding an almost scale-invariant spectrum of primordial density fluctuations without invoking inflation.\n\nAn illustrative example of such mechanisms is the cyclic Universe~\\cite{Steinhardt:2001st,Steinhardt:2002ih,Steinhardt:2002kw,Khoury:2003rt,Turok:2004yx,Khoury:2004xi,Lehners:2008vx} that, in contrast to the conventional cosmological framework, suggests a periodic history for the Cosmos. The model has been extensively studied and discussed in relation to a broad range of topics, including quantum gravity, modified gravity, gravitational waves and dark energy, see e.g., Refs.~\\cite{Boyle:2003km,Ashtekar:2003hd,Bojowald:2004kt,Xiong:2007cn,Frampton:2007cv,Narlikar:2007hip,Xiong:2007cn,Baum:2007de,Biswas:2008ti,Cailleteau:2009fv,Brandenberger:2009ic,El-Nabulsi:2011xss,Cai:2012ag,Cai:2010zma,Nojiri:2011kd,Chang:2012yk,Ivanov:2012hq,Saaidi:2012qp,Bars:2013vba,Tavakoli:2014mra,Oriti:2016qtz,deCesare:2016rsf,Pavlovic:2017umo,Saridakis:2018fth,Das:2018bzx,Ijjas:2018bko,Ahmed:2019bff,Li:2019laq,Scherrer:2019dkc,Ijjas:2021zwv,Gorkavyi:2021tbw,Martin-Benito:2021szh,Calcinari:2022iss,Giovannetti:2022qje,Giovannetti:2023psb} or Refs.~\\cite{Battefeld:2014uga,Brandenberger:2016vhg} for reviews. In broad terms, each cycle comprises a phase recasting the standard Hot Big Bang theory (during which large-scale structures take shape), followed by a phase of slow, accelerated expansion mirroring the present-day observational evidence for a Dark Energy dominated dynamics. In the cyclic Universe, this latter stage also serves to dilute inhomogeneities and flatten the spatial geometry. Subsequently, a contraction phase ensues, generating nearly scale-invariant density perturbations. Finally, the cycle concludes with a big-crunch/big-bang transition, during which matter and radiation are generated, setting the stage for the next cycle. \n\nNotice that both inflation and the cyclic Universe provide physical mechanisms to produce an almost scale-invariant spectrum of density perturbations~\\cite{Khoury:2001zk,Lehners:2007ac,Buchbinder:2007tw}. In addition, they can both explain observational facts such as the homogeneity in the cosmic microwave background (CMB) radiation~\\cite{Lehners:2013cka} and the fact that the present-day spatial geometry of the Universe appears to be flat, or at the very least nearly flat\\footnote{For recent discussions surrounding the spatial geometry of the Universe, see, e.g.,~\\cite{Park:2017xbl,Handley:2019tkm,DiValentino:2019qzk,Efstathiou:2020wem,DiValentino:2020hov,Benisty:2020otr,Vagnozzi:2020rcz,Vagnozzi:2020dfn,DiValentino:2020kpf,Yang:2021hxg,Cao:2021ldv,Dhawan:2021mel,Dinda:2021ffa,Gonzalez:2021ojp,Akarsu:2021max,Cao:2022ugh,Glanville:2022xes,Bel:2022iuf,Yang:2022kho,Stevens:2022evv,Favale:2023lnp}}. Therefore, at first glance, one might wonder how to distinguish between the two models. Focusing solely on scalar modes, this is a challenging knot to unravel~\\cite{Khoury:2003vb,Gratton:2003pe}. However, the two scenarios yield significantly distinct predictions for the stochastic background of gravitational waves~\\cite{Boyle:2003km}. Similar to scalar modes, inflation predicts a nearly scale-invariant (red-tilted) spectrum of tensor modes~\\cite{Baumann:2009ds,Martin:2013tda,Caprini:2018mtu}. Conversely, in the cyclic Universe, the tensor spectrum is typically blue-tilted, and its amplitude is many orders of magnitude lower than that predicted by inflation, remaining well below any observable threshold achievable in the near future. Consequently, any measurement of primordial gravitational waves (e.g., through the effects left in the CMB B-mode polarisation at large angular scales) would offer conclusive evidence for inflation, discounting the cyclic model. \n\nDespite this fact being acknowledged as a strength for inflation and perhaps a limitation in predictive capacity for the cyclic model, it is worth emphasising a few caveats surrounding this conclusion. Firstly, despite the best efforts, the detection of primordial tensor modes remains elusive at present~\\cite{BICEP:2021xfz}, making it impossible to discriminate between the two scenarios. Therefore, the cyclic Universe remains an alternative worth considering. Secondly, the inflationary predictions concerning the amplitude and tilt of the tensor spectrum depend significantly on the specific model. While well-known consistency relations among inflationary parameters can be derived within single-field slow-roll inflation minimally coupled to gravity~\\cite{Baumann:2009ds,Martin:2013tda}, these relations can be violated by various physical mechanisms. A long yet not exhaustive list of possibilities include considering modified gravity~\\cite{Kobayashi:2010cm,Kawasaki:2013xsa,Nozari:2016jmn,Giare:2020vss}, multi-field inflation~\\cite{Kaiser:2013sna,Price:2014ufa,Achucarro:2010da,DeAngelis:2023fdu,Giare:2023kiv}, additional (spectator) rolling axion fields~\\cite{Mukohyama:2014gba,Namba:2015gja,Peloso:2016gqs,Ozsoy:2020ccy}, couplings to axion-gauge or spin-2 fields~\\cite{Dimastrogiovanni:2016fuu,Iacconi:2019vgc}, breaking spatial and/or temporal diffeomorphism invariance~\\cite{Endlich:2012pz,Cannone:2014uqa,Graef:2015ova,Ricciardone:2016lym}, higher curvature corrections to the effective gravitational action~\\cite{Baumann:2015xxa,Giare:2020plo}, higher order operators in effective field theory~\\cite{Capurri:2020qgz,Giare:2022wxq}, violations of the null energy condition~\\cite{Cai:2022lec,Ye:2023tpz}, alternative vacuum state/initial conditions~\\cite{Ashoorioon:2013eia,Ashoorioon:2014nta,Choudhury:2023kam}, sound speed resonances~\\cite{Cai:2020ovp}, inflation in an Universe filled with an elastic medium~\\cite{Gruzinov:2004ty}, and possible effects/models inspired by quantum gravity~\\cite{Ashoorioon:2005ep,Brandenberger:2006xi,Brandenberger:2014faa,Baumgart:2021ptt}. Many of these more elaborated scenarios yield completely different predictions, often resulting in a blue-tilted spectrum and possibly leaving signatures in different cosmological and astrophysical observables~\\cite{Stewart:2007fu,Cai:2014uka,Wang:2014kqa,Kuroyanagi:2014nba,Kuroyanagi:2020sfw,Giare:2020vhn,Vagnozzi:2020gtf,Vagnozzi:2023lwo,Jiang:2023gfe,Oikonomou:2024aww}. Furthermore, models with an arbitrarily small tensor amplitude can always be constructed (see, e.g., Ref~\\cite{Stein:2022cpk}), making it virtually impossible to rule out inflation based solely on a lack of detection of primordial gravitational waves. This is a critique frequently raised against inflationary cosmology as it questions its actual predictive capability.\n\nAs concerns the cyclic Universe, since any difference with respect to inflation in terms of predictions is likely to be confined to the spectrum of tensor modes, it becomes interesting to test whether similar caveats apply or if the model demonstrates greater resilience. \n\nIn light of this, we review the production of primordial gravitational waves in a cyclic Universe, identifying (and eventually clarifying) some conceptual aspects related to its concrete predictivity. Specifically, prevailing calculations in the existing literature conventionally establish initial conditions for primordial scalar and tensor modes during the \\textit{ekpyrotic} contracting phase~\\cite{Boyle:2003km}. While for scalar perturbations the implications of setting the initial conditions in different phases have been examined~\\cite{Erickson:2006wc}, the calculation of the tensor spectrum has always been performed starting in the \\textit{ekpyrotic} phase, assuming a Bunch-Davies (BD) vacuum state and neglecting any potential contributions arising from tensor modes originating during the dark energy phase of the previous cycle. This leads us to question whether they exert any influence on the spectrum observed in the current cycle. Taking a broader perspective, one may wonder whether the predictions concerning tensor modes remain resilient throughout the diverse cycles of the model itself. Yet another aspect that is imperative to clarify is to what extent the predictions depend on the choice of the vacuum state, addressing the crucial question of what freedom exists in the cyclic Universe regarding the choice of the vacuum state and whether substantial alterations can arise in the tensor spectrum by assuming different vacua, akin to what is found in inflationary cosmology.\n\nTo address these points, we present a general model for the evolution of gravitational waves produced in a cyclic Universe, making no assumptions about the initial vacuum state and starting the calculation from the dark energy phase of the previous cycle. We find that the additional tensor models originated in the previous cycle have minimal impact on the tensor spectrum observed in the current cycle, except for corrections on scales as large as the comoving Hubble radius today that are due to sub-horizon modes produced towards the end of the previous dark energy phase. Most importantly, we find that non-BD quanta in the dark energy phase can easily overwhelm the energy density associated with the modulus field, potentially spoiling the model. Avoiding these backreaction effects sets restrictive constraints on deviations away from the BD vacuum during the dark energy phase, thereby limiting the overall freedom to consider alternative vacua in the cyclic Universe.\n\nThe paper is organised as follows. In \\autoref{construction of the model}, we introduce the cyclic Universe model and review its background dynamics. In \\autoref{primordial tensor spectrum}, we consider the evolution of gravitational waves in such a Universe, starting from the previous cycle's dark energy phase and deriving the evolution in full generality. In \\autoref{sec:discussion}, we discuss the implications for the model's predictions, deriving constraints on the choice of the vacuum state and analysing the contribution coming from tensor modes originated in the previous cycle. Finally, in \\autoref{sec:conc}, we derive our main conclusions.\n\n\\section{Cyclic model and Background Dynamics} \\label{construction of the model}\nWe consider a simple scalar field setup in which the dynamics of the cyclic model in the Einstein frame are well described by the 4$D$ effective Lagrangian~\\cite{Erickson:2006wc}\n\\begin{equation}\\label{lagrangian}\n    \\mathcal{L}=\\sqrt{-g}\\left(\\frac{M_{\\rm Pl}^2}{2}R-\\frac{1}{2}\\partial_\\mu\\phi\\partial^\\mu\\phi-V(\\phi)\\right),\n\\end{equation}\nwhere $g$ is the determinant of the metric $g_{\\mu \\nu}$, $R$ is the Ricci scalar and we adopt units where $c=1$. The scalar field $\\phi$ is a modulus field, driving the dark energy dominated phase and the subsequent \\textit{ekpyrotic} and contracting kinetic phases, which we discuss below. Assuming a spatially flat FRLW background, the scalar field satisfies the usual equation of motion\n\\begin{equation}\n    \\ddot{\\phi}+3H\\dot{\\phi}+V_{,\\phi}=0,\n    \\label{KG}\n\\end{equation}\nwhere dots denote derivatives with respect to cosmological time $t$. On the other hand, ignoring any coupling between the scalar field and the other standard model species and neglecting any additional contributions from the latter to the total Universe energy density, the evolution of the scale factor is governed by the Friedmann equation that, in terms of the Hubble parameter $H=\\dot{a}/a$, reads\n\\begin{equation}\n    H^2=\\frac{1}{3 M_{\\rm Pl}^2}\\biggl(\\frac{1}{2}\\dot{\\phi}^2 + V(\\phi)\\biggl).\n    \\label{H2}\n\\end{equation}\nIn what follows, to efficiently describe the dynamics of the cyclic Universe we focus on a phenomenological potential of the form\n\\begin{equation}\\label{potnetial} \n    V=V_0\\left(1-e^{-c\\phi/M_{\\rm Pl}}\\right)Y(\\phi),\n\\end{equation}\nwhere $V_0$ is of the same order of the vacuum energy observed in today’s Universe, $c$ is a positive constant value and $Y(\\phi)$ is a step function. Notice that our choice concerning the specific potential employed in the work is, in part, motivated by the fact that the exponential form is convenient for analysis, and in part from the fact that the same potential has been widely adopted in similar studies, allowing a direct comparison between our findings and other results documented in the existing literature. However, it is important to emphasise that cyclic models can emerge from a broad spectrum of different potentials that should ultimately emerge from the higher-dimensional theory. Without loss of generality, the only constraint comes from requiring an acceptable spectrum of scalar perturbations that implies considering a steep, strongly negatively curved region across observational ranges of the field to reproduce. \n\nHaving that said, the potential~\\eqref{potnetial} serves multiple purposes, including describing dark energy responsible for cosmic acceleration observed today. More importantly, it plays a crucial role in transitioning the Universe from accelerated expansion to contraction. This is achieved by rolling from positive to negative values of the potential until reaching a time where $H^2=0$ and consequently triggering a phase characterised by an equation of state $\\omega \\gg 1$. For instance, by solving Eq.~\\eqref{H2} it can be shown that when the negative potential dominates $V\\simeq -V_0e^{-c\\phi}$ (\\textit{i.e.} \\textit{ekpyrotic} phase), the scale factor behaves as~\\cite{Erickson:2006wc}\n\\begin{equation}\n    a(t)\\sim (-t)^{\\tilde{\\alpha}},\n\\end{equation}\nwhere $t$ has negative values and $\\tilde{\\alpha}=2/c^2$. At this point it is also convenient to introduce the conformal time $d\\tau=dt/a(t),$ which we will frequently use later. In terms of the conformal time, the scale factor during the \\textit{ekpyrotic} phase evolves as:\n\\begin{equation}\n    a(\\tau)\\sim \\biggl[(-1)^{\\tilde{\\alpha}}(\\tau-\\tilde{\\alpha} \\tau)^{\\tilde{\\alpha}/(1-\\tilde{\\alpha})}\\biggl]_{\\tau_i}^{\\tau_{f}},\n    \\label{scalefactor}\n\\end{equation}\nunderscoring that the Universe is gradually contracting while the scalar field slowly descends along its sharply decreasing negative potential, to produce an acceptable spectrum of cosmological scalar perturbations.\n\nIn the literature, the \\textit{ekpyrotic} phase is typically assumed as the starting point of the cycle where initial conditions of primordial perturbations are imposed, and the calculations of the relative scalar and tensor spectra begin. However, in this study, we want to extend the model to include the contribution of tensor perturbations produced during the dark energy phase of the previous cycle to investigate whether they could have any impact on the spectrum we observe in the current cycle and eventually clarify why (not). To do so, the overall strategy will be to start the calculation in the dark energy phase of the previous cycle (making no assumptions on the vacuum state) and evolve the system through four regimes. For this reason, before dealing with the explicit calculation of the tensor spectrum, given that in our case we consider one more phase than in previous studies, it is useful to dedicate the following two subsections to reviewing the background dynamics of the model. In the same spirit of the discussion outlined so far, we start from the dark energy phase of the previous cycle and evolve the scale factor, ensuring its continuity across the boundaries of each phase. Additionally, we derive constraints on the model's parameters based on minimum theoretical requirements, such as the continuity of $H(t)$ and the consistency of the theory across cycles.\n\n\\subsection{Evolution and continuity of the scale factor across stages}\n\n\\subsubsection{Dark Energy phase}\nWe start from the dark energy phase, in which the expansion rate $H$ is roughly constant and the scale factor $a(t)$ behaves as\n\\begin{equation}\n    a(t)=a(t_{\\rm{tr}})e^{H\\left(t-t_{\\rm{tr}}\\right)} \\;\\;\\;\\;t<t_{\\rm{tr}},\n    \\label{adarkenergy}\n\\end{equation}\n\nwith $t_{tr}$ transition time between dark energy and \\textit{ekpyrotic} phase. The equation above can be translated in terms of conformal time as\n\n\n\n\n\n\\begin{equation}\n    a(\\tau) = \\frac{1}{H(B-\\tau)},\n    \\label{adarkenergyconf}\n\\end{equation}\nwhere, for the continuity across $\\tau = \\tau_{\\rm{tr}}$, $B$ is fixed to\\footnote{The value of $B$ is achieved by considering $H_0, a(\\tau_{\\rm{r}})$ and $\\tau_{\\rm{r}}$ given in the next sections.}\n\\begin{equation}\\label{B_const}\n    B = \\frac{1}{a(\\tau_{\\rm{tr}})H}+\\tau_{\\rm{tr}}.\n\\end{equation}\nAdditionally, by means of Eq.~\\eqref{scalefactor}, we can infer \n\\begin{equation}\n    \\frac{a(\\tau_{\\rm{tr}})}{a(\\tau_{\\rm{end}})} = \\left(\\frac{\\tau_{\\rm{tr}}- \\tau_{\\rm{ek}}}{\\tau_{\\rm{end}}-\\tau_{\\rm{ek}}}\\right)^\\alpha,\n\\end{equation}\nwhere $\\alpha\\equiv \\tilde{\\alpha}/(1-\\tilde{\\alpha})$ and $\\tau_{\\rm{ek}}\\equiv (1-2\\alpha)\\tau_{\\rm{end}}$ is the conformal time corresponding to when the potential diverges to minus infinity.\n\n\\subsubsection{Ekpyrotic phase}\n\nAs a next step, we transition to the \\textit{ekpyrotic} phase. In this phase the potential becomes negative and the Einstein frame expansion forces the scale factor to contract\n\\begin{equation}\\label{ekpyrotic scale factor}\n    \\frac{a(\\tau)}{a(\\tau_{end})}=\\left(\\frac{\\tau-\\tau_{ek}}{\\tau_{end}-\\tau_{ek}}\\right)^\\alpha\\;\\;\\;\\;\\; \\tau_{tr}<\\tau<\\tau_{end}.\n\\end{equation}\nWe note again that, being $\\alpha\\ll 1$, the contraction is very slow.\n\n\\subsubsection{Contracting kinetic phase}\n\nOnce $\\tau > \\tau_{\\rm{end}}$ we enter the region where the effects of the potential are negligible, namely $\\phi<\\phi_{\\rm{end}}$. During this period -- known as contracting kinetic phase -- we have:\n\\begin{equation}\n    \\frac{a(\\tau)}{a(\\tau_{\\rm{r}})}=\\left(\\frac{-\\tau}{(1+\\chi)\\tau_{\\rm{r}}}\\right)^{\\frac{1}{2}}\\;\\;\\;\\;\\;\\tau_{\\rm{end}}<\\tau<0,\n    \\label{akineticcontraction}\n\\end{equation}\nwhere $\\chi$ is a small positive constant that measures the amount of radiation created at the bounce ($\\tau=0$). \n\n\\subsubsection{Expanding kinetic phase}\n\nFinally, for the last phase of the cycle, the so-called expanding kinetic phase, we get\n\\begin{equation}\n    a(\\tau)=\\left(\\frac{\\tau}{\\tau_{\\rm{r}}}\\right)^{\\frac{1}{2}},\\;\\;\\;\\;\\;0<\\tau<\\tau_{\\rm{r}},\n    \\label{aexpanding}\n\\end{equation}\nNotice that, for convenience, we work in a coordinate frame where the scale factor is set to $a(\\tau_{\\rm{r}}) = 1$ for the time $t_{\\rm{r}}$ corresponding to the beginning of the radiation-dominated era. The corresponding conformal time $\\tau_{\\rm{r}}= (2H_{\\rm{r}})^{-1}$ is constrained by the radiation temperature $T_{\\rm{r}}$, being $H_{\\rm{r}}\\propto T_{\\rm{r}}^2/M_{\\rm{Pl}}$. Following Ref.~\\cite{Khoury:2003rt}, we choose a quite conservative value  $T_{\\rm{r}} \\sim 10^7$ GeV, akin to that obtained in the more familiar standard cosmology at the end of the reheating phase following inflation. This choice also ensures that we can safely recover predictions of primordial Big Bang Nucleosynthesis (BBN).\n\n\\subsection{Parameter constraints}\n\n\\subsubsection{Continuity of the Hubble parameter}\n\nIn order to constrain the length of the \\textit{ekpyrotic} contracting phase, we require that the Hubble parameter returns to its original value after every cycle. Following Ref.~\\cite{Erickson:2006wc}, this implies that\n\\begin{equation}\nH_{\\rm{end}}/H_{\\rm{0}}\\approx \\sqrt{\\frac{-V_{\\rm{end}}}{V_0}}\n\\end{equation}\nwhere $V_{\\rm{end}}$ is the depth of the potential well and $V_0$ is the height of the potential plateau. The spectral range of perturbations produced when the field rolls from $V\\approx 0$ to $V\\approx -V_{\\rm{end}}$, satisfies\n\\begin{equation}\n    \\frac{k_{\\rm{max}}}{k_{\\rm{min}}}\\approx\\sqrt{\\frac{-V_{\\rm{end}}}{V_0}},\n    \\label{Hratio}\n\\end{equation}\nand it needs to span at least $N=60$ e-folds for the \\textit{ekpyrotic} phase to produce a scale-invariant spectrum over a broad enough range of scales for us to observe today~\\cite{Planck:2018jri,Planck:2018vyg}. Hence, the transition time between dark energy and \\textit{ekpyrotic} phase is constrained to\n\\begin{equation}\n    \\frac{H_{\\rm{tr}}}{H_{\\rm{end}}}=2\\, \\alpha\\, \\tau_{\\rm{end}} \\, \\frac{a_{\\rm{end}}\\, a_{\\rm{tr}}}{{(\\tau_{\\rm{tr}}-\\tau_{\\rm{ek}})}}<e^{-60}.\n    \\label{Htrans/Hend}\n\\end{equation}\nMoreover, as $a(\\tau)\\approx \\rm{const}$ during the \\textit{ekpyrotic} contracting phase \\cite{Erickson:2006wc}, from Eq.~\\eqref{Htrans/Hend} it follows that\n\\begin{equation}\n    \\left|\\tau_{\\rm{tr}}-\\tau_{\\rm{ek}}\\right|>2\\, \\alpha\\, \\tau_{\\rm{end}}\\, a_{\\rm{end}}^2\\, e^{60}.\n\\end{equation}\n\n\\subsubsection{Cycling constraint}\nTo place constraints on the duration of the kinetic evolution phases, we require that they must last enough time for the scalar field to have started at the potential minimum (\\textit{i.e.} $\\phi=\\phi_{\\rm{end}}$), moved off to the Bounce (\\textit{i.e.} $\\phi\\to-\\infty$), and returned all the way back past $\\phi_{\\rm{end}}$ and made it up to the potential plateau to begin a radiation-dominated Universe.\n\nBarring some brief $\\omega\\gg 1$ period (which divides the expanding kinetic phase into two parts) as the field moves back up past $\\phi_{\\rm{end}}$ to the potential plateau, from Eq.~\\eqref{KG} and Eq.~\\eqref{potnetial} we find\n\\begin{equation}\n    \\phi-\\phi_{end}=c_1 \\ln\\left(\\frac{t}{t_{end}}\\right),\n\\end{equation}\nwhere the factor $c_1^2=2/3$ comes from Eq.~\\eqref{H2} during kinetic domination, $t_{\\rm{end}}$ is the time taken to reach $\\phi_{\\rm{end}}$  starting at $\\phi\\to-\\infty$. \n\nNotice that in the region where $\\omega\\gg 1$, contributions from $Y(\\phi)$ becomes relevant and $V\\approx V_0\\left(1-e^{-c\\phi}\\right)$. Therefore, in this case, the time $t_{\\rm{r}}$ required to climb the potential well and reach the plateau at $V\\approx V_0$ can be bounded to \n\\begin{equation}\n    \\frac{t_{\\rm{r}}}{t_{\\rm{end}}}>\\left(\\frac{V_{\\rm{end}}}{V_0}\\right)^{\\sqrt{\\frac{3}{2 c^2}}}.\n    \\label{eq:tr_tend}\n\\end{equation}\nSince the time taken for the field to cross the negative region of the potential before radiation domination begins is given by\n\\begin{equation}\n    \\frac{t_{\\rm{r}}}{t_{\\rm{end}}}\\approx \\frac{\\sqrt{V_{\\rm{end}}}}{H_{\\rm{r}}},\n\\end{equation}\nfrom Eq.~\\eqref{eq:tr_tend} we can infer an upper limit on the Hubble parameter at $t_r$ which reads  \n\\begin{equation}\n    H_{\\rm{r}} \\lesssim \\frac{\\sqrt{V_{\\rm{end}}}}{M_{\\rm{Pl}}}\\left(\\frac{V_0}{V_{\\rm{end}}}\\right)^{\\frac{3}{2c^2}}.\n    \\label{Hr}\n\\end{equation}    \nThis upper limit constrains the ratio between $\\tau_r$ and $\\tau_{\\rm end}$ to\n\\begin{equation}\\label{gamma}\n    \\Gamma = \\left|\\frac{\\tau_{\\rm{r}}}{\\tau_{\\rm{end}}}\\right| \\gtrsim \\left(\\frac{V_{\\rm{end}}}{V_0}\\right)^{\\sqrt{\\frac{2}{3c^2}}}\\simeq 10^8,\n\\end{equation}\nwhere we used that the vacuum energy density $\\rho_{\\Lambda} = V_0 \\sim 10^{-120}M_{\\rm{Pl}}^4$ and $V_{\\rm{end}}\\sim 10^{-20} M_{\\rm{Pl}}^4$.\nFollowing Ref.~\\cite{Boyle:2003km}, throughout this paper, we always consider the dimensionless parameter $\\Gamma \\sim 10^8$.\\\\\n\\\\\n\\noindent In conclusion, the final constraints we derive for the cyclic model at the background level (and that are important to bear in mind for the following discussion on tensor perturbations) are:\n\\begin{align}\n    \\left|\\frac{\\tau_{\\rm{r}}}{\\tau_{\\rm{end}}}\\right|&\\gtrsim 10^8, & (\\tau_{\\rm{tr}} - \\tau_{\\rm{ek}})&\\gtrsim 10^9, & \\tau_{\\rm{r}} = \\frac{1}{2H_{\\rm{r}}}.\n\\end{align}\n\n\n\\section{General primordial tensor spectrum}\n\\label{primordial tensor spectrum}\nConsidering a spatially flat FLRW metric, in the synchronous gauge the perturbed line element reads:\n\\begin{equation}\nd s^2=a^2(\\tau)\\left[d \\tau^2-\\left(\\delta_{i j}+h_{i j}\\right) d x^i d x^j\\right].\n\\end{equation}\nTensor modes (i.e., metric perturbations) are described in terms of the transverse and traceless part of the symmetric 3×3 matrix $h_{ij}$. To characterise the contribution of each wavenumber $k$ to $h_{ij}(t,\\mathbf{x})$, we consider a Fourier representation $\\tilde{h}_{ij}(t,\\mathbf{k})$. Moving to the Fourier space, focusing on one particular polarisation state, and assuming isotropy, the gravitational wave field $h_{k}$ satisfies the following equation:\n\\begin{equation}\n    h_{k}'' + 2\\frac{a'}{a}h_k'+k^2h_k=0,\n\\end{equation}\nwhere $(..)'$ denotes the derivative with respect to conformal time $\\tau$. However, it is more convenient to use a new variable $f_k(\\tau)\\equiv a(\\tau)h_k(\\tau)$ satisfying\n\\begin{equation}\n\\label{BesselsEq}\nf_k''+\\left(k^2+\\frac{a''}{a}\\right)f_k=0.\n\\end{equation}\nAfter redefining $f_k=i\\sqrt{\\tau}\\,u_k$, Eq.~\\eqref{BesselsEq} assumes the more familiar form of a Bessel equation and, for each phase of the cyclic model, the general solution involves a linear combination of the Hankel functions of the first and second kind, that we denote as $H^{(1,2)}$. \n\nIn this section, we present a generalised calculation for the primordial tensor spectrum in the cyclic Universe. In \\autoref{sec.general_solution}, we derive the general solution of Eq.~\\eqref{BesselsEq} making no assumptions about the vacuum state of the theory and considering the contribution of tensor modes produced in the dark energy phase of the previous cycle rather than starting directly from the \\textit{ekpyrotic} phase of the present cycle. In \\autoref{sec.Matching_phases}, we require internal consistency in the evolution of the tensor mode amplitudes throughout the four phases of the model, ensuring that both $h_k(\\tau)$ and $h_k'(\\tau)$ remain continuous functions and matching the general solutions across the different phases. Finally, in \\autoref{sec.spectrum_today}, we derive the expression of the primordial tensor spectrum and briefly discuss its strain today.\n\n\\subsection{General Solutions in the different Phases}\n\n\\label{sec.general_solution}\n\n\\subsubsection{General solution in the Dark Energy phase}\nDuring the dark energy phase, $a(\\tau)$ is given by Eq.~\\eqref{adarkenergyconf}, and the general solution of Eq.~\\eqref{BesselsEq} reads \n\\begin{equation}\\label{DE solution}\n    f_k(\\eta)=\\sqrt{-k\\eta}\\left(D_1(k)H^{(1)}_\\frac{3}{2}(-k\\eta)+D_2(k)H^{(2)}_{\\frac{3}{2}}(-k\\eta)\\right)\\;\\;\\;\\tau<\\tau_{\\rm{tr}},\n\\end{equation}\nwhere $H_n^{(1)}$ and $H_n^{(2)}$ denote the Hankel functions of first and second kind respectively, $\\eta=\\tau-B$, where $B$ is given by Eq.~\\eqref{B_const} and, for each wave-number $k$, $D_{1,2}(k)$ are arbitrary constants. \n\n\\subsubsection{General solution in the \\textit{ekpyrotic} contraction}\nIn the stage of the \\textit{ekpyrotic} contraction, $a(\\tau)$ is given by Eq.~\\eqref{ekpyrotic scale factor} and the general solution reads\n\\begin{equation}\\label{ekpyrotic solution}\nf_k(\\tau)=\\sqrt{y}\\left(A_1(k)H^{(1)}_n(y)+A_2(k)H^{(2)}_{n}(y)\\right)\\;\\;\\;\\tau_{\\rm{tr}}<\\tau<\\tau_{\\rm{end}},\n\\end{equation}\nwhere $y\\equiv-k(\\tau-\\tau_{\\rm{ek}})$. \n\n\\subsubsection{General solution in the kinetic contracting phase}\nMoving to the kinetic contracting phase and making use of Eq.~\\eqref{akineticcontraction}, we achieve\n\\begin{equation}\n    f_k(\\tau)=\\sqrt{-k\\tau}\\left(B_1(k)H^{(1)}_0(-k\\tau)+B_2(k)H^{(2)}_{0}(-k\\tau)\\right)\\;\\;\\;\\tau_{\\rm{end}}<\\tau<0,\n\\end{equation}\n\n\\subsubsection{General solution in the kinetic expanding phase}\nConsidering Eq.~\\eqref{aexpanding} in the kinetic expanding phase the genral solution is\n\\begin{equation}\n    f_k(\\tau)=\\sqrt{k\\tau}\\left(C_1(k)H^{(1)}_0(k\\tau)+C_2(k)H^{(2)}_{0}(k\\tau)\\right)\\;\\;\\;0<\\tau<\\tau_{\\rm{r}}.\n\\end{equation}\n\n\n\\subsection{Matching phases}\n\\label{sec.Matching_phases}\nIn what follows we obtain expressions for the coefficients $D_{1,2}(k),A_{1,2}(k),B_{1,2}(k)$ and $C_{1,2}(k)$ by matching $h_k(\\tau)$ and $h_k(\\tau)'$ at the boundaries of each phase. \\\\\n\n\\subsubsection{Dark energy - \\textit{ekpyrotic}}\nAt the boundary between dark energy and \\textit{ekpyrotic} stage, we require continuity of $f_k(\\tau)$ namely\n\\begin{equation}\\label{dark energy-ekpyrotic matching}\n    \\sqrt{x_{\\rm{tr}}}\\left(D_1(k)H^{(1)}_\\frac{3}{2}(x_{\\rm{tr}})+D_2(k)H^{(2)}_{\\frac{3}{2}}(x_{\\rm{tr}})\\right)= \\sqrt{y_{\\rm{tr}}}\\left(A_1(k)H^{(1)}_n(y_{\\rm{tr}})+A_2(k)H^{(2)}_{n}(y_{\\rm{tr}})\\right),\n\\end{equation}\nwhere $x_{\\rm{tr}}=-k\\eta(\\tau_{\\rm{tr}})$, and continuity of $f'(\\tau)$ which we write in the matrix form $\\underline{\\underline{\\textbf{y}}}_1\\textbf{A}=\\underline{\\underline{\\textbf{x}}}_1\\textbf{D}$ where\n\n\n\n\n\n    \\\\\n\n\n\n\n \n\n\n\n\n\n\n\n    \n\n\n\\begin{align}\\label{transition matching}\n \\underline{\\underline{\\textbf{y}}}_1&=\\left(\n \\renewcommand*{\\arraystretch}{1.5}\n    \\begin{matrix}\n        \\sqrt{y_{\\rm{tr}}}H^{(1)}_n(y_{\\rm{tr}}) & \n        \\sqrt{y_{\\rm{tr}}}H^{(2)}_n(y_{\\rm{tr}})\\\\[5pt]\n        \\sqrt{y_{\\rm{tr}}}H^{(1)}_{n-1}(y_{\\rm{tr}})-\\frac{(n-\\frac{1}{2})}{\\sqrt{y_{\\rm{tr}}}}H^{(1)}_n(y_{\\rm{tr}})\n        & \\qquad\n        \\sqrt{y_{\\rm{tr}}}H^{(2)}_{n-1}(y_{\\rm{tr}})-\\frac{(n-\\frac{1}{2})}{\\sqrt{y_{\\rm{tr}}}}H^{(2)}_n(y_{\\rm{tr}})\n    \\end{matrix}\n    \\right),\\nonumber\\\\[10pt]\n    \\underline{\\underline{\\textbf{x}}}_1&=\\left(\n    \\begin{matrix}\n        \\sqrt{x_{\\rm{tr}}}H^{(1)}_\\frac{3}{2}(x_{\\rm{tr}}) & \\sqrt{x_{\\rm{tr}}}H^{(2)}_\\frac{3}{2}(x_{\\rm{tr}})\\\\[5pt]\n        \\sqrt{x_{\\rm{tr}}}H^{(1)}_\\frac{1}{2}(x_{\\rm{tr}})-\\frac{1}{\\sqrt{x_{\\rm{tr}}}}H^{(1)}_\\frac{3}{2}(x_{\\rm{tr}})\n        & \\qquad\n        \\sqrt{x_{\\rm{tr}}}H^{(2)}_\\frac{1}{2}(x_{\\rm{tr}})-\\frac{1}{\\sqrt{x_{\\rm{tr}}}}H^{(2)}_\\frac{3}{2}(x_{\\rm{tr}})\n    \\end{matrix}\n    \\right),\n\\end{align}\nand\n\\begin{equation}\n    \\textbf{A} = \\left(\\begin{matrix}\n        A_1\\\\A_2\n    \\end{matrix}\\right),\\;\\;\\;\\;\\;\\;\n    \\textbf{D} = \\left(\\begin{matrix}\n        D_1\\\\D_2\n    \\end{matrix}\\right),\\;\\;\\;\\;\\;\\; \\text{with} \\;\\; x_{\\rm{tr}}\\equiv-k\\tau_{\\rm{tr}}.\n\\end{equation}\n\n\\subsubsection{\\textit{Ekpyrotic} - kinetic contraction}\n\nSimilarly here, at $\\tau = \\tau_{\\rm{end}}$ namely the end of the \\textit{ekpyrotic} phase, we require $\\underline{\\underline{\\textbf{y}}}_2\\textbf{B}=\\underline{\\underline{\\textbf{x}}}_2\\textbf{A}$ where \n\n\n\n\\begin{align}\n \\underline{\\underline{\\textbf{y}}}_2&=\\left(\n    \\begin{matrix}\n        H^{(1)}_0(x_{\\rm{e}}) & H^{(2)}_0(x_{\\rm{e}})\\\\[5pt]\n        \\sqrt{x_{\\rm{e}}}H^{(1)}_{-1}(x_{\\rm{e}})+\\frac{H^{(1)}_0(x_{\\rm{e}})}{2\\sqrt{x_{\\rm{e}}}}\n        &\\qquad\n        \\sqrt{x_{\\rm{e}}}H^{(2)}_{-1}(x_{\\rm{e}})+\\frac{H^{(2)}_0(x_{\\rm{e}})}{2\\sqrt{x_{\\rm{e}}}}\n    \\end{matrix}\n    \\right),\\nonumber \\\\[10pt]\n    \\underline{\\underline{\\textbf{x}}}_2&=\\left(\n    \\begin{matrix}\n        \\sqrt{2\\alpha}H^{(1)}_n(2\\alpha x_{\\rm{e}}) & \n        \\sqrt{2\\alpha}H^{(2)}_n(2\\alpha x_{\\rm{e}})\\\\[5pt]\n        \\sqrt{2\\alpha x_{\\rm{e}}}H^{(1)}_{n-1}(2\\alpha x_{\\rm{e}})-\\frac{(n-\\frac{1}{2})}{\\sqrt{2\\alpha x_{\\rm{e}}}}H^{(1)}_n(2\\alpha x_{\\rm{e}})\n        & \\qquad\n        \\sqrt{2\\alpha x_{\\rm{e}}}H^{(2)}_{n-1}(2\\alpha x_{\\rm{e}})-\\frac{(n-\\frac{1}{2})}{\\sqrt{2\\alpha x_{\\rm{e}}}}H^{(2)}_n(2\\alpha x_{\\rm{e}})\n    \\end{matrix}\n    \\right),\n\\end{align}\nand \n\\begin{equation}\n    \\textbf{B}=\\left(\\begin{matrix}\n    B_1\\\\B_2\n\\end{matrix}\\right),\\;\\;\\;\\;\\;\\; \\text{with} \\;\\; x_{\\rm{e}}\\equiv-k\\tau_{\\rm{end}}.\n\\end{equation}\n\n\\subsubsection{Kinetic contraction- kinetic expansion}\nThe matching for these two final stages arises at $\\tau = 0$, which is trivial as we have\n\\begin{equation}\n    C_{1,2} = -\\sqrt{1+\\chi}B_{2,1}.\n\\end{equation}\n\n\n\\subsection{Strain spectrum today ($\\tau=\\tau_0$)}\n\\label{sec.spectrum_today}\n\nThe quantity in terms of which we assess the production of primordial gravitational waves in the cyclic Universe is the dimensionless strain spectrum \n\\begin{equation}\\label{general_strain_expreession}\n    \\Delta h = k^{\\frac{3}{2}}\\frac{\\left|h_k(\\tau)\\right|}{\\pi}.\n\\end{equation}\nIt is convenient to evaluate the dimensionless strain spectrum in the radiation-dominated epoch. Using the general solutions we derived and matched across the different phases of the model, it reads:\n\\begin{equation}\\label{strain_spectrum}\n    \\Delta h(k, \\tau_{r}) = \\frac{k^2\\sqrt{2\\,\\tau_{\\rm{r}}}}{a(\\tau_{\\rm{r}})\\pi M_{\\rm{Pl}}}\\left|C_1(k) H_0^{(1)}(x_{\\rm{r}}) + C_2(k)H_0^{(2)}(x_{\\rm{r}})\\right|,\n\\end{equation}\nwhere $x_{\\rm{r}}\\equiv k\\,\\tau_{\\rm{r}}$. The strain tensor spectrum today $\\Delta h(k, \\tau_0)$ can be easily related to the spectrum in the radiation dominated epoch by means of the transfer function formalism as:\n\\begin{equation}\\label{spectrum today}\n    \\Delta h(k, \\tau_0) \\equiv \\mathcal{T}(k) \\Delta h(k, \\tau_{\\rm{r}}), \n\\end{equation}\nwhere $\\mathcal{T}(k)$ is the transfer function, responsible for propagating the spectrum forwards to today's observed spectrum. Following Ref.~\\cite{Boyle:2003km}, we use a transfer function of the form \n\\begin{equation}\\label{transfer function}\n    \\mathcal{T}(k)\\approx \\left(\\frac{k_0}{k}\\right)^2\\left(1+\\frac{k}{k_{\\rm{eq}}}+\\frac{k^2}{k_{\\rm{eq}}k_{\\rm{r}}}\\right),\n\\end{equation}\nwhere $k_{\\rm{r}} = a_{\\rm{r}} H_{\\rm{r}} \\approx T_r^2/M_{\\rm{Pl}}\n$\nis the wave number of modes crossing the horizon at the start of radiation domination. \n\nSo far, the whole calculation is done by setting $a(\\tau_{\\rm{r}})=1$. However, for ease of comparison with data and the other results documented in the literature, it is convenient to return to the usual coordinate system where $a_0 = 1$. This can be easily done by means of the well-known inverse scaling between the scale factor and the CMB temperature (\\textit{i.e.} $a \\propto 1/T$). This relation fixes the ratio $a(\\tau_{0}) / a(\\tau_{\\rm{r}}) \\propto T_{\\rm{r}}/T_0 \\sim 10^{20}$. As a result, we can re-scale the comoving frequency accordingly, which now takes the more familiar value $\\hat k_{\\rm{r}}=k_{\\rm{r}}/a(\\tau_{0})=10^{-1}\\, \\text{Hz}$. From now on, we will name the re-scaled comoving frequency $\\hat k_{\\rm{r}}$ as $k_{\\rm{r}}$. Additionally, always following from the inverse proportionality between the scale factor and the CMB temperature, we expect a spectral range of modes entering between the start of radiation domination and today of the order of\n\\begin{equation}\n    \\frac{k_0}{k_{\\rm{r}}}\\propto \\frac{T_0}{T_{\\rm{r}}} \\sim 6.6 \\times 10^{-20}.\n\\end{equation}\nTaking in mind that during matter domination $H\\sim a^{-3/2}$, we get\n\\begin{equation}\n    \\frac{k_{\\rm{eq}}}{k_0}\\approx \\sqrt{1+z_{\\rm{eq}}} \\sim 10^2,\n\\end{equation}\nwhere $z_{\\rm{eq}}$ is the redshift at the equivalence.\n\nWe can also place a bound on the wave number of modes on the horizon at the previous dark energy-\\textit{ekpyrotic} transition, $k_{\\rm{tr}}$, which will be useful later. The wavelength of such modes can be estimated using the duration of the \\textit{ekpyrotic} phase, which we require to last at least 60 e-folds to effectively homogenise and flatten the Universe for the subsequent cycle. However, a much stronger constraint comes from Eq.~\\eqref{Hratio}, as the number of e-folds during the \\textit{ekpyrotic} phase is given by $N\\sim \\ln(H_{\\rm{end}}/H_0)\\approx \\ln(\\sqrt{-V_{\\rm{end}}/V_0})$. Taking our previous constraints on $V_0$ at today's dark energy density and $V_{\\rm{end}}$ at the GUT scale, we arrive at $N\\sim 115$. This forces our horizon to shrink by a factor of $\\sim 10^{50}$ during the dark \\textit{ekpyrotic} phase, and hence we would expect\n\\begin{equation}\n\\frac{k_{\\rm{end}}}{k_{\\rm{tr}}}\\approx 10^{50}.\n\\end{equation}\n\n\\section{Resilience through cycles and the vacuum state}\n\\label{sec:discussion}\nThe calculation of the spectrum of gravitational waves introduced in the previous section, in addition to considering the contribution of tensor modes produced in the dark energy phase of the previous cycle, is entirely general regarding the vacuum state and applies to any particular choice. This allows us to test several conceptual aspects of the theory (partially highlighted in the introduction), such as its resilience throughout cycles and the implications of the choice of the vacuum states. In this section, we examine both these issues in more detail. In \\autoref{Gravitational Waves From A non-Bunch Davies Vacuum}, we focus on the choice of the vacuum state of the theory, deriving novel constraints on the latter based on internal consistency through cycles and epochs. In \\autoref{analysis of gravitational waves}, we quantify the extent to which including the contribution from tensor modes originating during the dark energy phase of the previous cycle changes the spectrum of tensor perturbations observed in the current cycle.\n\n\\subsection{Gravitational waves from a non-Bunch Davies vacuum}\\label{Gravitational Waves From A non-Bunch Davies Vacuum}\n\nAs often speculated both in quantum field theory and effective field theory, the choice of the vacuum state represents an important topic of discussion and significance as it plays a crucial role in determining the properties of the theory and the physical predictions it makes. On the one hand, in quantum field theory, the vacuum state is the lowest-energy state of a quantum field, which usually corresponds to the state with no physical particles. On the other hand, effective field theories often deal with specific energy scales or regimes of a more fundamental theory (such as in our effective representation of the cyclic Universe), and the choice of the vacuum state may involve integrating out high-energy degrees of freedom and focusing on the low-energy behavior. This makes the choice of the vacuum state far from trivial when considering the possibility of new physics at sufficiently high energies. \n\nThe selection of the vacuum state has been the subject of intense study and attention in the context of inflationary models where considering more exotic (though physically motivated) vacuum states other than BD) can lead to markedly different predictions for the spectrum of scalar and tensor modes, see \\textit{e.g.} ~\\cite{Ashoorioon:2013eia,Ashoorioon:2014nta,Choudhury:2023kam} and references therein. Just to mention one example among many, even sticking to the framework of single-field inflation minimally coupled to gravity, considering an exotic vacuum can result in the violation of the usual slow-roll consistency relations allowing for a blue-tilted spectrum of gravitational waves, with an enhanced amplitude on small scales that can eventually produce observable signatures visible by CMB and Gravitational Waves experiments.\n\nHowever, the same issue has not been investigated in the cyclic Universe. Previous analyses of the spectrum of the gravitational wave assume a BD vacuum for perturbations produced in the \\textit{ekpyrotic} phase, enforcing the solution to converge to that of a plane wave in flat space at sufficiently short distances~\\cite{Boyle:2003km}. As argued in Ref.~\\cite{Steinhardt:2002ih}, this choice mostly relies on the classical treatment of the dynamics of the evolving modulus field at arbitrarily small length scales. However, just like in inflation, the choice may no longer be trivial if the dynamics of the modulus field is influenced by new physics at some characteristic energy scale $M$. As a result, one may wonder whether in the cyclic Universe, the choice of the vacuum state is somehow affected by the same level of \"arbitrariness\" as inflation, or if additional constraints can be derived based on the strong interconnection between the different phases and/or from the need to maintain consistency through cycles. A different, yet related, question is whether, also in the cyclic Universe, this potential degree of arbitrariness impacts the predictions for the tensor spectrum or if they remain robust under the choice of the vacuum state.\n\nHere, we take a first step forward in the discussion and, considering a non-BD vacuum state in the dark energy phase of the previous cycle, we argue that this initial state imposes a more fundamental constraint on the cyclic model, primarily due to the backreaction effects that a non-BD state could have on the background evolution of the modulus field. \n\nTo prove this point, following Refs.~\\cite{Aravind:2013lra,Ashoorioon:2013eia,Holman:2007na}, we allow the second Bogoliubov coefficient $\\beta_k$ to be non-zero. Notice that, since the Bogoliubov coefficient is related to $D_2$ by \n\\begin{equation}\n\\beta_k = 2D_2\\sqrt{(k/\\pi)},\n\\end{equation}\nand parameterizes deviations from the BD vacuum. As the effective theory becomes invalid at energy scales beyond those of new physics, we ensure that no modes are excited past this point, therefore requiring $\\beta_k \\to 0$ for $k > Ma(\\tau_c)$. In addition, we choose $\\beta_k$ of the form\n\\begin{equation}\n    \\beta_k \\sim \\beta_0 e^{-\\frac{k^2}{M^2 a\\left(\\tau_c\\right)^2}},\n    \\label{eq:non-BD-vscuum}\n\\end{equation}\nwhere the energy scale of the effective theory is $M\\sim 10^{-4}M_{\\rm{Pl}}$ and $\\beta_0$ is a proportionality factor. The effects produced by the backreaction in the dark energy-dominated phase set stringent limits on how large the non-vanishing $\\beta_k$ can be. In particular, requiring that the energy density of the non-BD quanta does not overwhelm the energy density associated with the modulus field -- hence spoiling the model -- implies\n\\begin{equation}\n    \\rho_{non-BD}\\sim\\frac{1}{a_{i}^4}\\int{\\frac{d^3k}{(2\\pi)^3}}\\left|\\beta_0\\right|^2 k\\sim\\left(\\frac{a(\\tau_c)}{a_{i}(\\tau)}\\right)^4\\left|\\beta_0\\right|^2 M^4\\ll M_{\\rm Pl}^2 H^2,\n    \\label{eq:rho_NBD}\n\\end{equation}\nwhere $a_{i}$ refers to the value of the scale factor when we set our initial vacuum conditions, and we have used $M_{\\rm PL}^2H^2$ as the energy density associated with the background evolution. We take the cutoff time $\\tau_c$, beyond which we cannot be certain of the modulus field dynamics to be the start of radiation domination regime in the previous cycle $\\tau_{r(pc)}$.\nAs long as $(a(\\tau_{r(pc)})/a(\\tau))^4 \\ll 1 $, Eq.~\\eqref{eq:rho_NBD} sets an upper bound for the parameter $\\beta_0$:\n\\begin{equation}\n    \\beta_0 < \\frac{H M_{\\rm Pl}}{M^2}\\sim 10^{-53}\n\\end{equation}\nwhere we used that, during the dark energy phase, the Hubble parameter is approximately constant $H\\simeq H_0=100\\,h \\simeq 2.1 h \\times 10^{-42}$ GeV. Notice that this bound is independent of the cutoff time $\\tau_c$ when starting the calculation in the dark energy phase. This is because of the three expansion phases preceding this epoch and the large net expansion of the scale factor from cycle to cycle, ensuring that $(a(\\tau_c)/a(\\tau_i))^4 \\ll 1$ for any $\\tau_c$.\n\nWe emphasise once again that avoiding problems with backreaction during the dark energy-dominated phase leads to an extremely restrictive constraint on deviations away from the BD vacuum. To gain a rough idea of how restrictive our bound is, we can compare constraints on the same cutoff scale obtained in inflationary cosmology. In that case, as highlighted in Ref.~\\cite{Holman:2007na}, the restriction reads $\\beta_0 < 10^{-6}$; i.e., about 47 orders of magnitude weaker than our bound. The reason for such a large difference lies in the fact that, although inflation and dark energy share several common aspects in terms of background dynamics, these two phases span energy scales that differ by over 100 orders of magnitude in characteristic energy density. The energy scale of inflation is much higher, making it way more challenging for perturbations to overwhelm the dynamics and allowing larger freedom for deviations away from BD.\n\nTo further appreciate the strength of our constraint, it is also worth briefly discussing what happens when setting the vacuum state in a different phase of the model rather than in the dark energy-dominated one. In particular, we focus on the \\textit{ekpyrotic} phase, which, as highlighted multiple times in this work, is the phase where initial conditions are typically fixed, and the calculation of the spectrum of tensor perturbations is initiated. In this phase, the Hubble parameter drastically grows, see Eq.~\\eqref{Hratio}, while the scale factor remains approximately constant. As a consequence, the constraint on $\\beta_0$ is relaxed up to $\\beta_0 < 10^{45}$ for $M \\sim 10^{-4}M_{\\rm Pl}$, allowing us complete freedom to choose a wide range of vacuum states in the \\textit{ekpyrotic} phase without compromising the model during this stage. That being said, it is necessary to ensure that significant deviations away from a BD vacuum during the \\textit{ekpyrotic} phase do not lead to other issues during the subsequent evolutionary states of the cycle. However, given that the evolution of the dark energy phase is governed by the background dynamics of the lowest energy scale and considering the cyclic nature of the model and its resilience through cycles (meaning that starting from the dark energy phase of the previous cycle does not produce differences in observable quantities as we prove in the next subsection), we argue that selecting the vacuum during the dark energy phase is the most restrictive and conservative choice to circumvent this issue entirely. This makes relying on the BD vacuum a safer assumption from a model-building perspective.\n\n\n\\subsection{Gravitational waves produced from different phases}\\label{analysis of gravitational waves}\n\nOur general calculation for the strain spectrum of gravitational waves enables us to investigate how the predictions change when incorporating the contribution from tensor modes originating in the dark energy phase of the previous cycle. We can then test the robustness of these predictions by comparing our results with those already documented in the literature, derived from starting in the \\textit{ekpyrotic} contracting phase.\n\nIn this section, we compare the predictions for the strain spectrum of gravitational waves obtained in the following two cases:\n\\begin{itemize}\n\\item[\\textit{(a)}] Using our general calculation and starting in the dark energy phase of the previous cycle.  In this case we impose BD vacuum conditions on the coefficients of the dark energy stage $D_1$ and $D_2$ given by:\n\\begin{align}\\label{DE conditions}\nD_1 &= \\frac{1}{2}\\sqrt{\\frac{\\pi}{k}}, & D_2 &= 0.\n\\end{align}\n\n\\item[\\textit{(b)}] Starting the calculation in the \\textit{ekpyrotic} phase (disregarding the matching at $\\tau=\\tau_{tr}$) and imposing BD initial conditions as done in Res.~\\cite{Boyle:2003km} \n\\begin{align}\\label{ekpyrotic conditions}\n    A_1 &= \\frac{1}{2}\\sqrt{\\frac{\\pi}{k}}, & A_2 &= 0.\n\\end{align}\n\\end{itemize}\nNotice that, although our calculation is fully generic concerning the choice of the vacuum state, as we proved in the previous subsection, deviations away from the BD vacuum state in the dark energy phase are strongly constrained, providing us with a valid physical reason to impose this vacuum state in the dark energy phase for the case \\textit{(a)}. Instead, for the case \\textit{(b)}, we impose the BD initial conditions in the \\textit{ekpyrotic} phase to work in the same framework as Ref.~\\cite{Boyle:2003km} and allow direct comparison.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.7\\columnwidth]{Fig1.png}\n    \\caption{Strain spectrum plotted against physical frequency $k$, of gravitational waves produced starting the cycle with the dark energy phase in blue, and with the \\textit{ekpyrotic} phase in orange. BD initial conditions are assumed for both spectra. $k_0, k_{eq}, k_r$ and $k_{end}$ are the comoving frequencies on the horizon today, at matter-radiation equivalence, at the start of radiation domination, and at the \\textit{ekpyrotic}-kinetic transition respectively.}\n    \\label{fig:Strain spectra}\n\\end{figure}\n\n\\begin{figure}[htb!]\n    \\centering\n    \\includegraphics[width=0.75\\textwidth]{Fig2.png}\n    \\caption{Illustrative plot of the comoving Hubble horizon, $1/aH$, throughout the \\textit{ekpyrotic}, kinetic contracting, kinetic expanding, radiation domination, and finally matter domination phases in magenta, orange, dark blue, green and cyan respectively. Key modes on the horizon are illustrated as horizontal dashed lines, along with a label of their comoving wavenumber, and some less important modes are labelled.}\n    \\label{fig:horizon plot}\n\\end{figure}\n\nAfter matching the relevant phases, the strain spectra of tensor modes predicted in the two cases can be derived using Eq.~\\eqref{strain_spectrum} and Eq.~\\eqref{transfer function} and are shown in \\autoref{fig:Strain spectra} in blue for the case \\textit{(a)} and in orange for the case \\textit{(b)}. As evident from the figure, a difference of up to an order of magnitude in the strain $\\Delta h$ is observed for modes $k_0$ at the present-day horizon.\n\nThis difference can be understood by considering the evolution of modes produced during the dark energy phase of the previous cycle. To further clarify this point, we refer to \\autoref{fig:horizon plot} where we show the evolution of the comoving Hubble horizon, $1/aH$, throughout the different phases of the model. From the figure we note that the horizon at the end of the dark energy phase (\\textit{i.e.} $k_{\\rm tr}^{-1}$) is several orders of magnitude greater than the present-day horizon (\\textit{i.e.} $k_0^{-1}$) ensuring that none of these super horizon modes can have re-entered by today or in any subsequent cycle.  Modes produced during this dark energy phase that exit the horizon become frozen and subsequently experience further stretching throughout this epoch.\n\nOn the other hand, sub-horizon modes produced in the same phase oscillate with decaying amplitude $h \\propto a^{-1}$. This feature can be shown by solving Eq.~\\eqref{BesselsEq} in the dark energy background dynamics described by Eq.~\\eqref{adarkenergy}. In view of that, we expect sub-horizon modes produced deep within the dark energy phase (as well as in any previous phase of the previous cycle) to decay away to negligible amplitudes when compared to modes produced at the end of the same phase or during the subsequent \\textit{ekpyrotic} phase. Instead, sub-horizon modes produced \\textit{near} the end of the dark energy phase can survive into the \\textit{ekpyrotic} phase of the next cycle, acting as extra quanta in the vacuum initial conditions for the subsequent \\textit{ekpyrotic} phase. This contribution is encoded in the coefficient $A_2$ in Eq.~\\eqref{ekpyrotic solution} and can lead to observable effects on scales corresponding to the long wavelength portion of the strain spectrum. Referring back to \\autoref{fig:Strain spectra}, we can appreciate how the differences produced by amplitude oscillations expected from an under-damped simple harmonic oscillator solution affect only the range of frequencies between $k_0$ and $k_{\\rm eq}$ before becoming frozen in during the \\textit{ekpyrotic} phase.\n\nMoving forward, as outlined in Ref.~\\cite{Erickson:2006wc}, the subsequent phases will provide a red-tilt to the scale-invariant dark energy spectrum. Notice that for an \\textit{ekpyrotic} phase lasting $N \\sim 115$ e-folds, the magnitude of the coefficient $A_2$ becomes $\\mathcal{O}(10^{10})$ between $k_0$ and $k_{\\rm eq}$ then rapidly decays to order $\\mathcal{O}(10^{-15})$ for modes around $k_{\\rm r}$. This explains the discrepancy between imposing BD initial conditions in the dark energy phase given by Eq.~\\eqref{DE conditions}, and in the \\textit{ekpyrotic} phase, given by Eq.~\\eqref{ekpyrotic conditions}. \n\nWe conclude with a last important remark: as argued in \\autoref{Gravitational Waves From A non-Bunch Davies Vacuum}, to preserve the background evolution of the modulus field, it is important to have a BD-like vacuum state during the dark energy phase. This requirement implies that perturbations existing in the current cycle must decay to negligible levels in amplitude compared to the energy density present in the BD vacuum, preventing them from acting as additional quanta on top of the vacuum. We emphasise that this condition is satisfied by the evolution of tensor perturbations after the bounce, particularly during the subsequent radiation and matter-dominated phases. In particular, the transfer function, Eq.~\\eqref{transfer function}, ensures that the amplitudes of short-wavelength modes re-entering the horizon first (that are potentially the most problematic), are decreased by a factor $\\left( k_0/k \\right)^2$. Consequently, the amplitude of the shortest wavelength modes in the observable spectrum ($k \\sim k_{\\rm end}$), is suppressed by more than $20$ orders of magnitude during the radiation and matter-dominated phases. As a result, the evolution through these phases guarantees the decaying amplitude of all modes in every cycle preceding each dark energy phase and restoring the vacuum to a BD state in the latter. This shields the dark energy phase from possible backreaction effects, underscoring the resilience of the model and demonstrating once again that, from a theoretical standpoint, fixing initial conditions in this phase is much safer from a model-building perspective.\n\n\n\\section{Conclusion}\n\\label{sec:conc}\n\nIn this study, we investigate the production of gravitational waves in a cyclic Universe, focusing on certain conceptual aspects of the theory, such as the resilience of observable predictions against the phase of the cycle in which initial conditions are set and the choice of the vacuum state.\n\nIn most of the analyses carried out in the literature, the \\textit{ekpyrotic} phase is typically assumed as the starting point of the cycle where initial conditions of primordial perturbations are imposed, and the calculations of the relative spectra begin.  While for scalar perturbations the implications of setting the initial conditions in different phases of the theory have been examined in a few studies surrounding this topic, to the best of our knowledge, the calculation of the tensor spectrum has always been performed starting in the \\textit{ekpyrotic} phase, assuming a BD vacuum state and neglecting any potential contributions arising from tensor modes originating during the dark energy phase of the previous cycle.\n\nIn light of this, a few (we believe) interesting questions remained somewhat pending. For instance, one might wonder whether setting initial conditions in the dark energy phase of the previous cycle and considering the potential contribution of tensor modes generated in this phase could lead to any observable differences in the gravitational wave strain spectrum observed in the current cycle and eventually clarify why (not). Most importantly, in effective field theory descriptions of the cyclic Universe, the choice of the vacuum state may involve integrating out high-energy degrees of freedom, focusing on low-energy behaviors of the modulus field. This makes the choice far from trivial when considering the possibility of new physics acting at sufficiently high characteristic energy scales. Consequently, one might wonder about the implications of assuming a BD vacuum state in the \\textit{ekpyrotic} phase and, more broadly, what freedom exists in the cyclic Universe regarding the choice of the vacuum state and how such freedom affects the predictions for the spectrum of gravitational waves.\n\nFuelled by these questions, we consider a cyclic Universe described by the effective 4D Lagrangian \\eqref{lagrangian} with a potential given by Eq.~\\eqref{potnetial}. After reviewing the background dynamics of the model (\\autoref{construction of the model}), we focus on the production and evolution of tensor modes. In \\autoref{primordial tensor spectrum}, we analytically solve the equation of motion of the gravitational wave field through all the different phases of the cycle by starting from the dark energy phase of the previous cycle and making no assumptions about the vacuum state. The results presented in this section generalise the predictions for the tensor spectrum to include the contribution of tensor perturbations produced during the dark energy phase of the previous cycle and apply to any choice of the vacuum state of the theory. Therefore, they extend the treatment presented so far in the literature, allowing us to gain important insights into the issues posed earlier.\n\nAs argued in \\autoref{sec:discussion}, our findings reveal a significant resilience of the cyclic Universe model concerning predictions for the spectrum of primordial gravitational waves, with the most relevant results reading as follows.\n\\begin{itemize}\n\\item \\textit{Initial Conditions:} To quantify the impact of the contribution arising from tensor modes produced in the previous dark energy phase of the cycle, in \\autoref{analysis of gravitational waves} we compared the spectra obtained by starting from the \\textit{ekpyrotic} contracting phase (in orange in \\autoref{fig:Strain spectra}) and the dark energy phase of the previous cycle (in blue in \\autoref{fig:Strain spectra}), assuming in both cases a BD vacuum. The two spectra are essentially identical except for a difference up to an order of magnitude in the strain $\\Delta h$ for modes on the scale $k_0$ at the present-day horizon. The reason is that the horizon at the end of the dark energy phase is several orders of magnitude greater than the present-day horizon (see also \\autoref{fig:horizon plot}), implying that none of these super-horizon modes can have re-entered by today or in any subsequent cycle. On the other hand, sub-horizon modes produced in the same phase oscillate with decaying amplitude $h \\propto a^{-1}$. Therefore, sub-horizon modes produced deep within the dark energy phase (as well as in any previous phase of the previous cycle) decay away to negligible amplitudes, while sub-horizon modes produced near the end of the dark energy phase can survive into the \\textit{ekpyrotic} phase of the next cycle, being responsible for the small differences in the strain spectrum observed around $k_0$.\n\n\\item \\textit{Vacuum state:} Although including tensor modes originating from the previous dark energy phase does not lead to significant differences in the spectrum of primordial perturbations, we argue that starting the calculation in this phase imposes a more fundamental constraint on the choice of the vacuum state of the model. To highlight this aspect, in \\autoref{Gravitational Waves From A non-Bunch Davies Vacuum} we consider a non-BD vacuum state in the dark energy phase parametrized in terms of the second Bogoliubov coefficient given by Eq.~\\eqref{eq:non-BD-vscuum}. We show that requiring the energy density of the non-BD quanta not to overwhelm the energy density associated with the modulus field -- hence, potentially spoiling the model -- implies an extremely restrictive constraint on deviations away from the BD vacuum. On the other hand, for the same non-BD vacuum state, the constraint is completely lost in the \\textit{ekpyrotic} phase where we are left with complete freedom to choose a wide range of vacuum states without compromising the model during this stage (but possibly spoiling the subsequent evolution). As a result, selecting the vacuum during the dark energy phase (where we are basically forced to BD) seems to be the most restrictive and conservative choice, as well as a safer assumption from a model-building perspective. Given the cyclic nature of the model and the resilience of its predictions against the phase where we set initial conditions (as proven in the previous point), this strongly reduces our freedom to consider exotic vacuum states in the cyclic Universe.\n\\end{itemize}\n\n\\begin{acknowledgments}\nCvdB is supported (in part) by the Lancaster–Manchester–Sheffield Consortium for Fundamental Physics under STFC grant: ST/X000621/1. This article is based upon work from COST Action CA21136 Addressing observational tensions in cosmology with systematics and fundamental physics (CosmoVerse) supported by COST (European Cooperation in Science and Technology). \n\\end{acknowledgments}\n\n{}\n\n\n\\end{document}\n"}
{"paper_id": "2403-00533", "version": "2403-00533v2", "root_file_path": "d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\data_raw\\2403-00533\\tex\\2403-00533v2\\JCAP.tex", "metadata": {"total_length": 64048, "merged_count": 1, "merged_files": ["JCAP.tex"], "missing_files": []}, "content": "\\documentclass[a4paper,11pt]{article}\n\\pdfoutput=1 \n             \n\n\\usepackage{jcappub} \n                     \n\n\\usepackage[T1]{fontenc} \n\n\n\\title{Gravitational waves in a cyclic Universe: resilience through cycles and vacuum state}\n\n\n\n\n\n\n\n\n\n\\author[a]{Mariaveronica De Angelis,}\n\\author[a]{Adam Smith,}\n\\author[a]{William Giar\\`e,}\n\\author[a]{Carsten van de Bruck}\n\n\\affiliation[a]{School of Mathematics and Statistics, The University of Sheffield, Hounsfield Road, S3 7RH Sheffield, United Kingdom}\n\n\n\n\n\n\n\n\n\n\n\n\\emailAdd{mdeangelis1@sheffield.ac.uk}\n\\emailAdd{asmith69@sheffield.ac.uk}\n\\emailAdd{w.giare@sheffield.ac.uk}\n\\emailAdd{c.vandebruck@sheffield.ac.uk}\n\n\n\n\\abstract{\nWe present a generalised calculation for the spectrum of primordial tensor perturbations in a cyclic Universe, making no assumptions about the vacuum state of the theory and accounting for the contribution of tensor modes produced in the dark energy phase of the previous cycle. We show that these modes have minimal impact on the spectrum observed in the current cycle, except for corrections on scales as large as the comoving Hubble radius today. These corrections are due to sub-horizon modes produced towards the end of the dark energy phase, persisting into the \\textit{ekpyrotic} phase of the next cycle as additional quanta. In relation to the vacuum state, we argue that non-Bunch-Davies quanta can easily overwhelm the energy density driving the dark energy phase, potentially compromising the model. Therefore, avoiding backreaction effects sets restrictive constraints on deviations away from the Bunch-Davies vacuum during this phase, limiting the overall freedom to consider alternative vacua in the cyclic Universe.}\n\n\\begin{document}\n\\maketitle\n\\flushbottom\n\n\n\\section{Introduction}\\label{intro}\n\nThe most compelling observational evidence supporting cosmological inflation~\\cite{Guth:1980zm,Linde:1981mu,Albrecht:1982wi,Vilenkin:1983xq} as the leading theory of the early Universe is currently provided by the Planck satellite measurement of the spectral index of scalar perturbations, $n_s = 0.9649\\pm0.0042$~\\cite{Planck:2018jri}. In the simplest single-field slow-roll inflationary models, the spectrum of scalar modes is expected to be almost but not exactly flat~\\cite{Mukhanov:1981xt, Bardeen:1983qw,Hawking:1982cz,Guth:1982ec}, with deviations from flatness are quantified in terms of how much $n_s$ deviates from 1~\\cite{Lidsey:1995np,Lyth:1998xn,Baumann:2009ds,Martin:2013tda}. As a result, the Planck data seem to be in excellent agreement with the theoretical predictions of inflationary models~\\cite{Planck:2018jri,Planck:2018vyg}, ruling out a Harrison-Zeldovich scale-invariant spectrum~\\cite{Harrison:1969fb,Zeldovich:1972zz,Peebles:1970ag} (corresponding to $n_s=1$) at a statistical level exceeding $8.5$ standard deviations and lending weight to the inflationary paradigm.\n\nThat being said, with no aim to downplay the significance of this result or its interpretation, it is crucial to emphasise that, on its own, it does not provide conclusive evidence for cosmological inflation. Even hinging on a certain level of optimism and setting aside the uncertainty surrounding constraints on $n_s$ from CMB experiments other than Planck\\footnote{Over the years, constraints on the spectral index have been released by a multitude of Planck-independent CMB experiments such as WMAP~\\cite{WMAP:2012fli,WMAP:2012nax}, the Atacama Cosmology Telescope (ACT)~\\cite{ACT:2020frw, ACT:2020gnv}, and the South Pole Telescope (SPT)~\\cite{SPT-3G:2014dbx, SPT-3G:2021eoc}. When considering these data at face value, Planck is currently the only experiment excluding $n_s=1$ at a statistical significance much larger than $3\\sigma$. Conversely, ACT shows a preference for $n_s=1$~\\cite{ACT:2020gnv,Giare:2022rvg}. Different combinations of these data overall support the result $n_s\\ne1$, although sometimes they lead to discordant results in terms of the other inflationary parameters or the preferred inflationary models~\\cite{Forconi:2021que,Giare:2023wzl}.} -- or the potential implications arising from the well-known tensions~\\cite{Bernal:2016gxb,Verde:2019ivm,DiValentino:2020zio,DiValentino:2021izs,Abdalla:2022yfr} characterising the recent debate\\footnote{For studies suggesting potential implications of cosmological tensions for inflation, see, e.g., Refs.~\\cite{DiValentino:2018zjj,Ye:2021nej,Ye:2022efx,Jiang:2022uyg,Jiang:2022qlj,Takahashi:2021bti,Lin:2022gbl,Hazra:2022rdl,Braglia:2021sun,Keeley:2020rmo,Jiang:2023bsz}}\n--  alternative theoretical mechanisms have been put forth, yielding an almost scale-invariant spectrum of primordial density fluctuations without invoking inflation.\n\nAn illustrative example of such mechanisms is the cyclic Universe~\\cite{Steinhardt:2001st,Steinhardt:2002ih,Steinhardt:2002kw,Khoury:2003rt,Turok:2004yx,Khoury:2004xi,Lehners:2008vx} that, in contrast to the conventional cosmological framework, suggests a periodic history for the Cosmos. The model has been extensively studied and discussed in relation to a broad range of topics, including quantum gravity, modified gravity, gravitational waves and dark energy, see e.g., Refs.~\\cite{Boyle:2003km,Ashtekar:2003hd,Bojowald:2004kt,Xiong:2007cn,Frampton:2007cv,Narlikar:2007hip,Baum:2007de,Biswas:2008ti,Cailleteau:2009fv,Brandenberger:2009ic,El-Nabulsi:2011xss,Cai:2012ag,Cai:2010zma,Nojiri:2011kd,Chang:2012yk,Ivanov:2012hq,Saaidi:2012qp,Bars:2013vba,Tavakoli:2014mra,Oriti:2016qtz,deCesare:2016rsf,Pavlovic:2017umo,Saridakis:2018fth,Das:2018bzx,Ijjas:2018bko,Ahmed:2019bff,Li:2019laq,Scherrer:2019dkc,Barca:2019ane,Ijjas:2021zwv,Gorkavyi:2021tbw,Martin-Benito:2021szh,Calcinari:2022iss,Giovannetti:2022qje,Giovannetti:2023psb} or Refs.~\\cite{Battefeld:2014uga,Brandenberger:2016vhg} for reviews. In broad terms, each cycle comprises a phase recasting the standard Hot Big Bang theory (during which large-scale structures take shape), followed by a phase of slow, accelerated expansion mirroring the present-day observational evidence for a Dark Energy dominated dynamics. In the cyclic Universe, this latter stage also serves to dilute inhomogeneities and flatten the spatial geometry. Subsequently, a contraction phase ensues, generating nearly scale-invariant density perturbations. Finally, the cycle concludes with a big-crunch/big-bang transition, during which matter and radiation are generated, setting the stage for the next cycle. \n\nNotice that both inflation and the cyclic Universe provide physical mechanisms to produce an almost scale-invariant spectrum of density perturbations~\\cite{Khoury:2001zk,Lehners:2007ac,Buchbinder:2007tw}. In addition, they can both explain observational facts such as the homogeneity in the cosmic microwave background (CMB) radiation~\\cite{Lehners:2013cka} and the fact that the present-day spatial geometry of the Universe appears to be flat, or at the very least nearly flat\\footnote{For recent discussions surrounding the spatial geometry of the Universe, see, e.g.,~\\cite{Park:2017xbl,Handley:2019tkm,DiValentino:2019qzk,Efstathiou:2020wem,DiValentino:2020hov,Benisty:2020otr,Vagnozzi:2020rcz,Vagnozzi:2020dfn,DiValentino:2020kpf,Yang:2021hxg,Cao:2021ldv,Dhawan:2021mel,Dinda:2021ffa,Gonzalez:2021ojp,Akarsu:2021max,Cao:2022ugh,Glanville:2022xes,Bel:2022iuf,Yang:2022kho,Stevens:2022evv,Favale:2023lnp}}. Therefore, at first glance, one might wonder how to distinguish between the two models. Focusing solely on scalar modes, this is a challenging knot to unravel~\\cite{Khoury:2003vb,Gratton:2003pe}. However, the two scenarios yield significantly distinct predictions for the stochastic background of gravitational waves~\\cite{Boyle:2003km}. Similar to scalar modes, inflation predicts a nearly scale-invariant (red-tilted) spectrum of tensor modes~\\cite{Baumann:2009ds,Martin:2013tda,Caprini:2018mtu}. Conversely, in the cyclic Universe, the tensor spectrum is typically blue-tilted, and its amplitude is many orders of magnitude lower than that predicted by inflation, remaining well below any observable threshold achievable in the near future. Consequently, any measurement of primordial gravitational waves (e.g., through the effects left in the CMB B-mode polarisation at large angular scales) would offer conclusive evidence for inflation, discounting the cyclic model. \n\nDespite this fact being acknowledged as a strength for inflation and perhaps a limitation in predictive capacity for the cyclic model, it is worth emphasising a few caveats surrounding this conclusion. Firstly, despite the best efforts, the detection of primordial tensor modes remains elusive at present~\\cite{BICEP:2021xfz}, making it impossible to discriminate between the two scenarios. Therefore, the cyclic Universe remains an alternative worth considering. Secondly, the inflationary predictions concerning the amplitude and tilt of the tensor spectrum depend significantly on the specific model. While well-known consistency relations among inflationary parameters can be derived within single-field slow-roll inflation minimally coupled to gravity~\\cite{Baumann:2009ds,Martin:2013tda}, these relations can be violated by various physical mechanisms. A long yet not exhaustive list of possibilities include considering modified gravity~\\cite{Kobayashi:2010cm,Kawasaki:2013xsa,Nozari:2016jmn,Giare:2020vss}, multi-field inflation~\\cite{Kaiser:2013sna,Price:2014ufa,Achucarro:2010da,DeAngelis:2023fdu,Giare:2023kiv}, additional (spectator) rolling axion fields~\\cite{Mukohyama:2014gba,Namba:2015gja,Peloso:2016gqs,Ozsoy:2020ccy}, couplings to axion-gauge or spin-2 fields~\\cite{Dimastrogiovanni:2016fuu,Iacconi:2019vgc}, breaking spatial and/or temporal diffeomorphism invariance~\\cite{Endlich:2012pz,Cannone:2014uqa,Graef:2015ova,Ricciardone:2016lym}, higher curvature corrections to the effective gravitational action~\\cite{Baumann:2015xxa,Giare:2020plo}, higher order operators in effective field theory~\\cite{Capurri:2020qgz,Giare:2022wxq}, violations of the null energy condition~\\cite{Cai:2022lec,Ye:2023tpz}, alternative vacuum state/initial conditions~\\cite{Ashoorioon:2013eia,Ashoorioon:2014nta,Choudhury:2023kam}, sound speed resonances~\\cite{Cai:2020ovp}, inflation in an Universe filled with an elastic medium~\\cite{Gruzinov:2004ty}, and possible effects/models inspired by quantum gravity~\\cite{Ashoorioon:2005ep,Brandenberger:2006xi,Brandenberger:2014faa,Baumgart:2021ptt}. Many of these more elaborated scenarios yield completely different predictions, often resulting in a blue-tilted spectrum and possibly leaving signatures in different cosmological and astrophysical observables~\\cite{Stewart:2007fu,Cai:2014uka,Wang:2014kqa,Kuroyanagi:2014nba,Kuroyanagi:2020sfw,Giare:2020vhn,Vagnozzi:2020gtf,Vagnozzi:2023lwo,Jiang:2023gfe,Oikonomou:2024aww}. Furthermore, models with an arbitrarily small tensor amplitude can always be constructed (see, e.g., Ref~\\cite{Stein:2022cpk}), making it virtually impossible to rule out inflation based solely on a lack of detection of primordial gravitational waves. This is a critique frequently raised against inflationary cosmology as it questions its actual predictive capability.\n\nAs concerns the cyclic Universe, since any difference with respect to inflation in terms of predictions is likely to be confined to the spectrum of tensor modes, it becomes interesting to test whether similar caveats apply or if the model demonstrates greater resilience. \n\nIn light of this, we review the production of primordial gravitational waves in a cyclic Universe, identifying (and eventually clarifying) some conceptual aspects related to its concrete predictivity. Specifically, prevailing calculations in the existing literature conventionally establish initial conditions for primordial scalar and tensor modes during the \\textit{ekpyrotic} contracting phase~\\cite{Boyle:2003km}. While for scalar perturbations the implications of setting the initial conditions in different phases have been examined~\\cite{Erickson:2006wc}, the calculation of the tensor spectrum has always been performed starting in the \\textit{ekpyrotic} phase, assuming a Bunch-Davies (BD) vacuum state and neglecting any potential contributions arising from tensor modes originating during the dark energy phase of the previous cycle. This leads us to question whether they exert any influence on the spectrum observed in the current cycle. Taking a broader perspective, one may wonder whether the predictions concerning tensor modes remain resilient throughout the diverse cycles of the model itself. Yet another aspect that is imperative to clarify is to what extent the predictions depend on the choice of the vacuum state, addressing the crucial question of what freedom exists in the cyclic Universe regarding the choice of the vacuum state and whether substantial alterations can arise in the tensor spectrum by assuming different vacua, akin to what is found in inflationary cosmology.\n\nTo address these points, we present a general model for the evolution of gravitational waves produced in a cyclic Universe, making no assumptions about the initial vacuum state and starting the calculation from the dark energy phase of the previous cycle. We find that the additional tensor models originated in the previous cycle have minimal impact on the tensor spectrum observed in the current cycle, except for corrections on scales as large as the comoving Hubble radius today that are due to sub-horizon modes produced towards the end of the previous dark energy phase. Most importantly, we find that non-BD quanta in the dark energy phase can easily overwhelm the energy density associated with the modulus field, potentially spoiling the model. Avoiding these backreaction effects sets restrictive constraints on deviations away from the BD vacuum during the dark energy phase, thereby limiting the overall freedom to consider alternative vacua in the cyclic Universe.\n\nThe paper is organised as follows. In \\autoref{construction of the model}, we introduce the cyclic Universe model and review its background dynamics. In \\autoref{primordial tensor spectrum}, we consider the evolution of gravitational waves in such a Universe, starting from the previous cycle's dark energy phase and deriving the evolution in full generality. In \\autoref{sec:discussion}, we discuss the implications for the model's predictions, deriving constraints on the choice of the vacuum state and analysing the contribution coming from tensor modes originated in the previous cycle. Finally, in \\autoref{sec:conc}, we derive our main conclusions.\n\n\\section{Cyclic model and Background Dynamics} \\label{construction of the model}\nWe consider a simple scalar field setup in which the dynamics of the cyclic model in the Einstein frame are well described by the 4$D$ effective Lagrangian~\\cite{Erickson:2006wc}\n\\begin{equation}\\label{lagrangian}\n    \\mathcal{L}=\\sqrt{-g}\\left(\\frac{M_{\\rm Pl}^2}{2}R-\\frac{1}{2}\\partial_\\mu\\phi\\partial^\\mu\\phi-V(\\phi)\\right),\n\\end{equation}\nwhere $g$ is the determinant of the metric $g_{\\mu \\nu}$, $R$ is the Ricci scalar and we adopt units where $c=1$. The scalar field $\\phi$ is a modulus field, driving the dark energy dominated phase and the subsequent \\textit{ekpyrotic} and contracting kinetic phases, which we discuss below. Assuming a spatially flat FRLW background, the scalar field satisfies the usual equation of motion\n\\begin{equation}\n    \\ddot{\\phi}+3H\\dot{\\phi}+V_{,\\phi}=0,\n    \\label{KG}\n\\end{equation}\nwhere dots denote derivatives with respect to cosmological time $t$. On the other hand, ignoring any coupling between the scalar field and the other standard model species and neglecting any additional contributions from the latter to the total Universe energy density, the evolution of the scale factor is governed by the Friedmann equation that, in terms of the Hubble parameter $H=\\dot{a}/a$, reads\n\\begin{equation}\n    H^2=\\frac{1}{3 M_{\\rm Pl}^2}\\biggl(\\frac{1}{2}\\dot{\\phi}^2 + V(\\phi)\\biggl).\n    \\label{H2}\n\\end{equation}\nIn what follows, to efficiently describe the dynamics of the cyclic Universe we focus on a phenomenological potential of the form\n\\begin{equation}\\label{potnetial} \n    V=V_0\\left(1-e^{-c\\phi/M_{\\rm Pl}}\\right)Y(\\phi),\n\\end{equation}\nwhere $V_0$ is of the same order of the vacuum energy observed in today’s Universe, $c$ is a positive constant value and $Y(\\phi)$ is a step function. Notice that our choice concerning the specific potential employed in the work is, in part, motivated by the fact that the exponential form is convenient for analysis, and in part from the fact that the same potential has been widely adopted in similar studies, allowing a direct comparison between our findings and other results documented in the existing literature. However, it is important to emphasise that cyclic models can emerge from a broad spectrum of different potentials that should ultimately emerge from the higher-dimensional theory. Without loss of generality, the only constraint comes from requiring an acceptable spectrum of scalar perturbations that implies considering a steep, strongly negatively curved region across observational ranges of the field to reproduce. \n\nHaving that said, the potential~\\eqref{potnetial} serves multiple purposes, including describing dark energy responsible for cosmic acceleration observed today. More importantly, it plays a crucial role in transitioning the Universe from accelerated expansion to contraction. This is achieved by rolling from positive to negative values of the potential until reaching a time where $H^2=0$ and consequently triggering a phase characterised by an equation of state $\\omega \\gg 1$. For instance, by solving Eq.~\\eqref{H2} it can be shown that when the negative potential dominates $V\\simeq -V_0e^{-c\\phi}$ (\\textit{i.e.} \\textit{ekpyrotic} phase), the scale factor behaves as~\\cite{Erickson:2006wc}\n\\begin{equation}\n    a(t)\\sim (-t)^{\\tilde{\\alpha}},\n\\end{equation}\nwhere $t$ has negative values and $\\tilde{\\alpha}=2/c^2$. At this point it is also convenient to introduce the conformal time $d\\tau=dt/a(t),$ which we will frequently use later. In terms of the conformal time, the scale factor during the \\textit{ekpyrotic} phase evolves as:\n\\begin{equation}\n    a(\\tau)\\sim \\biggl[(-1)^{\\tilde{\\alpha}}(\\tau-\\tilde{\\alpha} \\tau)^{\\tilde{\\alpha}/(1-\\tilde{\\alpha})}\\biggl]_{\\tau_i}^{\\tau_{f}},\n    \\label{scalefactor}\n\\end{equation}\nunderscoring that the Universe is gradually contracting while the scalar field slowly descends along its sharply decreasing negative potential, to produce an acceptable spectrum of cosmological scalar perturbations.\n\nIn the literature, the \\textit{ekpyrotic} phase is typically assumed as the starting point of the cycle where initial conditions of primordial perturbations are imposed, and the calculations of the relative scalar and tensor spectra begin. However, in this study, we want to extend the model to include the contribution of tensor perturbations produced during the dark energy phase of the previous cycle to investigate whether they could have any impact on the spectrum we observe in the current cycle and eventually clarify why (not). To do so, the overall strategy will be to start the calculation in the dark energy phase of the previous cycle (making no assumptions on the vacuum state) and evolve the system through four regimes. For this reason, before dealing with the explicit calculation of the tensor spectrum, given that in our case we consider one more phase than in previous studies, it is useful to dedicate the following two subsections to reviewing the background dynamics of the model. In the same spirit of the discussion outlined so far, we start from the dark energy phase of the previous cycle and evolve the scale factor, ensuring its continuity across the boundaries of each phase. Additionally, we derive constraints on the model's parameters based on minimum theoretical requirements, such as the continuity of $H(t)$ and the consistency of the theory across cycles.\n\n\\subsection{Evolution and continuity of the scale factor across stages}\n\n\\subsubsection{Dark Energy phase}\nWe start from the dark energy phase, in which the expansion rate $H$ is roughly constant and the scale factor $a(t)$ behaves as\n\\begin{equation}\n    a(t)=a(t_{\\rm{tr}})e^{H\\left(t-t_{\\rm{tr}}\\right)} \\;\\;\\;\\;t<t_{\\rm{tr}},\n    \\label{adarkenergy}\n\\end{equation}\n\nwith $t_{tr}$ transition time between dark energy and \\textit{ekpyrotic} phase. The equation above can be translated in terms of conformal time as\n\n\n\n\n\n\\begin{equation}\n    a(\\tau) = \\frac{1}{H(B-\\tau)},\n    \\label{adarkenergyconf}\n\\end{equation}\nwhere, for the continuity across $\\tau = \\tau_{\\rm{tr}}$, $B$ is fixed to\\footnote{The value of $B$ is achieved by considering $H_0, a(\\tau_{\\rm{r}})$ and $\\tau_{\\rm{r}}$ given in the next sections.}\n\\begin{equation}\\label{B_const}\n    B = \\frac{1}{a(\\tau_{\\rm{tr}})H}+\\tau_{\\rm{tr}}.\n\\end{equation}\nAdditionally, by means of Eq.~\\eqref{scalefactor}, we can infer \n\\begin{equation}\n    \\frac{a(\\tau_{\\rm{tr}})}{a(\\tau_{\\rm{end}})} = \\left(\\frac{\\tau_{\\rm{tr}}- \\tau_{\\rm{ek}}}{\\tau_{\\rm{end}}-\\tau_{\\rm{ek}}}\\right)^\\alpha,\n\\end{equation}\nwhere $\\alpha\\equiv \\tilde{\\alpha}/(1-\\tilde{\\alpha})$ and $\\tau_{\\rm{ek}}\\equiv (1-2\\alpha)\\tau_{\\rm{end}}$ is the conformal time corresponding to when the potential diverges to minus infinity.\n\n\\subsubsection{Ekpyrotic phase}\n\nAs a next step, we transition to the \\textit{ekpyrotic} phase. In this phase the potential becomes negative and the Einstein frame expansion forces the scale factor to contract\n\\begin{equation}\\label{ekpyrotic scale factor}\n    \\frac{a(\\tau)}{a(\\tau_{end})}=\\left(\\frac{\\tau-\\tau_{ek}}{\\tau_{end}-\\tau_{ek}}\\right)^\\alpha\\;\\;\\;\\;\\; \\tau_{tr}<\\tau<\\tau_{end}.\n\\end{equation}\nWe note again that, being $\\alpha\\ll 1$, the contraction is very slow.\n\n\\subsubsection{Contracting kinetic phase}\n\nOnce $\\tau > \\tau_{\\rm{end}}$ we enter the region where the effects of the potential are negligible, namely $\\phi<\\phi_{\\rm{end}}$. During this period -- known as contracting kinetic phase -- we have:\n\\begin{equation}\n    \\frac{a(\\tau)}{a(\\tau_{\\rm{r}})}=\\left(\\frac{-\\tau}{(1+\\chi)\\tau_{\\rm{r}}}\\right)^{\\frac{1}{2}}\\;\\;\\;\\;\\;\\tau_{\\rm{end}}<\\tau<0,\n    \\label{akineticcontraction}\n\\end{equation}\nwhere $\\chi$ is a small positive constant that measures the amount of radiation created at the bounce ($\\tau=0$). \n\n\\subsubsection{Expanding kinetic phase}\n\nFinally, for the last phase of the cycle, the so-called expanding kinetic phase, we get\n\\begin{equation}\n    a(\\tau)=\\left(\\frac{\\tau}{\\tau_{\\rm{r}}}\\right)^{\\frac{1}{2}},\\;\\;\\;\\;\\;0<\\tau<\\tau_{\\rm{r}},\n    \\label{aexpanding}\n\\end{equation}\nNotice that, for convenience, we work in a coordinate frame where the scale factor is set to $a(\\tau_{\\rm{r}}) = 1$ for the time $t_{\\rm{r}}$ corresponding to the beginning of the radiation-dominated era. The corresponding conformal time $\\tau_{\\rm{r}}= (2H_{\\rm{r}})^{-1}$ is constrained by the radiation temperature $T_{\\rm{r}}$, being $H_{\\rm{r}}\\propto T_{\\rm{r}}^2/M_{\\rm{Pl}}$. Following Ref.~\\cite{Khoury:2003rt}, we choose a quite conservative value  $T_{\\rm{r}} \\sim 10^7$ GeV, akin to that obtained in the more familiar standard cosmology at the end of the reheating phase following inflation. This choice also ensures that we can safely recover predictions of primordial Big Bang Nucleosynthesis (BBN).\n\n\\subsection{Parameter constraints}\n\n\\subsubsection{Continuity of the Hubble parameter}\n\nIn order to constrain the length of the \\textit{ekpyrotic} contracting phase, we require that the Hubble parameter returns to its original value after every cycle. Following Ref.~\\cite{Erickson:2006wc}, this implies that\n\\begin{equation}\nH_{\\rm{end}}/H_{\\rm{0}}\\approx \\sqrt{\\frac{-V_{\\rm{end}}}{V_0}}\n\\end{equation}\nwhere $V_{\\rm{end}}$ is the depth of the potential well and $V_0$ is the height of the potential plateau. The spectral range of perturbations produced when the field rolls from $V\\approx 0$ to $V\\approx -V_{\\rm{end}}$, satisfies\n\\begin{equation}\n    \\frac{k_{\\rm{max}}}{k_{\\rm{min}}}\\approx\\sqrt{\\frac{-V_{\\rm{end}}}{V_0}},\n    \\label{Hratio}\n\\end{equation}\nand it needs to span at least $N=60$ e-folds for the \\textit{ekpyrotic} phase to produce a scale-invariant spectrum over a broad enough range of scales for us to observe today~\\cite{Planck:2018jri,Planck:2018vyg}. Hence, the transition time between dark energy and \\textit{ekpyrotic} phase is constrained to\n\\begin{equation}\n    \\frac{H_{\\rm{tr}}}{H_{\\rm{end}}}=2\\, \\alpha\\, \\tau_{\\rm{end}} \\, \\frac{a_{\\rm{end}}\\, a_{\\rm{tr}}}{{(\\tau_{\\rm{tr}}-\\tau_{\\rm{ek}})}}<e^{-60}.\n    \\label{Htrans/Hend}\n\\end{equation}\nMoreover, as $a(\\tau)\\approx \\rm{const}$ during the \\textit{ekpyrotic} contracting phase \\cite{Erickson:2006wc}, from Eq.~\\eqref{Htrans/Hend} it follows that\n\\begin{equation}\n    \\left|\\tau_{\\rm{tr}}-\\tau_{\\rm{ek}}\\right|>2\\, \\alpha\\, \\tau_{\\rm{end}}\\, a_{\\rm{end}}^2\\, e^{60}.\n\\end{equation}\n\n\\subsubsection{Cycling constraint}\nTo place constraints on the duration of the kinetic evolution phases, we require that they must last enough time for the scalar field to have started at the potential minimum (\\textit{i.e.} $\\phi=\\phi_{\\rm{end}}$), moved off to the Bounce (\\textit{i.e.} $\\phi\\to-\\infty$), and returned all the way back past $\\phi_{\\rm{end}}$ and made it up to the potential plateau to begin a radiation-dominated Universe.\n\nBarring some brief $\\omega\\gg 1$ period (which divides the expanding kinetic phase into two parts) as the field moves back up past $\\phi_{\\rm{end}}$ to the potential plateau, from Eq.~\\eqref{KG} and Eq.~\\eqref{potnetial} we find\n\\begin{equation}\n    \\phi-\\phi_{end}=c_1 \\ln\\left(\\frac{t}{t_{end}}\\right),\n\\end{equation}\nwhere the factor $c_1^2=2/3$ comes from Eq.~\\eqref{H2} during kinetic domination, $t_{\\rm{end}}$ is the time taken to reach $\\phi_{\\rm{end}}$  starting at $\\phi\\to-\\infty$. \n\nNotice that in the region where $\\omega\\gg 1$, contributions from $Y(\\phi)$ becomes relevant and $V\\approx V_0\\left(1-e^{-c\\phi}\\right)$. Therefore, in this case, the time $t_{\\rm{r}}$ required to climb the potential well and reach the plateau at $V\\approx V_0$ can be bounded to \n\\begin{equation}\n    \\frac{t_{\\rm{r}}}{t_{\\rm{end}}}>\\left(\\frac{V_{\\rm{end}}}{V_0}\\right)^{\\sqrt{\\frac{3}{2 c^2}}}.\n    \\label{eq:tr_tend}\n\\end{equation}\nSince the time taken for the field to cross the negative region of the potential before radiation domination begins is given by\n\\begin{equation}\n    \\frac{t_{\\rm{r}}}{t_{\\rm{end}}}\\approx \\frac{\\sqrt{V_{\\rm{end}}}}{H_{\\rm{r}}},\n\\end{equation}\nfrom Eq.~\\eqref{eq:tr_tend} we can infer an upper limit on the Hubble parameter at $t_r$ which reads  \n\\begin{equation}\n    H_{\\rm{r}} \\lesssim \\frac{\\sqrt{V_{\\rm{end}}}}{M_{\\rm{Pl}}}\\left(\\frac{V_0}{V_{\\rm{end}}}\\right)^{\\frac{3}{2c^2}}.\n    \\label{Hr}\n\\end{equation}    \nThis upper limit constrains the ratio between $\\tau_r$ and $\\tau_{\\rm end}$ to\n\\begin{equation}\\label{gamma}\n    \\Gamma = \\left|\\frac{\\tau_{\\rm{r}}}{\\tau_{\\rm{end}}}\\right| \\gtrsim \\left(\\frac{V_{\\rm{end}}}{V_0}\\right)^{\\sqrt{\\frac{2}{3c^2}}}\\simeq 10^8,\n\\end{equation}\nwhere we used that the vacuum energy density $\\rho_{\\Lambda} = V_0 \\sim 10^{-120}M_{\\rm{Pl}}^4$ and $V_{\\rm{end}}\\sim 10^{-20} M_{\\rm{Pl}}^4$.\nFollowing Ref.~\\cite{Boyle:2003km}, throughout this paper, we always consider the dimensionless parameter $\\Gamma \\sim 10^8$.\\\\\n\\\\\n\\noindent In conclusion, the final constraints we derive for the cyclic model at the background level (and that are important to bear in mind for the following discussion on tensor perturbations) are:\n\\begin{align}\n    \\left|\\frac{\\tau_{\\rm{r}}}{\\tau_{\\rm{end}}}\\right|&\\gtrsim 10^8, & (\\tau_{\\rm{tr}} - \\tau_{\\rm{ek}})&\\gtrsim 10^9, & \\tau_{\\rm{r}} = \\frac{1}{2H_{\\rm{r}}}.\n\\end{align}\n\n\n\\section{General primordial tensor spectrum}\n\\label{primordial tensor spectrum}\nConsidering a spatially flat FLRW metric, in the synchronous gauge the perturbed line element reads:\n\\begin{equation}\nd s^2=a^2(\\tau)\\left[d \\tau^2-\\left(\\delta_{i j}+h_{i j}\\right) d x^i d x^j\\right].\n\\end{equation}\nTensor modes (i.e., metric perturbations) are described in terms of the transverse and traceless part of the symmetric 3×3 matrix $h_{ij}$. To characterise the contribution of each wavenumber $k$ to $h_{ij}(t,\\mathbf{x})$, we consider a Fourier representation $\\tilde{h}_{ij}(t,\\mathbf{k})$. Moving to the Fourier space, focusing on one particular polarisation state, and assuming isotropy, the gravitational wave field $h_{k}$ satisfies the following equation:\n\\begin{equation}\n    h_{k}'' + 2\\frac{a'}{a}h_k'+k^2h_k=0,\n\\end{equation}\nwhere $(..)'$ denotes the derivative with respect to conformal time $\\tau$. However, it is more convenient to use a new variable $f_k(\\tau)\\equiv a(\\tau)h_k(\\tau)$ satisfying\n\\begin{equation}\n\\label{BesselsEq}\nf_k''+\\left(k^2+\\frac{a''}{a}\\right)f_k=0.\n\\end{equation}\nAfter redefining $f_k=i\\sqrt{\\tau}\\,u_k$, Eq.~\\eqref{BesselsEq} assumes the more familiar form of a Bessel equation and, for each phase of the cyclic model, the general solution involves a linear combination of the Hankel functions of the first and second kind, that we denote as $H^{(1,2)}$. \n\nIn this section, we present a generalised calculation for the primordial tensor spectrum in the cyclic Universe. In \\autoref{sec.general_solution}, we derive the general solution of Eq.~\\eqref{BesselsEq} making no assumptions about the vacuum state of the theory and considering the contribution of tensor modes produced in the dark energy phase of the previous cycle rather than starting directly from the \\textit{ekpyrotic} phase of the present cycle. In \\autoref{sec.Matching_phases}, we require internal consistency in the evolution of the tensor mode amplitudes throughout the four phases of the model, ensuring that both $h_k(\\tau)$ and $h_k'(\\tau)$ remain continuous functions and matching the general solutions across the different phases. Finally, in \\autoref{sec.spectrum_today}, we derive the expression of the primordial tensor spectrum and briefly discuss its strain today.\n\n\\subsection{General Solutions in the different Phases}\n\n\\label{sec.general_solution}\n\n\\subsubsection{General solution in the Dark Energy phase}\nDuring the dark energy phase, $a(\\tau)$ is given by Eq.~\\eqref{adarkenergyconf}, and the general solution of Eq.~\\eqref{BesselsEq} reads \n\\begin{equation}\\label{DE solution}\n    f_k(\\eta)=\\sqrt{-k\\eta}\\left(D_1(k)H^{(1)}_\\frac{3}{2}(-k\\eta)+D_2(k)H^{(2)}_{\\frac{3}{2}}(-k\\eta)\\right)\\;\\;\\;\\tau<\\tau_{\\rm{tr}},\n\\end{equation}\nwhere $H_n^{(1)}$ and $H_n^{(2)}$ denote the Hankel functions of first and second kind respectively, $\\eta=\\tau-B$, where $B$ is given by Eq.~\\eqref{B_const} and, for each wave-number $k$, $D_{1,2}(k)$ are arbitrary constants. \n\n\\subsubsection{General solution in the \\textit{ekpyrotic} contraction}\nIn the stage of the \\textit{ekpyrotic} contraction, $a(\\tau)$ is given by Eq.~\\eqref{ekpyrotic scale factor} and the general solution reads\n\\begin{equation}\\label{ekpyrotic solution}\nf_k(\\tau)=\\sqrt{y}\\left(A_1(k)H^{(1)}_n(y)+A_2(k)H^{(2)}_{n}(y)\\right)\\;\\;\\;\\tau_{\\rm{tr}}<\\tau<\\tau_{\\rm{end}},\n\\end{equation}\nwhere $y\\equiv-k(\\tau-\\tau_{\\rm{ek}})$. \n\n\\subsubsection{General solution in the kinetic contracting phase}\nMoving to the kinetic contracting phase and making use of Eq.~\\eqref{akineticcontraction}, we achieve\n\\begin{equation}\n    f_k(\\tau)=\\sqrt{-k\\tau}\\left(B_1(k)H^{(1)}_0(-k\\tau)+B_2(k)H^{(2)}_{0}(-k\\tau)\\right)\\;\\;\\;\\tau_{\\rm{end}}<\\tau<0,\n\\end{equation}\n\n\\subsubsection{General solution in the kinetic expanding phase}\nConsidering Eq.~\\eqref{aexpanding} in the kinetic expanding phase the genral solution is\n\\begin{equation}\n    f_k(\\tau)=\\sqrt{k\\tau}\\left(C_1(k)H^{(1)}_0(k\\tau)+C_2(k)H^{(2)}_{0}(k\\tau)\\right)\\;\\;\\;0<\\tau<\\tau_{\\rm{r}}.\n\\end{equation}\n\n\n\\subsection{Matching phases}\n\\label{sec.Matching_phases}\nIn what follows we obtain expressions for the coefficients $D_{1,2}(k),A_{1,2}(k),B_{1,2}(k)$ and $C_{1,2}(k)$ by matching $h_k(\\tau)$ and $h_k(\\tau)'$ at the boundaries of each phase. \\\\\n\n\\subsubsection{Dark energy - \\textit{ekpyrotic}}\nAt the boundary between dark energy and \\textit{ekpyrotic} stage, we require continuity of $f_k(\\tau)$ namely\n\\begin{equation}\\label{dark energy-ekpyrotic matching}\n    \\sqrt{x_{\\rm{tr}}}\\left(D_1(k)H^{(1)}_\\frac{3}{2}(x_{\\rm{tr}})+D_2(k)H^{(2)}_{\\frac{3}{2}}(x_{\\rm{tr}})\\right)= \\sqrt{y_{\\rm{tr}}}\\left(A_1(k)H^{(1)}_n(y_{\\rm{tr}})+A_2(k)H^{(2)}_{n}(y_{\\rm{tr}})\\right),\n\\end{equation}\nwhere $x_{\\rm{tr}}=-k\\eta(\\tau_{\\rm{tr}})$, and continuity of $f'(\\tau)$ which we write in the matrix form $\\underline{\\underline{\\textbf{y}}}_1\\textbf{A}=\\underline{\\underline{\\textbf{x}}}_1\\textbf{D}$ where\n\n\n\n\n\n    \\\\\n\n\n\n\n \n\n\n\n\n\n\n\n    \n\n\n\\begin{align}\\label{transition matching}\n \\underline{\\underline{\\textbf{y}}}_1&=\\left(\n \\renewcommand*{\\arraystretch}{1.5}\n    \\begin{matrix}\n        \\sqrt{y_{\\rm{tr}}}H^{(1)}_n(y_{\\rm{tr}}) & \n        \\sqrt{y_{\\rm{tr}}}H^{(2)}_n(y_{\\rm{tr}})\\\\[5pt]\n        \\sqrt{y_{\\rm{tr}}}H^{(1)}_{n-1}(y_{\\rm{tr}})-\\frac{(n-\\frac{1}{2})}{\\sqrt{y_{\\rm{tr}}}}H^{(1)}_n(y_{\\rm{tr}})\n        & \\qquad\n        \\sqrt{y_{\\rm{tr}}}H^{(2)}_{n-1}(y_{\\rm{tr}})-\\frac{(n-\\frac{1}{2})}{\\sqrt{y_{\\rm{tr}}}}H^{(2)}_n(y_{\\rm{tr}})\n    \\end{matrix}\n    \\right),\\nonumber\\\\[10pt]\n    \\underline{\\underline{\\textbf{x}}}_1&=\\left(\n    \\begin{matrix}\n        \\sqrt{x_{\\rm{tr}}}H^{(1)}_\\frac{3}{2}(x_{\\rm{tr}}) & \\sqrt{x_{\\rm{tr}}}H^{(2)}_\\frac{3}{2}(x_{\\rm{tr}})\\\\[5pt]\n        \\sqrt{x_{\\rm{tr}}}H^{(1)}_\\frac{1}{2}(x_{\\rm{tr}})-\\frac{1}{\\sqrt{x_{\\rm{tr}}}}H^{(1)}_\\frac{3}{2}(x_{\\rm{tr}})\n        & \\qquad\n        \\sqrt{x_{\\rm{tr}}}H^{(2)}_\\frac{1}{2}(x_{\\rm{tr}})-\\frac{1}{\\sqrt{x_{\\rm{tr}}}}H^{(2)}_\\frac{3}{2}(x_{\\rm{tr}})\n    \\end{matrix}\n    \\right),\n\\end{align}\nand\n\\begin{equation}\n    \\textbf{A} = \\left(\\begin{matrix}\n        A_1\\\\A_2\n    \\end{matrix}\\right),\\;\\;\\;\\;\\;\\;\n    \\textbf{D} = \\left(\\begin{matrix}\n        D_1\\\\D_2\n    \\end{matrix}\\right),\\;\\;\\;\\;\\;\\; \\text{with} \\;\\; x_{\\rm{tr}}\\equiv-k\\tau_{\\rm{tr}}.\n\\end{equation}\n\n\\subsubsection{\\textit{Ekpyrotic} - kinetic contraction}\n\nSimilarly here, at $\\tau = \\tau_{\\rm{end}}$ namely the end of the \\textit{ekpyrotic} phase, we require $\\underline{\\underline{\\textbf{y}}}_2\\textbf{B}=\\underline{\\underline{\\textbf{x}}}_2\\textbf{A}$ where \n\n\n\n\\begin{align}\n \\underline{\\underline{\\textbf{y}}}_2&=\\left(\n    \\begin{matrix}\n        H^{(1)}_0(x_{\\rm{e}}) & H^{(2)}_0(x_{\\rm{e}})\\\\[5pt]\n        \\sqrt{x_{\\rm{e}}}H^{(1)}_{-1}(x_{\\rm{e}})+\\frac{H^{(1)}_0(x_{\\rm{e}})}{2\\sqrt{x_{\\rm{e}}}}\n        &\\qquad\n        \\sqrt{x_{\\rm{e}}}H^{(2)}_{-1}(x_{\\rm{e}})+\\frac{H^{(2)}_0(x_{\\rm{e}})}{2\\sqrt{x_{\\rm{e}}}}\n    \\end{matrix}\n    \\right),\\nonumber \\\\[10pt]\n    \\underline{\\underline{\\textbf{x}}}_2&=\\left(\n    \\begin{matrix}\n        \\sqrt{2\\alpha}H^{(1)}_n(2\\alpha x_{\\rm{e}}) & \n        \\sqrt{2\\alpha}H^{(2)}_n(2\\alpha x_{\\rm{e}})\\\\[5pt]\n        \\sqrt{2\\alpha x_{\\rm{e}}}H^{(1)}_{n-1}(2\\alpha x_{\\rm{e}})-\\frac{(n-\\frac{1}{2})}{\\sqrt{2\\alpha x_{\\rm{e}}}}H^{(1)}_n(2\\alpha x_{\\rm{e}})\n        & \\qquad\n        \\sqrt{2\\alpha x_{\\rm{e}}}H^{(2)}_{n-1}(2\\alpha x_{\\rm{e}})-\\frac{(n-\\frac{1}{2})}{\\sqrt{2\\alpha x_{\\rm{e}}}}H^{(2)}_n(2\\alpha x_{\\rm{e}})\n    \\end{matrix}\n    \\right),\n\\end{align}\nand \n\\begin{equation}\n    \\textbf{B}=\\left(\\begin{matrix}\n    B_1\\\\B_2\n\\end{matrix}\\right),\\;\\;\\;\\;\\;\\; \\text{with} \\;\\; x_{\\rm{e}}\\equiv-k\\tau_{\\rm{end}}.\n\\end{equation}\n\n\\subsubsection{Kinetic contraction- kinetic expansion}\nThe matching for these two final stages arises at $\\tau = 0$, which is trivial as we have\n\\begin{equation}\n    C_{1,2} = -\\sqrt{1+\\chi}B_{2,1}.\n\\end{equation}\n\n\n\\subsection{Strain spectrum today ($\\tau=\\tau_0$)}\n\\label{sec.spectrum_today}\n\nThe quantity in terms of which we assess the production of primordial gravitational waves in the cyclic Universe is the dimensionless strain spectrum \n\\begin{equation}\\label{general_strain_expreession}\n    \\Delta h = k^{\\frac{3}{2}}\\frac{\\left|h_k(\\tau)\\right|}{\\pi}.\n\\end{equation}\nIt is convenient to evaluate the dimensionless strain spectrum in the radiation-dominated epoch. Using the general solutions we derived and matched across the different phases of the model, it reads:\n\\begin{equation}\\label{strain_spectrum}\n    \\Delta h(k, \\tau_{r}) = \\frac{k^2\\sqrt{2\\,\\tau_{\\rm{r}}}}{a(\\tau_{\\rm{r}})\\pi M_{\\rm{Pl}}}\\left|C_1(k) H_0^{(1)}(x_{\\rm{r}}) + C_2(k)H_0^{(2)}(x_{\\rm{r}})\\right|,\n\\end{equation}\nwhere $x_{\\rm{r}}\\equiv k\\,\\tau_{\\rm{r}}$. The strain tensor spectrum today $\\Delta h(k, \\tau_0)$ can be easily related to the spectrum in the radiation dominated epoch by means of the transfer function formalism as:\n\\begin{equation}\\label{spectrum today}\n    \\Delta h(k, \\tau_0) \\equiv \\mathcal{T}(k) \\Delta h(k, \\tau_{\\rm{r}}), \n\\end{equation}\nwhere $\\mathcal{T}(k)$ is the transfer function, responsible for propagating the spectrum forwards to today's observed spectrum. Following Ref.~\\cite{Boyle:2003km}, we use a transfer function of the form \n\\begin{equation}\\label{transfer function}\n    \\mathcal{T}(k)\\approx \\left(\\frac{k_0}{k}\\right)^2\\left(1+\\frac{k}{k_{\\rm{eq}}}+\\frac{k^2}{k_{\\rm{eq}}k_{\\rm{r}}}\\right),\n\\end{equation}\nwhere $k_{\\rm{r}} = a_{\\rm{r}} H_{\\rm{r}} \\approx T_r^2/M_{\\rm{Pl}}\n$\nis the wave number of modes crossing the horizon at the start of radiation domination. \n\nSo far, the whole calculation is done by setting $a(\\tau_{\\rm{r}})=1$. However, for ease of comparison with data and the other results documented in the literature, it is convenient to return to the usual coordinate system where $a_0 = 1$. This can be easily done by means of the well-known inverse scaling between the scale factor and the CMB temperature (\\textit{i.e.} $a \\propto 1/T$). This relation fixes the ratio $a(\\tau_{0}) / a(\\tau_{\\rm{r}}) \\propto T_{\\rm{r}}/T_0 \\sim 10^{20}$. As a result, we can re-scale the comoving frequency accordingly, which now takes the more familiar value $\\hat k_{\\rm{r}}=k_{\\rm{r}}/a(\\tau_{0})=10^{-1}\\, \\text{Hz}$. From now on, we will name the re-scaled comoving frequency $\\hat k_{\\rm{r}}$ as $k_{\\rm{r}}$. Additionally, always following from the inverse proportionality between the scale factor and the CMB temperature, we expect a spectral range of modes entering between the start of radiation domination and today of the order of\n\\begin{equation}\n    \\frac{k_0}{k_{\\rm{r}}}\\propto \\frac{T_0}{T_{\\rm{r}}} \\sim 6.6 \\times 10^{-20}.\n\\end{equation}\nTaking in mind that during matter domination $H\\sim a^{-3/2}$, we get\n\\begin{equation}\n    \\frac{k_{\\rm{eq}}}{k_0}\\approx \\sqrt{1+z_{\\rm{eq}}} \\sim 10^2,\n\\end{equation}\nwhere $z_{\\rm{eq}}$ is the redshift at the equivalence.\n\nWe can also place a bound on the wave number of modes on the horizon at the previous dark energy-\\textit{ekpyrotic} transition, $k_{\\rm{tr}}$, which will be useful later. The wavelength of such modes can be estimated using the duration of the \\textit{ekpyrotic} phase, which we require to last at least 60 e-folds to effectively homogenise and flatten the Universe for the subsequent cycle. However, a much stronger constraint comes from Eq.~\\eqref{Hratio}, as the number of e-folds during the \\textit{ekpyrotic} phase is given by $N\\sim \\ln(H_{\\rm{end}}/H_0)\\approx \\ln(\\sqrt{-V_{\\rm{end}}/V_0})$. Taking our previous constraints on $V_0$ at today's dark energy density and $V_{\\rm{end}}$ at the GUT scale, we arrive at $N\\sim 115$. This forces our horizon to shrink by a factor of $\\sim 10^{50}$ during the dark \\textit{ekpyrotic} phase, and hence we would expect\n\\begin{equation}\n\\frac{k_{\\rm{end}}}{k_{\\rm{tr}}}\\approx 10^{50}.\n\\end{equation}\n\n\\section{Resilience through cycles and the vacuum state}\n\\label{sec:discussion}\nThe calculation of the spectrum of gravitational waves introduced in the previous section, in addition to considering the contribution of tensor modes produced in the dark energy phase of the previous cycle, is entirely general regarding the vacuum state and applies to any particular choice. This allows us to test several conceptual aspects of the theory (partially highlighted in the introduction), such as its resilience throughout cycles and the implications of the choice of the vacuum states. In this section, we examine both these issues in more detail. In \\autoref{Gravitational Waves From A non-Bunch Davies Vacuum}, we focus on the choice of the vacuum state of the theory, deriving novel constraints on the latter based on internal consistency through cycles and epochs. In \\autoref{analysis of gravitational waves}, we quantify the extent to which including the contribution from tensor modes originating during the dark energy phase of the previous cycle changes the spectrum of tensor perturbations observed in the current cycle.\n\n\\subsection{Gravitational waves from a non-Bunch Davies vacuum}\\label{Gravitational Waves From A non-Bunch Davies Vacuum}\n\nAs often speculated both in quantum field theory and effective field theory, the choice of the vacuum state represents an important topic of discussion and significance as it plays a crucial role in determining the properties of the theory and the physical predictions it makes. On the one hand, in quantum field theory, the vacuum state is the lowest-energy state of a quantum field, which usually corresponds to the state with no physical particles. On the other hand, effective field theories often deal with specific energy scales or regimes of a more fundamental theory (such as in our effective representation of the cyclic Universe), and the choice of the vacuum state may involve integrating out high-energy degrees of freedom and focusing on the low-energy behavior. This makes the choice of the vacuum state far from trivial when considering the possibility of new physics at sufficiently high energies. \n\nThe selection of the vacuum state has been the subject of intense study and attention in the context of inflationary models where considering more exotic (though physically motivated) vacuum states other than BD) can lead to markedly different predictions for the spectrum of scalar and tensor modes, see \\textit{e.g.} ~\\cite{Ashoorioon:2013eia,Ashoorioon:2014nta,Choudhury:2023kam} and references therein. Just to mention one example among many, even sticking to the framework of single-field inflation minimally coupled to gravity, considering an exotic vacuum can result in the violation of the usual slow-roll consistency relations allowing for a blue-tilted spectrum of gravitational waves, with an enhanced amplitude on small scales that can eventually produce observable signatures visible by CMB and Gravitational Waves experiments.\n\nHowever, the same issue has not been investigated in the cyclic Universe. Previous analyses of the spectrum of the gravitational wave assume a BD vacuum for perturbations produced in the \\textit{ekpyrotic} phase, enforcing the solution to converge to that of a plane wave in flat space at sufficiently short distances~\\cite{Boyle:2003km}. As argued in Ref.~\\cite{Steinhardt:2002ih}, this choice mostly relies on the classical treatment of the dynamics of the evolving modulus field at arbitrarily small length scales. However, just like in inflation, the choice may no longer be trivial if the dynamics of the modulus field is influenced by new physics at some characteristic energy scale $M$. As a result, one may wonder whether in the cyclic Universe, the choice of the vacuum state is somehow affected by the same level of \"arbitrariness\" as inflation, or if additional constraints can be derived based on the strong interconnection between the different phases and/or from the need to maintain consistency through cycles. A different, yet related, question is whether, also in the cyclic Universe, this potential degree of arbitrariness impacts the predictions for the tensor spectrum or if they remain robust under the choice of the vacuum state.\n\nHere, we take a first step forward in the discussion and, considering a non-BD vacuum state in the dark energy phase of the previous cycle, we argue that this initial state imposes a more fundamental constraint on the cyclic model, primarily due to the backreaction effects that a non-BD state could have on the background evolution of the modulus field. \n\nTo prove this point, following Refs.~\\cite{Aravind:2013lra,Ashoorioon:2013eia,Holman:2007na}, we allow the second Bogoliubov coefficient $\\beta_k$ to be non-zero. Notice that, since the Bogoliubov coefficient is related to $D_2$ by \n\\begin{equation}\n\\beta_k = 2D_2\\sqrt{(k/\\pi)},\n\\end{equation}\nand parameterizes deviations from the BD vacuum. As the effective theory becomes invalid at energy scales beyond those of new physics, we ensure that no modes are excited past this point, therefore requiring $\\beta_k \\to 0$ for $k > Ma(\\tau_c)$. In addition, we choose $\\beta_k$ of the form\n\\begin{equation}\n    \\beta_k \\sim \\beta_0 e^{-\\frac{k^2}{M^2 a\\left(\\tau_c\\right)^2}},\n    \\label{eq:non-BD-vscuum}\n\\end{equation}\nwhere the energy scale of the effective theory is $M\\sim 10^{-4}M_{\\rm{Pl}}$ and $\\beta_0$ is a proportionality factor. The effects produced by the backreaction in the dark energy-dominated phase set stringent limits on how large the non-vanishing $\\beta_k$ can be. In particular, requiring that the energy density of the non-BD quanta does not overwhelm the energy density associated with the modulus field -- hence spoiling the model -- implies\n\\begin{equation}\n    \\rho_{non-BD}\\sim\\frac{1}{a_{i}^4}\\int{\\frac{d^3k}{(2\\pi)^3}}\\left|\\beta_0\\right|^2 k\\sim\\left(\\frac{a(\\tau_c)}{a_{i}(\\tau)}\\right)^4\\left|\\beta_0\\right|^2 M^4\\ll M_{\\rm Pl}^2 H^2,\n    \\label{eq:rho_NBD}\n\\end{equation}\nwhere $a_{i}$ refers to the value of the scale factor when we set our initial vacuum conditions, and we have used $M_{\\rm PL}^2H^2$ as the energy density associated with the background evolution. We take the cutoff time $\\tau_c$, beyond which we cannot be certain of the modulus field dynamics to be the start of radiation domination regime in the previous cycle $\\tau_{r(pc)}$.\nAs long as $(a(\\tau_{r(pc)})/a(\\tau))^4 \\ll 1 $, Eq.~\\eqref{eq:rho_NBD} sets an upper bound for the parameter $\\beta_0$:\n\\begin{equation}\n    \\beta_0 < \\frac{H M_{\\rm Pl}}{M^2}\\sim 10^{-53}\n\\end{equation}\nwhere we used that, during the dark energy phase, the Hubble parameter is approximately constant $H\\simeq H_0=100\\,h \\simeq 2.1 h \\times 10^{-42}$ GeV. Notice that this bound is independent of the cutoff time $\\tau_c$ when starting the calculation in the dark energy phase. This is because of the three expansion phases preceding this epoch and the large net expansion of the scale factor from cycle to cycle, ensuring that $(a(\\tau_c)/a(\\tau_i))^4 \\ll 1$ for any $\\tau_c$.\n\nWe emphasise once again that avoiding problems with backreaction during the dark energy-dominated phase leads to an extremely restrictive constraint on deviations away from the BD vacuum. To gain a rough idea of how restrictive our bound is, we can compare constraints on the same cutoff scale obtained in inflationary cosmology. In that case, as highlighted in Ref.~\\cite{Holman:2007na}, the restriction reads $\\beta_0 < 10^{-6}$; i.e., about 47 orders of magnitude weaker than our bound. The reason for such a large difference lies in the fact that, although inflation and dark energy share several common aspects in terms of background dynamics, these two phases span energy scales that differ by over 100 orders of magnitude in characteristic energy density. The energy scale of inflation is much higher, making it way more challenging for perturbations to overwhelm the dynamics and allowing larger freedom for deviations away from BD.\n\nTo further appreciate the strength of our constraint, it is also worth briefly discussing what happens when setting the vacuum state in a different phase of the model rather than in the dark energy-dominated one. In particular, we focus on the \\textit{ekpyrotic} phase, which, as highlighted multiple times in this work, is the phase where initial conditions are typically fixed, and the calculation of the spectrum of tensor perturbations is initiated. In this phase, the Hubble parameter drastically grows, see Eq.~\\eqref{Hratio}, while the scale factor remains approximately constant. As a consequence, the constraint on $\\beta_0$ is relaxed up to $\\beta_0 < 10^{45}$ for $M \\sim 10^{-4}M_{\\rm Pl}$, allowing us complete freedom to choose a wide range of vacuum states in the \\textit{ekpyrotic} phase without compromising the model during this stage. That being said, it is necessary to ensure that significant deviations away from a BD vacuum during the \\textit{ekpyrotic} phase do not lead to other issues during the subsequent evolutionary states of the cycle. However, given that the evolution of the dark energy phase is governed by the background dynamics of the lowest energy scale and considering the cyclic nature of the model and its resilience through cycles (meaning that starting from the dark energy phase of the previous cycle does not produce differences in observable quantities as we prove in the next subsection), we argue that selecting the vacuum during the dark energy phase is the most restrictive and conservative choice to circumvent this issue entirely. This makes relying on the BD vacuum a safer assumption from a model-building perspective.\n\n\n\\subsection{Gravitational waves produced from different phases}\\label{analysis of gravitational waves}\n\nOur general calculation for the strain spectrum of gravitational waves enables us to investigate how the predictions change when incorporating the contribution from tensor modes originating in the dark energy phase of the previous cycle. We can then test the robustness of these predictions by comparing our results with those already documented in the literature, derived from starting in the \\textit{ekpyrotic} contracting phase.\n\nIn this section, we compare the predictions for the strain spectrum of gravitational waves obtained in the following two cases:\n\\begin{itemize}\n\\item[\\textit{(a)}] Using our general calculation and starting in the dark energy phase of the previous cycle.  In this case we impose BD vacuum conditions on the coefficients of the dark energy stage $D_1$ and $D_2$ given by:\n\\begin{align}\\label{DE conditions}\nD_1 &= \\frac{1}{2}\\sqrt{\\frac{\\pi}{k}}, & D_2 &= 0.\n\\end{align}\n\n\\item[\\textit{(b)}] Starting the calculation in the \\textit{ekpyrotic} phase (disregarding the matching at $\\tau=\\tau_{tr}$) and imposing BD initial conditions as done in Res.~\\cite{Boyle:2003km} \n\\begin{align}\\label{ekpyrotic conditions}\n    A_1 &= \\frac{1}{2}\\sqrt{\\frac{\\pi}{k}}, & A_2 &= 0.\n\\end{align}\n\\end{itemize}\nNotice that, although our calculation is fully generic concerning the choice of the vacuum state, as we proved in the previous subsection, deviations away from the BD vacuum state in the dark energy phase are strongly constrained, providing us with a valid physical reason to impose this vacuum state in the dark energy phase for the case \\textit{(a)}. Instead, for the case \\textit{(b)}, we impose the BD initial conditions in the \\textit{ekpyrotic} phase to work in the same framework as Ref.~\\cite{Boyle:2003km} and allow direct comparison.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.7\\columnwidth]{Fig1.png}\n    \\caption{Strain spectrum plotted against comoving frequency $k$, of gravitational waves produced starting the cycle with the dark energy phase in blue, and with the \\textit{ekpyrotic} phase in orange. BD initial conditions are assumed for both spectra. $k_0, k_{eq}, k_r$ and $k_{end}$ are the comoving frequencies on the horizon today, at matter-radiation equivalence, at the start of radiation domination, and at the \\textit{ekpyrotic}-kinetic transition respectively.}\n    \\label{fig:Strain spectra}\n\\end{figure}\n\n\\begin{figure}[htb!]\n    \\centering\n    \\includegraphics[width=0.75\\textwidth]{Fig2.png}\n    \\caption{Illustrative plot of the comoving Hubble horizon, $1/aH$, throughout the \\textit{ekpyrotic}, kinetic contracting, kinetic expanding, radiation domination, and finally matter domination phases in magenta, orange, dark blue, green and cyan respectively. Key modes on the horizon are illustrated as horizontal dashed lines, along with a label of their comoving wavenumber, and some less important modes are labelled.}\n    \\label{fig:horizon plot}\n\\end{figure}\n\nAfter matching the relevant phases, the strain spectra of tensor modes predicted in the two cases can be derived using Eq.~\\eqref{strain_spectrum} and Eq.~\\eqref{transfer function} and are shown in \\autoref{fig:Strain spectra} in blue for the case \\textit{(a)} and in orange for the case \\textit{(b)}. As evident from the figure, a difference of up to an order of magnitude in the strain $\\Delta h$ is observed for modes $k_0$ at the present-day horizon.\n\nThis difference can be understood by considering the evolution of modes produced during the dark energy phase of the previous cycle. To further clarify this point, we refer to \\autoref{fig:horizon plot} where we show the evolution of the comoving Hubble horizon, $1/aH$, throughout the different phases of the model. From the figure we note that the horizon at the end of the dark energy phase (\\textit{i.e.} $k_{\\rm tr}^{-1}$) is several orders of magnitude greater than the present-day horizon (\\textit{i.e.} $k_0^{-1}$) ensuring that none of these super horizon modes can have re-entered by today or in any subsequent cycle.  Modes produced during this dark energy phase that exit the horizon become frozen and subsequently experience further stretching throughout this epoch.\n\nOn the other hand, sub-horizon modes produced in the same phase oscillate with decaying amplitude $h \\propto a^{-1}$. This feature can be shown by solving Eq.~\\eqref{BesselsEq} in the dark energy background dynamics described by Eq.~\\eqref{adarkenergy}. In view of that, we expect sub-horizon modes produced deep within the dark energy phase (as well as in any previous phase of the previous cycle) to decay away to negligible amplitudes when compared to modes produced at the end of the same phase or during the subsequent \\textit{ekpyrotic} phase. Instead, sub-horizon modes produced \\textit{near} the end of the dark energy phase can survive into the \\textit{ekpyrotic} phase of the next cycle, acting as extra quanta in the vacuum initial conditions for the subsequent \\textit{ekpyrotic} phase. This contribution is encoded in the coefficient $A_2$ in Eq.~\\eqref{ekpyrotic solution} and can lead to observable effects on scales corresponding to the long wavelength portion of the strain spectrum. Referring back to \\autoref{fig:Strain spectra}, we can appreciate how the differences produced by amplitude oscillations expected from an under-damped simple harmonic oscillator solution affect only the range of frequencies between $k_0$ and $k_{\\rm eq}$ before becoming frozen in during the \\textit{ekpyrotic} phase.\n\nMoving forward, as outlined in Ref.~\\cite{Erickson:2006wc}, the subsequent phases will provide a red-tilt to the scale-invariant dark energy spectrum. Notice that for an \\textit{ekpyrotic} phase lasting $N \\sim 115$ e-folds, the magnitude of the coefficient $A_2$ becomes $\\mathcal{O}(10^{10})$ between $k_0$ and $k_{\\rm eq}$ then rapidly decays to order $\\mathcal{O}(10^{-15})$ for modes around $k_{\\rm r}$. This explains the discrepancy between imposing BD initial conditions in the dark energy phase given by Eq.~\\eqref{DE conditions}, and in the \\textit{ekpyrotic} phase, given by Eq.~\\eqref{ekpyrotic conditions}. \n\nWe conclude with a last important remark: as argued in \\autoref{Gravitational Waves From A non-Bunch Davies Vacuum}, to preserve the background evolution of the modulus field, it is important to have a BD-like vacuum state during the dark energy phase. This requirement implies that perturbations existing in the current cycle must decay to negligible levels in amplitude compared to the energy density present in the BD vacuum, preventing them from acting as additional quanta on top of the vacuum. We emphasise that this condition is satisfied by the evolution of tensor perturbations after the bounce, particularly during the subsequent radiation and matter-dominated phases. In particular, the transfer function, Eq.~\\eqref{transfer function}, ensures that the amplitudes of short-wavelength modes re-entering the horizon first (that are potentially the most problematic), are decreased by a factor $\\left( k_0/k \\right)^2$. Consequently, the amplitude of the shortest wavelength modes in the observable spectrum ($k \\sim k_{\\rm end}$), is suppressed by more than $20$ orders of magnitude during the radiation and matter-dominated phases. As a result, the evolution through these phases guarantees the decaying amplitude of all modes in every cycle preceding each dark energy phase and restoring the vacuum to a BD state in the latter. This shields the dark energy phase from possible backreaction effects, underscoring the resilience of the model and demonstrating once again that, from a theoretical standpoint, fixing initial conditions in this phase is much safer from a model-building perspective.\n\n\n\\section{Conclusion}\n\\label{sec:conc}\n\nIn this study, we investigate the production of gravitational waves in a cyclic Universe, focusing on certain conceptual aspects of the theory, such as the resilience of observable predictions against the phase of the cycle in which initial conditions are set and the choice of the vacuum state.\n\nIn most of the analyses carried out in the literature, the \\textit{ekpyrotic} phase is typically assumed as the starting point of the cycle where initial conditions of primordial perturbations are imposed, and the calculations of the relative spectra begin.  While for scalar perturbations the implications of setting the initial conditions in different phases of the theory have been examined in a few studies surrounding this topic, to the best of our knowledge, the calculation of the tensor spectrum has always been performed starting in the \\textit{ekpyrotic} phase, assuming a BD vacuum state and neglecting any potential contributions arising from tensor modes originating during the dark energy phase of the previous cycle.\n\nIn light of this, a few (we believe) interesting questions remained somewhat pending. For instance, one might wonder whether setting initial conditions in the dark energy phase of the previous cycle and considering the potential contribution of tensor modes generated in this phase could lead to any observable differences in the gravitational wave strain spectrum observed in the current cycle and eventually clarify why (not). Most importantly, in effective field theory descriptions of the cyclic Universe, the choice of the vacuum state may involve integrating out high-energy degrees of freedom, focusing on low-energy behaviors of the modulus field. This makes the choice far from trivial when considering the possibility of new physics acting at sufficiently high characteristic energy scales. Consequently, one might wonder about the implications of assuming a BD vacuum state in the \\textit{ekpyrotic} phase and, more broadly, what freedom exists in the cyclic Universe regarding the choice of the vacuum state and how such freedom affects the predictions for the spectrum of gravitational waves.\n\nFuelled by these questions, we consider a cyclic Universe described by the effective 4D Lagrangian \\eqref{lagrangian} with a potential given by Eq.~\\eqref{potnetial}. After reviewing the background dynamics of the model (\\autoref{construction of the model}), we focus on the production and evolution of tensor modes. In \\autoref{primordial tensor spectrum}, we analytically solve the equation of motion of the gravitational wave field through all the different phases of the cycle by starting from the dark energy phase of the previous cycle and making no assumptions about the vacuum state. The results presented in this section generalise the predictions for the tensor spectrum to include the contribution of tensor perturbations produced during the dark energy phase of the previous cycle and apply to any choice of the vacuum state of the theory. Therefore, they extend the treatment presented so far in the literature, allowing us to gain important insights into the issues posed earlier.\n\nAs argued in \\autoref{sec:discussion}, our findings reveal a significant resilience of the cyclic Universe model concerning predictions for the spectrum of primordial gravitational waves, with the most relevant results reading as follows.\n\\begin{itemize}\n\\item \\textit{Initial Conditions:} To quantify the impact of the contribution arising from tensor modes produced in the previous dark energy phase of the cycle, in \\autoref{analysis of gravitational waves} we compared the spectra obtained by starting from the \\textit{ekpyrotic} contracting phase (in orange in \\autoref{fig:Strain spectra}) and the dark energy phase of the previous cycle (in blue in \\autoref{fig:Strain spectra}), assuming in both cases a BD vacuum. The two spectra are essentially identical except for a difference up to an order of magnitude in the strain $\\Delta h$ for modes on the scale $k_0$ at the present-day horizon. The reason is that the horizon at the end of the dark energy phase is several orders of magnitude greater than the present-day horizon (see also \\autoref{fig:horizon plot}), implying that none of these super-horizon modes can have re-entered by today or in any subsequent cycle. On the other hand, sub-horizon modes produced in the same phase oscillate with decaying amplitude $h \\propto a^{-1}$. Therefore, sub-horizon modes produced deep within the dark energy phase (as well as in any previous phase of the previous cycle) decay away to negligible amplitudes, while sub-horizon modes produced near the end of the dark energy phase can survive into the \\textit{ekpyrotic} phase of the next cycle, being responsible for the small differences in the strain spectrum observed around $k_0$.\n\n\\item \\textit{Vacuum state:} Although including tensor modes originating from the previous dark energy phase does not lead to significant differences in the spectrum of primordial perturbations, we argue that starting the calculation in this phase imposes a more fundamental constraint on the choice of the vacuum state of the model. To highlight this aspect, in \\autoref{Gravitational Waves From A non-Bunch Davies Vacuum} we consider a non-BD vacuum state in the dark energy phase parametrized in terms of the second Bogoliubov coefficient given by Eq.~\\eqref{eq:non-BD-vscuum}. We show that requiring the energy density of the non-BD quanta not to overwhelm the energy density associated with the modulus field -- hence, potentially spoiling the model -- implies an extremely restrictive constraint on deviations away from the BD vacuum. On the other hand, for the same non-BD vacuum state, the constraint is completely lost in the \\textit{ekpyrotic} phase where we are left with complete freedom to choose a wide range of vacuum states without compromising the model during this stage (but possibly spoiling the subsequent evolution). As a result, selecting the vacuum during the dark energy phase (where we are basically forced to BD) seems to be the most restrictive and conservative choice, as well as a safer assumption from a model-building perspective. Given the cyclic nature of the model and the resilience of its predictions against the phase where we set initial conditions (as proven in the previous point), this strongly reduces our freedom to consider exotic vacuum states in the cyclic Universe.\n\\end{itemize}\n\n\\begin{acknowledgments}\nWG and CvdB are supported by the Lancaster–Sheffield Consortium for Fundamental Physics under STFC grant: ST/X000621/1. This article is based upon work from COST Action CA21136 Addressing observational tensions in cosmology with systematics and fundamental physics (CosmoVerse) supported by COST (European Cooperation in Science and Technology). \n\\end{acknowledgments}\n\n\\bibliographystyle{JHEP}\n\n\n\\end{document}"}
{"paper_id": "2403-00534", "version": "2403-00534v1", "root_file_path": "d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\data_raw\\2403-00534\\tex\\2403-00534v1\\1rdm.tex", "metadata": {"total_length": 59864, "merged_count": 2, "merged_files": ["1rdm.tex", "pretex.tex"], "missing_files": []}, "content": "\\documentclass[aps,twocolumn,floatfix,superscriptaddress,longbibliography]{revtex4-2}\n\\usepackage[utf8]{inputenc}\n\\usepackage{times}\n\\usepackage{tikz}\n\\usepackage[T1]{fontenc}\n\\renewcommand{\\baselinestretch}{1.0}\n\n\\usepackage{bm}\n\\usepackage{times}\n\n\n% <BEGIN_FILE: pretex.tex>\n \\usepackage{algorithm,algorithmic}\n\n\n\n\n \n\n \n \n\\usepackage{mathtools}\n\\usepackage{amsmath}\n\\usepackage[shortlabels]{enumitem}\n\n\n\\usepackage{graphicx,epic,eepic,epsfig,amsmath,latexsym,amssymb,verbatim,color}\n \n\n\n\\usepackage{bbm}\n \n\n\\usepackage{float}\n\n\\usepackage{tikz}\n\\usetikzlibrary{chains}\n\\usetikzlibrary{fit}\n\\usepackage{pgflibraryarrows}\t\t\n\\usepackage{pgflibrarysnakes}\t\t\n\n\\usepackage{epsfig}\n\\usetikzlibrary{shapes.symbols,patterns} \n\\usepackage{pgfplots}\n\n\\usepackage[strict]{changepage}\n\\usepackage{hyperref}\n\\hypersetup{colorlinks=true,citecolor=blue,linkcolor=blue,filecolor=blue,urlcolor=blue,breaklinks=true}\n\n\\usepackage[marginal]{footmisc}\n\\usepackage{url}\n\\usepackage{theorem}\n\n\n\n \n\n\n\n\n\n\n\n\\newtheorem{definition}{Definition}\n\\newtheorem{proposition}{Proposition}\n\\newtheorem{lemma}[proposition]{Lemma}\n\n\\newtheorem{fact}{Fact}\n\\newtheorem{theorem}[proposition]{Theorem}\n\n\\newtheorem{corollary}[proposition]{Corollary}\n\\newtheorem{conjecture}{Conjecture}\n\n \n\n\\def\\squareforqed{\\hbox{\\rlap{$\\sqcap$}$\\sqcup$}}\n\\def\\qed{\\ifmmode\\squareforqed\\else{\\unskip\\nobreak\\hfil\n\\penalty50\\hskip1em\\null\\nobreak\\hfil\\squareforqed\n\\parfillskip=0pt\\finalhyphendemerits=0\\endgraf}\\fi}\n\\def\\endenv{\\ifmmode\\;\\else{\\unskip\\nobreak\\hfil\n\\penalty50\\hskip1em\\null\\nobreak\\hfil\\;\n\\parfillskip=0pt\\finalhyphendemerits=0\\endgraf}\\fi}\n\\newenvironment{proof}{\\noindent \\textbf{{Proof~} }}{\\hfill $\\blacksquare$}\n\n\n\n\\newcounter{remark}\n\\newenvironment{remark}[1][]{\\refstepcounter{remark}\\par\\medskip\\noindent\n\\textbf{Remark~\\theremark #1} }{\\medskip}\n\n\n\n\\newcounter{example}\n\\newenvironment{example}[1][]{\\refstepcounter{example}\\par\\medskip\\noindent\n\\textbf{Example~\\theexample #1} }{\\medskip}\n\n\\newcommand{\\exampleTitle}[1]{\\textbf{(#1)}}\n\\newcommand{\\remarkTitle}[1]{\\textbf{(#1)}}\n\\newcommand{\\proofComment}[1]{\\exampleTitle{#1}}\n\n\\newenvironment{beweis}[1]{\\noindent\\textbf{Proof~{#1}~} }{\\qed}\n\n\n\\mathchardef\\ordinarycolon\\mathcode`\\:\n\\mathcode`\\:=\\string\"8000\n\\def\\vcentcolon{\\mathrel{\\mathop\\ordinarycolon}}\n\\begingroup \\catcode`\\:=\\active\n  \\lowercase{\\endgroup\n  \\let :\\vcentcolon\n  }\n\n\n\\usepackage{cleveref}\n\\usepackage{graphicx}\n\\usepackage{xcolor}\n\n\\RequirePackage[framemethod=default]{mdframed}\n\\newmdenv[skipabove=7pt,\nskipbelow=7pt,\nbackgroundcolor=darkblue!15,\ninnerleftmargin=5pt,\ninnerrightmargin=5pt,\ninnertopmargin=5pt,\nleftmargin=0cm,\nrightmargin=0cm,\ninnerbottommargin=5pt,\nlinewidth=1pt]{tBox}\n\n\n\\newmdenv[skipabove=7pt,\nskipbelow=7pt,\nbackgroundcolor=blue2!25,\ninnerleftmargin=5pt,\ninnerrightmargin=5pt,\ninnertopmargin=5pt,\nleftmargin=0cm,\nrightmargin=0cm,\ninnerbottommargin=5pt,\nlinewidth=1pt]{dBox}\n\\newmdenv[skipabove=7pt,\nskipbelow=7pt,\nbackgroundcolor=darkkblue!15,\ninnerleftmargin=5pt,\ninnerrightmargin=5pt,\ninnertopmargin=5pt,\nleftmargin=0cm,\nrightmargin=0cm,\ninnerbottommargin=5pt,\nlinewidth=1pt]{sBox}\n\\definecolor{darkblue}{RGB}{0,76,156}\n\\definecolor{darkkblue}{RGB}{0,0,153}\n\\definecolor{blue2}{RGB}{102,178,255}\n\\definecolor{darkred}{RGB}{195,0,0}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\newcommand{\\nc}{\\newcommand}\n\\nc{\\rnc}{\\renewcommand}\n\\nc{\\beg}{\\begin{equation}}\n\\nc{\\eeq}{{\\end{equation}}}\n\\nc{\\beqa}{\\begin{eqnarray}}\n\\nc{\\eeqa}{\\end{eqnarray}}\n\\nc{\\lbar}[1]{\\overline{#1}}\n\\nc{\\bra}[1]{\\langle#1|}\n\\nc{\\ket}[1]{|#1\\rangle}\n\\nc{\\ketbra}[2]{|#1\\rangle\\!\\langle#2|}\n\\nc{\\braket}[2]{\\langle#1|#2\\rangle}\n\\newcommand{\\braandket}[3]{\\langle #1|#2|#3\\rangle}\n\n\\nc{\\proj}[1]{| #1\\rangle\\!\\langle #1 |}\n\\nc{\\avg}[1]{\\langle#1\\rangle}\n\n\\nc{\\rank}{\\operatorname{Rank}}\n\\nc{\\smfrac}[2]{\\mbox{$\\frac{#1}{#2}$}}\n\\nc{\\tr}{\\operatorname{Tr}}\n\\nc{\\ox}{\\otimes}\n\\nc{\\dg}{\\dagger}\n\\nc{\\dn}{\\downarrow}\n\\nc{\\cA}{{\\cal A}}\n\\nc{\\cB}{{\\cal B}}\n\\nc{\\cC}{{\\cal C}}\n\\nc{\\cD}{{\\cal D}}\n\\nc{\\cE}{{\\cal E}}\n\\nc{\\cF}{{\\cal F}}\n\\nc{\\cG}{{\\cal G}}\n\\nc{\\cH}{{\\cal H}}\n\\nc{\\cI}{{\\cal I}}\n\\nc{\\cJ}{{\\cal J}}\n\\nc{\\cK}{{\\cal K}}\n\\nc{\\cL}{{\\cal L}}\n\\nc{\\cM}{{\\cal M}}\n\\nc{\\cN}{{\\cal N}}\n\\nc{\\cO}{{\\cal O}}\n\\nc{\\cP}{{\\cal P}}\n\\nc{\\cQ}{{\\cal Q}}\n\\nc{\\cR}{{\\cal R}}\n\\nc{\\cS}{{\\cal S}}\n\\nc{\\cT}{{\\cal T}}\n\\nc{\\cV}{{\\cal V}}\n\\nc{\\cX}{{\\cal X}}\n\\nc{\\cY}{{\\cal Y}}\n\\nc{\\cZ}{{\\cal Z}}\n\\nc{\\cW}{{\\cal W}}\n\\nc{\\csupp}{{\\operatorname{csupp}}}\n\\nc{\\qsupp}{{\\operatorname{qsupp}}}\n\\nc{\\var}{{\\operatorname{var}}}\n\\nc{\\rar}{\\rightarrow}\n\\nc{\\lrar}{\\longrightarrow}\n\\nc{\\polylog}{{\\operatorname{polylog}}}\n\n\\nc{\\wt}{{\\operatorname{wt}}}\n\\nc{\\av}[1]{{\\left\\langle {#1} \\right\\rangle}}\n\\nc{\\supp}{{\\operatorname{supp}}}\n\n\\nc{\\argmin}{{\\operatorname{argmin}}}\n\n\n\\def\\a{\\alpha}\n\\def\\b{\\beta}\n\\def\\di{\\diamondsuit}\n\\def\\d{\\delta}\n\\def\\e{\\epsilon}\n\\def\\ve{\\varepsilon}\n\\def\\z{\\zeta}\n\\def\\h{\\eta}\n\\def\\t{\\theta}\n\\def\\i{\\mathbf{i}}\n\\def\\k{\\kappa}\n\\def\\ll{\\lambda}\n\\def\\m{\\mu}\n\\def\\n{\\nu}\n\\def\\x{\\xi}\n\\def\\p{\\pi}\n\\def\\r{\\rho}\n\\def\\s{\\sigma}\n\\def\\ta{\\tau}\n\\def\\u{\\upsilon}\n\\def\\ph{\\varphi}\n\\def\\c{\\chi}\n\\def\\ps{\\psi}\n\\def\\o{\\omega}\n\n\\def\\G{\\Gamma}\n\\def\\D{\\Delta}\n\\def\\T{\\Theta}\n\\def\\L{\\Lambda}\n\\def\\X{\\Xi}\n\\def\\P{\\Pi}\n\\def\\S{\\Sigma}\n\\def\\U{\\Upsilon}\n\\def\\Ph{\\Phi}\n\\def\\Ps{\\Psi}\n\\def\\O{\\Omega}\n\\def\\w{\\omega}\n\n\\nc{\\RR}{{{\\mathbb R}}}\n\\nc{\\CC}{{{\\mathbb C}}}\n\\nc{\\FF}{{{\\mathbb F}}}\n\\nc{\\NN}{{{\\mathbb N}}}\n\\nc{\\ZZ}{{{\\mathbb Z}}}\n\\nc{\\PP}{{{\\mathbb P}}}\n\\nc{\\QQ}{{{\\mathbb Q}}}\n\\nc{\\UU}{{{\\mathbb U}}}\n\\nc{\\EE}{{{\\mathbb E}}}\n\\nc{\\id}{{\\operatorname{id}}}\n\\newcommand{\\cvartheta}{{\\widetilde\\vartheta}}\n\\nc{\\CHSH}{{\\operatorname{CHSH}}}\n\n\\newcommand{\\Op}{\\operatorname}\n\n\\nc{\\be}{\\begin{equation}}\n\\nc{\\ee}{{\\end{equation}}}\n\\nc{\\bea}{\\begin{eqnarray}}\n\\nc{\\eea}{\\end{eqnarray}}\n\\nc{\\<}{\\langle}\n\\rnc{\\>}{\\rangle}\n\n\\nc{\\rU}{\\mbox{U}}\n\\def\\lpmm{ \\left(\\rule{0pt}{1.8ex}\\right. \\! }\n\\def\\rpmm{ \\! \\left.\\rule{0pt}{1.8ex}\\right) }\n\\def\\lbm{ \\left[\\rule{0pt}{2.1ex}\\right. }\n\\def\\rbm{ \\left.\\rule{0pt}{2.1ex}\\right] }\n\\def\\lpm{ \\left(\\rule{0pt}{2.1ex}\\right. \\!}\n\\def\\rpm{ \\!\\left.\\rule{0pt}{2.1ex}\\right) }\n\\def\\lbL{ \\left[\\rule{0pt}{2.4ex}\\right. \\!}\n\\def\\rbL{ \\!\\left.\\rule{0pt}{2.4ex}\\right] }\n\\def\\lpL{ \\left(\\rule{0pt}{2.4ex}\\right.\\!\\!}\n\\def\\rpL{ \\!\\! \\left.\\rule{0pt}{2.4ex}\\right)}\n\\def\\lpH{ \\left(\\rule{0pt}{3.0ex}\\right.\\!\\!}\n\\def\\rpH{ \\!\\! \\left.\\rule{0pt}{3.0ex}\\right)}\n\\def\\llH{ \\left|\\rule{0pt}{3.0ex}\\right.\\!}\n\\def\\rlH{ \\!\\left.\\rule{0pt}{3.0ex}\\right|}\n\\def\\llL{ \\left|\\rule{0pt}{2.4ex}\\right.\\!}\n\\def\\rlL{ \\!\\left.\\rule{0pt}{2.4ex}\\right|}\n\\def\\llm{ \\left|\\rule{0pt}{2.1ex}\\right.\\!}\n\\def\\rlm{ \\!\\left.\\rule{0pt}{2.1ex}\\right|}\n\n\n\\nc{\\ob}[1]{#1}\n\n\\nc{\\SEP}{{\\text{\\rm SEP}}}\n\\nc{\\NS}{{\\text{\\rm NS}}}\n\\nc{\\LOCC}{{\\text{\\rm LOCC}}}\n\\nc{\\PPT}{{\\text{\\rm PPT}}}\n\\nc{\\EXT}{{\\text{\\rm EXT}}}\n\n\\nc{\\Sym}{{\\operatorname{Sym}}}\n\n\n\n\\nc{\\ERLO}{{E_{\\text{r,LO}}}}\n\\nc{\\ERLOCC}{{E_{\\text{r,LOCC}}}}\n\\nc{\\ERPPT}{{E_{\\text{r,PPT}}}}\n\\nc{\\ERLOCCinfty}{{E^{\\infty}_{\\text{r,LOCC}}}}\n\\nc{\\Aram}{{\\operatorname{\\sf A}}}\n\\newtheorem{problem}{Problem}\n\\newtheorem{note}[problem]{Note}\n\n\\newcommand{\\Choi}{Choi-Jamio\\l{}kowski }\n\\newcommand{\\eps}{\\varepsilon}\n\\newcommand{\\CPTP}{\\text{\\rm CPTP}}\n\n\n\\usepackage{tikz}\n\n\\usepackage{hyperref}\n\\hypersetup{colorlinks=true,citecolor=blue,linkcolor=blue,filecolor=blue,urlcolor=blue,breaklinks=true}\n\n\n\\makeatletter\n\\def\\grd@save@target#1{\n  \\def\\grd@target{#1}}\n\\def\\grd@save@start#1{\n  \\def\\grd@start{#1}}\n\\tikzset{\n  grid with coordinates/.style={\n    to path={\n      \\pgfextra{\n        \\edef\\grd@@target{(\\tikztotarget)}\n        \\tikz@scan@one@point\\grd@save@target\\grd@@target\\relax\n        \\edef\\grd@@start{(\\tikztostart)}\n        \\tikz@scan@one@point\\grd@save@start\\grd@@start\\relax\n        \\draw[minor help lines,magenta] (\\tikztostart) grid (\\tikztotarget);\n        \\draw[major help lines] (\\tikztostart) grid (\\tikztotarget);\n        \\grd@start\n        \\pgfmathsetmacro{\\grd@xa}{\\the\\pgf@x/1cm}\n        \\pgfmathsetmacro{\\grd@ya}{\\the\\pgf@y/1cm}\n        \\grd@target\n        \\pgfmathsetmacro{\\grd@xb}{\\the\\pgf@x/1cm}\n        \\pgfmathsetmacro{\\grd@yb}{\\the\\pgf@y/1cm}\n        \\pgfmathsetmacro{\\grd@xc}{\\grd@xa + \\pgfkeysvalueof{/tikz/grid with coordinates/major step}}\n        \\pgfmathsetmacro{\\grd@yc}{\\grd@ya + \\pgfkeysvalueof{/tikz/grid with coordinates/major step}}\n        \\foreach \\x in {\\grd@xa,\\grd@xc,...,\\grd@xb}\n        \\node[anchor=north] at (\\x,\\grd@ya) {\\pgfmathprintnumber{\\x}};\n        \\foreach \\y in {\\grd@ya,\\grd@yc,...,\\grd@yb}\n        \\node[anchor=east] at (\\grd@xa,\\y) {\\pgfmathprintnumber{\\y}};\n      }\n    }\n  },\n  minor help lines/.style={\n    help lines,\n    step=\\pgfkeysvalueof{/tikz/grid with coordinates/minor step}\n  },\n  major help lines/.style={\n    help lines,\n    line width=\\pgfkeysvalueof{/tikz/grid with coordinates/major line width},\n    step=\\pgfkeysvalueof{/tikz/grid with coordinates/major step}\n  },\n  grid with coordinates/.cd,\n  minor step/.initial=.2,\n  major step/.initial=1,\n  major line width/.initial=2pt,\n}\n\\makeatother\n\n\n\n\\usepackage{thmtools}\n\\usepackage{thm-restate}\n\\usepackage{etoolbox}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\makeatletter\n\\def\\problem@s{}\n\\newcounter{problems@cnt}\n\\declaretheorem[name=\\textcolor{darkred}{Problem}]{thmproblem}\n\\newenvironment{problems}{\n  \\stepcounter{problems@cnt}\n  \n  \n  \\begingroup\\edef\\x{\\endgroup\n    \\noexpand\\restatable{thmproblem}{problem\\Alph{problems@cnt}}\n  }\\x\n}{\\endrestatable\n  \n  \n  \\xappto\\problem@s{\n    \\expandafter\\noexpand\\csname problem\\Alph{problems@cnt}\\endcsname*\n  }\n}\n\\newcommand{\\allproblems}{\\problem@s}\n\\makeatother\n\n% <END_FILE: pretex.tex>\n\n\\definecolor{beamer}{rgb}{0.2,0.2,0.7}\n\\definecolor{colorone}{rgb}{1,0.36,0.03}\n\\definecolor{colortwo}{rgb}{0.4,0.77,0.17}\n\\definecolor{colorthree}{rgb}{0.01,0.51,0.93}\n\\definecolor{colorfour}{rgb}{0.47,0.26,0.58}\n\\definecolor{colorfive}{rgb}{0.12,0.55,0.16}\n\\usepackage{tcolorbox}\n\\usepackage{relsize}\n\\usepackage{graphicx}\n\\usepackage{subfigure}\n\\usepackage{booktabs}\n\\usepackage{array}\n\n\n\\newcommand{\\angs}{$\\mathring{A}$}\n\\newcommand{\\invangs}{$\\mathring{A}^{-1}$}\n\n\\allowdisplaybreaks\n\n\\begin{document}\n\n\\title{N-representable one-electron reduced density matrix reconstruction with frozen core electrons}\n \n\\author{Sizhuo Yu}\n\\email{sizhuo.yu@centralesupelec.fr}\n\\affiliation{Université Paris-Saclay, CentraleSupélec, CNRS, Laboratoire SPMS, F 91190 Gif-sur-Yvette, France}\n\n\\author{Jean-Michel Gillet}\n\\affiliation{Université Paris-Saclay, CentraleSupélec, CNRS, Laboratoire SPMS, F 91190 Gif-sur-Yvette, France}\n\n\\begin{abstract}\nRecent advances in quantum crystallography have shown that, beyond conventional charge density refinement, a one-electron reduced density matrix (1-RDM) satisfying N-representability conditions can be reconstructed using jointly experimental X-ray structure factors (XSF) and directional Compton profiles (DCP) through semi-definite programming.\nSo far, such reconstruction methods for  1-RDM, not constrained to idempotency, had been tested only on a toy model system (CO$_2$).\nIn this work, a new method is assessed on crystalline urea (CO(NH$_2$)$_2$) using static (0 K) and dynamic (50 K) artificial-experimental data.\nAn improved model, including symmetry constraints and frozen-core electron contribution, is introduced to better handle the increasing system complexity. \nReconstructed 1-RDMs, deformation densities and DCP anisotropy are analyzed, and it is demonstrated that the changes in the model significantly improve the reconstruction's quality against insufficient information and data corruption.\nThe robustness of the model and the strategy are thus shown to be well-adapted to address the reconstruction problem from actual experimental scattering data.\n\\end{abstract}\n\n\\date{\\today}\n\n\\maketitle\n\n\n\n\n\\section{Introduction}\\label{sec-intro}\nWhile N-electron wave-functions provide the most complete and exact description of electronic structure in crystals, their experimental determination is still out of reach due to their exponentially large complexity for real systems. \nMoreover, in Coulson's words: \"a conventional many-electron wave-function tells us more than we need to know.\"\\cite{coulsonPresentStateMolecular1960} \nIt is then worth considering the one(two)-electron reduced density matrices (1,2-RDM) as compact substitutes for wave-functions since they involve significantly fewer parameters. As of today, the incompleteness of N-representability conditions\\cite{Liu2007}, which ensure that a reduced density matrix can be associated with a complete N-body density matrix, and the lack of experimental observables with sufficient information content still pose daunting obstacles to the reconstruction of 2-RDMs.\nTherefore, 1-RDMs, which do not suffer from the same impediment and still contain valuable quantum mechanical information, are considered suitable candidates for modelling electron behaviour from experimental data. \nThe reconstruction process, however, remains a challenging task. \nFirstly, N-representability conditions still need to be fulfilled for an experimentally reconstructed 1-RDM to be physically meaningful. Secondly, from a pure measurement perspective, as the 1-RDM contains both position and momentum space information, it cannot be obtained using a single experimental technique to this day and to the best of our knowledge.\n\nThe challenge of 1-RDM reconstruction from experimental data was initiated by Clinton and coworkers in the 1960s using a drastic idempotency condition as a means to ensure N-representability \n \\cite{Clinton1969_1, Clinton1969_2, Clinton1969_3, Clinton1969_4, Clinton1969_5}.\nBased on a series of works combining position and momentum space data on isolated atoms, Schmider and coworkers \\cite{schmiderReconstructionOneParticle1992} argued that the idempotency condition would hinder the recovery of electron (static and dynamical) correlation effect in the reconstructed density matrix. \nThe potential presence of such information in position space was recently confirmed by an X-ray constrained wave-function refinement on urea and alanine \\cite{hupfEffectsExperimentallyObtained2023}. \nThe authors argue that evidence of significant deviation from the Hartree-Fock description can be found using high-resolution X-ray diffraction structure factors. Any single-determinant-based model would forbid access to such subtle features in the data. \nAdopting a formal perspective, Mazziotti and coworkers discussed \\cite{mazziotti2007reduced} different strategies to include N-representability conditions in a series of articles and proposed a semidefinite programming (SDP) formulation of the  1,2-RDM reconstruction problem \\cite{foleyMeasurementdrivenReconstructionManyparticle2012}. On more practical grounds,\nfollowing Schmider and coworkers' seminal work,  several papers reported the joint use of X-ray diffraction structure factors (SF) and directional Compton profiles (DCP) to explore different non-single-determinant models and strategies for 1-RDM modelling in both magnetic and non-magnetic molecular compounds \\cite{schmiderAtomicOrbitalsCompton1993,schmiderInferenceOneParticleDensity1993,schwarzDensityMatricesPosition1994b,schmiderLowMomentumElectrons1996,gueddidaDevelopmentJointRefinement2018,gueddidaJointRefinementModel2018,debruyneInferringOneelectronReduced2020,launayNRepresentableOneelectronReduced2021}. However, all SDP-based reconstruction attempts of 1-RDM, which have been put forward, were applied to isolated atoms or molecules with at most 2 or 3 atoms.\n\nThe present work further investigates the 1-RDM reconstruction problem in molecular crystals by building upon the convex optimization approach put forward in \\cite{debruyneInferringOneelectronReduced2020,launayNRepresentableOneelectronReduced2021}, scaling up the system size from modest dry ice (CO$_2$) to the more realistic urea (CO(NH$_2$)$_2$) crystal. The purpose is thus to demonstrate the potential of an improved method more suitable to practical applications and its aptness to compensate for sparse momentum space data. To address the challenges posed by a significant increase in system size, we propose the implementation of symmetry constraints and the possibility of freezing core-electron contributions. For the first time, approximate energy and virial ratio are used to determine the optimal data set for the 1-RDM model refinement.\n\nThis article is structured as follows: In Sec. 2, we explain how the 1-RDM reconstruction can be formulated as a convex optimization problem, with the N-representability condition, symmetry and frozen core electrons as convex constraints. The method used for reconstruction, deconvoluted from thermal motion, is briefly reviewed. In Sec. 3, we showcase the importance of the joint use of position and momentum space data even when Compton scattering data is suspected to be poorly informative. Additional degradation due to noise and temperature effects and the improved robustness using symmetry and frozen core constraints are illustrated. The conclusion and future directions are given in the last section.\n\n\n\\section{Methods}\n\\subsection{1-RDM reconstruction using least-square fitting}\n\nFor a spin-traced(spin-free) pure-state N-electron system, the 1-RDM can be derived by integrating out the $N-1$ coordinates of the N-electron density matrix, i.e.\n\\begin{equation}\n    \\Gamma^{(1)}(\\mathbf{r}, \\mathbf{r}') =  N \\int \\psi(\\mathbf{r}, \\mathbf{r_2}, ... \\mathbf{r_N} )\\psi^* (\\mathbf{r}', \\mathbf{r}_2, ..., \\mathbf{r_N}) d\\mathbf{r}_2 ... d\\mathbf{r}_N.\n    \\label{eq_pure1rdm}\n\\end{equation}\nwhere $\\psi(\\mathbf{r}, \\mathbf{r_2}, ... \\mathbf{r_N} )$ is the pure-state N-electron wavefunction. A mixed-state system 1-RDM is a mere convex combination of pure-state 1-RDMs. \n\nIt is well-known \\cite{lowdinQuantumTheoryManyParticle1955a} that the 1-RDM can be conveniently approximated using a discrete one-electron basis set $\\{\\phi_i\\}$ as\n\\begin{equation}\n    \\Gamma^{(1)}(\\mathbf{r}, \\mathbf{r}') = \\sum_{ij} P_{ij} \\phi_i(\\mathbf{r}) \\phi^*_j (\\mathbf{r}').\n    \\label{eq_1rdmP}\n\\end{equation}\nIf the basis set is kept fixed, the 1-RDM is determined once the population matrix $\\mathbf{P}$ in \\eqref{eq_1rdmP} is found. The number of parameters in the model is thus solely conditioned by the size of the population matrix and, therefore, by the number of basis functions. In this work, the basis functions are atomic orbitals, but plane waves could also be considered, if needed, for strongly delocalised electron systems.\n\nThe 1-RDM is directly connected to the  mean electron density distribution in position space through its  diagonal elements \n\\begin{equation}\\label{1RDM2density}\n \\rho(\\mathbf{r}) = \\Gamma^{(1)}(\\mathbf{r}, \\mathbf{r}).  \n\\end{equation}\nFurthermore, the 1-RDM encapsulates momentum space information through a 6D Fourier-Dirac transform \\cite{Weyrich1996} \n\\begin{equation}\n    n(\\mathbf{p}) = \\frac{1 }{ (2\\pi\\hbar)^3}\\int\\Gamma^{(1)}(\\mathbf{r}, \\mathbf{r}+\\mathbf{t})e^{-i\\mathbf{p}\\cdot\\mathbf{t}/\\hbar} d^3t d^3r,\n\\end{equation} \nwith $n(\\mathbf{p})$ being the momentum density.\nThis double connection to both axes of phase space strongly suggests there is little hope of reconstructing a good quality 1-RDM from data provided by a single experimental technique.\n\nThanks to very efficient refinement methods and models \\cite{gattiModernChargeDensityAnalysis2012a}, high-resolution X-ray structure factors (SF), which are obtained by elastic coherent X-ray diffraction, are almost routinely used in the reconstruction of position space electron density. Using \\eqref{1RDM2density}, the relationship between SF and 1-RDM is simply\n\\begin{equation}\n    F(\\mathbf{q}) = \\int \\Gamma^{(1)}(\\mathbf{r}, \\mathbf{r}) \\exp (-i \\mathbf{r} \\cdot \\mathbf{q}) d \\mathbf{r}\n    \\label{eq_sfraw}.\n\\end{equation}\nOn the other hand, directional Compton profiles (DCP) are measured by deep inelastic incoherent X-ray scattering. Within the Impulse Approximation \\cite{phillipsXRayDeterminationElectron1968}, they give access to projections of momentum space electron density, i.e.\n\\begin{align}\n J(q,\\mathbf{u})& = \\int n(\\mathbf{p}) \\delta(\\mathbf{p} \\cdot \\mathbf{u} - q) d \\mathbf{p}\\\\ &=\\frac{1 }{ 2\\pi\\hbar}\\int\\Gamma^{(1)}(\\mathbf{r}, \\mathbf{r}+t \\mathbf{u})e^{-iqt/\\hbar} dt d^3r,\n    \\label{eq_dcpraw}\n\\end{align}\nwhere $\\mathbf{u}$ is the unit vector giving the direction in momentum space onto which the electron density is projected. It is colinear with the scattering vector of the Compton measurement.\n\nThe model used in this work is based on expression \\eqref{eq_1rdmP}. The determination of the best population matrix given a set of SF and DCP thus requires expressing experimental observable values as functions of matrix $\\mathbf{P}$ using the operator form\n\\begin{equation}\nF(\\mathbf{q}) = \\text{Tr} (\\mathbf{F}_\\mathbf{q} \\mathbf{P})~~\\text{and}~~\nJ(\\mathbf{q}) = \\text{Tr} (\\mathbf{J}_\\mathbf{q} \\mathbf{P}),\n\\label{eq_FJops}\n\\end{equation}\nwith $\\mathbf{F}_\\mathbf{q}$ and $\\mathbf{J}_\\mathbf{q}$ being the SF and DCP operators respectively. For conciseness, $\\mathbf{q}$ stands for $(q,\\mathbf{u})$ in the Compton profile matrix element of \\eqref{eq_FJops}. To proceed any further, the operators $\\mathbf{F}_\\mathbf{q}$ and $ \\mathbf{J}_\\mathbf{q}$  matrix elements need to be written in the basis-set representation as\n\\begin{align}\n\\begin{split}\n(\\mathbf{F}_\\mathbf{q})_{ij} = \\int \\phi_i^*(\\mathbf{r})\\phi_j(\\mathbf{r})e^{-i\\mathbf{q}\\cdot\\mathbf{r}} d^3r,\\\\\n(\\mathbf{J}_\\mathbf{q})_{ij} = \\frac{1}{2\\pi \\hbar}\\int \\phi_i^*(\\mathbf{r})\\phi_j(\\mathbf{r}+t \\mathbf{u})e^{-iqt} dtd^3r.\n\\end{split}\n\\label{eq_FJelt}\n\\end{align}\n\nIn this work, particular attention has been paid to the reliability of the final 1-RDM reconstruction. If one assumes that error bars on data points are uncorrelated and follow a normal distribution law, for an unbiased model, the most probable population matrix $\\mathbf{P}$ is found by solving the minimization problem.\n\\begin{equation}\n    \\text{argmin}_{\\mathbf{P}} ~ \\sum_i \\left (\\frac{ \\text{Tr}(\\mathbf{O}_i \\mathbf{P}) - O^{\\text{exp}}_i }{\\sigma_i}\\right )^2,\n    \\label{eq_chi2}\n\\end{equation}\nwhere the model is expected to yield the mean value for each observable datum represented by $\\mathbf{O}_i$ while its actual experimental measurement gives $ O^{\\text{exp}}_i$ with the associated estimated variance $\\sigma_i^2$. In our case, each data point originates either from X-ray diffraction or Compton scattering measurements so that $\\mathbf{O}_i = \\mathbf{F}_{\\mathbf{q}_i}$ or, $\\mathbf{O}_j =\\mathbf{J}_{\\mathbf{q}_j}$ for different scattering vectors. \nIn the present work, for a given basis set of Gaussian contracted Slater-type orbitals, the closed form of each matrix element \\eqref{eq_FJelt} is calculated prior to refinement using a Mathematica code \\cite{Mathematica}. \n\nThe minimization of \\eqref{eq_chi2} is a convex least-squares fitting problem. The following section will explain how, together with the necessary N-representability conditions, the reconstruction problem falls into a convex optimization problem called semidefinite programming \\cite{boyd2004convex}.\n\n\\subsection{Constraints: N-representability, symmetry and frozen core}\n\\label{sec_constraint} \nThe N-representability conditions must be satisfied to ensure that the population matrix yields a physically meaningful density matrix. It is worth noting that the N-representability conditions are significantly more difficult if one requires the system to be in a pure state instead of a statistical mixture of quantum states.  \\textit{Pure} N-representability and \\textit{ensemble} N-representability are generally employed to distinguish the respective situations \\cite{Chakraborty2015Nrep}. We have chosen to consider the latter case for both practical reasons and because the system cannot always be exactly in its ground state, without interacting with the environment. Consequently, for an \\textit{ensemble} N-representable 1-RDM, the population matrix $\\mathbf{P}^\\perp$ for a closed-shell system, associated with an orthonormal basis set, must satisfy the following constraints,\n\\begin{subequations}\\label{Nrepcond10}\n    \\begin{align}\n      \\mathbf{P}^\\perp &\\succcurlyeq 0,\\label{Nrepcond11} \\\\\n        2 \\mathbf{I} - \\mathbf{P}^\\perp &\\succcurlyeq 0, \\label{Nrepcond12}\\\\\n        \\text{Tr}(\\mathbf{P}^\\perp) &= N,\n    \\label{Nrepcond13}\n    \\end{align}\n\\end{subequations}\ntogether with the obvious condition that\n$\\mathbf{P}^\\perp $ \nbeing Hermitian. Here $\\mathbf{I}$ is the identity matrix, and the symbol $\\succcurlyeq$ means the matrix is semidefinite positive, which is equivalent to stating that all eigenvalues are non-negative. Hence, constraint (11b) requires the eigenvalues of $\\mathbf{P}^\\perp$ to be smaller than 2. As previously mentioned, the present basis set is made of Slater-type atomic orbitals (expressed as Gaussian contractions), which are not mutually orthogonal. A Lowdin orthogonalization is thus performed on the atomic-orbital basis set prior to the reconstruction.\n\nAll constraints listed in \\eqref{Nrepcond10} are convex; thus, the convexity of the minimization of Eq.~\\eqref{eq_chi2} is preserved. Moreover, the semi-definite positivity of $\\mathbf{P}^\\perp$ imposed in \\eqref{Nrepcond11} makes it possible to use the tools of semi-definite programming \\cite{foleyMeasurementdrivenReconstructionManyparticle2012, debruyneInferringOneelectronReduced2020}. Access to the solution is thereby significantly facilitated.  \n\nThe model developed in this work is specifically adapted to molecular crystals for which a single group of atoms can be considered to form a specific entity. It is assumed that this group, referred to as \"the molecule\", does not share any charge with other entities in the same or neighbouring unit cells. The 1-RDM model is thus a mere molecular 1-RDM onto which translation and rotational symmetry operations can be applied to generate the density matrix of the entire crystal. These operations are fully taken into account in the present work.\n\nThe symmetry invariance at the molecular level can also be considered. The population matrix is thus required to be a direct sum of matrices in the invariant subspaces of each symmetry operator. In other words, $\\mathbf{P}$ should be block-diagonal when using the symmetry-adapted orbitals as the new basis, i.e. $\\mathbf{S}^{T} \\mathbf{P} \\mathbf{S} = \\bigoplus_{j=1}^{n} \\mathbf{P}_j$ where $\\mathbf{S}$ transforms the basis of atomic orbitals into symmetry adapted orbitals, $n$ the number of irreducible representations and $\\mathbf{P}_j$ the block matrices associated with each irreducible representation.\n\nThe new model also allows for freezing core-electron contributions. It effectively reduces the model's active space, hence the number of parameters to be determined in the population matrix.\nAs a consequence, illustrated in the next section, the computational cost is lowered, and the robustness of the result is improved against noise contamination and thermal-induced effects.\nIt can be best observed on core electrons' spatial density distribution, contributing to sharp peaks near each nucleus. \nTherefore, accurately reproducing such features for a population matrix model would require knowledge of high-$q$ structure factors, which may present an experimental challenge at usual temperatures. \nHere, an alternate but common approach was chosen.  A single-determinant calculation of the wave-function is performed from which core-electron molecular orbitals are extracted to construct an approximate core-electron density matrix. As a result, the model population matrix is given by $\\mathbf{P}^\\perp = \\mathbf{P}'^\\perp + \\mathbf{P}_{\\text{core}}^\\perp$, with $\\mathbf{P}_{\\text{core}}^\\perp$ being the frozen-core-electron population matrix. \nThe latter represents a fixed number of electrons and is -by construction- idempotent. The optimization is thus forced to search for the optimal solution in the subspace orthogonal to that spanned by the core electron's orbitals if the N-representability on the total 1-RDM is to be preserved.\n\nCombining symmetry and frozen-core conditions and assuming a non-magnetic system, the N-representability constraints become\n\\begin{subequations}\\label{eq_cons_symcore_all}\n    \\begin{align}\n    \\mathbf{P}'^\\perp &\\succcurlyeq 0, \n    \\label{eq_cons_symcore_1}\n    \\\\\n    2 \\mathbf{I} - \\mathbf{P}'^\\perp - \\mathbf{P}^\\perp_{\\text{core}}&\\succcurlyeq 0, \\\\\n    \\text{Tr}(\\mathbf{P}'^\\perp + \\mathbf{P}^\\perp_{\\text{core}}) &= N \\\\\n    \\mathbf{S'}^{T} (\\mathbf{P}'^\\perp + \\mathbf{P}^\\perp_{\\text{core}}) \\mathbf{S'} &= \\bigoplus_{j=1}^{n} \\mathbf{P}_j,\n    \\label{eq_cons_symcore_4}\n    \\end{align}\n\\end{subequations}\nwith $\\mathbf{P}'^\\perp$ being the population matrix for valence electrons and $N$ the total electron number for a single molecule. $\\mathbf{S'}$ transforms the orthogonalized atomic basis into the symmetry adapted basis. \nWe remark that with \\eqref{eq_cons_symcore_1} - \\eqref{eq_cons_symcore_4}, the optimization problem can still be modelled with SDP.\nIn this work, the constrained optimization problem is solved from the closed form of our model using the CVXPY package \\cite{diamond2016cvxpy}.\n\n\\begin{figure}[!htbp]\n    \\centering\n \\includegraphics[width=9cm]{Fig1.pdf}\n    \\caption{a) The unit cell of urea crystal ($P\\overline{4}21m$ with a=5.66 \\angs, c=4.71 \\angs) b) In dashed-green the path along which the 1-RDM values displayed in this article have been computed. The path is a succession of segments passing through O-C-N-H atoms.}\n    \\label{fig1}\n\\end{figure}\n\n\\subsection{Reconstruction from non-zero temperature data}\n\\label{sec_dw}\nIt must be noted that the reconstruction method is inherently temperature-independent, since the 1-RDM describes both mixed states and pure states. However, comparison with first-principle calculations is generally best done at the zero-Kelvin limit. It is thus helpful to deconvolute thermal motion effects to recover the ideal static 1-RDM. In this work, it is assumed that, given the large photon-electron energy transfer involved in the Compton scattering process,\nDCPs are hardly affected by nuclear agitation at reasonably low temperature \\cite{sternemannInfluenceLatticeDynamics2000,dugdaleThermalDisorderCorrelation1998,matsudaRayComptonScattering2020}. Therefore, temperature-induced alteration of experimental data is only taken into consideration for X-ray structure factors. In this case, the model is modified so that the SF matrix elements include an anisotropic Debye-Waller factor.\n\\begin{equation}\\label{DWSFmatrix}\n    (\\mathbf{F}_\\mathbf{q})_{ij}= e^{-\\mathbf{q}\\cdot\\widehat {B}_a\\cdot\\mathbf{q}}\\int \\phi_i^*(\\mathbf{r})\\phi_j(\\mathbf{r})e^{-i\\mathbf{q}\\cdot\\mathbf{r}} d^3r\n\\end{equation}\nwhere $\\widehat {B}_a$ is the thermal displacement tensor for nucleus $a$ on which both basis functions $\\phi_i$ and $\\phi_j$ are centred. No change is applied when the basis functions are associated with different atoms. More sophisticated temperature schemes are worth considering \\cite{stevensCalculationDynamicElectron1977}. For example, the Mulliken partitioning approach to two-centre contribution was implemented in our previous work \\cite{launayNRepresentableOneelectronReduced2021} and should be used with real data. However, the usual independent-atom model was chosen to prevent unfair similarity with the computational method used to generate the reference data \\cite{erbaAccurateDynamicalStructure2013a}. It has been checked that this simple approach allows for a fair deconvolution of thermal agitation effects when data is not contaminated with noise.\n\n\n\\section{Results}\n\nThe model explained above is well suited to molecular crystals and should be assessed for realistic systems. In particular, for such an approach which combines different experiments, it is necessary to evaluate the impact of data quality on the 1-RDM reconstruction.\n\nThe urea crystal (CO(NH$_2$)$_2$) has been chosen for two specific reasons: firstly, it has long been considered a \"standard\" test system in the field of charge density reconstruction. Several bond types are represented, among which highly mobile and delocalized electron density contributes to non-linear optical properties \\cite{cassidyNonlinearOpticalProperties1979,westComprehensiveAnalysisTerms2015}. Secondly, because of the interest it has attracted over the years, high-quality structure factors \\cite{zavodnikElectronDensityStudy1999,birkedalChargeDensityUrea2004} and Compton profiles data \\cite{shuklaHydrogenBondingUrea2001} are available from the literature. It thus positions urea as a legitimate candidate for a first phase-space-derived reconstruction of experimental 1-RDM on a molecular compound. Additionally, the urea molecule is significantly larger than our previous test systems and possibly one of the largest molecules onto which Compton measurement has ever been reported \\cite{shuklaHydrogenBondingUrea2001}. It can thus be considered a significant step in the quest for 1-RDM reconstruction.\nThis paper is the last stage of model calibration before a final reconstruction from true experimental data is undertaken.\n\nWe use here the same strategy for model assessment as for smaller systems, and described in previous papers \\cite{debruyneInferringOneelectronReduced2020,launayNRepresentableOneelectronReduced2021}: a reference 1-RDM is obtained from a periodic DFT calculation using the B3LYP functional \\cite{beckeDensityFunctionalThermochemistry1993} and a pob-DZVP basis set \\cite{peintingerConsistentGaussianBasis2013} using the CRYSTAL14 program \\cite{Dovesi2014Crystal}. The nuclei positions are those given by \\cite{Worsham1986} and derived from neutron diffraction data.\nArtificial-experimental data points are then generated based on this DFT-derived 1-RDM. 50-K-structure-factors are computed up to $\\sin\\theta/\\lambda=$1.1 \\invangs after atomic displacement parameters have been obtained using the dedicated option of CRYSTAL14 \\cite{erbaAccurateDynamicalStructure2013a}.\nCompton profiles are little affected by thermal motion at such low temperatures, and no particular treatment is applied in their case.\nThe CRYSTAL14 SF and DCP values are considered ideal mean values on which a Gaussian noise distribution is centred for each data point.\nConsequently, noise-contaminated data is also considered in our test reconstructions.\nThe reconstructed density matrix is obtained by determining a population matrix for a basis of poorer quality than that employed for artificial-data generation. An inevitable bias in the model is therefore introduced.\nThe basis set for the 1-RDM model is thus taken as a simple 6-31G basis set, with additional $p$-orbitals on hydrogen atoms.\n\n\\subsection{Reconstructions from ideal data}\nThe best reconstruction result is expected when data is obtained without thermal motion and noise. \nThe use of artificial data cannot be circumvented to test this optimal case. \nObserving what type of reconstruction results from the sole use of X-ray diffraction data is then quite illustrative. \nThe artificial-experimental set includes 3627 SF with $\\sin\\theta/\\lambda < 1.1 $ \\invangs. \n\nInspection of the 1-RDM $\\Gamma(\\mathbf{r}, \\mathbf{r}')$ on the O-C-N-H path as a 2D function clearly shows that the SF-only derived 1-RDM lacks most of the off-diagonal regions important features (Fig. \\ref{fig2}). It is consistent with conclusions drawn from previous works on a much smaller system. \nIn such a case, inferring the off-diagonal region is inherently difficult because SF are solely related to the position space density, therefore to the diagonal component $\\rho(\\mathbf{r}) = \\Gamma(\\mathbf{r}, \\mathbf{r})$. Only constraints on the model are likely to improve the off-diagonal description. This is an important criterion to assess the quality of the 1-RDM reconstruction since, in essence, off-diagonal parts are conditioned by the bonding mechanisms and how different locations interfere to shape the wave-function.\n\nA second step is to include noise-free Compton data in the observables. In all the following cases, 8 non-equivalent crystalline directions are used ([100], [110], [111], [210], [211], [310], [311], [321]). For each direction, data points are taken every 0.1 a.u.. This value corresponds to usual Compton spectrometer resolutions and prevents significant correlation between consecutive points. The maximum momentum value is set to 10 a.u.. The data set thus contains 800 DCP values in total. Obviously, a noise-free refinement case does not justify any weighting scheme, and the $\\sigma_i$ in the objective function \\eqref{eq_chi2} are uniformly taken to be 1.\n\n\n\\begin{figure}[!tp]\n    \\centering\n    \\includegraphics[width=0.5\\textwidth]{Fig2.pdf}\n    \\caption{a) The reference 1-RDM along the O-C-N-H path was calculated from CRYSTAL14. The reconstructed 1-RDMs with and without the DCP artificial-data are shown respectively in the upper left and lower right corner of (b). The contours are drawn at $\\pm 10^{-2} \\times 2^n e \\cdot \\text{\\angs}^{-3}, ~n \n    \\in [0, 20]$ where the positive(negative) contours are shown in solid (dashed) lines with blue (red) shades. }\n    \\label{fig2}\n\\end{figure}\n\nAs displayed in Fig.~\\ref{fig2}(b), the reconstructed 1-RDM now exhibits very marginal deviation from the reference. Slight differences persist in the off-diagonal regions $\\Gamma(r, r'\\neq r)$. \nA discrepancy is observed when the reconstructed 1-RDM are visualized along the two different O-C-N-H paths. Such a discrepancy is corrected once the symmetry restriction is imposed.\n\nThe virial ratio $-V / 2T$ is calculated for the reconstructed 1-RDMs, where the two-electron potential energy is estimated using the 2-RDM expression ansatz $\\Gamma^{(2)}(\\mathbf{r'}_1,\\mathbf{r'}_2;\\mathbf{r}_1,\\mathbf{r}_2)=\\Gamma^{(1)}(\\mathbf{r}_1,\\mathbf{r'}_1)\\Gamma^{(1)}(\\mathbf{r'}_2,\\mathbf{r}_2)-\\Gamma^{(1)}(\\mathbf{r'}_1,\\mathbf{r}_2)\\Gamma^{(1)}(\\mathbf{r'}_2,\\mathbf{r}_1)$. \nThe virial ratios for reconstruction with and without DCP are 0.996 and 0.934, respectively, confirming the role of Compton data in reaching a more pertinent solution.\nThe distinction between the two reconstructions showcases the importance of momentum space measurement even for a system like urea crystal, where the DCP anisotropy does not exceed 1\\%  of the total electron number (see Fig.~\\ref{fig_dcp}). \nNote that the good post-refinement virial ratio is a mere consequence of the reconstruction quality and did not require any ad-hoc constraint in our model or the objective function.\n\nIn the following paragraphs, possible sources of reconstruction errors will be discussed in more detail, and emphasis will be put on techniques for improving the model's robustness.\n\n\\subsection{Closer to real life: noise and temperature effects}\\label{section_noisetemp}\n\n\n\\begin{figure}[!htb]\n    \\centering\n    \\includegraphics[width=0.5\\textwidth]{Fig3.pdf}\n    \\caption{a) The reference (upper left) and reconstructed (lower right) 1-RDM $\\Gamma(\\mathbf{r}, \\mathbf{r}')$ with 0 K 1\\% noisy data.  b) The reconstruction 1-RDMs from 50 K 1\\% noisy data with (upper left) and without (lower right) restrictions. c) Estimated standard deviations for reconstructions shown in (b).\n    \\\\\n    The contours for (a) and (b) are the same as in Fig.~\\ref{fig2},  and the positive(negative) contours are shown in solid (dashed) lines with blue (red) shades. For (c) the contours are drawn at $10^{-4} \\times 2^n e \\cdot \\text{\\angs}^{-3}, ~n \\in [0, 12].$\n    \\\\\n    (N. 0K w/o R. = Noised 0 K without Restriction)}\n    \\label{fig3}\n\\end{figure}\n\nWhen real experimental data is used, noise contamination cannot be avoided. This section first considers the effect of statistical noise and, as a common practice, assumes no bias in the model. Then, the thermal motion of nuclei is introduced, and we study how it combines with statistical noise to deteriorate the reconstructed 1-RDM further. \n\nArtificial data is now contaminated by a random noise generated according to a Gaussian law. \nFor example, the SF data values become $F'(\\mathbf{q}) = F(\\mathbf{q}) + n \\times \\epsilon (\\mathbf{q})$ with $~\\epsilon \\sim \\mathcal{N}(0, \\vert F (\\mathbf{q}) \\vert )$. The noise level is chosen to be of the order of 1\\% by setting $n=0.01$. \nA similar procedure is applied to the DCP values. \nNotice that, given the weak Compton anisotropy in this system, the chosen noise level wipes out most of the directional information from the Compton scattering spectrum. We have found that such a noise model results in highly unbalanced weight in the objective function. Hence, an unweighted version of \\eqref{eq_chi2} is used in practice.\n\nAs expected, the 1-RDM reconstruction from noisy data now exhibits stronger deviations from the reference. \nIt can be seen in Fig. \\ref{fig3} (a). The modest discrepancies in the diagonal part of the RDM can be emphasized by looking at the electron deformation density displayed in Fig.~ \\ref{fig_deform} (a). As a reminder, the deformation density is the difference between the total electron density and the sum of independent atoms densities, i.e. promolecular density. The latter is obtained from CRYSTAL14 software using the same basis set as the reference calculation (pob-DZVP).\nAs anticipated, discrepancies are more significant in the off-diagonal region of the 1-RDM (Fig.~\\ref{fig3} (a)). The model, constrained by the N-representability conditions, obviously struggles to get sensible information from the weak Compton anisotropies buried under the noise. It is evidenced by the noticeable mismatch of anisotropy oscillations shown in  Fig.~\\ref{fig_dcp} (filled red triangles). Nevertheless, it can be seen that the deviation of reconstructed DCP anisotropy is still moderate, possibly due to the information carried by structure factors data. This assumption is validated after observing further deterioration when SF are removed from the data (filled blue triangles).\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.5\\textwidth]{Fig4.pdf}\n    \\caption{a) Deformation density on C-O-N reference plan (left) and reconstruction with 0 K 1\\% noisy data (right). b) Deformation density reconstructed from 50 K 1\\% noisy data with (left) and without (right) core electron and symmetry restrictions. \\\\\n    The contours are drawn at the same levels as Fig.~\\ref{fig2}.}\n    \\label{fig_deform}\n\\end{figure}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.5\\textwidth]{Fig5.pdf}\n    \\caption{Reference (dots) and reconstructed (lines) DCP anisotropy at [110] direction without (circles) and with 1\\% noise (triangles). The dotted line shows the reconstruction without the use of SF artificial-data. The purple shaded area indicates the standard deviation of reconstruction upon resampling from the noise distribution. }\n    \\label{fig_dcp}\n\\end{figure}\n\nFor the proposed model, deconvolution of temperature effects constitutes a difficult challenge.\nIn contrast to most common electron density reconstructions, using, for example, the widely spread kappa-refinement pseudo-atom multipolar model \\cite{hansenTestingAsphericalAtom1978, gattiModernChargeDensityAnalysis2012a}, our approach to 1-RDM determination relies on a linear expression \\eqref{eq_1rdmP} which, combined with linear constraints (Sec. \\ref{sec_constraint}), makes it possible to use positive semi-definite programming methodology.\nInsertion of the Debye-Waller formulation to account for the thermal effect destroys such a linearity. While an alternative formulation is currently under development, it was decided for the present work to explore the possibility of treating sequentially both problems. First, an ab-initio 1-RDM is computed with the basis set of the model.\nThen, the Atomic Displacement Parameters (ADP) are determined from high-order structure factors ($\\sin\\theta/\\lambda>0.7$ \\invangs) as explained in Sec.~\\ref{sec_dw}. Then, these ADP values $\\mathbf{B}$ are fixed and incorporated into the model. Therefore, the model remains linear for the 1-RDM refinement step, since a mere factor $e^{-\\mathbf{q} \\cdot \\mathbf{B} \\cdot \\mathbf{q}}$  is added to the SF operators.\nThe quality of such ADPs depends heavily on the model basis-set, and one cannot expect the refined $\\mathbf{P}$ matrix to be exempt from thermal motion contamination.\nAs shown in the lower panel of Fig.~\\ref{fig3}(b) and Fig.~\\ref{fig_deform}(b), the reconstructed 1-RDM and deformation density continue to worsen, which is clear evidence that the thermal motion effect has not been thoroughly deconvoluted.\nAlthough Compton data is assumed to be unperturbed at such low temperatures, the off-diagonal region continues deteriorating. It must be attributed to the sparsity of reliable information from momentum space, which cannot be compensated for by the SFs. \nOur independent-atom Debye-Waller description's poor performance is clearly shown by unphysical electron depletion in the vicinity of nitrogen centres shown in Fig.~\\ref{fig_deform}(b). Further, it is confirmed by the significant differences between the refined and reference (from CRYSTAL14) ADP for the nitrogen nuclei (about 25\\% discrepancy). When real data is involved, this very crude scheme will necessitate the addition of the previously mentioned two-centre terms in \\eqref{DWSFmatrix} and a more thorough inclusion of the Debye-Waller contribution in the general refinement.  The feature is currently being implemented.\n\n\n\\subsection{Further restrictions: frozen-core and symmetry}\n\nIn the previous section, we discussed how the combination of noise and thermal motion affects the 1-RDM reconstruction using \\eqref{eq_1rdmP}. \nTo mitigate such problems, a possible method is to reduce the degree of freedom of the model, thus making it more robust against noise contamination.\nAs introduced in Sec.~\\ref{sec_constraint}, one would naturally first invoke the necessity of applying symmetry restrictions to the model.\nAn overall improvement in reconstruction quality is observed as unnecessary free parameters are eliminated.\n\nA further limitation of the active space is obtained by freezing the core electron contribution to the density matrix. \nThis well-spread procedure does not affect our ability to absorb momentum space data, which primarily describes delocalised valence electrons. \nOn the SF side, freezing the core component of the 1-RDM helps stabilise the refinement against high-order reflections, which are the most affected by the noise and nuclear motion, while preserving nearly all the model's flexibility. \nIn this subsection, we report the impact of such a scheme under the non-ideal reconstruction scenarios.\n\nWhen the frozen-core and symmetry restrictions are added to those concerning N-representability, Fig.~\\ref{fig3} (b - upper panel) shows that the distortion in the reconstructed 1-RDM is greatly reduced.\nIn this case, even in the presence of noise and thermal agitation, the model catches most of the features observed in the reference 1-RDM (Fig.~\\ref{fig3}(a). Note that the most significant discrepancy is in the off-diagonal region corresponding to the long-range interaction between hydrogen and carbon, which are second neighbours. Such a striking improvement confirms that limiting the active space can effectively improve the reconstruction's robustness against noise. \nThe standard deviation on the reconstructed 1-RDMs with and without additional constraints (Fig. \\ref{fig3}(c)) was estimated upon resampling from the Gaussian noise distribution. \nIt is observed that the restriction of active space distinctly diminishes the uncertainty of the reconstruction.\n\nInterestingly, such a betterment in the 1-RDM modelling brings only minor changes to the resulting deformation density near the nuclei. Similarly, no major improvement is observed for the DCP anisotropy reconstruction (see Supplementary Information). This seemingly paradoxical observation can be resolved when adopting an optimization problem perspective.\n\nAs mentioned earlier, the 1-RDM reconstruction was defined through \\eqref{eq_chi2} as a least-squares minimization problem given the SF and DCP data.\nTherefore, introducing constraints such as \\eqref{eq_cons_symcore_1} - \\eqref{eq_cons_symcore_4} can only result in a new optimal solution with higher $\\chi^2$ value, i.e. a worse fit to the SF and DCP.\nConsequently, the DCP anisotropies and deformation density are not likely to be improved because they only depend on our ability to fit the Compton data and a set of Fourier coefficients of the electron density.\nHowever, a 1-RDM is a function in 6-D space which contains more information than its limited number of projections given by the data values.\nIn such a case, it is well-founded to believe that restricting the size of solution space effectively regularizes the model, giving it stronger predictive ability.\n\n\\begin{figure}[!htb]\n    \\centering\n    \\includegraphics[width=0.46\\textwidth]{Fig5.pdf}\n    \\caption{Mean field energy (see text) of one urea molecule reconstructed from 1\\% noisy data. Dashed and solid lines with circle and triangle data points represent the reconstruction with 0 K and 50 K SF data, respectively, and with identical Compton data. Blue lines show the reconstruction with no additional constraints. Light blue and red lines show the results when symmetry and core electron constraints are used. The purple dotted line shows the virial ratio of a 0 K 1\\% noisy data reconstruction with additional restrictions.}\n    \\label{fig5}\n\\end{figure}\n\nEstimating the total electron energy from experimental SF is a well-known, difficult challenge.\nAdding Compton scattering information does not significantly facilitate the task. However, on a mere relative scale, the energy criterion can be employed to compare the performances of different refinement strategies.\nThroughout this work, a recurrent question has been to evaluate the optimal cut-off value in $\\sin\\theta/\\lambda$ for the structure factors. In a perfect world, free from thermal motion, high-Miller-indices reflections should be retained as long as they rise above statistical noise.\nThe solid curve in Fig. \\ref{fig5}  shows that, for an ideal 0 K set of SF, the total electron energy stabilizes for any cut-off value above 0.7 \\invangs.\nIt is no longer true when data values are affected by temperature agitation.\nIn the 50 K case (dashed curve), SF corresponding to $\\sin\\theta/\\lambda > 0.7$ \\invangs contribute to a significant deterioration of the reconstruction from an electron-energy perspective.\nAs shown in Fig. \\ref{fig5} (all dashed-curves), minimum energy is reached when only reflections lower than 0.7 \\invangs are included in the set. Then, as one increases the Ewald sphere radius,  the total energy starts rising continuously. This confirms that the additional high-order reflections, which are the most affected by thermal motion, are not sufficiently well deconvoluted by the one-centre Debye-Waller model and merely contribute to perturbing the refinement process.\n\nWhen symmetry enforcement alone is applied, an overall betterment can already be observed under 0 K and 50 K scenarios. \nMore remarkably, introducing an additional frozen-core component not only further reduces the perturbation instilled by high-order reflections but also dramatically improves the reconstruction ability when only a small amount of reflection data is available.\nThus, both restrictions effectively increase the stability of the model by filtering out most of the perturbation brought by thermal motion and noise contamination and by reducing the number of unnecessary free parameters.\nIn addition, the behaviour of the fully restricted model in the reduced $q_{\\text{max}}$ domain suggests the possibility of reconstructing the 1-RDM with a limited amount of low-angle SF data, those that describe the most diffused electrons.\n\nLet us insist that the Hartree-Fock-like energy computed here is only meaningful as an indicator of the reconstruction quality since it uses both position and momentum space electron densities. However, due to the intrinsic difficulty of predicting energy from 1-RDMs, the question of whether one could accurately determine the total (or interaction) energy from scattering experiments should be left for more careful examination and discussion.\n\n\\section{Conclusion and Discussion}\nIn this work, an improved 1-RDM reconstruction method has been tested on a system which is significantly larger than those previously investigated \\cite{debruyneInferringOneelectronReduced2020, launayNRepresentableOneelectronReduced2021}. \nThe crucial role played by momentum space information, originating from Compton scattering data, is confirmed. It is instrumental in the quality of the reconstruction, even when the weak anisotropy is buried under statistical noise.\nThe two main additions to the model, symmetry restrictions and frozen-core contributions, are evidenced to drastically stabilize the 1-RDM reconstruction process against statistical noise and temperature effects.\nMeanwhile, with no additional constraints, it is shown that the resulting energies, evaluated from the modelled 1-RDM, closely satisfy the virial theorem. \nAs a consequence, the approximated total energy and virial ratio were found to be valuable indicators to identify an optimal portion of the Ewald sphere, which balances pertinent information and noise contamination.\n\n\nHowever, proper deconvolution of temperature-induced nuclear motion remains a challenging problem. \nIn the current approach, two main obstacles have been identified. Firstly, our choice of limiting the flexibility of the temperature model and the basis set to avoid bias in the assessment. \nSecondly, the necessity of keeping the 1-RDM model linear.  \nBoth inevitably led to strong discrepancies in atomic displacement parameters but allowed for a reliable assessment of the model's stability. Moreover, we have good reasons to believe that using a non-linear version of the optimization, including two-centre temperature factors and a better basis set, will drastically improve the performance when real experimental data is considered.\n\nThe current method for 1-RDM reconstruction is essentially a statistical inference procedure. Therefore, the quality of its outcome depends not only on the data distribution but also on the prior distribution of the model. In the present stage, a uniform prior was used, which means no prior knowledge is assumed. In the future, one could consider a more informed prior, for example, a Gaussian distribution centred on a lower-level theory calculation. The use of additional priors will help the model's performance, especially in the case of poor data quality.\n\nFinally, our results illustrate that 1-RDM reconstruction is achievable for a system of moderate size from X-ray structure factors and directional Compton profile measurements, even when the momentum space information is drastically limited. In the next step, such a method can be readily applied to actual experimental data.\n\n\\textbf{Acknowledgements: } The authors thank Devinder Sivia, Pietro Cortona and Julie McDonald for their insightful comments and discussions. S. Y. gratefully acknowledges funding from the Chinese Scholarship Council. Part of the computations are conducted on the clusters of Paris-Saclay University, which is gratefully acknowledged.\n\n\n\n\n\n\\bibliographystyle{apsrev4-1}\n\n\n\n\n\n\\onecolumngrid\n\\newpage\n\n\n\\section*{Supplementary Information}\n\n\\setcounter{figure}{0}\n\\renewcommand{\\thefigure}{S\\arabic{figure}}\n\n\\setcounter{table}{0}\n\\renewcommand{\\thetable}{S\\arabic{table}}\n\n\\setcounter{page}{1}\n\\subsection{Fitting score}\n\nHere, we list the unweighted fitting scores $\\chi^2$ for different data sets and reconstructions. We note $\\tilde{\\chi}^2_{\\text{total}}$ the value of the objective function in the optimization procedure and $\\chi^2_{\\text{total}}$ the fitting score computed on the entire range of 0 K, noise-free artificial-data. \n\nFirstly, it can be observed that while the optimal objective function values for the 0 K and 50 K scenarios are similar, the $\\chi^2_{\\text{SF}}$ is substantially larger than the $\\tilde{\\chi}^2_{\\text{SF}}$ in the 50 K case, suggesting that the thermal motion is not perfectly accounted for by \\eqref{DWSFmatrix}. Secondly, in both the 0 K and 50 K cases, restrictions lead to an increase in the optimal fitting score $\\tilde{\\chi}^2_{\\text{total}}$ (for noised 0 K and 50 K data), but a decrease of the ${\\chi}^2_{\\text{total}}$ (noise-free 0 K). Such an observation quantitatively confirms the assumption that restrictions act as regularization, thereby improving reconstruction results.\n\n\\begin{table}[!htp]\n    \\centering\n    \\begin{tabular}{|c||c|c|c||c|c|c|}\n         \\hline\n         Scenario & $\\tilde{\\chi}^2_{\\text{SF}}$ & $\\tilde{\\chi}^2_{\\text{DCP}}$ & $\\tilde{\\chi}_{\\text{total}}^2 $\n         & $\\chi^2_{\\text{SF}}$ & $\\chi^2_{\\text{DCP}}$ & $\\chi_{\\text{total}}^2$\n         \\\\\n         \\hline\n         0 K 1\\% noise & 2.683 & 2.084 & 4.766 & 1.798 & 0.871 & 2.669\n         \\\\\n         0 K 1\\% noise sym. & 3.090 & 2.272 & 5.362 & 1.336 & 0.734 & 2.070\n         \\\\\n         0 K 1\\% noise sym. \\& core & 3.190 & 2.271 & 5.362 & 1.234 & 0.741 & 1.975\n         \\\\\n         50 K 1\\% noise & 2.597 & 2.258 & 4.855 & 22.065 & 0.896 & 22.961\n         \\\\\n         50 K 1\\% noise sym. & 2.964 & 2.662 & 5.626 & 17.375 & 0.724 & 18.099\n         \\\\\n         50 K 1\\% noise sym. \\& core & 3.144 & 2.741 & 5.885 & 14.176 & 0.757 & 14.933 \n         \\\\\n         \\hline\n    \\end{tabular}\n    \\caption{$\\tilde{\\chi}^2_{\\text{SF}} = \\sum_\\mathbf{q} (\\tilde{F}(\\mathbf{q}) - \\text{Tr} ( \\mathbf{P} \\mathbf{F}_\\mathbf{q}) )^2 $ for $\\vert \\mathbf{q} \\vert < 0.7$ and $\\tilde{F}$ being the 1\\% noisy artificial-data. Similarly, $\\tilde{\\chi}^2_{\\text{DCP}}$ is the objective function for noisy data. As a reminder,  $\\tilde{\\chi}^2_{\\text{total}} = \\tilde{\\chi}^2_{\\text{SF}} + \\tilde{\\chi}^2_{\\text{DCP}}$ is the objective function being optimized, and the noised SF artificial-data is at respectively 0 K and 50 K for two different scenarios. $\\chi^2_{\\text{total}}, \\chi^2_{\\text{SF}}, \\chi^2_{\\text{DCP}}$ refer to the fitting score with respect to the noise-free artificial-data using the entire Ewald sphere up to 1.1 \\invangs (always at 0 K).}\n    \\label{tab:my_label}\n\\end{table}\n\n\\subsection{Mean-field energies}\n\nWe list the energies evaluated for different reconstructions. Note that the potential energies, hence the total energies, are calculated with a Hartree-Fock Hamiltonian operator, which allows us to calculate an approximate energy with 1-RDMs. For reference, the energies of the DFT 1-RDM with the pob-DZVP basis set are $T_{\\text{DFT}}  = 223.315, ~ E_{\\text{DFT}} = -225.005$. However, one should keep in mind that these energies are evaluated with different methods and different basis functions, and are not meant to be compared directly.\n\n\\begin{table}[!htp]\n    \\centering\n    \\begin{tabular}{|c||c|c|c||c|c|}\n         \\hline\n         Scenario & $T$ & $V_{\\text{HF}}$ & $E_{\\text{HF}}$\n         & Virial ratio\n         \\\\\n         \\hline\n         0 K 1\\% noise &  224.554 & -447.229 & -222.675 & 0.99581\n         \\\\\n         0 K 1\\% noise sym. & 224.387 & -447.546 & -223.159 & 0.99726\n         \\\\\n         0 K 1\\% noise sym. \\& core & 224.909 & -448.237 & -223.327 & 0.99648\n         \\\\\n         50 K 1\\% noise & 223.844 & -445.405 & -221.561 & 0.99490\n         \\\\\n         50 K 1\\% noise sym. & 223.676 & -446.073 & -222.397 & 0.99714\n         \\\\\n         50 K 1\\% noise sym. \\& core & 224.884 & -447.832 & -222.949 & 0.99570\n         \\\\\n         \\hline\n    \\end{tabular}\n    \\caption{The energies and Virial ratios evaluated for different reconstructed 1-RDMs. For potential energies $V_{\\text{HF}}$, a mean-field Hartree-Fock energy operator is employed.}\n    \\label{tab:my_label}\n\\end{table}\n\n\n\\end{document}\n\n"}
{"paper_id": "2403-00535", "version": "2403-00535v1", "root_file_path": "d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\data_raw\\2403-00535\\tex\\2403-00535v1\\Winnberg-etal-jpg.tex", "metadata": {"total_length": 231271, "merged_count": 2, "merged_files": ["Winnberg-etal-jpg.tex", "appendix.tex"], "missing_files": []}, "content": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\documentclass{aa}\n\n\n\n\n\\usepackage{txfonts}\n\n\\usepackage[english]{babel}  \n\\usepackage{graphicx}  \n\\usepackage[utf8]{inputenc}  \n\\usepackage{microtype}  \n\\usepackage{booktabs}  \n\\usepackage{rotating}  \n\\usepackage{lscape}\n\\usepackage[squaren]{SIunits}\n\\usepackage{natbib}\n\\usepackage{color}\n\\usepackage{amsmath}\n\\usepackage{amssymb}\n\\maxdeadcycles=200\n\n\\newcommand{\\water}{H$_2$O}\n\\newcommand{\\micron}{$\\mu$m}\n\\newcommand{\\recta}[4]{$\\alpha=#1^{\\rm{h}}\\,#2^{\\rm{m}}\\,#3\\fs#4$}\n\\newcommand{\\dec}[3]{$\\delta=#1\\degr\\,#2\\arcmin\\,#3\\arcsec$}\n\\newcommand{\\decli}[4]{$\\delta=#1\\degr\\,#2\\arcmin\\,#3\\farcs#4$}\n\\newcommand{\\powerten}[1]{$10^{#1}$}\n\\newcommand{\\vc}{$v_{\\rm{c}}$}\n\\newcommand{\\ve}{$v_{\\rm{e}}$}\n\\newcommand{\\vch}{$v_{\\rm{c}}^{\\rm{H_2O}}$}\n\\newcommand{\\veh}{$v_{\\rm{e}}^{\\rm{H_2O}}$}\n\\newcommand{\\vp}{$v_{\\rm{p}}$}\n\\newcommand{\\vcoh}{$v_{\\rm{c}}^{\\rm{OH}}$}\n\\newcommand{\\vchzweio}{$v_{\\rm{c}}^{\\rm{H}_2\\rm{O}}$}\n\\newcommand{\\dvoh}{$\\Delta\\rm{v}^{\\rm{OH}}$}\n\\newcommand{\\dvhzweio}{$\\Delta\\rm{v}^{\\rm{H}_2\\rm{O}}$}\n\\newcommand{\\Sp}{$S_{\\rm{p}}$}\n\\newcommand{\\SI}{$S_{\\rm{I}}$}\n\\newcommand{\\smax}{$S_{\\rm{max}}$}\n\\newcommand{\\sir}{$S_{25}$}\n\\newcommand{\\Lhzweio}{${\\cal L}^{\\rm{H_2O}}$}\n\\newcommand{\\Lsun}{$L_{\\sun}$}\n\\newcommand{\\Myr}{$M_{\\sun}$\\,yr$^{-1}$}\n\n\\newcommand{\\mdot}{$\\dot{M}$}\n\\newcommand{\\Teff}{$T_{\\mathrm{eff}}$}\n\n\\newcommand{\\Lup}{$L_{\\rm H_2O}^{\\rm up}$}\n\\newcommand{\\gm}{$\\;\\;\\;\\;\\;$}\n\\newcommand{\\vgm}{$\\;\\;\\;\\;\\;\\;\\;$}\n\n\\newcommand{\\TAStar}{$T^*\\hskip-2pt_{\\rm A}$}\n\\newcommand{\\kms}{km\\,s$^{-1}$}\n\\newcommand{\\Lfir}{$L_{\\rm fir}$}\n\\newcommand{\\Msol}{M$_{\\odot}$}\n\\newcommand{\\Lsol}{L$_{\\odot}$}\n\\newcommand{\\Tex}{$T_{\\rm ex}$}\n\\newcommand{\\Tmb}{$T_{\\rm mb}$}\n\\newcommand{\\ITmb}{$\\int T_{\\rm mb} {\\rm dv}$}\n\\newcommand{\\ITAstar}{$\\int T^*\\hskip-2pt_{\\rm A} {\\rm dv}$}\n\\newcommand{\\Vlsr}{$V_{\\rm lsr}$}\n\\newcommand{\\degs}{$^{\\circ}$}\n\\newcommand{\\pad}{.\\hskip-2pt$^\\circ$}\n\\newcommand{\\pam}{.\\hskip-2pt$^{\\prime}$}\n\\newcommand{\\pas}{.\\hskip-2pt$^{\\prime\\prime}$}\n\\newcommand{\\pats}{.\\hskip-2pt$^s$}\n\\newcommand{\\magn}{.\\hskip-2pt$^m$}\n\\newcommand{\\gsim}{\\;\\lower.6ex\\hbox{$\\sim$}\\kern-7.75pt\\raise.65ex\\hbox{$>$}\\;}\n\\newcommand{\\lsim}{\\;\\lower.6ex\\hbox{$\\sim$}\\kern-7.75pt\\raise.65ex\\hbox{$<$}\\;}\n\\newcommand{\\Done}{D$^\\prime$}\n\\newcommand{\\Dtwo}{D$^{\\prime\\prime}$}\n\\newcommand{\\Gone}{G$^\\prime$}\n\\newcommand{\\Gtwo}{G$^{\\prime\\prime}$}\n\n\\begin{document} \n\n\\title{Water vapour masers in long-period variable stars}\n\\subtitle{III. Mira variables\n\\object{U\\,Her} and \\object{RR\\,Aql}\\thanks{The maser spectra and the VLA data cubes are available via anonymous ftp to cdsarc.u-strasbg.fr (130.79.128.5) or via http://cdsarc.u-strasbg.fr/cgi-bin/qcat?J/A+A/}}\n\n\\author {A.~Winnberg \\inst{1}\n        \\and J.~Brand \\inst{2}\n        \\and D.~Engels \\inst{3}}\n\n\\offprints{J. Brand or D. Engels,\\\\\n\\email{brand@ira.inaf.it, dengels@hs.uni-hamburg.de}\n}\n\n\\institute{Onsala Rymdobservatorium, Observatoriev\\\"{a}gen,\n           S--43992 Onsala, Sweden\n      \\and INAF - Istituto di Radioastronomia \\& Italian ALMA Regional Centre, Via P. Gobetti 101,\n           I--40129 Bologna, Italy\n      \\and Hamburger Sternwarte, Universit\\\"{a}t Hamburg, Gojenbergsweg 112,\n           D--21029 Hamburg, Germany\n           }\n\n\\date{Received date; accepted date: 12/2/24}\n\n\\abstract {Water maser emission is often found in the circumstellar envelopes of evolved stars, i.e. asymptotic-giant-branch stars and red supergiants with oxygen-rich chemistry. The \\water\\ emission shows strong variability in evolved stars of all these types.}\n{We wish to understand the reasons for the strong variability of water masers emitted at 22 GHz. In this paper we study U Her and RR Aql as representatives of Mira variable stars.}\n{We monitored U\\,Her and RR\\,Aql in the 22-GHz maser line of water vapour with single-dish telescopes. The monitoring period covered about two decades between 1990 and 2011, with a gap between 1997 and 2000 in the case of RR\\,Aql. Observations were taken also in 1987 and 2015 before and after the period of contiguous monitoring. In addition, maps were obtained in the period 1990--1992 of U\\,Her with the Very Large Array.}\n{We find that the strongest emission in U\\,Her is located in a shell with boundaries 11 -- 25 AU. The gas crossing time is 8.5 years. We derive lifetimes for individual maser clouds of $\\le$4 years, based on the absence of detectable line-of-sight velocity drifts of the maser emission. The shell is not evenly filled, and its structure is maintained on timescales much longer than those of individual maser clouds. Both stars show brightness variability on several timescales. The prevalent variation is periodic, following the optical variability of the stars with a lag of 2--3 months. Superposed are irregular fluctuations, of a few months' duration, of increased or decreased excitation at particular locations, and long-term systematic variations on timescales of a decade or more.}\n{The properties of the maser emission are governed by those of the stellar wind while traversing the \\water\\ maser shell. Inhomogeneities in the wind affecting the excitation conditions and prevalent beaming directions likely cause the variations seen on timescales longer than the stellar pulsation period. We propose the existence of long-living regions in the shells, which maintain favourable excitation conditions on timescales of the wind crossing times through the shells or orbital periods of (sub-)stellar companions. The \\water\\ maser properties in these two Mira variables are remarkably similar to those in the semiregular variables studied in our previous papers, regarding shell location, outflow velocities, and lifetimes. The only difference is the regular brightness variations of the Mira variables caused by the periodic pulsation of the stars.}\n\n\\keywords{Water masers -- Stars: AGB and post-AGB, U~Her, RR~Aql --\ncircumstellar matter}\n\n\\maketitle\n\n\n\n\\section{\\label{intro} Introduction}\nMaser emission of SiO, \\water\\ and OH is frequently found in the circumstellar shells or envelopes (CSEs) of oxygen-rich stars on the asymptotic giant branch (AGB) and in several red supergiants (RSGs). Within the CSEs, where conditions are favourable for the excitation of one or another of these masers depends on local density, temperature and dynamics and thus in practice on distance to the stellar surface. In the case of Mira variables the \\water\\ masers are typically found at radii of 5 to 50 AU \\citep{bowers93, bowers94, colomer00, bains03, imai03, xu22}.\n\nEarly observing programs to monitor water masers found strong variability in their spectra \\citep[and references therein]{schwartz74, berulis83, habing96} particularly noticeable in the integrated maser emission \\citep{berulis98}. Depending on the type of star observed and the duration of the monitoring, several types of variability can be recognised. The first and often most evident is a variation in delayed sync with the light variations of the central star (same period but with an offset in phase); superposed on this regular variation there often is an erratic variability, occurring on shorter timescales, including bursts of individual maser lines lasting weeks to months. If the monitoring takes place over long periods of time, variability in overall brightness of the maser emission may be detected, lasting many years \\citep{brand20} and may have repetitive patterns ('superperiods';  \\citealt{rudnitskii05}). \n\n\n\n\n\\begin{table*}\n  \\caption{Basic information on the two Mira variables monitored in the period 1990--2011.}\n\\label{centralcoords} \n\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{rlllrrrclcc}\n\\hline\\noalign{\\smallskip}\n\\multicolumn{1}{c}{Name} &  \\multicolumn{2}{c}{$\\alpha$\\,\\,\\,\\, (J2000)\\,\\,\\,\\, \n $\\delta$} & \n \\multicolumn{1}{c}{$D^{\\rm a}$} &\n  \\multicolumn{1}{c}{$V_{\\ast}^{\\rm b}$} & \n \\multicolumn{1}{c}{$V_{\\rm exp}^{\\rm b}$} & \n \\multicolumn{1}{c}{$V_{\\rm b}, V_{\\rm r}$} & \n \\multicolumn{1}{c}{$P_{\\rm opt}$} &\n \\multicolumn{1}{c}{TJD$_{max}$} &\n\\multicolumn{1}{c}{$P_{\\rm rad}$} &\n\\multicolumn{1}{c}{$\\phi_{\\rm lag}$} \\\\\n\\multicolumn{1}{c}{} &\n\\multicolumn{1}{l}{\\, h\\,\\,  m\\, \\,   s}\n & \\multicolumn{1}{l}{\\, \\,  $\\circ$\\,\\,\\, $\\prime$\\, \\,  $\\prime\\prime$}\n & \\multicolumn{1}{c}{pc} \n & \\multicolumn{1}{c}{\\kms}\n & \\multicolumn{1}{c}{\\kms}\n & \\multicolumn{1}{c}{\\kms}\n & \\multicolumn{1}{c}{days}\n & \\multicolumn{1}{c}{days}\n & \\multicolumn{1}{c}{days} \n & \\multicolumn{1}{c}{} \\\\\n\\hline\\noalign{\\smallskip}\nU~Her & 16:25:47.5 & +18:53:33&266$^{+32}_{-18}$ &$-$15.0&13.1 &$-$23.3,  $-$7.1 & 405 &6668$\\pm$3 &407$\\pm$11 &0.16 \\\\[0.1cm]\nRR~Aql& 19:57:36.1 & $-$01:53:11&410$^{+12}_{-11}$&28.5&9.0 &23.2, 31.9 & 400&6487$\\pm$5 &400$\\pm$5 &0.21 \\\\[0.1cm]\n\\noalign{\\smallskip}\n\\hline\n\\end{tabular}}\n\\\\[0.1cm]\nReferences. $^{\\rm (a)}$ For distances: \nU Her: \\cite{vlemmings07};  \nRR Aql: \\cite{sun22}.\n$^{\\rm (b)}$ For stellar systemic and expansion velocities.\nU Her:~\\cite{gon-alfonso98}; RR Aql:~\\cite{danilovich15}.\n\\end{table*}\n\n\nMaps made from interferometric observations taken many months apart show that also the distribution of the maser emission sites in the CSEs changes considerably \\citep{johnston85}. The masers are thought to reside in clouds of size 2--5 AU \\citep{bains03, richards11} embedded in the stellar wind, which in the case of Semiregular and Mira variables are identifiable for at most a few years \\citep{bains03, winnberg08}. The crossing times through the \\water\\ maser shells, located within $\\sim$50 stellar radii, have timescales of decades, so that the disappearance of the emission of particular maser features after few years would indicate that the clouds either dissipate or change their beaming direction \\citep{bains03, richards12}.\n\nBesides brightness variations, also variations of the velocities of the maser lines were studied. Velocity drifts attributed to the passage of shocks in the \\water\\ maser shell were reported for several stars \\citep{shintani08}. The monitoring of the velocity variations through high-resolution interferometry, make it possible to trace the structure of the stellar wind passing through the shell, as shown recently by \\cite{xu22} for the Mira variable BX\\,Cam (IRC+70066). \n\nIn order to improve the understanding of the properties of \\water\\ maser variability for different types of late-type stars, we started in 1987 the Medicina/Effelsberg monitoring program of several such stars using the Medicina 32-m and Effelsberg 100-m radio telescopes. With data covering 20--30 years, we expect to elucidate the changes of maser excitation conditions within the \\water\\ maser shells, which in AGB stars are crossed by the stellar wind on timescales of the same order. The sample included Semi-regular Variables (SRV), Mira variables, OH/IR stars and RSGs. For each class of stars we added several interferometric observations of a prototypical object using the Very Large Array (VLA), to study the development of the emission pattern in the maps and the response of the single-dish spectra to it.\n\n\nIn our first two papers we presented the results for the SRVs in our sample: RX~Boo and SV~Peg (\\citealt{winnberg08}; hereafter Paper I), and R~Crt and RT~Vir (\\citealt{brand20}; hereafter paper II). In the period 1990--1992 the \\water\\ maser emission of RX~Boo, taken as representative of the class, was found in an incomplete ring with an inner radius of 15~AU and a shell thickness of 22~AU. The variability of \\water\\ masers in RX~Boo, as well as in SV~Peg, R~Crt and RT~Vir, is due to the emergence and disappearance of maser clouds with lifetimes of $\\sim$1 year. The maser emission regions do not evenly fill the shell of RX~Boo, as indicated by the asymmetry in the spatial distribution, which persists at least an order of magnitude longer. An exception to the generally short lifetime of individual maser clouds is the \"11~\\kms\\ feature\" in RT~Vir, originating in a cloud with an estimated lifetime of $>$ 7.5 years \\citep{brand20}. \n\nIn this paper we present the results for approximately two decades of monitoring of the Mira variables U~Her and RR~Aql. We chose U~Her as the representative star of the class of Mira variables.  Interferometric maps were taken for this star between 1990 and 1992. Preliminary results of the U~Her observations were reported in \\cite{engels99} and \\cite{winnberg11}. In addition, here we will use also other interferometric maps from the literature made in the same period as the single-dish monitoring program. The results of the Mira-like variable stars IK~Tau, and of R~Cas, R~Leo and $\\chi$~Cyg \nwill be the subject of  separate papers. The results for the RSGs will be presented in a forthcoming paper.\n\nIn Table \\ref{centralcoords} we present some basic information. It gives the name of the object in column (Col.) 1; the coordinates are in Cols. 2 and 3; in Col. 4 we show the distance, the references for which are given in the footnote. \nAll linear sizes in this paper are scaled to these distances. The radial velocity of the star,  $V_{\\ast}$, and the final expansion velocity in the CSE, $V_{\\rm exp}$, are given in Cols. 5 and 6. These velocities are our best estimates using the data obtained from observations of molecular emission (mostly CO) by the references listed in the footnote. In Col.~7 we give the (blue and red) boundaries $V_{\\rm b}$, $V_{\\rm r}$ of the range in velocity, over which \\water\\ emission was found during the monitoring period. Col. 8 gives the optical pulsation period $P_{\\rm opt}$, and Col. 9 the date TJD$_{max}$\\footnote{Truncated Julian Date, TJD=JD-2440000.5} of the last optical maximum before the monitoring started. Col. 10 shows the radio pulsation period $P_{\\rm rad}$, and in Col. 11 we give the lag $\\phi_{\\rm lag}$ of the phase of the radio light curve with respect to the optical phase. The entries for Cols. 7--11 for the individual stars are taken from the sub-sections in this paper, where the \\water\\ maser properties are analysed individually. \n \nThis paper is organised as follows: in Sect.~\\ref{sec:observations} we describe the observations, and in Sect.~\\ref{sec:presdata} we explain methods and definitions to present the data. In Sect.~\\ref{sec:uher} we analyse the single dish and interferometric data of U\\,Her, and present the model and 3-dimensional structure of the circumstellar envelope of U\\,Her. The single dish data for RR\\,Aql are discussed in Sect.~\\ref{sec:rraql}. The properties of the \\water\\ maser emission in the circumstellar envelopes of Mira variables are discussed in Sect.~\\ref{sec:miraprop}, while our findings are summarised in Sect.~\\ref{sec:conclusions}.\n\n\\section{Observations  \\label{sec:observations}}\nSingle dish observations of the \\water\\ maser line at 22235.08~MHz were made with the Medicina 32-m and Effelsberg 100-m telescopes at typical intervals of a few months. Initial observations began in 1987 with the Medicina telescope, and the regular monitoring for the stars discussed here was performed between 1990 and 2011. Some additional spectra were taken in 2015. For both stars \none spectrum taken between 1987 and 1989 has been published before, by \\cite{comoretto90}. \nThe Effelsberg telescope participated in the monitoring program between 1990 and 1999, and in the case of U~Her until 2002. \nVLA observations of U~Her were made on four occasions in the period 1990--1992.\n\n\\subsection{Medicina}\nBetween March 1987 and March 2011, and again in 2015, we searched for H$_2$O($6_{16}-5_{23}$) (22.2350798~GHz) maser emission with the Medicina 32-m telescope\\footnote{The Medicina 32--m VLBI radiotelescope is operated by INAF--Istituto di Radioastronomia.} towards the stars listed in Table~\\ref{centralcoords}. We used a digital autocorrelator backend with a bandwidth of 10~MHz and 1024 channels, resulting in a resolution of 9.76~kHz (0.132 \\kms); the half-power beam width (HPBW) at 22~GHz was $\\sim$1\\pam 9. During this period the sample was observed four to five times per year in separate sessions. For more information on the changes in the system during these years, see Paper~I.\n\nThe telescope pointing model was typically updated a few times per year, and quickly checked every few weeks by observing strong maser sources (e.g. W3~OH, Orion-KL, W49~N, Sgr~B2, and W51). The pointing accuracy was always better than 25\\arcsec; the rms residuals from the pointing model were of the order of 8\\arcsec--10\\arcsec. \n\nObservations were taken in total power mode, with both ON and OFF scans of 5~min duration. The OFF position was taken \\mbox{1\\pad25 E} of the source position to rescan the same path as the ON scan. Usually two ON/OFF pairs were taken at each position. Only the left-hand circular (LHC) polarisation output from the receiver was registered\\footnote{In Paper I this was erroneously reported as only RHC (right-hand circular).}. In 2015 both polarisations were recorded (and averaged during data reduction).\nThe observations were embedded in a larger program. We could thus determine the antenna gain as a function of elevation by observing several times during the day the continuum source DR~21 (for which we assumed a flux density of 16.4 Jy after scaling the value of 17.04~Jy given by \\cite{ott94} for the ratio of the source size to the Medicina beam) at a range of elevations. Antenna temperatures were derived from total power measurements in position switching mode. The integration time at each position was 10 sec with 400~MHz bandwidth. \n\nThe daily gain curve was determined by fitting a polynomial curve to the DR~21 data; this was then used to convert antenna temperature to flux density for all spectra taken that day. From the dispersion of the single measurements around the curve, we found the typical calibration uncertainty to be 20\\%.\n\n\n\\begin{table}\n\\caption[]{\\label{tab:VLAspec} VLA map specifications for U~Her}\n\\begin{flushleft}\n\\begin{tabular}{lrrrrr}\n\\hline\\noalign{\\smallskip}\nDate &  \\multicolumn{3}{c}{HPBW} & rms & $S/N$ \\\\\n & maj.a.  & min.a. & p. a.  &  & \\\\\n & (\\arcsec) & (\\arcsec) & (\\degr) & (Jy/b.) & \\\\\n\\noalign{\\smallskip} \\hline\\noalign{\\smallskip}\n1990 Feb. 26 & 0.091 & 0.081 & 71.31 & 0.012 & 1080 \\\\\n1990 June 03 & 0.130 & 0.106 & 54.79 & 0.015 & 500 \\\\\n1991 Oct. 20 & 0.409 & 0.106 & $-65.62$ & 0.021 & 380 \\\\\n1992 Dec. 28 & 0.075 & 0.072 & $-24.46$ & 0.017 & 420 \\\\\n\\noalign{\\smallskip} \\hline\n\\end{tabular}\n\\end{flushleft}\n\\emph{maj.a.}: half-power beam width (HPBW) for the major axis of\nthe best-fit three-dimensional Gaussian component to the synthesised beam \\\\\n\\emph{min.a.}: HPBW for the minor axis \\\\\n\\emph{p.a.}: position angle of the major axis (E of N)\\\\\n\\emph{rms}: the root-mean-square noise fluctuations in signal-free\nchannels in units of Jansky per beam area \\\\\n\\emph{S/N}: signal-to-noise ratio or `dynamic range' in the\nchannel with strongest signal\n\\end{table}\n\n\n\\subsection{Effelsberg}\nBetween 1990 and 1999 we observed the sources \nwith the Effelsberg 100-m antenna\\footnote{The Effelsberg 100-m radiotelescope is operated by the Max-Planck-Institut f\\\"ur Radioastronomie, Bonn}. \nTo observe the $6_{16}\\rightarrow 5_{23}$ transition of the water molecule, 18--26~GHz receivers with cooled masers as pre-amplifier were used until 1999. Only one polarisation direction, the LHC, was recorded, as circumstellar water masers were found to be unpolarised to limits of a few percent (\\citealt{barvainis89}). U~Her was observed also in 2002 using the 1.3cm prime-focus receiver, which measured two linear polarisations averaged during post-processing. At 1.3~cm wavelength the beam width is $\\sim$40\\arcsec\\ (HPBW).  We observed in total power mode integrating ON and OFF the source in general for 3--10~min each. \n`ON-source' the telescope was positioned on the coordinates given in Table \\ref{centralcoords}, while the `OFF-source' position was displaced 3\\arcmin\\ to the east of the source. \n\nUntil 1999 the backend consisted of a 1024 channel autocorrelator, while in 2002 an 8192 channel autocorrelator was used (4096 channels per polarisation). Observations were made with a bandwidth of 6.25~MHz (5 MHz in 2002), centred on the stellar radial velocity. The velocity coverage was $70$ or $80$~\\kms\\ and the velocity resolution 0.08~\\kms\\ (0.016 \\kms\\ in 2002). \nFor procedures to reduce the spectra and for the calibration we refer to Paper I. We estimate that the flux densitiy values are not reliable to better than 30~\\%. \n\n\\subsection{VLA observations}\nU~Her was observed with the Very Large Array (VLA)\\footnote{The VLA is operated by the National Radio Astronomy Observatory, which is a facility of the National Science Foundation operated under cooperative agreement by Associated Universities, Inc.\n} on four occasions between February 1990 and December 1992. All 27 antennas were used yielding synthesised beamwidths down to $\\sim$70 mas (Table \\ref{tab:VLAspec}). For three of the four epochs we used the largest extent (\"A\" configuration), while the October 1991 observations were carried out with a hybrid configuration (\"BnA\"). We chose a backend bandwidth of 3.125~MHz to obtain a total velocity range of 42~\\kms\\ and the bandwidth was split into 64 channels, yielding a velocity resolution of 0.66~\\kms. Data from the right and left circular polarization modes were averaged. Typical integration times were 30~min on the star and 12~min on the phase calibrator J1608+1029 with a sampling time of 30~s. Flux calibration was obtained relative to 3C286 that was assumed to have a flux density of 2.55~Jy and 3C84 was used to correct for the bandpass shapes.\n\n\\begin{figure*}\n\\resizebox{18cm}{!}{\n\\includegraphics{uher_selspec_070121.jpg}}\n\\caption{Selected H$_2$O maser spectra of U~Her. The calendar date of the observation is indicated on the top left above each panel, the TJD (JD-2440000.5), \non the top right.}\n\\label{fig:uher_sel}\n\\end{figure*}\n\n\\section{Presentation of the data \\label{sec:presdata}}\nBefore we present and discuss the data on the stars in our sample, we need to describe the tools and define the parameters we used in our analysis. For each star we also show a selection of the spectra taken over the years, in the sub-sections where they are presented. See Fig. \\ref{fig:uher_sel} as an example. All maser spectra for the stars are presented in Fig. \\ref{fig:uher_all} and \\ref{fig:rraql_all} (Appendix C).\n\n\\subsection{Diagnostic plots} \\label{diagplots}\n\nFor each star we show a number of plots that summarise the behaviour of the water maser emission in time, intensity and velocity-range. We give a brief description of these diagnostic plots, and refer to Paper II for more details. \n\n\\smallskip\n{\\it FVt-plot:}\\ The time variation of the maser emission is visualised by plotting the flux density versus time and line-of-sight (los) velocity, $V_{\\rm los}$, in a so-called FVt-diagram (cf. \\citealt{felli07}). An example is shown in Fig.~\\ref{fig:uher-fvt}.\nBetween consecutive observations linear interpolation was applied; when there is a long time-interval between two consecutive observations this produces an apparent persistence or increase in the lifetime of a feature.\nAlthough we also took 5 spectra in 2015, the last spectra used in the FVt-plots are from March 2011, to avoid a 4-year gap. \n\n\\smallskip\n{\\it Upper envelope spectrum:}\\ this was obtained by assigning to each velocity channel the maximum (if  ${>} 3\\sigma$, after resampling to a resolution of 0.3 \\kms) signal detected during our observations (including spectra taken before and after the monitoring period 1990 -- 2011.\nThis 'envelope' represents the maser spectrum if all velocity components were to emit at their maximum level and at the same time.\nSee Fig.~\\ref{fig:uher-upenv} for an example. \n\n\\smallskip\n{\\it Lower envelope spectrum}:\\ as the upper envelope, but obtained by finding the minimum flux density in each velocity channel, setting it to zero, unless it is ${>} 3\\sigma$ (after resampling to a resolution of 0.3 \\kms). \nAn example is shown in Fig.~\\ref{fig:uher-loenv}. \n\n\\smallskip\n{\\it Detection-rate histogram:}\\ this shows the rate-of-occurrence of maser emission above the 3$\\sigma$ noise level for each velocity channel (for 0.3 \\kms\\ resolution), both in absolute numbers (left axis) as in percentage (right axis). This simply counts for each channel the number of times the flux density in the channel is greater than the 3$\\sigma$ noise level of the spectrum.\nAn example is shown in Fig.~\\ref{fig:uher-histo}. \n\n\\smallskip\n{\\it Radio (maser) light curves}\\ are obtained by plotting integrated flux densities versus TJD or versus optical phase. \nThe integrated flux density $S(\\rm tot)$  is determined over a fixed velocity interval encompassing all velocities $V_{\\rm los}$ at which maser emission was detected. \nThe optical phase $\\varphi_{\\rm s}$ is obtained from a fit with a sine-function to the optical light curve (for details see Sect.~\\ref{opt-radio-variability}).\n\n\\begin{figure*}\n\\begin{minipage}[t]{17cm}\n \\begin{minipage}[t]{8.5cm}\n  \\begin{flushleft}\n   {\\includegraphics*[width=8cm,angle=0]{uher-FVt-TJD-100220.jpg}}\n  \\end{flushleft}\n \\end{minipage}\n \\begin{minipage}[t]{8.5cm}\n  \\begin{flushright}\n        {\\includegraphics*[width=8cm,angle=0]{uher-plotcomps-fvt-nostrong-140223.jpg}}\n  \\end{flushright}\n \\end{minipage}\n\\end{minipage}\n\\caption{{\\it Left:} Flux density versus line-of-sight as a function of time (FVt)-plot for U~Her. Each horizontal dotted line indicates an observation (spectra taken within 4 days from each other were averaged). Data are resampled to a resolution of 0.3~\\kms\\ and only emission at levels $\\geq 3\\sigma$ and $\\geq$ 1~Jy is shown. The first spectrum in this plot was taken on 16 February 1990; JD = 2447938.5, TJD = 7938. Last spectrum shown is for 20 March 2011. \\\\{\\it Right:} Spectral components identified by the component fit of the single-dish spectra as listed in  Tables~\\ref{tab:compUHerB-E} and \\ref{tab:compUHerG-M} (see also Sect.~\\ref{sdd_LineProfAna}). The component designations are given above the plot. \nFeatures which have been detected in adjacent spectra, are connected by solid lines.}\n\\label{fig:uher-fvt}\n\\end{figure*}\n\n\\subsection{Velocities and velocity ranges \\label{velodef}}\n\n\\noindent\nIn the following we define velocities and velocity ranges that we shall use in the analysis of the spectra. Only a short description is given here; for a detailed definition we refer to \nPaper II.\nThe observed velocity ranges of the \\water\\ maser emission are analysed in the frame of the 'standard model' for CSEs in evolved stars \\citep{hoefner18}. This model assumes that the stars have radially symmetric outflowing winds, which form a spherical shell of dust and gas around them. \nThe winds are accelerated so that the outflow velocity $V_{\\rm out}$ is increasing with radial distance from the star before it reaches \nthe final expansion velocity $V_{\\rm exp}$.\n\nThe velocity range over which \\water\\ maser emission can be expected is \nconstrained by the velocity ranges given by the OH maser and CO thermal emission. Both species are found beyond the typical \\water\\ maser shells in regions where the wind acceleration has already ceased and the outflow velocity is constant \\citep{hoefner18}. \nThen $V_{\\rm out} \\le V_{\\rm exp}$\nand the observed \\water\\ maser velocities $V_{\\rm los}$  \nare expected in the range \n$V_{\\ast}-V_{\\rm exp} \\le V_{\\rm los} \\le V_{\\ast}+V_{\\rm exp}$.\n\nWe call the blue and red extremes of \nthe observed \\water\\ maser velocity range  $V_{\\rm b}$ and $V_{\\rm r}$, respectively. We use the  detection-rate histogram for the determination of the observed maximum extent of the \\water\\ maser velocity range $\\Delta V_{\\rm los} = V_{\\rm r} - V_{\\rm b}$ (hereafter 'maximum velocity range') valid for the period of observations. \nThis method gives accurate values ($\\approx 0.15$ \\kms) \nfor the maximum velocity range $\\Delta V_{\\rm los}$. In the case of spherical symmetry, we expect that the centre of the \\water\\ maser velocity range is \n$(V_{\\rm b}+V_{\\rm r})/2 = V_{\\ast}$. \n\nOne should note that the velocity range of individual observations and the maximum velocity range do vary with time because of two effects. First, for periods of time the outermost features may fall in brightness below the detection limit leading to an apparent variation of the observed velocity range. And second, maser emission might be excited out to larger/smaller distances \nfor periods of time leading to a real increase/decrease of the maximum velocity range. \n\n\\noindent\n\n\\section{U~Her \\label{sec:uher}}\nU\\,Her is a long-period variable AGB star at a distance of $266^{+32}_{-18}$ pc (Table \\ref{centralcoords}), based on the OH maser parallax measured by \\cite{vlemmings07}. We prefer the distance obtained by radio interferometry, because there is a large difference between the distances measured by the two astrometric satellites Hipparcos ($235^{+58}_{-39}$ pc; \\citealt{vanleeuwen07, vanleeuwen08}) and Gaia EDR3 ($424^{+14}_{-13}$ pc; \\citealt{gaiacol20}), which may be caused by uncertainties introduced by stellar activity on optical parallaxes of nearby AGB stars \\citep{chiavassa18}. Radial velocity determinations of U\\,Her agree within $\\sim$0.5 \\kms\\ centred on $V_{\\ast} = -15.0$ \\kms. The final expansion velocity $V_{\\rm exp}$ in the CSE can be as high as 20 \\kms\\ \\citep{gottlieb22}, but here we use a more conservative value $V_{\\rm exp} = 13.1$ \\kms\\ (see Table \\ref{centralcoords}). \n\nThe \\water\\ maser of U~Her was first detected in 1969 by \\cite{schwartz70b, schwartz70a} as a single feature at $-15$ \\kms\\ (their detection limit was $\\approx$ 10~Jy). Until 1984 the maser was observed several times with detections in the velocity range $-24$ to $-7$ \\kms. \nThe strongest peak was found either at $-15$ or $-17$ \\kms\\ \\citep[and references therein]{engels88}. Interferometric observations were made until the early time of our monitoring program with the VLA in 1983, 1988 and 1990\n\\citep{lane87, bowers94, colomer00} and with MERLIN in 1985 \\citep{yates94}. They found the masers to be located in an unevenly filled ring-like structure with typical inner and outer radii of $\\sim$10 and $\\sim$20 AU, respectively. \n\n\n\\begin{figure}\n\\resizebox{9cm}{!}{\\rotatebox{270}{\n\\includegraphics\n{uher-upenv-041018.jpg}}}\n\\caption{Upper envelope spectrum for U~Her; 1987-2015.}\n\\label{fig:uher-upenv}\n\\end{figure}\n\n\\begin{figure}\n\\resizebox{9cm}{!}{\\rotatebox{270}{\n\\includegraphics\n{uher-loenv-041018.jpg}}}\n\\caption{Lower envelope spectrum for U~Her; 1987-2015.}\n\\label{fig:uher-loenv}\n\\end{figure}\n\n\\subsection{Single-dish data}\n\\subsubsection{\\label{sdd_MaserSpec} Variations in brightness of the \\water\\ maser profile}\nOur observations of U~Her cover more than 28 years, from March 1987 to October 2015. Contiguous monitoring was made between 1990 and 2011 with typically 5--6 observations per year. Depending on telescope (i.e. Effelsberg or Medicina), date of observation and integration time, the rms sensitivity of the observations was very inhomogeneous ranging from 0.1 to $\\sim$4 Jy, depending on resolution and date. At the 0.3~\\kms\\ resolution used here, after mid-1991, with few exceptions all rms were $<$ 1.0~Jy. All 137 spectra taken are shown in the Appendix (Fig.~\\ref{fig:uher_all}). Sample spectra showing typical profiles are given in Fig. \\ref{fig:uher_sel}. \n\nA general view of the properties of the profile variations is given in the FVt plot (Fig. \\ref{fig:uher-fvt}, left panel) covering the years 1990 -- 2011. The profile is usually dominated by emission  in the velocity range $-16 < V_{\\rm los} < -14$ \\kms\\ close to the stellar radial velocity $V_\\ast = -15.0$ \\kms, while in the outer parts of the profile  ($V_{\\rm los} < -18$ and $> -14$ \\kms) the emission is much weaker and at $V_{\\rm los} > -14$ \\kms\\ appeared more or less regularly only around the maximum of the periodic stellar light variations. \nIn the velocity range $-18 < V_{\\rm los} < -16$ \\kms\\ emission was generally present but never dominating the profile; the peak at TJD = 8681 (29 February 1992) is caused by a spectral component at $-18.2$ \\kms, just outside this\nrange (component \\Dtwo\\ in Table \\ref{tab:components}).\nThe plot clearly demonstrates that the maser emission is responding in strength to the periodic variability of the star. \nThere is also an apparent broadening of the profile at regular time intervals, likewise connected to the pulsational period of the star. As will be shown in Section \\ref{opt-radio-variability}, the radio emission varies with the optical period but is lagging behind the optical one by about three months. In 1987 and 2015 the profiles were similar to those in 1990--2011 but the emission in the outer parts of the profile was not detected (Fig. \\ref{fig:uher_all}, Appendix C).\n\nOn top of the regular component of variability, non-regular flux density variations of individual maser features occurred, which led to strong profile variations over the years. This is exemplified by the upper envelope spectrum (Fig. \\ref{fig:uher-upenv}), where the strongest feature is at $-18.3$ \\kms. This feature was strong for about 18 months between January 1991 (TJD $\\sim$8250) and July 1992 (TJD $\\sim$8800) (Fig.~\\ref{fig:uher_sel} and Fig.~\\ref{fig:uher_all}; Appendix C). In the following this period will be referred to as the '1991/1992 peculiar phase'. The feature brightened again in autumn 1996 (TJD = 10352) for less than a year. No comparable brightenings were observed redwards of $-14$ \\kms. In contrast, the second prominent feature in the upper envelope spectrum at $-15$ \\kms\\ was permanently present, even in 1987 and 2015 prior to and after the phase of contiguous observations (see the lower envelope spectrum, Fig. \\ref{fig:uher-loenv}). \nThe emission close to the borders of the velocity range at $V_{\\rm los} < -20$ and $V_{\\rm los} > -12$ \\kms\\ (cf. also Fig. \\ref{fig:uher_sel}) is usually weak and becomes strong only occasionally. After 1996 (TJD $\\ga$ 10500) the blue-shifted emission at velocities $V_{\\rm los} < -18$ \\kms\\ faded away and after 2003 (TJD $\\ga$13000) it was not detected anymore by us. The long-term brightness variations are reflected in the FVt-plot (Fig. \\ref{fig:uher-fvt}) as prominent asymmetry in the observed velocity range over time.\n\n\\begin{figure}\n\\resizebox{9cm}{!}{\\rotatebox{270}{\n\\includegraphics\n{uher-histo-220119.jpg}}}\n\\caption{Detection rate histogram for U~Her; 1987-2015.}\n\\label{fig:uher-histo}\n\\end{figure}\n\n\\subsubsection{\\label{sdd_MaserVel} Variations of the \\water\\ maser velocity range}\nThe detection rate histogram (Fig. \\ref{fig:uher-histo}) confirms that the dominant spectral features occurred between $-16$ and $-14$ \\kms. It also shows that the total velocity range over which emission was detected is $-23.3 < V_{\\rm los} <  -7.1$ \\kms\\ (Table \\ref{centralcoords}), which is symmetric with respect to the stellar radial velocity. The FVt-plot shows also that the width of the observed velocity range is varying. This is caused by the  drop of the maser brightness at the weaker outer parts of the maser profile below the threshold of the FVt-plot ($\\sim$1 Jy) during the faint part of the stellar variability cycle. \n\nThe blue border of the \\water\\ maser profile of U~Her had been a point of discussion in the past, after  emission had been detected at velocities $\\sim$2 \\kms\\ bluewards of the velocity range covered by the OH 1667 MHz maser emission and other molecular species \\citep{engels88,bowers94}.  However, given the final expansion  velocity as obtained from more recent CO observations (see Table 1) the extreme blue \\water\\ maser velocities at $<-23$ \\kms\\ \\citep{engels88}, seen before the start of our observations are not 'forbidden' by the 'standard model' anymore, and instead asymmetries in the OH maser shell could be responsible for the lack of OH maser emission at very blue velocities. \n\n\n\\subsubsection{Periodicity in the optical and radio light curves\n\\label{opt-radio-variability}}\nAs is evident from the FVt-plot (Fig. \\ref{fig:uher-fvt}), the \\water\\ maser variations of U~Her show periodic behaviour, which is caused by the maser's strong response to the stellar brightness variations. \n\nWe created the radio light curve of U~Her using the integrated flux density determined over a fixed velocity interval encompassing all velocities at which maser emission was detected. The optical data (V-band) were taken from AAVSO (Kafka, 2021\\footnote{Observations from the AAVSO International Database, https://www.aavso.org}) for the years 1986--2015 encompassing the monitoring program and consisted of $>$2400 observations, while the radio data consisted of 137 observations. For both data sets a Fourier analysis was made to search for periodicity. The Lomb periodogram \\citep{press92} of the optical data showed a well defined period $P_{\\rm opt} = 405\\pm2$ days, in agreement with the VizieR\\footnote{Ochsenbein F., et al., The VizieR database of astronomical catalogues, DOI = 10.26093/cds/vizier; \\cite{ochsenbein00}}\nperiod of 406 days. The periodogram of the radio light curve confirmed the optical period ($P_{\\rm rad} = 407$ days; Table \\ref{centralcoords}), albeit with much larger uncertainties. \nTo analyse the maser variations in relation to the optical variations of the star, we modelled in the following the optical and maser light curves by sine-waves with a common period and related the model light curves to each other. \n\n\\subsubsection{The model for the optical light curve \\label{sdd_OptModelLcurve} }\nThe \\water\\ maser variations are not in phase with the optical variations. It is well-established that for Mira variables they lag behind several weeks to months \\citep{staley94, berulis98, shintani08}. To study this behaviour quantitatively we set the optical reference phase $\\varphi_{\\rm s} = 0$ at the maximum of the optical model sine curve. These maxima are delayed in the mean by $25\\pm10$ days relative to the real optical maxima ($\\Delta\\varphi_{\\rm s} = 0.06\\pm0.025$ in units of phase). The delay is caused by the asymmetry of the optical light curve of U~Her with a steeper rise to the maximum and a slower decline to the minimum and the scatter is due to the varying time differences between two real optical maxima. We found $\\Delta T = 406 \\pm 15$ days as the average time difference between two consecutive maxima, with extreme time differences of 374 and 426 days. The choice to link the optical phase to the model sine curve is therefore the only way to define an optical reference phase independent from the details of the optical light curve or the choice of the time interval over which the light curve is analysed. Adopting this approach, radio-optical phase lags can be compared between stars having different quality of the sampling of their optical light curves. Using for example the {\\it mean} time difference between the observed optical maxima as reference is an alternative way to determine the delay $\\Delta\\varphi_{\\rm s}$, but this method would be restricted to stars where the optical maxima are well observed. Our optical model light curve has a period $P_{\\rm opt} = 405$ days and a reference epoch for maxima TJD$_{max} = 6668 \\pm 3$ days (Table~\\ref{centralcoords}). \n\n\\begin{figure}\n\\includegraphics[angle=-90,width=\\columnwidth]\n{uher_lcurve_2.jpg}\n\\caption{U~Her \\water\\ maser light curve. Plotted are integrated fluxes $S(\\rm tot)$ in the velocity range $-24 < V_{\\rm los} <-6$ \\kms\\ in Jy \\kms\\ vs. optical phase   $\\varphi_{\\rm s}$. For better visualization the data are repeated for a second period. $\\varphi_{\\rm s} = 0$ is defined as the time of maximum optical brightness. Datapoints marked by an asterisk (*) are from Effelsberg, the plusses (+) are Medicina data. Overplotted are average integrated fluxes in phase bins of 0.1 (red), and a sine curve (blue) which was obtained by a fit to the 1990--2011 radio measurements with a period of $P_{\\rm opt}=405$ days. The sine curve is delayed by $\\phi_{\\rm lag} = 0.16$, i.e. by 64 days with respect to the optical maximum.} \n\\label{fig:uher-lcurve}\n\\end{figure}\n\n\\subsubsection{The phase lag between optical and radio light curve \\label{sdd_phase-lag} }\nThe lag of the radio light curve relative to the optical one was determined with the fit of a sine curve to the radio data using the optical period, and the amplitude as free parameter. Only radio observations during the continuous monitoring between 1990 and 2011 were used, and observations taken within 3 days were averaged. The final data-set to determine the radio light curve consisted of 125 maser spectra. The resulting lag of $\\phi_{\\rm lag} = 0.16$ (Table \\ref{centralcoords}) is only weakly depending on the choice of the amplitude. The radio light curve is shown in Fig.~ \\ref{fig:uher-lcurve} as a function of the optical phase $\\varphi_{\\rm s}$. It is immediately clear that the scatter in integrated fluxes $S$ is large for any particular phase, indicating that the luminosity variations of the star can explain only part of the maser variability seen. To visualize the periodic component of the variations we overplotted a binned light curve (average integrated fluxes in bins of 0.1 in phase) and the sine curve obtained from the fit using the optical period. In Fig.~ \\ref{fig:uher-lcurve} the integrated flux densities $S(\\rm tot)$ obtained with the Effelsberg telescope appear to be systematically brighter than those obtained with the Medicina telescope. This is a selection effect caused by the general brightness decrease of U\\,Her's maser light curve (see Fig.~\\ref{fig:uher-stot-tjd}) and the limitation of the Effelsberg observations to the first years. This leads to a fraction of observations during bright maser phases being significantly higher for the Effelsberg than for the Medicina radio telescope.\n\n\n\\begin{figure}\n\\includegraphics[angle=-90,width=\\columnwidth]\n{uher-Stot-TJD-exp-270923.jpg}\n\\caption{U~Her \\water\\ maser light curve, showing the total flux (integrated between $V_{\\rm los}$ $-24$~\\kms\\ and $-6$~\\kms) as a function of TJD. The vertical dashed lines indicate the (modelled) optical maxima with $P = 405$~days.\n} \n\\label{fig:uher-stot-tjd}\n\\end{figure}\n\n\\begin{figure}\n\\includegraphics[angle=-90,width=\\columnwidth]\n{uher-Sratio-blue-red-exp-270923.jpg}\n\\caption{The ratio $S(\\rm blue)$/$S(\\rm red)$ of the U~Her \\water\\ maser emission, of the $V_{\\rm los} < -15$~\\kms\\ [$S(\\rm blue)$] and $> -15$~\\kms\\ [$S(\\rm red)$] part of the maser velocity range with respect to the stellar velocity, as a function of TJD. The vertical dashed lines indicate the (modelled) optical maxima with $P = 405$~days.\n} \n\\label{fig:uher-sratio-tjd}\n\\end{figure}\n\n\\subsubsection{\\label{sdd_longterm_lc} The long-term radio light curve}\nIn addition to the periodic maser brightness variations, additional brightness changes are seen also on timescales shorter and longer than the stellar period. \n\nIn Fig.~\\ref{fig:uher-stot-tjd} we plot the total flux of the U~Her \\water\\ maser as a function of time between 1990 and 2015. As is evident also here, the variations in the maser emission follow the optical variations of the star, indicated by the dashed lines that mark the TJD of the (modelled) stellar maxima (see Sect.~\\ref{sdd_OptModelLcurve}). Although the dominance of the emission in the $-16$ to $-14$ \\kms\\ velocity interval after 1992 suggests some long-term continuity, this continuity is restricted to velocities and not to brightness levels. The radio light curve shown in Fig.~\\ref{fig:uher-stot-tjd} indicates a clear decrease by a factor of 4 of the average brightness level  between 1990 ($S(\\rm tot) \\sim 200$ Jy \\kms) and 2011 ($S(\\rm tot) \\sim 50$ Jy \\kms). In 2015 the brightness level had increased again to ($S(\\rm tot) \\sim 125$ Jy \\kms), while in April 1984 the total flux was 185 Jy \\kms\\ \\citep{engels88}. The strong emission in February 1992 during the ’1991/1992 peculiar phase’ (see Sect.~\\ref{sdd_MaserSpec}) could have been a burst. \nAfter this phase emissions at $V_{\\rm los} < -$17 \\kms\\ dropped sharply and the total flux went through a weak phase lasting until 1995, when a brightness increase of the $-15.5$ \\kms\\ feature brought the total flux back to a level following the long-term decline of the average brightness.\\\\\n\nThe 1990--2011 long-term brightness decrease is not uniform over the velocity range, but due to a systematic brightness decrease of the emission that is blue-shifted with respect to the stellar velocity ($V_*$ = $-$15 \\kms). As shown in Figure~\\ref{fig:uher-sratio-tjd} the ratio $S(\\rm blue)$/$S(\\rm red)$ between the blue- and red-shifted total flux is continuously decreasing between $\\sim$1992 and $\\sim$2007. During the ’1991/1992 peculiar phase’ $S(\\rm blue)$ was $\\sim$15 times stronger than $S(\\rm red)$, while in 2007/2008 the ratio could be as small as $\\sim$0.5. The strength of the red-shifted emission in the period 2007 -- 2011 (TJD $>$ 13500) appears to be due to the shift of the peak emission in the  $-16$ to $-14$ \\kms\\ velocity interval by $< 1$~\\kms\\ to the red (see the FVt-diagram,  Fig.~\\ref{fig:uher-fvt}, left). The choice of the stellar radial velocity influences the ratio quantitatively but its trend remains for any radial velocity within the dominant $-16$  to $-14$ \\kms\\ interval. The $S(\\rm blue)$/$S(\\rm red)$ ratio and its variation indicate an asymmetry of the excitation conditions in the front part of the \\water\\ maser shell of U~Her, where the blue-shifted emission comes from, compared to the rear part, where the red-shifted emission originates. \n\nThe radio light curve (Fig. \\ref{fig:uher-stot-tjd}) shows three rather bright maxima compared to the times before and after. They are the possible burst in the  ’1991/1992 peculiar phase’ (peak emission at TJD = 8682), the maximum in 2000 (peak on TJD = 11640) and the maximum in 2007 (peak on TJD = 14389). We consider them as short-term fluctuations rather than as evidence of 'super-periodicity', because the time intervals between the peaks with a duration of 6.8 and 7.3 stellar cycles do not match. The next maximum would have been expected in April - September 2015. We have observations in this time interval, but no information on the brightness levels before and after. It is therefore not possible to decide if the brightness levels observed in 2015 belong to a local maximum or are part of a general increase of the brightnesses.\n\nBesides our monitoring program, U~Her's \\water\\ maser has been observed with single-dish telescopes only occasionally by other groups. Excluding the ’1991/1992 peculiar phase’, the strongest maser feature was consistently reported at $\\sim -14.5$ \\kms\\ by \\cite{comoretto90} for March 1987, by \\cite{kim10} for June 2009, and by \\cite{neufeld17} for May 2016. In 1991 the strongest peaks were at $-16$ \\citep{takaba94} and $-19$ \\kms\\ \\citep{takaba01} in accordance with our observations.\n\n\n\\subsubsection{\\label{sdd_LineProfAna}  Velocity variations of individual \\water\\ maser features}\nBesides the regular periodic and long-term brightness variations of U~Her's \\water\\ maser emission, also small changes in velocity of the maser features are apparent in the FVt-plot (Fig. \\ref{fig:uher-fvt}). For their analysis we decomposed the \\water\\ maser spectra 1987 -- 2015 into separate features by fitting multiple Gaussian line profiles. The details of the fitting technique are described in Paper I. These maser features can be traced over some period of time in several consecutive spectra, fade away, and may reappear at later times perhaps with a slightly different velocity. As in the semi-regular variable stars (Papers I and II), the full width at half maximum (FWHM) of strong features (visible as distinct peaks in the spectra) is $\\sim$1~\\kms, and therefore features with FWHM $\\ga$ 2~\\kms\\ are most probably blends. \nWe assume that maser features in adjacent (in time) spectra with velocity differences $\\la$0.5~\\kms\\ belong to a unique emission region in the \\water\\ maser shell, which persisted over this period of time (i.e. the time between the two observations) and varied in intensity.\n\nFor accounting purposes all spectral features were grouped according to their velocities into {\\it maser spectral components}. The assignment of the features to the spectral components in the four velocity intervals ($< -18$, $-18$ to $-16$, $-16$ to $-14$ and $>-14$ \\kms\\ as introduced in Sect. \\ref{sdd_MaserSpec}) is discussed in Appendix A. Tables \\ref{tab:compUHerB-E} and \\ref{tab:compUHerG-M} list the spectral features identified by the fitting procedure and their assignments.\n\nThe spectral components are labeled with capital letters A, B, ... M in order of increasing velocity $V_{\\rm los}$. Their labeling is synchronized with the labels of the spatial components (to be introduced in Sect. \\ref{id_spatial_c}), so that corresponding spectral and spatial components share the same label. Due to strong blending in velocity space, in each of the four velocity ranges only few (one to four) spectral components could be defined. In total we identified eleven spectral components. Not all spatial components could be identified in the single-dish spectra, especially not the fainter ones, and therefore there are no spectral components matching the spatial components A1, F1+F2, H1+H2, and J1+J2 (cf. Table \\ref{tab:components} in Sect. \\ref{id_spatial_c}). \nTwo spectral components (D at $\\sim-18.5$ \\kms\\ and G at $\\sim-15.0$ \\kms) are obvious blends with velocity separations less than the FWHM of the features. In Tables \\ref{tab:compUHerB-E} and \\ref{tab:compUHerG-M}   the subcomponents making up the spectral components D and G were labeled \\Done, \\Dtwo\\ and \\Gone, \\Gtwo\\ respectively. \n\nThe maser spectral components identified in individual spectra (Tables \\ref{tab:compUHerB-E} and \\ref{tab:compUHerG-M}) are graphically displayed in Fig. \\ref{fig:uher-fvt} (right panel), where it can be compared directly with the FVt-plot. Often, changes of the spectral component peak velocities $V_{los}$ (taken from Tables~\\ref{tab:compUHerB-E} -- \\ref{tab:compUHerG-M}) in all four velocity ranges occur on timescales of many months by more than $0.5$~\\kms, although not in a systematic way. An example are the peak velocities of spectral component \\Gone\\ in 2007 -- 2011 (TJD $\\ga 14000$) with velocities $-16.0 \\le V_{los} \\le -14.8$. We interpret the meandering of the velocities as a superposition of blended maser features varying in brightness asynchronously and coming perhaps over some time from different locations.  \n\nThe interpretation of the short-term velocity variations is less ambiguous in the $>-14$ \\kms\\ velocity range, where fewer spectral features are apparent and therefore blending is less of a problem.  \nComponent I shows evidence for blending between 1990 and 1996 ($\\sim 7900 <$ TJD $< \\sim 10200$) with peak velocities varying back and forth by almost 1 \\kms\\ ($-13.7$ to $-12.7$ \\kms; see Table \\ref{tab:compUHerG-M}), \nwhile the velocity remained almost constant at $-12.9$ \\kms\\ thereafter until TJD $\\sim$ 14500. It then reappeared at the same velocity in 2015. Component K has a peak velocity of $\\sim-11$ \\kms\\ and was detectable only after 1994 (TJD $>$ 9750), also without significant velocity variations. Component L was seen only in two epochs 1990--1992 (TJD $<$ 8900) at $-10.2$ \\kms\\ and 2007--2010 (14000 $<$ TJD $<$ 15300) at $-9.7$ \\kms, and it is unclear if the emissions in these two epochs are related to each other. Finally, component M at $\\sim-8$ was only seen at the beginning of the monitoring program, in parallel to component L (1990-1992), while the maser emission in U~Her was strong over the full profile. There were too few appearances to draw conclusions on its velocity variations.   \n\nIn the velocity range covered by components I to M we would expect shifts of increasing velocity (components becoming redder),\nif the emission regions would persist and move with the expanding CSE. This is not the case here, so that the emission of these components must have come from different emission clouds in the course of the monitoring period. \nThe absence of long-term velocity shifts of the spectral components will be discussed further in Sect. \\ref{region-lifetime}. \n\n\\begin{figure*}\n\\centering\n\\resizebox{17cm}{!}{\n\\includegraphics{uherfeb90cont.jpg}}\n\\caption{Sample \\water\\ maser images of U~Her from February 1990 covering the velocity range between $-16$ and $-14$ \\kms\\ with the strongest emission. The synthesised FWHM beam size (major axis: 0\\pas09; minor axis: 0\\pas08; position angle of the major axis: 71\\degr) is shown in the left panel  (-15.9~\\kms). The images are oriented along right ascension and declination and the angular scales are relative to the position \\recta{16}{25}{47}{39}, \n\\dec{+18}{53}{32.9} (J2000).  Brightness contours are -0.25 (dashed contour), 0.25, 2.5, 5, 50, and 150~Jy per beam area (1.4 \\powerten{-13}~sr). }\n\\label{fig:channel_maps}\n\\end{figure*}\n\n\\subsection{Interferometric data }\nThe VLA observations of the \\water\\ masers in U~Her were made with the aim to identify the emission sites in the CSE and breaking the spatial degeneracy in the single dish data. Due to the limited spatial resolution this was only partially successful. The size of the emission sites is not specified a priori, but we refer to the most compact gas clumps hosting maser emission as 'maser clouds'. The data analysis of the images yields maser spatial components, which in general will be superpositions of several maser clouds close to each other in space as well as in velocity.\n\n\n\\begin{table}[!t]\n\\caption[]{\\label{tab:components} Spatial (\"Spat\") and spectral (\"Spec\") components of U~Her 1990 -- 1992.}\n\\begin{tabular}{rlrrrrl}\n\\hline\\noalign{\\smallskip}\nSpat & Date & $V_{\\rm los}$  & S$_p$ & Xoff & Yoff & Spec \\\\\n &      & [km/s] & [Jy] & [mas] & [mas] &  \\\\\n\\noalign{\\smallskip}\\hline\\noalign{\\smallskip}\nA1 & 91 Oct. & $-$23.8 & 0.4    &  +26  &  +30  &  --  \\\\[0.1cm]   \nB1 & 90 Feb. & $-$21.5 & 2.9    &  +59  &  +32  &  B   \\\\        \nB1 & 90 Jun. & $-$21.2 & 0.1    &  +21  &  +6   &      \\\\[0.1cm] \nB2 & 91 Oct. & $-$22.0 & 25.0   &  $-$44  &  $-$6   &  B    \\\\[0.1cm]  \nC1 & 90 Feb. & $-$19.9 & 1.5    &  $-$33  &  $-$48  &  C   \\\\        \nC1 & 91 Oct. & $-$19.9 & 41.0   &  $-$38  &  $-$54  &      \\\\        \nC1 & 92 Dec. & $-$20.2 & 0.1    &  $-$56  &  $-$54  &      \\\\[0.1cm]  \nC2 & 90 Feb. & $-$19.9 & 0.6    &  +45  &  +42  &      \\\\[0.1cm]  \nC3 & 91 Oct. & $-$19.2 & 5.0    &  $-$46  &  +58  &      \\\\[0.1cm] \nD1 & 90 Feb. & $-$18.2 & 52.0   &  $-$9   &  $-$60  &  \\Dtwo  \\\\        \nD1 & 90 Jun. & $-$18.1 & 7.3    &  $-$9   &  $-$60  &      \\\\        \nD1 & 92 Dec. & $-$18.2 & 3.0    &  $-$8   &  $-$68  &      \\\\[0.1cm] \nD2 & 90 Feb. & $-$17.9 & 12.0   &  $-$25  &  +46  &   \\Dtwo    \\\\        \nD2 & 90 Jun. & $-$17.9 & 2.5    &  $-$27  &  +46  &      \\\\\nD2 & 91 Oct. & $-$17.9 & 5.0    &  $-$20  &  +52  &      \\\\[0.1cm] \nE1 & 90 Feb. & $-$17.2 & 0.8    &  +61  &  +62  &      \\\\[0.1cm] \nE2 & 90 Feb. & $-$16.9 & 2.4    &  $-$63  &  $-$36  &      \\\\[0.1cm] \nE3 & 90 Feb. & $-$16.9 & 10.0   &  $-$1   &  $-$58  &  E   \\\\        \nE3 & 91 Oct. & $-$17.2 & 12.0   &  $-$32  &  $-$36  &      \\\\        \nE3 & 92 Dec. & $-$17.2 & 2.0    &  $-$8   &  $-$60  &      \\\\[0.1cm] \nE4 & 90 Jun. & $-$16.6 & 2.7    &  $-$37  &  +10  &      \\\\[0.1cm] \nE5 & 91 Oct. & $-$16.6 & 1.8    &  $-$44  &  +60  &      \\\\        \nE5 & 92 Dec. & $-$17.2 & 3.5    &  $-$34  &  +66  &      \\\\[0.1cm] \nF1 & 90 Feb. & $-$15.9 & 5.0    &  $-$25  &  +42  &      \\\\        \nF1 & 91 Oct. & $-$15.9 & 2.2    &  $-$48  &  +30  &      \\\\[0.1cm]         \nF2 & 92 Dec. & $-$15.9 & 1.5    &  +24  &  +32  &      \\\\[0.1cm] \nG1 & 90 Feb. & $-$14.9 & 230.0  &  +15  &  $-$54  &  \\Gone + \\Gtwo   \\\\        \nG1 & 90 Jun. & $-$15.0 & 145.0  &  +15  &  $-$52  &   \\\\ \nG1 & 92 Dec. & $-$15.3 & 14.0   &  +14  &  $-$46  &      \\\\[0.1cm] \nG2 & 90 Jun. & $-$14.9 & 5.0    &  $-$21  &  +8   &      \\\\[0.1cm]         \nG3 & 92 Dec. & $-$15.5 & 4.0    &  $-$74  &  $-$10  &      \\\\[0.1cm] \nG4 & 92 Dec. & $-$15.4 & 3.0    &   +8  &  +42  &      \\\\[0.1cm] \nH1 & 90 Feb. & $-$14.3 & 5.0    &  $-$43  &  +32  &      \\\\        \nH1 & 90 Jun. & $-$13.9 & 3.8    &  $-$51  &  +32  &      \\\\        \nH1 & 92 Dec. & $-$13.9 & 0.7    &  $-$76  &  +10  &      \\\\[0.1cm]   \nH2 & 91 Oct. & $-$14.5 & 0.3    &  $-$44  &  +42  &      \\\\        \nH2 & 92 Dec. & $-$13.9 & 1.0    &   $-$4  &  +54  &      \\\\[0.1cm] \nI1 & 90 Feb. & $-$12.9 & 1.0    &  +49  &  $-$14  &  I   \\\\        \nI1 & 90 Jun. & $-$13.3 & 0.8    &  +31  &  $-$4   &      \\\\[0.1cm]  \nI2 & 90 Feb. & $-$12.6 & 1.0    &  $-$35  &  +44  &      \\\\[0.1cm] \nJ1 & 90 Jun. & $-$12.0 & 0.3    &  $-$29  &  +34  &      \\\\         \nJ1 & 92 Dec. & $-$12.0 & 0.1    &  $-$28  &  +74  &      \\\\[0.1cm] \nJ2 & 92 Dec. & $-$11.6 & 0.1    &  $-$34  &  $-$28  &      \\\\[0.1cm] \nL1 & 90 Feb. & $-$10.0 & 1.4    &  $-$67  &  +4   &  L    \\\\       \nL1 & 90 Jun. & $-$10.0 & 0.8    &  $-$67  &  +6   &      \\\\[0.1cm] \nM1 & 90 Feb. & $-$7.7  & 0.3    &  $-$43  &  +36  &  M   \\\\        \nM1 & 90 Jun. & $-$8.0  & 0.2    &  $-$49  &  +32  &      \\\\[0.1cm] \nM2 & 92 Dec. & $-$8.3  & 0.1    &  $-$20  &  $-$10  &      \\\\\n\\noalign{\\smallskip}\\hline\n\\end{tabular}\n\\end{table}\n\n\n\\subsubsection{\\label{id_spatial_c} Spatial component identification}\nThe VLA interferometric data consist of one data cube for each of the four epochs, containing 63 channel maps each.  The maps are separated in velocity by 0.658~\\kms. An example is given in Fig. \\ref{fig:channel_maps} where maps of the 3 channels with the strongest emission seen in the first VLA epoch (February 1990) are shown. The sensitivities measured in a line-free channel were 12--21 mJy/beam. In order to single out maser components the data cubes were analysed within AIPS\\footnote{Astronomical Image Processing System,\\\\ www.aips.nrao.edu/index.shtml.} in a three step process. First, the individual channel maps were analysed one by one by fitting multiple 2D Gaussians, then spatial components were identified by comparing the fit results in neighbouring channel maps, and finally these components were verified in velocity space. \nThe details of this analysis are described in Paper I. \n\nAbout twelve spatial maser components were identified in each VLA observing epoch. These components are listed in Table \\ref{tab:components}, which gives the spatial component (Spat), the VLA observing epoch, the line-of-sight velocity $V_{\\rm los}$ and peak flux density $S_{\\rm p}$ of the spatial components identified, the spatial offsets (Xoff and Yoff) from the adopted map centre (defined in Sect. \\ref{map-alignment}) and the associated (single dish) spectral component (Spec) from Tables \\ref{tab:compUHerB-E} and \\ref{tab:compUHerG-M} in the Appendix. Spectral component B had different spatial counterparts in 1990 and 1991. Spectral component \\Done\\ is not listed as it appeared only after 1990--1992. Spatial component G1 is likely of composite nature, as the corresponding spectral component G is according to our analysis of the spectral profiles made up by two components (\\Gone\\ and \\Gtwo\\ in Table~\\ref{tab:compUHerG-M}). \n\nFor the epoch June 1990 the components can be compared to those found by \\cite{colomer00}, who used the VLA to observe U~Her one day apart from our observation. As in the case of RX~Boo (see Paper I) they found about the same number of components (13 vs. 12). However, their components are spread over a smaller velocity range of $-18.3 \\le  V_{\\rm los} -10.2$ \\kms, because they did not detect the faint components B1 ($V_{\\rm los} = -21.2$ \\kms, $S_\\nu = 0.1$ Jy) \nand M1 ($V_{\\rm los} = -8.0$ \\kms, $S_\\nu = 0.2$ Jy) (cf. Table~\\ref{tab:components}).\nOur strongest spatial component in June 1990, G1 ($V_{\\rm los} = -15.0$ \\kms) is split by the 3-dimensional Gaussian fitting program of Colomer et al. into three spatial components in the velocity range $-15.3 < V_{\\rm los} < -14.6$ \\kms. This corroborates our conclusion that G1 is of composite nature. Common components in both June 1990 maps are present outside the very crowded main velocity range $-16 < V_{\\rm los} < -14$ \\kms, if flux densities surpassed 1 Jy, whereas weaker spatial components \nwere not recognized by the fitting program of Colomer et al.. As discussed in Paper I, the two fitting methods lead to different results for weaker components and regions of high spatial blending. The overall spatial distributions of both maps is however similar, so that the projected angular shell sizes are similar. \n\n\\subsubsection{Alignment of the maps \\label{map-alignment}}\nThe maps taken between 1990 and 1992 were aligned to a common origin using spatial components present over two or more observing epochs and assuming that the components are located in a ring-like structure around the star. Matched components are given a common designation in Table \\ref{tab:components}. For example at $V_{\\rm los} \\approx -20.0$ \\kms\\  the strong C1 spatial component seen in October 1991, is identified in February 1990 and December 1992 as a weak component, while other spatial components (C2, C3) identified at (or close to) this velocity are clearly coming from different parts of the shell.  \nAs in the case of RX~Boo (Paper I), the 1990 maps had many components in common, while components in 1991 and 1992 were difficult to identify with components seen in the other years. The identification was further complicated by the poor east-west resolution in October 1991, and the ’1991/1992 peculiar phase', in which the masers were at that time. As discussed in Sect. \\ref{sdd_MaserSpec}, the maser emission at $V_{\\rm los} \\leq -18$ \\kms\\ was prominent around the turn of the year 1991/1992, while in other epochs this emission was relatively weak and emission from the  $-16 < V_{\\rm los} < -14$ \\kms\\ velocity range prevailed. In October 1991 spatial components B2 and C1 were strongest (Table \\ref{tab:components}), while the strongest component during the other three epochs (G1) could not be identified. \nThe components used to align the December 1992 map with the maps from 1990 were D1 and G1, which were strong in both years. C1 and D2 were used to align the October 1991 map. After alignment these components scattered in position by $\\le12$ mas.\n\nA plot of all spatial components on the sky relative to a common origin as given in Table \\ref{tab:components} is shown in Fig. \\ref{fig:uher-rainbow}\\footnote{Note that Fig.~2 in \\cite{winnberg11} erroneously shows the mirror image of this distribution.}.  The distribution of the components suggests a ring-like structure. To find the most likely position of the star, a circle was fitted according to a least-squares method to all components having radial velocities between $-18$ and $-14$ \\kms. They were considered as being likely 'tangential components', able to outline the ring-like structure of the projected shell. The fit was carried out without weights and the center of the circle was used as our best guess for the stellar position, and as common origin of the plot and of the component offsets in Table \\ref{tab:components}. The best fit gave a radius for the circle of 57 mas ($\\sim15$ AU).\n\nSpatial coincidences among other components were searched for in the aligned maps.  Coincident spatial components detected in different epochs were given a common label. After subtraction of coincident components we ended up with 28 different spatial components (hereafter 'merged spatial components') of which half were present in at least two maps. Positional deviations between maps were $\\la15$~mas, although in a few ambiguous cases we accepted as coincidences also components with deviations up to 45~mas (cf. B1, H2, J1 in Table \\ref{tab:components}). \nThe number of merged spatial components found and the accuracies in velocities and positions are very similar to the results obtained for the SRV RX~Boo in Paper I. \n\n\\begin{figure}\n\\includegraphics[width=\\columnwidth]\n{fig10-jb-OK-nogrid.jpg}\n\\caption{All the spatial components of U Her listed in Table 3 plotted on the sky. Each component is represented by a symbol surrounded by a circle with a diameter $d$ depending on flux density $S_\\nu$: $d = 8\\,({\\rm log}\\,S_\\nu  + 1.3)$. \nThe dates are represented by different symbols: Feb. 90 by small circles; Jun. 90 by asterisks; Oct. 91  by plus signs; Dec. 92 by crosses. The circles around the components are colour coded according to the line-of-sight velocity of the component (see the scale below the map). The dashed circle with a radius of 57~mas has been obtained from a fit to the components with line-of-sight velocities $-18 < V_{\\rm los} < -14$ \\kms\\ (see text) and the origin of the plot has been moved to the center of this circle. The filled black circle at the center symbolizes the central star with a diameter of 10.65~mas \\citep{vanbelle96}.} \n\\label{fig:uher-rainbow}\n\\end{figure}\n\n\\subsubsection{Cross correlation of single-dish and interferometric data 1990–1992}\n The assignment in Table \\ref{tab:components} of spectral components identified in Sect. \\ref{sdd_LineProfAna} to the spatial components identified in Sect. \\ref{id_spatial_c} was made using the velocities in common. Spatial component A1 detected in October 1991 at $-23.8$ \\kms\\ with a peak flux density of 0.4 Jy was not present in any of our spectra. Its velocity is lower by 0.5 \\kms\\ than the blue border of the \\water\\ maser velocity range that we determined in Sect. \\ref{sdd_MaserVel} from the single-dish spectra. Due to blending in velocity space also other weaker spatial components ($\\le 10$ Jy: spatial components F, H, J) could not be assigned to individual spectral components.\nAn exception are the spatial components M1 and M2 at $\\sim-8$ \\kms\\ in the extreme red part of the velocity range. Their emission could be detected in the spectra due to absence of  stronger maser emission at neighboring velocities. \nThe brighter spectral components \\Done\\ and K at $-18.9\\pm0.2$ and $-10.8\\pm0.3$ \\kms\\ respectively (see Table \\ref{tab:compUHerB-E} and \\ref{tab:compUHerG-M}) were not seen in our spectra before 1993. Accordingly, spectral component \\Done\\ was not assigned to any spatial component, and spectral component K is absent from Table \\ref{tab:components} because no emission was seen at the corresponding velocities in the VLA maps 1990 -- 1992. Spatial component G1 is a blend of two emission sites, which could be identified as sub-components \\Gone\\ and \\Gtwo\\ of spectral component G in the single-dish spectra due to their superior velocity resolution.\n\nFor brighter spatial/spectral components the cross-correlation was not unambiguous at several velocities, due to blending in velocity and position. One case is spectral component B with peak velocities $-21.1\\pm0.5$ \\kms, which\nwas the strongest in the maser profile probably only for a couple of days \nduring the '1991/1992 peculiar phase'.\nThe VLA map of 20 October 1991 was made close to the maximum of this phase and the emission was detected in the east part of the shell (spatial component B2\nat $-22.0$ \\kms). The peak velocity of B2 is 1.4 \\kms\\ lower than the peak velocity $-$20.6 \\kms\\ of the spectral component B measured 6 and 13 days later (cf. Table~\\ref{tab:compUHerB-E}). Such a large velocity difference between spatial and spectral components was not seen by any other component, and may indicate the presence of brief emission bursts on the timescales of many days.\n\nThe peculiarity of the '1991/1992 peculiar phase' is evident also from the result that in 1990 the spectral component B maser emission came from a different part of the \\water\\ maser shell (spatial component B1 in the north-east part of the shell). There was no emission at corresponding velocities in December 1992. In parallel to spectral component B also component C  reached a maximum between October 1991 and April 1992 (see Table \\ref{tab:compUHerB-E}). This emission was located in the south-west of the shell, and in this case emission from this part at that velocity was seen also in the maps from 1990 and 1992 (spatial component C1).\n\n The cross-correlation is also complex for velocities $V_{\\rm los} \\ge -18$ \\kms. Spatial components D1 and D2 ($V_{\\rm los} \\approx -18$ \\kms, velocity difference $\\approx 0.2$ \\kms) cannot be separated in the single-dish spectra, and coincide in velocity with spectral component \\Dtwo. Spectral component \\Done\\ was not prominent in 1990--1992. Spatial component E3 is the strongest component among five in the velocity range $-17.2  < V_{\\rm los} < -16.6$ \\kms. It was not identified in the spectra of 1990, but was detected as spectral component E in 1991 and 1992. As E3 comes in velocity space from close to the brighter spectral component \\Dtwo, it was not distinguished in the 1990 spectra because of blending. \n\nIn the $-16 < V_{\\rm los} < -14$ \\kms\\ range the dominating spectral feature G, composed of \\Gone\\ and \\Gtwo\\ (velocity separation in 1990: 0.7 \\kms), was identified spatially as one component G1, \ncaused by the insufficient spectral resolution of the VLA data ($\\sim$0.7 \\kms). The maps show however, that both spectral features came from the same region in the southern part of the shell. Spatial component G1 was detected in 1990 as well as in Dec. 1992 and had been therefore a dominant emission region over a time range of at least 3 years, except for a few months in 1991/1992. \n\nAt velocities $\\ge-14.5$ \\kms\\ of the nine different spatial components listed in Table \\ref{tab:components} only three (I1, L1, and M1 from the 1990 maps) could be distinguished also in the spectra (Table \\ref{tab:compUHerG-M}). The velocities corresponding to spatial components H1/H2 ($V_{\\rm los} \\approx -14.1$ \\kms) are strongly blended in the spectra by the dominating spectral component \\Gtwo . In the 1991 and 1992 maps spatial components at larger velocities ($V_{\\rm los} >-14$ \\kms) were either absent (October 1991) or extremely weak (0.1 Jy).\n\n\n\\subsection{An \\water\\ maser shell model for U~Her \\label{sec:shell-model}}\n\n\\subsubsection{The projected structure}\nThe distribution of all spatial components observed in U\\,Her 1990-1992 (Fig. \\ref{fig:uher-rainbow}) is best described as an incomplete ring with most components located in the half of the ring between position angles 170 and 350$^\\circ$ counting from North over East.\n\nFollowing the analysis and discussion of the location of the \\water\\ masers in the circumstellar envelope of the semi-regular variable RX~Boo (Paper I), we assume that the \\water\\ masers are embedded in an isotropically expanding envelope. In this case, the relationship between the line-of-sight velocities $V_{\\rm los}$ of the maser spatial components relative to that of the star and their projected distances $r_{\\rm p}$ from the star is given by\n\\begin{equation} \\label{velocity-law}\n\\left( \\frac{r_{\\rm p}}{r}\\right) ^2 + \\left( \\frac{V_{\\rm los} - V_*}{V_{\\rm out}}\\right) ^2 = 1\n\\end{equation}\nwhere $r$ is the radial distance, and $V_{\\rm out}$ is the outflow velocity of the components,\nand $V_*$ is the radial velocity of the star. \n\nUsing $V_* = -15.0$ \\kms\\ (Table \\ref{centralcoords}), we plotted in Fig.~\\ref{fig:uher-absvexp} the relative line-of-sight velocity $\\mid V_{\\rm los}-V_*\\mid$ in absolute values of all 48 spatial components against their projected distance $r_{\\rm p}$. $\\mid V_{\\rm los}-V_*\\mid$ and $r_{\\rm p}$ were calculated using $V_{\\rm los}$-, Xoff-, Yoff-values from Table\\,\\ref{tab:components}. For a shell-like distribution of the masers we expect, according to Eq.~(\\ref{velocity-law}), elliptical inner and outer boundaries for their locations in Fig.~\\ref{fig:uher-absvexp}. Unlike in the corresponding diagram of RX~Boo (Paper\\;I), there is no sharp inner boundary, but an outer boundary can be defined, by fitting a quarter of an ellipse to the six outer masers components marked by surrounding circles in Fig.~\\ref{fig:uher-absvexp}, using the 'least-squares method'. From this fit we conclude that the outflow velocity in the envelope of U~Her at $\\sim$24~AU from the star is about 10~\\kms. \n\n\\begin{figure}\n\\includegraphics[width=\\columnwidth]\n{fig11-dieter-010923-nogrid.jpg}\n\\caption{Relative line-of-sight velocity $\\mid V_{\\rm los} - V_*\\mid$ versus projected distance $r_{\\rm p}$ for all spatial components of U\\,Her from Table~\\ref{tab:components} identified in the four epochs. The six outer points surrounded by circles were used for the fit with a quart ellipse.}\n\\label{fig:uher-absvexp}\n\\end{figure}\n\n\\begin{figure}\n\\includegraphics[width=\\columnwidth]\n{fig12-dieter-040923-nogrid.jpg}\n\\caption{The model outflow velocity for U Her as a function of the distance from the star. It approaches the final expansion velocity (horizontal straight line) asymptotically. The dots along the curve give the radial distances and outflow velocities of the 28 merged spatial components (see text).}\n\\label{fig:uher-expansion}\n\\end{figure}\n\n\\subsubsection{The 3-dimensional shell structure} \\label{sec:3D-struct}\nAs in Paper I we assume that the outflow velocity law is exponential in nature and is leading asymptotically to the final expansion velocity, $V_{\\rm exp}$ \n\\begin{equation}\nV_{\\rm out}(r) = V_{\\rm exp} \\left\\lbrace  1 - {\\rm exp}\\,\\left[ -k (r - r_0)\\right]\n\\right\\rbrace \n\\label{eq:exp_model}\n\\end{equation} \nwhere $k$ is a scaling factor and $r_0$ is a radial offset. \n\nEquation (\\ref{eq:exp_model}) contains three constant parameters ($V_{\\rm exp}$, $k$ and $r_0$) and in order to find plausible values for them we need to estimate the coordinate values for three points on the outflow law.  For the value of $V_{\\rm exp}$ (the outflow velocity at $r = \\infty$) we adopted $V_{\\rm exp} = 13.1$ \\kms\\ (Table~\\ref{centralcoords}). \nBased on the fit of the ellipse shown in Fig.~\\ref{fig:uher-absvexp} to determine an outer boundary of the shell, we adopted an outflow velocity of 9.6~\\kms\\ at a distance of 23.9~AU from  the star. We also assumed that the outflow velocity at the photosphere $r_0=1.4$ AU is\n$V_{\\rm out}=0$ \\kms.\n \nThus the value of the parameter $k$ can be determined from the condition that the stellar wind passes through the position \\mbox{($r_1$, $V_1$) = (23.9, 9.6).}  Using \\begin{equation}\nk = \\frac{{\\rm ln}\\,V_{\\rm exp} - {\\rm ln}\\,(V_{\\rm exp} - V_1)}{r_1 - r_0}\n\\label{eq:k_value}\n\\end{equation}\nwe obtain $k = 0.059\\,{\\rm AU}^{-1}$. \n\nFor the case of non-tangential movements of maser components, the model allows one to calculate the observed line-of-sight velocities $V_{\\rm los}$ and projected distances $r_{\\rm p}$ \nfrom the equations\n\\begin{equation}\nV_{\\rm los} - V_* = V_{\\rm out}(r)\\,{\\rm sin}\\,\\theta\n\\label{eq:v-los}\n\\end{equation}\nand\n\\begin{equation}\nr_{\\rm p} = r\\,{\\rm cos}\\,\\theta\n\\label{eq:r-p}\n\\end{equation}\nwhere $\\theta$ is the aspect angle between the normal to the line of sight and the radius from the star to the maser. The aspect angle is $-90^\\circ \\le \\theta < 0^\\circ$ for $V_{\\rm los} - V_{*} < 0$ \\kms\\  and viceversa. \n\nCombining Eqs. (\\ref{eq:exp_model}), (\\ref{eq:v-los}), and (\\ref{eq:r-p}) gives a relation, which uniquely determines the distance $r$ of the maser cloud from the star for the time of the distance measurement, using the adopted outflow law and the observed quantities ($V_{\\rm los} - V_{*}$) and $r_{\\rm p}$\n\\begin{equation}\nV_{\\rm los} = V_{\\rm exp} \\left\\lbrace  1 - {\\rm exp}\\,\\left[ -k (r - r_0)\\right]\n\\right\\rbrace \\cdot \\sqrt{1 - (r_{\\rm p}/r)^2} + V_*\n    \\label{eq:vlos-r-rp}\n\\end{equation}\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=6.25cm]\n{fig13alt-de-v3-nogrid.jpg}\n\n\\caption{The three-dimensional distribution of the U Her merged maser spatial components and their associated velocities according to the outflow velocity law given by Eq. \\ref{eq:exp_model}. The distribution is seen from three cardinal directions: on the sky (a), ``from the side'' (b) and ``from above'' (c). In (b) and (c) the observer is to the left. The maser components are represented by filled circles with colours according to their line-of-sight velocities (see scale at bottom). Their diameters are proportional to the logarithm of their flux density.  The central star is symbolised by a black circle at (0,0).\n}\n\\label{fig:uher-3Dvert}\n\\end{figure}\n\nIn Fig.~\\ref{fig:uher-expansion}, the adopted outflow velocity law with the parameters $k = 0.059$~AU$^{-1}$, $r_0 = 1.4$ AU and $V_{\\rm exp} = 13.1$ \\kms\\ is shown graphically, together with the radial distances $r$ of the maser spatial components calculated with Eq.\\,(\\ref{eq:vlos-r-rp}). The projected distances $r_p = \\sqrt{{\\rm Xoff}^2 + {\\rm Yoff}^2}$ used for these calculations were averages over the epochs in which the components were detected (Table\\,\\ref{tab:components}). The uncertainties in positions and velocities lead to errors in the radial distances $r$ of a few AU, so that their locations on the velocity curve mainly delineate the typical distance range of the maser components. The main conclusion is that the maser shell is primarily located between $\\sim$11 and $\\sim$25 AU. The outflow velocity within these boundaries increases from 5.6 to 9.8 \\kms. The maser components outside this shell ($>25$ AU) are the spatial components A1 and M1 which have line-of-sight velocities $V_{\\rm los}$ at the extreme ends of the observed velocity range, and have the largest outflow velocities $V_{\\rm out}$. The component inside this shell is the tangential component G2 with  $V_{\\rm los}-V_* = -0.1$ \\kms\\ close to the line-of-sight toward the star itself. The two outliers at $r>25$ AU with flux densities $<1$\\,Jy indicate that weak and short-lived maser activity can occur outside the shell delineated by the strong maser components. The same is true for the regions inside the inner boundary of the shell, in which the masers are usually suppressed because there the acceleration of the wind is relatively high. Due to projection effects the shell radius ($r=15$ AU), derived from the spatial distribution of the maser components on the sky (Fig. \\ref{fig:uher-rainbow}), is $\\sim$80\\% of the mean radius of the shell ($r\\sim18$ AU) given by the midpoint between inner and outer boundary of the 3D model. \n\nThe 3D-distribution of the merged maser spatial components and their associated velocities is shown in Fig.~\\ref{fig:uher-3Dvert} as seen from three different directions: on the sky (a), ‘from the side’ (b) and ‘from above’ (c). The dominance of tangential masers is seen clearly in all three diagrams. \nIn panel (c) there is a scarcity of (red-shifted) components seen on the backside of the shell. This is also evident in the FVt-plot (Fig.\\,\\ref{fig:uher-fvt}, left), where there is less emission at red-shifted velocities, and in Fig. \\ref{fig:uher-sratio-tjd} where the blue-shifted integrated emission prevailed between 1991 and 2007. However, one should keep in mind that an observer seeing U Her from directions deviating considerably from the geocentric one would see a different set of maser components, because a maser beams in a preferred direction, which is governed by the direction of greatest elongation of the maser cloud. \nThus, Fig.~\\ref{fig:uher-3Dvert} is merely showing the three-dimensional positions of the maser components that we see from Earth. \nOf course a similar situation occurs for our present point of observation: there may be maser spots that we do not see because their emission may happen to be beamed in the wrong direction from our vantage point.\n\nHaving adopted an outflow law it is of interest to investigate the associated time scale for the gas to travel through the maser shell. Thus, we have integrated the function $V_{\\rm out}^{-1}$, where $V_{\\rm out}$ is described by Eq.\\,(\\ref{eq:exp_model}), along the radius $r$ between the shell boundaries. We find that it takes $\\sim$8.5 years for gas to travel through the shell (11 -- 25 AU), where most of the \\water\\ masers reside.\n\n\\subsection{Other \\water\\ maser shell observations of U~Her \\label{shell-size}}\n\\subsubsection{Maser shell sizes}\nOur shell model can be directly compared to the results of the contemporaneous 22 GHz VLA observations of \\cite{colomer00} from June 1990 discussed in Sect. \\ref{id_spatial_c}. Using a 3D fitting program they advocate a thick maser shell with an inner radius of 45 mas ($\\sim$12 AU) and an outer radius of 70 mas ($\\sim$19 AU), in which the molecules flow outwards with a velocity of $\\sim$6 \\kms. A comparison with our model expansion velocity law (Fig. \\ref{fig:uher-expansion}) shows that their shell boundaries delineate a narrow 7~AU-wide shell in the central part of our shell model covering the strongest spatial maser components. The average radius of this shell is $\\sim$15.5 AU compared to $\\sim$18 AU, the average radius of our shell model. The two independent analyses highlight the uncertainties in  the determination of the boundaries, with the shell boundaries for 1990 of Colomer et al. being a conservative estimate.\n\nAfter our VLA observations 1990 -- 1992, the \\water\\ maser emission of U~Her was mapped several times and these observations can be used to search for variations in the spatial distribution of the masers over a longer time range. Interferometric observations with better spatial resolution than achievable with the VLA were made with MERLIN 1994, 2000 and 2001 \\citep{bains03, richards12}, and with the VLBA in 1995 \\citep{marvel96}. They confirm the overall ring-like geometry of the \\water\\ maser shell. However, the boundaries of this shell determined from the different observations are not well defined. The most compelling determination is by \\cite{richards12} who, for the epochs 2000 and 2001, found  projected inner and outer radii of 10 and 40 AU, respectively. While the inner radius is compatible with the inner boundary determined here and by \\cite{colomer00}, the outer radius is significantly larger. \nHowever, the outer shell at radii $>30$ AU was only sparsely populated by maser components in 2000/2001 \\citep{richards12}, and these were not detected  1990 -- 1992.\nThe decrease in brightness of the maser emission with radial distance makes the determination of the outer shell boundary much more dependent on instrumental sensitivity compared to the inner boundary.\nThe conclusion from all \\water\\ maser shell observations available is that the \\water\\ masers are located in a shell within the expanding spherical wind of U~Her with most of the sites with stronger emission located at a radial distance of about 15--20~AU. Nevertheless, VLBA observations by \\cite{vlemmings02b,vlemmings05} challenged the characterization of U~Her's \\water\\ maser shell as a persistent ring-like distribution of distinct emission regions.  \n\n\\subsubsection{Constraints on shell geometry due to spatial resolution effects}\nTo study magnetic field strengths in the CSE of U~Her, 22~GHz observations with the VLBA were made on 13 December 1998 by \\cite{vlemmings02b} and  on 20 April 2003 by \\cite{vlemmings05} with an average beam width of 0.5 mas. They noted a significant change in spectrum and spatial distribution between the two observations. The spatial features detected in 1998 (optical phase $\\phi_{\\rm s} = 0.44$) were at velocities $-19.3$ to $-17.6$ (likely part of our spectral components \\Done\\ and \\Dtwo), while in 2003 ($\\phi_{\\rm s} = 0.97$) they detected features at  $-15.9$ to $-14.5$ (\\Gone\\ and \\Gtwo). Our single-dish spectra taken close to their observations (12 December  1998 and 2 April 2003, see Fig. \\ref{fig:uher_all}) show a stronger profile change only in the red wing ($>-14$ \\kms), but not at the velocities with VLBA detections by Vlemmings et al.. During both epochs spectral components \\Gone\\ and \\Gtwo\\ were strongest, while \\Done+\\Dtwo\\ was 2--3 times weaker. It is therefore conceivable that the spatial distribution as such did not change significantly between the two epochs, but the sizes of the spatial components did, leading to different components  being resolved out by the VLBA in the two epochs. This explanation is corroborated by the failure of \\cite{imai97b} to detect U~Her's \\water\\ maser emission in 1994/1995 during a VLBI experiment (resolution 2.1 mas), indicating that the dominating spectral components \\Gone\\ and \\Gtwo\\ had significantly larger sizes. \n\\cite{richards11} give typical sizes 2--5 AU (8--19 mas) for \\water\\ maser clouds of U~Her, so that losses of spatial features due to the largest recoverable scale for imaging with the VLBA are plausible. \n\n\\subsubsection{Maser amplification in 2001 by stellar emission?}\n\\cite{vlemmings02a} mapped the maser on 20 May 2001 with MERLIN (beam size $\\le30$ mas), when the strongest spectral maser feature was present at $-15.6$ \\kms\\ (spectral component \\Gone, see Table \\ref{tab:compUHerG-M} and Fig. \\ref{fig:uher_all} in the Appendix). They found the position of the feature to match with the location of the star at this epoch, determined using own and HIPPARCOS proper motion measurements of U\\,Her, and argued that the maser feature is amplified by the stellar radiation in the background. They note that this result would place the star not in the center but on the ring-like distribution of the maser spots.\n\nIn the monitoring data we find no evidence for extraordinary amplification of the \\Gone\\ component. If the \\Gone\\ component would have moved as part of the stellar wind in radial direction within the line-of-sight to the star, a systematic blue-shift of its velocity $V_{\\rm los}$ over time is expected, which is not observed. Alternatively, if \\Gone\\ would have moved in tangential direction the stellar amplification would be only a temporary effect during the time of eclipse. Adopting a U~Her maser cloud size of 2--5 AU \\citep{richards11}, a stellar diameter of 2.8 AU \\citep{vanbelle96, ragland06} and a velocity of $\\sim$7 \\kms\\ perpendicular to the line-of-sight (see Fig. \\ref{fig:uher-expansion})  leads to a duration of the eclipse of several years. Therefore, a monitoring program of such an eclipsing event is expected to observe a temporary year-long flare of the emission on top on the regular maser variations. The occurrence of such a flare of the \\Gone\\ emission can be  definitely ruled out in the years around 2001. \n\nUsing the stellar position and proper motion given by GAIA DR3 we recalculated the position of the star for the MERLIN observation of \\cite{vlemmings02a} and found an offset of the position of the maser spot from the star of $\\Delta \\alpha = -37.4$ mas and $\\Delta \\delta = 0.3$ mas, which places the maser spot $\\sim$10 AU west from the star and at the inner boundary of the \\water\\ maser shell as determined earlier.\n\nBased on the lack of brightness and velocity variations of spectral component \\Gone, which could be related to an eclipse event, and based on the stellar position in 2001 according to recent GAIA astrometry, we conclude that the assumption of the stellar position close to the center of the ring-like distribution of spatial components, as made in Sect. \\ref{map-alignment}, is still a valid approach. The location of the \\water\\ masers in a shell within the spherically expanding stellar wind, remains therefore the most plausible geometric configuration.\n\n\n\\begin{table*}\n\\begin{center}\n\\caption[]{\\label{tab:emission-regions} Regions in U\\,Her's \\water\\ maser shell contributing emission to the prominent spectral components G (composed of \\Gone\\ and \\Gtwo\\ at $V_{\\rm los}\\sim-15\\pm0.5$ \\kms) and \\Dtwo\\ ($V_{\\rm los}\\sim-18$ \\kms).}\n\\begin{tabular}{lrrrrrrl}\n\\hline\\noalign{\\smallskip}\nMap Date &  Xoff & Yoff & $V_{\\rm los}$  & $S_\\nu$ & Region & Instr. & Reference \\\\\n         & [mas] & [mas]&[\\kms]& [Jy]  &        &           \\\\\n\\noalign{\\smallskip} \\hline\\noalign{\\smallskip}\n1988 Dec. &  5   & -46  & -14.7 & $\\sim$210 & SE & VLA  & \\cite{bowers94}\\\\\n1990 Feb. & 15   & -54  & -14.9 & 230.0   & SE & VLA  & this paper, Table \\ref{tab:components} \\\\\n1990 June & 15   & -52  & -15.0 & 145.0   & SE & VLA  & this paper, Table \\ref{tab:components} \\\\\n1990 June & 22   & -42  & -14.6 & 117.2   & SE & VLA  & \\cite{colomer00} \\\\\n1992 Dec. & 14   & -46  & -15.3 &  14.0   & SE & VLA & this paper, Table \\ref{tab:components} \\\\\n1994 Apr. & 21   & -63  & -14.8 &   8.4   & SE & MERLIN  & \\cite{richards12} \\\\\n1994 Apr. & 21   & -46  & -14.8 &   2.2   & SE & MERLIN  & \\cite{richards12} \\\\\n1995 June & 23   & -55  & -14.8 &   4.1   & SE & VLBA  & \\cite{marvel96} \\\\\n2000 May  & -    & -    &  -    &  n.d.   & SE & MERLIN  & \\cite{richards12}$^\\dagger$\\\\\n2001 Apr. & -    & -    &  -    &  n.d.   & SE & MERLIN  & \\cite{richards12}$^\\dagger$ \\\\\n\\noalign{\\smallskip} \\hline\\noalign{\\smallskip}\n1994 Apr. & -65   & -8  & -15.2 &  10.7   & W & MERLIN  & \\cite{richards12} \\\\\n1995 June & -36   & -21 & -15.5 &  10.6   & W & VLBA  & \\cite{marvel96} \\\\\n2000 May  & -50   & -2  & -15.0 & 141.0   & W & MERLIN  & \\cite{richards12} \\\\\n2001 Apr. & -59   & -8  & -15.9 &  38.0   & W & MERLIN  & \\cite{richards12} \\\\\n\\noalign{\\smallskip} \\hline\\noalign{\\smallskip}\n1988 Dec. & -8   & -51  & -17.6 & $\\sim$110 & SW & VLA  & \\cite{bowers94}\\\\\n1990 Feb. & -9   & -60  & -18.2 & 52.0    & SW & VLA  & this paper, Table \\ref{tab:components} \\\\\n1990 June & -9   & -60  & -18.1 &  7.3    & SW & VLA  & this paper, Table \\ref{tab:components} \\\\\n1990 June & -2   & -52  & -18.3 &  11.0   & SW & VLA  & \\cite{colomer00} \\\\\n1992 Dec. & -8   & -68  & -18.2 &   3.0 & SW & VLA & this paper, Table \\ref{tab:components} \\\\\n1994 Apr. & -30  & -62  & -17.9 &   2.4 & SW & MERLIN  & \\cite{richards12} $^{\\dagger\\dagger}$\\\\\n1995 June & -26  & -44  & -17.9 &   2.0 & SW & VLBA  & \\cite{marvel96}$^{\\dagger\\dagger}$ \\\\\n2000 May  & -69  & -13  & -17.8 &  12.3 & SW & MERLIN  & \\cite{richards12}$^{\\dagger\\dagger}$ \\\\\n2001 Apr. & -17  & -15  & -17.9 &   4.7 & SW & MERLIN  & \\cite{richards12}$^{\\dagger\\dagger}$ \\\\\n\\noalign{\\smallskip} \\hline \\noalign{\\smallskip}\n\\end{tabular}\\\\\n\\end{center}\n{\\it Note:} The position of the star in the maps presented by \\cite{colomer00} and \\cite{marvel96} was set by us to ($-$20,+43) and (+35,+20) mas, respectively,   relative to their map origin.\\\\ $^\\dagger$ n.d. = not detected. The non-detection of component G in 2000/2001 means that it is not seen in the south-east quadrant anymore.\\\\ $^{\\dagger\\dagger}$ The spatial components in the south-west quadrant might have originated in different regions before and after 1993.\n\\end{table*}\n\n\n\\subsection{Lifetime of emission regions \\label{region-lifetime}}\nThe analysis of the line profile variations in Sect. \\ref{sdd_LineProfAna} found the velocities of the spectral components remarkably constant over the monitoring period with only small shifts back and forth on timescales of months. The small shifts were assumed to be caused by blending caused by different maser clouds with similar velocities and unrelated brightness fluctuations. They may reflect the asynchronous formation and dissolution of contributing maser clouds or a variation of excitation conditions on these timescales.\n\nWe also found that some less blended spectral components could be followed without significant variations in velocity over longer time ranges. An example is component K, which could be detected continuously for 5 years (March 1995 -- April 2000; TJD = 9790 -- 11640; (see Fig. \\ref{fig:uher-fvt}, and Table \\ref{tab:compUHerG-M} in the Appendix) at $-10.9\\pm0.2$ \\kms, i.e. with a rather small velocity dispersion. Later it reappeared frequently close to the maxima of the stellar variability cycle. If this emission component comes from an individual maser cloud, one has to conclude  that U\\,Her's \\water\\ maser shell can host individual clouds with a range of lifetimes of 0.5 to many years. Alternatively, short-living clouds would have to appear regularly with always similar line-of-sight velocity $V_{\\rm los}$. \n\nIn the first case of a long-living maser cloud, which moves within the expanding CSE, systematic shifts of the projected velocity $V_{\\rm los}$ are expected, while the cloud is accelerated outward. This is not seen in any of the spectral components, and so the second case must apply and their emission must come from different short-living spatial components in the course of the monitoring period.\n\n\\subsubsection{Maser cloud lifetime constraints \\label{region-lifetime-1}}\nWe will now use the absence of systematic velocity shifts to derive upper limits for the lifetime of individual maser clouds. \n\nFor the outflow velocity curve shown in Fig.~\\ref{fig:uher-expansion}, we found in Sect. \\ref{sec:3D-struct} a crossing time of $\\sim${8.5} years for a mass element moving radially through the \\water\\ maser shell with inner and outer boundaries of 11 and 25 AU, and with an increase of its outflow velocity from 5.6 to 9.8~\\kms. Moving in the line-of-sight, such a hypothetical mass element showing maser emission all the time would show an acceleration of $a = 0.5$~\\kms yr$^{-1}$.  \n\nA maser cloud observed in two epochs with a time difference $\\Delta t$ and moving along a straight trajectory in the shell with an angle $\\theta$ with respect to the plane of the sky will experience a shift in line-of-sight velocity $\\delta V_{\\rm los} = \\delta V_{\\rm out}(r) \\cdot \\sin{\\theta}$ (cf. Eq. \\ref{eq:v-los}), with $\\delta V_{\\rm out}(r)$ the increase of the outflow velocity. For simplicity, we approximate the acceleration within the \\water\\ maser shell with the average velocity increase $a = 0.5$~\\kms yr$^{-1}$, i.e.  $\\delta V_{\\rm out}(r) = a \\cdot \\Delta t$ leading to the relation\n\\begin{equation}\n    \\delta V_{\\rm los} =  a \\cdot \\Delta t \\cdot \\sin{\\theta}.\n    \\label{eq:vlos-shift}\n\\end{equation}     \nIn the following we adopt a conservative value $\\mid$$\\delta V_{\\rm los}$$\\mid\\ < 0.5$ \\kms\\ for recognizable velocity shifts due to participation in the stellar outflow of any of U\\,Her's spectral components. For a given time interval $\\Delta t$, only maser components with an aspect angle obeying \n\\begin{equation}\n    \\mid\\sin{\\theta}\\mid = \\delta V_{\\rm los} / a / \\Delta t < 1.0 / \\Delta t\n    \\label {aspect-angle-limit}\n\\end{equation}\nwould not have been detected as having a drifting line-of-sight-velocity. \nA special case of Eq.\\,(\\ref{aspect-angle-limit}) is a tangential movement of the maser clouds ($\\theta \\approx 0$), where a constant velocity $V_{\\rm los}$ (i.e. $\\delta V_{\\rm los} \\approx 0$) can be expected for maser clouds traceable over long time intervals. Equation (\\ref{eq:v-los}) demands that in this case $V_{\\rm los} \\approx V_*$. Allowing an uncertainty of 0.5 \\kms\\ for the stellar radial velocity $V_* = -15.0$ \\kms, such a tangential movement would be able to explain the absence of velocity shifts for spectral components  \\Gone\\ ($\\sim -15.5$ \\kms) and \\Gtwo\\ ($\\sim -14.5$ \\kms). However, as our monitoring period is longer than the crossing time in U~Her's \\water\\ maser shell, more than one cloud must have contributed even for its \\Gone+\\Gtwo\\ spectral components.\n\nFor the case of non-tangential movements of clouds and $\\mid V_{\\rm los} - V_*\\mid\\ > 0.5$ \\kms, the line-of-sight velocity $V_{\\rm los}$ is determined by  $(V_{\\rm los} - V_*) = V_{\\rm out}(r) \\cdot \\sin{\\theta}$ (Eq. \\ref{eq:v-los}). A measurement of the projected distance $r_p$ of the cloud from the star and use of Eq.\\,(\\ref{eq:vlos-r-rp})\nuniquely determines the distance $r$ of the maser cloud from the star for the time of the projected distance measurement \nand fixes the aspect angle $\\theta$ according to Eq. (\\ref{eq:r-p}). After a time interval $\\Delta t$ a new measurement of $V_{\\rm los}$ should show a shift $\\delta V_{\\rm los} = a \\cdot \\Delta t \\cdot \\sin{\\theta}$ (Eq. \\ref{eq:vlos-shift}). A limit $\\delta V_{\\rm los}$(max) on $\\delta V_{\\rm los}$ constrains the time difference $\\Delta t$, for  which Eq. (\\ref{eq:vlos-shift}) would not be violated. The maser emission seen in observations separated by more than the corresponding $\\Delta t$(max) and having $\\delta V_{\\rm los} \\le \\delta V_{\\rm los}$(max) can therefore not originate from the same cloud. We interpret therefore $\\Delta t(max)$ as the lifetime of the cloud.\n\nAs an example, we will discuss the lifetime of spectral component E of U~Her seen almost permanently during the monitoring period. We detected the component in 1990 at $V_{\\rm los} = -16.9$~\\kms\\ (Table \\ref{tab:compUHerB-E} in the Appendix) and associated to it the spatial component E3 located $\\approx$60 mas ($r_{\\rm p} \\approx 16$ AU) south from the star, as judged from the February 1990 and December 1992 positions (Table \\ref{tab:components}). The projected outflow velocity is $V_{\\rm los} - V_* = -1.9$ \\kms , \nand using Eq. (\\ref{eq:vlos-r-rp}) we can derive a radial distance $r\\sim17$~AU, and from this\n$V_{\\rm out}(r) = 7.9$~\\kms\\ and $\\theta = -14^\\circ$, using Eqs. (\\ref{eq:exp_model}) and (\\ref{eq:v-los}). Adopting $\\delta V_{\\rm los} < 0.5$ \\kms\\ and $a = 0.5$~\\kms yr$^{-1}$, we find $\\Delta t(max) =  \\delta V_{\\rm los} / a /  \\sin{\\theta} \\approx 4$ yr.\n\n\\begin{figure*}\n\\resizebox{18cm}{!}{\n\\includegraphics{rraql_selspec_070121.jpg}}\n\\caption{Selected H$_2$O maser spectra of RR~Aql. The calendar date of the observation is indicated on the top left above each panel, the TJD (JD-2440000.5), on the top right.}\n\\label{fig:rraql_sel}\n\\end{figure*}\n\nLifetimes of at most several years are also found for other spectral components and varying the outflow velocity model derived in Sect. \\ref{sec:shell-model}. They are compatible with our series of VLA observations 1990 -- 1992, where the identification of common spatial maser components proved to be difficult over even shorter time ranges (see Sect. \\ref{map-alignment}). They corroborate the findings of \\cite{bains03}, who give lifetimes $\\la2$ years for maser clouds of U~Her with typical sizes 2--4 AU. \n\n\\subsubsection{Long-living regions favourable to \\water\\ maser emission \\label{region-lifetime-2}}\nThe limited lifetime of the individual maser clouds are in apparent contrast to the persistent presence of several spectral components over the full monitoring period (Fig. \\ref{fig:uher-fvt}).  The most notable components are \\Gone\\ and \\Gtwo\\ in the $-16$ to $-14$ \\kms\\ velocity region, which maintained their dominant role in the spectral profile after 1991 over more than 20 years. As we have argued in the previous Section, this is due to many consecutive components superposed over time at the same velocity. This suggests that the dominant emission region in U\\,Her's \\water\\ maser shell may not have moved away from its 1990-1992 location.   \n\nIn the standard model having a continuous homogeneous outflowing stellar wind there is no reason for the existence of preferred locations in the \\water\\ maser shell for particular emission sites. If the \\Gone\\ and \\Gtwo\\ components would come from short-lived single maser clouds, one would expect that their emission comes from random positions within the shell having the right projected line-of-sight velocity. In Table \\ref{tab:emission-regions} we compiled the strongest spatial maser components with velocities compatible with \\Gone\\ and \\Gtwo\\ identified in a number of interferometric maps covering the years 1988 - 2002. The table gives the offsets Xoff, Yoff of the spatial components from the position of the star, their velocity $V_{\\rm los}$, flux density $S_\\nu$, and a label for the region (see below). Furthermore, the date of the map, the interferometer used and the reference are given. The position of the star is either taken as given in the references or determined (see Note to Table \\ref{tab:emission-regions}) assuming a ring-like distribution of the maser emission as described in Sect. \\ref{map-alignment}.\n\nWe found that the strong ($S_\\nu > 200$ Jy in 1988--1990) spectral component has spatial counterparts between 1988 and 1995 in the south-east (SE in Tab. \\ref{tab:emission-regions}) quadrant of the maps. The flux density decreased to $<10$ Jy levels in 1994/1995 and the component was not detected afterwards. In 2000/2001 emission at \\Gone + \\Gtwo\\ velocities is seen in the west (W) of the MERLIN maps. Spectral component \\Dtwo, which decreased in brightness after 1988, can be identified at least until end of 1992 with spatial components found in the south-west (SW) quadrant, while afterwards the relation with spatial components in the same quadrant having velocities compatible with \\Dtwo\\ is inconclusive due to the large deviations of the offsets from those in the 1988--1992 period.\n\nThe frequent interferometic observations of U~Her's \\water\\ maser emission show that the regions in the CSE with conditions favourable to water maser excitation can survive longer than a few years, in contrast to what was found for single maser clouds. Spectral components with only small variations in velocity are therefore originating from multiple maser clouds, which are preferentially formed in particular regions of the shell, which for at least 6.5 years (in the case of  spectral component G) can maintain their favourable \\water\\ maser excitation condition. The origin of such regions will be discussed in Sect. \\ref{sdd-cse-asym}.\n\n\n\\begin{figure*}\n\\begin{minipage}[t]{17cm}\n \\begin{minipage}[t]{8.5cm}\n  \\begin{flushleft}\n   {\\includegraphics*[width=8cm,angle=0]{rraql-FVt-combifig-071021.jpg}}\n     \\end{flushleft}\n \\end{minipage}\n\n \\begin{minipage}[t]{8.5cm}\n  \\begin{flushright}\n      {\\includegraphics*[width=8cm,angle=0,scale=0.9]{rraql-plotcomps-110422-v3.jpg}}      \n  \\end{flushright}\n \\end{minipage}\n\\end{minipage}\n\\caption{As Fig.~\\ref{fig:uher-fvt}, but for RR~Aql. {\\it Left:} First spectrum: 17 February 1990; JD = 2447939.5, TJD = 7939. and last spectrum 25 June 1997 (lower panel). First spectrum 20 December 2000 and last spectrum: 20 March 2011 (upper panel). {\\it Right:} Spectral components identified by the component fit of the single-dish spectra as listed Table \\ref{tab:compRRAql}  in the Appendix.}\n\\label{fig:rraql-fvt}\n\\end{figure*}\n\n\\section{RR~Aql \\label{sec:rraql}}\nRR\\,Aql is a long-period variable AGB star having a distance of $410^{+12}_{-11}$ pc (Table \\ref{centralcoords}), based on the \\water\\ and SiO maser parallaxes measured by \\cite{sun22}. We adopt as radial velocity of the star $V_{\\ast} = 28.5\\pm0.5$ \\kms\\ and as final expansion velocity $V_{\\rm exp} = 9$ \\kms, as determined from circumstellar CO and SiO thermal emission (Table \\ref{centralcoords}).\n\nThe \\water\\ maser of RR~Aql was first detected in 1971 by \\cite{dickinson73} as a single strongly variable feature at $\\sim$27 \\kms. A VLBI observation in 1976 by \\cite{spencer79} showing a maser feature at 25.9 \\kms\\ with a flux density 353\\,Jy indicated that the maser could reach levels of several hundred Jy. \nRegular observations of the maser were first made within the Pushchino monitoring program starting in 1980 \\citep[hereafter B98]{berulis98}. The characteristics of the maser variations until 1997 are discussed in B98, who included also the maser observations published in the literature during this time interval. The brightest \\water\\ maser features in RR\\,Aql usually peaked at velocities $26-29$ \\kms\\ close to the stellar velocity. The brightness variations were indeed strong with phases where peak flux densities of several hundred Jy were reached, and occasions in the 1970s where the maser was not detected (i.e. $F_\\nu \\la 10$ Jy). \n\nThe \\water\\ maser region of RR~Aql was mapped on three occasions between 1981 and 1988 with the VLA.  The diameter of the maser region was confined to about 100 mas  (radius $\\sim$21 AU), \nwith clustering of the maser components mostly in two locations with a north-south orientation \\citep{johnston85, lane87, bowers94}. A VLBI observation of \\cite{imai97a} in 1995 was probably resolving out the emission, although the maser seemed to have been in a bright phase. Based on a total-power spectrum, they reported for 17-20 January 1995 a flux density of 296 Jy of the \\water\\ maser feature at 28.52 \\kms, which is a factor of $\\sim$4 higher than the brightness level observed before and after this date by B98, who concluded that the VLBI observation must have taken place during a transient flare of the maser. However, also our spectrum from 18 January 1995 showed a peak flux density of only $\\sim$40\\,Jy at 29.0 \\kms, which leaves the high flux densities during the VLBI observations unexplained. VLBA observations in 2017/2018 \\citep{sun22}, after the end of the monitoring program reported here, detected distinct maser emission regions with velocities in the range  $26-29$ \\kms, which had dominated the maser profile since the discovery of the maser. The latter regions were found at a distance of $\\sim$28 AU from the star, which is compatible with the shell dimensions inferred from the 1980s VLA observations.\n\n\\subsection{Variations of the \\water\\ maser profile}\nRR\\,Aql was first observed at Medicina in 1987 \\citep{comoretto90}, and we monitored the star regularly between 1990 and 2011. Additional observations were made in 2015. As before 1987, the strongest maser features were always detected close to the stellar velocity ($\\sim 26-31$ \\kms). In Fig.~\\ref{fig:rraql_sel} representative maser spectra from our observations taken between 1990 and 2011 are shown, while the complete set of 89 spectra is displayed in Fig.~\\ref{fig:rraql_all} (Appendix). An overview on the general behaviour of the maser variations is shown in the FVt plot (Fig. \\ref{fig:rraql-fvt}, left panel). Because of only occasional observations between June 1995 and December 2001, the plot is split into two panels omitting 3.5 years between July 1997 and November 2000. As in U~Her, the profile in general is varying with stellar pulsation in brightness and velocity range  and is dominated by three features (cf. upper envelope spectrum Fig. \\ref{fig:rraql-upenv}), which change their relative strengths over time. The central emission at $\\sim$29 \\kms\\ (spectral component C hereafter) is always present (cf. lower envelope spectrum Fig. \\ref{fig:rraql-loenv}) and is the dominating emission for most of the time. A second feature at $<28$ \\kms\\ (spectral component A; Fig. \\ref{fig:rraql-fvt}, right panel) is blue-shifted and is the strongest only occasionally: In 1991--1992 (TJD $\\sim 8200-8900$) and  September/October 2001 (TJD $\\sim 12200$). The third feature at $>$30 \\kms\\ (spectral component E) is red-shifted and was always weaker than the other two. This feature reached a $\\sim$50 Jy level between September 2004 and February 2005 (TJD $\\sim 13200-13500$), while it rarely surpassed 10 Jy for the rest of the time. It was often not detected during the faint phase of the optical brightness variations. \n\n\\begin{figure}\n\\resizebox{9cm}{!}{\\rotatebox{270}{\n\\includegraphics\n{rraql-upenv-081021.jpg}}}\n\\caption{Upper envelope spectrum for RR~Aql; 1987-2015.}\n\\label{fig:rraql-upenv}\n\\end{figure}\n\n\\begin{figure}\n\\resizebox{9cm}{!}{\\rotatebox{270}{\n\\includegraphics\n{rraql-loenv-081021.jpg}}}\n\\caption{Lower envelope spectrum for RR~Aql; 1987-2015.}\n\\label{fig:rraql-loenv}\n\\end{figure}\n\n\\begin{figure}\n\\resizebox{9cm}{!}{\\rotatebox{270}{\n\\includegraphics\n{rraql-histo-081021.jpg}}}\n\\caption{Detection rate histogram for RR~Aql; 1987-2015.}\n\\label{fig:rraql-histo}\n\\end{figure}\n\n\\subsection{\\water\\ maser velocity range  \\label{rraql-vel-range}}\nAs velocity boundaries of the maximum \\water\\ maser velocity range $\\Delta V_{\\rm los}$ of RR~Aql we determined $V_{\\rm b} = 23.2$ and $V_{\\rm r} = 31.9$ \\kms\\ (Table \\ref{centralcoords}) \nfrom the detection rate histogram (Fig.~\\ref{fig:rraql-histo}). The blue boundary is not well defined as the emission between $V_{\\rm los} =$ 21.5 and 24 \\kms\\ is generally weak (seen only marginally in  Fig.~\\ref{fig:rraql-upenv}) \nand surpasses flux densities $>$1\\,Jy usually only at velocities $V_{\\rm los} > 25$ \\kms\\ as evident from the FVt-plot (Fig. \\ref{fig:rraql-fvt}). \n\nAs in U~Her, the FVt-plot shows that the width of the observed velocity range is varying. The width is smaller during the faint part of the stellar variability cycle, when the weaker outer parts of the maser profile fall below the threshold of the FVt-plot ($\\sim1$ Jy). Close to the maxima of the stellar light curve, maser emission is seen over the maximum velocity range, except for the emission at $<$25 \\kms, which was detectable only in 1991 (TJD$\\sim$8600). In 2009, toward the end of the monitoring program, spectral component E at 30.5 \\kms\\ was becoming weaker and indeed was then only marginally detected in some of our 2010 and 2015 observations close to the optical maxima (cf. Appendix \\ref{sdd_LineFitRes_Appendix_RRAql}). \n\nThe maximum \\water\\ maser velocity range $\\Delta V_{\\rm los}$ \nbetween 23.2 and 31.9 \\kms\\ is asymmetric with respect to $V_{\\ast} = 28.5$ with emission from the front side of the shell reaching higher outflow velocities ($V_{\\rm out} \\sim 4.5-6.5$ \\kms) compared to the back side ($V_{\\rm out} \\sim 3$ \\kms). It is quite likely that this velocity range is only representative for the brightest maser features. \\cite{johnston85} detected in March 1982 with their sensitive VLA observations (rms $\\sim$ 0.35 Jy) emission between 19.7 and 36.9 \\kms. Such a large \\water\\ maser emission range was never confirmed later on, but it may indicate that in RR\\,Aql \\water\\ maser emission can be present over almost the full velocity range $V_{*} -V_{\\rm exp} < V_{\\rm los} < V_{*} + V_{\\rm exp}$ ($19.5-37.5$ \\kms; Table \\ref{centralcoords}) determined by the stellar wind, but is detectable only under exceptional circumstances.\n\n\\begin{figure}\n\\includegraphics[angle=-90,width=\\columnwidth]\n{rraql_lcurve_2.jpg}\n\\caption{ RR~Aql \\water\\ maser light curve.  See Fig. \\ref{fig:uher-lcurve} for details. $S(\\rm tot)$ was determined in the velocity range $22 < V_{\\rm los} < 34$ \\kms. The sine curve (blue) was obtained by a fit to the 1990--2011 radio measurements with a period $P_{\\rm opt} = 400$ days and is delayed by $\\phi_{\\rm lag} = 0.21$, i.e. by 84 days with respect to the optical light curve.}\n\\label{fig:rraql-lcurve}\n\\end{figure}\n\n\\subsection{\\label{sdd_LineProfAna_RRAql} \\water\\ maser line profile analysis}\nTo probe velocity variations of the features we made a similar profile analysis as for U\\,Her (Sect. \\ref{sdd_LineProfAna}) by fitting Gaussian profiles to the spectra. The\nassignment of the features to the spectral components is discussed in Appendix\n\\ref{sdd_LineFitRes_Appendix_RRAql}. We found five features labelled as maser spectral components A--E, which could be isolated by the fitting procedure. These components identified in individual spectra are graphically displayed in Fig. \\ref{fig:rraql-fvt} (right panel), where a direct comparison with the FVt-plot is possible. The corresponding flux densities and velocities are listed in Table \\ref{tab:compRRAql} \n(Appendix). Components A, C and E are the three dominant features in the maser profile, while components B and D could be isolated by the Gaussian fit only for some time lasting one to six years. \n\nThree to five spectral components are sufficient to describe the \\water\\ maser profile in the velocity range $26 < V_{\\rm los} <32$.  While in this range weaker maser components could not be identified, their presence is however likely, because such weaker maser emission is seen at velocities $23 - \\sim27$ \\kms\\ (see Appendix \\ref{sdd_LineFitRes_Appendix_RRAql}), although we failed to identify individual components. In contrast, there is no evidence in our spectra for emission at velocities larger than $V_{\\rm los} > 32$ \\kms, which leads to the observed asymmetry of $\\Delta V_{\\rm los}$ with respect to the stellar velocity.\n\nIn RR~Aql's \\water\\ maser shell, the components A--E represent the regions with the strongest emission only (at least over some time). Due to strong blending in velocity space they overlap in the maser profile and inhibit the identification of a spectral component, if the brightness contrast with respect to stronger neighbouring components becomes too large. We consider therefore the absence of spectral component B as a distinguishable feature in the maser profiles over more than $\\sim$15 years (1995 -- 2011, cf. Appendix  \\ref{sdd_LineFitRes_Appendix_RRAql}) not as evidence for extinction, but merely as inability to identify the component because of its relative weakness and blending with components A and C. In 2015 the dominant feature had a velocity $V_{\\rm los} \\approx 28.5$, which is in between the velocities seen for components B and C before, and which we assigned tentatively to component B. Also the identification of component D over only 1.5 years (Fig. \\ref{fig:rraql-fvt}, right panel) is mostly due to blending at the other times. \n\nBlending affects also the determination of the velocities of the spectral components. As is evident from Fig. \\ref{fig:rraql-fvt} (right panel) the components showed non-systematic variations in peak velocity within $\\sim1$ \\kms\\ over time. These variations are likely caused by blending of several maser components with velocity differences smaller than the typical line widths. This is corroborated by our experience that the profiles of the spectral components occasionally split into two or even three peaks. It is also expected from interferometric maps of RR Aql's \\water\\ maser shell, which showed for example in 1988 about two dozen maser spots separated in velocity by 0.3 \\kms\\ only \\citep{bowers94}. \n\nAs in U~Her, a remarkable property of the \\water\\ maser profile of RR~Aql is the apparent longevity of the spectral components in particular A and C in RR~Aql, which showed also the strongest emission over almost the full monitoring period of $\\sim$22 years. No systematic velocity shifts are recognizable, leading to the conclusion that\nthe emission regions in RR\\,Aql have properties as those of U~Her, in particular short lifetimes of at most a few years for individual maser clouds.\n\n\n\\begin{figure}\n\\includegraphics[angle=-90,width=\\columnwidth]\n{rraql-Stot-TJD-exp-270923.jpg}\n\\caption{RR\\,Aql \\water\\ maser light curves of the integrated flux $S(\\rm tot)$ from Pushchino 1980 -- 1997 (red) and Medicina/Effelsberg 1987 -- 2015. Vertical dotted lines are the (modelled) optical maxima with $P = 400$~days.}\n\\label{fig:rraql-lcurve-TJD}\n\\end{figure}\n\n\\begin{figure}\n\\includegraphics[angle=-90,width=\\columnwidth]\n{rraql-S-blue-red-TJD-exp-270923.jpg}\n\\caption{RR\\,Aql Medicina/Effelsberg \\water\\ maser light curves 1990 -- 1997 of RR\\,Aql showing the emission in the $V_{\\rm los} < 28.5$ and $> 28.5$ \\kms\\ part of the maser velocity range in blue and red color respectively. The sum of both (in black) is the integrated flux $S(\\rm tot)$ as shown in Fig. \\ref{fig:rraql-lcurve-TJD}. Vertical dotted lines as in Fig.~\\ref{fig:rraql-lcurve-TJD}.}\n\\label{fig:rraql-lcurve-blue-red}\n\\end{figure}\n\n\\begin{figure}\n\\includegraphics[angle=-90,width=\\columnwidth]\n{rraql-Sratio-blue-red-exp-270923.jpg}\n\\caption{Ratio $R = S(\\rm blue)$/$S(\\rm red)$ of the \\water\\ maser emission of RR~Aql of the $V_{\\rm los} < 28.5$ ($S(\\rm blue)$) and $> 28.5$ \\kms\\ ($S(\\rm red)$) part of the maser velocity range in 1987 -- 2015 of RR Aql. The vertical dotted lines are optical maxima as in Fig. \\ref{fig:rraql-lcurve-TJD}.}\n\\label{fig:rraql-lcurve-blue-red-ratio}\n\\end{figure}\n\n\n\n\\subsection{\\water\\ maser light curve}\nFigure \\ref{fig:rraql-lcurve} shows the maser light curve of RR~Aql using observations between 1990 and 2011, relative to the phase $\\varphi_{\\rm s}$ of the optical light curve. This radio light curve, based on integrated flux densities $S$(tot), and its relation to the optical light curve was analysed as for U~Her. $S$(tot) was determined over the velocity range  $22 < V_{\\rm los} < 34$ \\kms. \nThe general pattern as seen in the corresponding light curve of U~Her (Fig.~\\ref{fig:uher-lcurve}) is present here as well. The selection effect discussed in Section \\ref{sdd_phase-lag}, also\nin Fig.~\\ref{fig:rraql-lcurve} leads to on average higher integrated flux densities $S$(tot) of the Effelsberg- compared to the Medicina observations. While the maser emission varies on average following the optical light curve, the emission for a particular phase shows a large scatter.\nA fit of a sine wave to the radio data was made separately for the time ranges 1990 -- 1997 and 2000 -- 2011. We found little evidence for periodic variability in the first time range and a single dominating period of $405\\pm10$ days in the second time range. A fit to the complete data set 1990 -- 2011 yielded a period of $400\\pm5$ days (Table \\ref{centralcoords}), which coincides with the mean optical period in the time range 1987--2015. \nOur optical model light curve for RR\\,Aql has a period $P_{\\rm opt} = 400\\pm2$ days and a reference epoch for maxima TJD$_{max} = 6487 \\pm 5$ days (Table \\ref{centralcoords}); these numbers were determined as described in Sect. \\ref{sdd_OptModelLcurve} for U~Her. \nThe lag $\\phi_{\\rm lag} = 0.21$ (Table \\ref{centralcoords}) of the radio light curve is similar to the one of U\\,Her ($\\phi_{\\rm lag} = 0.16$) and other Mira variables of the sample that we monitored (Brand et al., in preparation). \n\nThe significant brightness scatter seen in Fig. \\ref{fig:rraql-lcurve} is caused by non-periodic brightness variations, which are caused by relative strength variations of the different spectral components. An overview of the integrated flux density light curve with (truncated) Julian Date is shown in Fig. \\ref{fig:rraql-lcurve-TJD}. The curve is dominated by the periodic variations of the maser integrated flux density, but superposed is an apparently bright period (8500 $<$ TJD $<$ 9500; $\\sim$1991 -- $\\sim$1995) and a relatively faint period after TJD $\\approx 13500$ (2005--2011). A zoom-in on the light curve for 1990--1997 is shown in Fig.~ \\ref{fig:rraql-lcurve-blue-red}, where we show also the emission contributions $S$(blue) and $S$(red) of the blue-shifted $V_{\\rm los} < 28.5$ \\kms\\ and  red-shifted $>28.5$ velocity range respectively, where $V_{*} = 28.5$ \\kms\\ is the adopted stellar radial velocity. Almost all the extra emission seen in this period is coming from the blue-shifted velocity range and is caused by the extraordinary strength of spectral component B in this epoch. Except for a few months in 2001 (TJD $\\approx 12200$), where spectral component A was strong, the emission in the blue-shifted velocity range remained modest after 2000, and the \\water\\ maser light curve was reflecting mostly the variations of spectral component C. After 2005 the light curve was following the overall decline of component C. Such long-term trends can be followed on timescales of several years longer than the stellar period (1.1 yr).\n\nAnother way to show the role of the secular variations of the individual spectral components is shown in Fig.~\\ref{fig:rraql-lcurve-blue-red-ratio}, where the ratio $R = S(\\rm blue)$/$S(\\rm red)$ is given as function of (truncated) Julian date.  The prevalence of $S(\\rm blue)$ is obvious in 8000 $<$ TJD $<$ 9000, but also the phase of dominance of the emission $S(\\rm red)$ in the red-shifted velocity range is evident around in 13000 $<$ TJD $<$ 14500. Typical spectra in these phases are shown in Fig.~\\ref{fig:rraql_sel}: On 25 October 1991 (TJD = 8554) showing spectral components A and B dominating and in 2005/2007 (TJD = 13359/14116) showing spectral component C dominating.\n\n\\subsection{Comparison with the Pushchino and VERA-Iriki monitoring programs \\label{sec:push-vera}}\nIn constrast to U~Her, RR~Aql was monitored also by other groups, allowing verification of our results. The Pushchino \\water\\ maser monitoring program (B98) of RR Aql between 1980 and 1997 overlapped with ours in 1990 -- 1997. Figure~\\ref{fig:rraql-lcurve-TJD} shows the light curve of integrated flux density of the \\water\\ maser between 1980 and 2011 combining observations of both monitoring programs. Both light curves are consistent with each other, although smaller brightness differences occur. This is caused likely by short-term brightness variations of the maser, leading to small brightness differences for observations made within several days . \n\nThe Pushchino light curve shows  the regular brightness variations connected to the stellar pulsations clearly. In agreement with our average phase lag $\\phi_{\\rm lag} = 0.21$, B98 could show that these variations are following the optical variations of the star with a delay of 10--30\\% of the length of the period. The Pushchino light curve also confirms the long-term change of the average brightness level in $\\sim$1991 -- $\\sim$1995 (8500 $<$ TJD $<$ 9500), caused by spectral component B at 28~\\kms\\ reaching flux density levels $>$400~Jy.\n\nParallel to our monitoring program, observations were made also within the VERA-Iriki monitoring program by \\cite{shintani08} between 2003 and 2006. Within the 3.3 years of the VERA-Iriki observations no long-term trends could be studied. In accordance with our observations, spectral components A, C and E were found to be dominant also in their observations. \n\nThe strongest features reported by single epoch observations overlapping with our monitoring program before 2000 were in April/May 1991 at 27.9 \\kms\\ (our spectral component B) \\citep{takaba94} and in October/November 1991 at 26.9~\\kms\\ (our  spectral component A) \\citep{takaba01}.  These velocities are in agreement with our strong components seen at these times. In October 1991 spectral components A and B were competing in brightness, and in our spectrum of 25 October 1991 component B was actually 40\\% brighter than component A. After 2000 only one single epoch observation was made. In accordance with our observations that component E became very weak after May 2009 (see Appendix B.3), \\cite{kim10} showed from their June 2009 observation that the component was indeed getting fainter.\n\n\n\\section{\\water\\ maser properties in Mira variable stars \\label{sec:miraprop}}\n\n\\noindent\n\n\\subsection{\\water\\ maser\\ and optical variability -- phase lags}\nThe prevailing pattern of the  \\water\\ maser brightness variations in U\\,Her and RR\\,Aql are the periodic variations, which respond to the brightness variations of the central stars. Their maser variations lag behind the optical variations by $\\phi_{\\rm lag} = 0.16$ and 0.21, respectively. Similar lags were also found for two other Mira variables o~Cet and R~Cas, monitored by us, and discussed in a separate paper (Brand et al., in preparation). The retarded maser variations are a well known pattern for Mira variables (\\citealt{schwartz74}; \\citealt{little-marenin91}; \\citealt{rudnitskii05} and references therein) and red supergiants \\citep{pashchenko99, lekht05}, where the stellar brightness variations have a large amplitude. This differs from the pattern seen in semi-regular variables  of type SRb, such as RX~Boo and SV~Peg (Paper I), R~Crt and RT~Vir (\\citealt{lekht99}; Paper II), which do not show large-amplitude periodic variations. The delayed response of maser emission to stellar brightness variations is a general characteristic, and this behaviour is well documented also for SiO masers \\citep{pardo04} and OH masers \\citep{fillit77, etoka00}.\n\nOften, the lags have been interpreted as the result of the travel time of propagating shock waves through the \\water\\ maser zone  \\citep{lekht01, pashchenko04}. \\cite{shintani08} observed periodic velocity variations of maser features in several stars as a general pattern, which they consider as confirmation for the passage of shocks, which first accelerate and later decelerate affected maser clouds. However, lags are also seen in the infrared \\citep{lockwood71, smith02, smith06, ita21}. Infrared, SiO and \\water\\ maser emissions all peak in brightness in the CSE at different distances, so that the lags, having a uniform magnitude, cannot be due to travel time effects.\nGuided by the findings of \\cite{shintani08} of periodic velocity shifts in a large number of \\water\\ maser features of several stars, we searched for patterns in the apparent velocity variations of the major spectral components \\Gone\\ and \\Gtwo\\ of U~Her and component C of RR~Aql. We found no regularity in the peak velocity changes over time, and conclude that the variations confined to $<0.5$ \\kms\\ are due to the influence of weaker maser features with similar velocities on the peak velocities derived from the Gaussian fits. \n\nFor the Mira variable BX\\,Cam, periodic velocity variations were not found either, but systematic velocity drifts $\\le$1.3 \\kms\\ over timescales of 2--3 years were detected in many maser spectral features, with the blue/red-shifted components decreasing/increasing in line-of-sight velocity $V_{\\rm los}$, which is consistent with expansion \\citep{xu22}.\n\nIf the shock waves propagate radially outwards, their influence on the velocities $V_{\\rm los}$ of the stronger spectral components of U\\,Her and RR\\,Aql furthermore is diminished due the small inclination angles of the outflow directions with respect to the plane of the sky. We were not able to find any evidence for the presence of shock waves in the \\water\\ maser shells of the two Mira variables. \nWe consider therefore the more likely explanation of the lags of the maser brightness variations relative to the optical to be the presence of strong titanium oxide (TiO) absorptions in the visual band at stellar maximum, as was invoked to explain the lags in the infrared by \\cite{smith06}. They find that the TiO absorption truncates the rise in the optical light before the maximum in the near-infrared is achieved. Therefore, the phase-lags are not relevant for the understanding of the conditions in the stellar wind, which allow the emergence of \\water\\ maser emission.\nThe phases of the \\water\\ maser light curves are therefore probably better indicators of the phases of the stellar bolometric variations than those of the much more frequently available optical light curves.\n\n\\subsection{Short- and long-term maser variability}\nWe found compelling evidence that the \\water\\ maser brightness variations in the SRVs R~Crt and RT~Vir \nare a superposition of two types of variations with different timescales. There are short-term fluctuations on timescales $\\la$1.5 yr, and long-term variations on timescales of decades (Paper~II). In the case of the Mira variables U~Her and RR~Aql these two types of variations are also present and lead to significant cycle-to-cycle variations of the otherwise periodically varying maser light curves (Figs. \\ref{fig:uher-lcurve} and  \\ref{fig:rraql-lcurve}). \n\n The short term fluctuations seen in integrated flux density are caused by random brightness variations of the individual spectral components, as has been shown for example for the 1991 -- $\\sim$1995 absolute maximum of RR~Aql's light curve (Fig. \\ref{fig:rraql-lcurve-blue-red} and Sect. \\ref{sec:push-vera}). These short-term variations are probably controlled by the coming and going of individual maser clouds, as the limited lifetimes we found for the maser clouds of U~Her indicate.\n \n Very strong short-term fluctuations (\"bursts\") of individual spectral components at times can dominate the maser profile. Such bursts \n  found in Mira and SR variables by the Pushchino monitoring programs \\citep{lekht99, esipov99, pashchenko04} and observed by us for example in RX~Boo (Paper I), in R~Crt and RT~Vir (Paper II), were not seen in the case of U~Her (1990 -- 2015, Fig. \\ref{fig:uher-stot-tjd}) or RR~Aql (1980 -- 2015, Fig. \\ref{fig:rraql-lcurve-TJD}) (although as mentioned in Sect.~\\ref{sdd_longterm_lc} the strong emission during the '1991/1992 peculiar phase' of U\\,Her might have been a burst). We assume that bursts are common but that their frequency of occurrence is not large, so that the absence of bursts in the latter two cases could be due to still insufficient time coverage of the maser light curves. \n\nLong-term maser brightness variations lasting many pulsation cycles are evident for the two Mira variables, if the \\water\\ maser light curves are displayed as function of time, either integrated over the full profiles or in selected velocity ranges. There is overall dimming and brightening of the different spectral components unrelated to each other, which determine the overall brighteness level but also their relative contributions to it. This basically reflects the change of the excitation conditions in the different parts of the \\water\\ maser shell, which occur on timescales which are closer to the crossing time (8.5 years in the  case of U\\,Her) of material through their \\water\\ maser shell than to the period of stellar pulsations. \n\n\\subsection{Lifetimes of \\water\\ maser clouds}\nIt was not possible to obtain constraints on the lifetimes of individual maser clouds in the winds of the two Mira variables based on the emergence and disappearance of individual maser features in the single-dish spectra. This is due to the wealth of components, which overlap in velocity space. The relative brightness fluctuations lead to small velocity shifts for the spectral components, so that individual maser components may be detectable individually in the spectral profiles only during short times when they are strong. \n\nThe smaller number of spectral components identified in the \\water\\ maser profile of RR~Aql compared to U~Her is likely also a consequence of blending in velocity space. The final expansion velocity of RR~Aql's wind is about 65\\% of that of the wind of U~Her (Table \\ref{centralcoords}), so that the acceleration in the \\water\\ maser shell is smaller. Given the same spectral resolution, separating the spectral components is then more difficult, assuming that the number of maser clouds and the line width distribution of the maser features are comparable in both stars. The spatially resolved maps are the only means to break the degeneracy present in velocity space. We identified nearly 30 different spatial components in the 1990--1992 VLA maps of U~Her compared to fewer than 10 spectral components in the single dish spectra detected during this period. \nBased on the VLA map from \\cite{bowers94} a similar discrepancy in the numbers of spatial and spectral components is present also for RR~Aql.\n\nThe lifetimes can be constrained by the apparent absence of  acceleration of the spectral components (cf. Sect.\\,\\ref{region-lifetime-1}). In both stars they showed constant line-of-sight velocities over the monitoring period independently from their location in the maser profile. If the emission clouds participate in the stellar outflow, their outflow velocities would increase with time and hence also their absolute projected velocities, \nmodified by the inclination of the outflow direction with respect to the plane of the sky. The expected velocity drifts are not seen (see the FVt-plots Fig. \\ref{fig:uher-fvt} and \\ref{fig:rraql-fvt}). This observation can be reconciled, if the emission clouds are themselves short-lived and new clouds regularly emerge in a longer-living (stationary?) larger region within the shell, as we have discussed for U\\,Her (c.f. Sect. \\ref{region-lifetime-2}). The clouds would then be created with similar line-of-sight velocities, and following each other would make up a spectral component with constant velocity. The presence of such regions is also\nsuggested by the mapping observations of BX\\,Cam in 2012--2014 by \\cite{matsuno20} and in 2018--2021 by \\cite{xu22}, where the emission sites were found in the same parts of the maser shell, despite the stellar wind material in the shell having been exchanged completely between the epochs, given the crossing time of 7.6 years reported by the latter authors.\n\nAs the clouds follow the stellar wind with increasing outflow velocities, the spectral components in principle cannot have a strictly constant velocity. Our velocity outflow model (Eq. \\ref{eq:exp_model}) for U\\,Her predicts an acceleration of 0.5 \\kms yr$^{-1}$ in the center of the shell. Adopting a typical lifetime of 2 years \\citep{bains03}, a maser cloud therefore will have changed its outflow velocity by 1.0 \\kms\\ and its line-of-sight  velocity $V_{\\rm los}$ by $\\le$1 \\kms. Velocity drifts of this size are in agreement with the systematic increase/decrease of line-of-sight velocities by up to $\\sim$0.4 \\kms yr$^{-1}$ observed for the \\water\\ maser spectral features in BX\\,Cam by \\cite{xu22}. \nAccording to our model, the cloud also will have moved radially by $3-4$\\, AU during its 2 years lifetime. \nFor U\\,Her, \\cite{richards12} found an increase of the line-of-sight velocities with distance from the star and derived a velocity gradient in the plane of the sky of $0.31\\pm0.28$ \\kms\\ AU$^{-1}$. A gradient of 0.31 \\kms\\ AU$^{-1}$ translates to a drift in outflow velocity by $0.9-1.2$ \\kms\\ for a movement over $3-4$\\, AU, which is compatible with our estimate of its change in outflow  velocity derived above from the model. We conclude that the line-of-sight velocity drifts of the spectral components, which are expected to be $\\le$1 \\kms\\ over two years are masked in our single-dish data, because the size of the drifts is of the order of the uncertainties in our velocity determinations due to blending with neighbouring features. Components with aspect angles $>30^{\\circ}$ would have line-of-sight velocity drifts, which surpass our limit of 0.5~\\kms. Of the spatial components plotted in Fig. \\ref{fig:uher-3Dvert} that have aspect angles $\\theta>30^{\\circ}$, only B and C are strong enough to be seen \ncontiguously for over more than a year. \nThese components are seen between 1990 and 1996 (see Fig. \\ref{fig:uher-fvt}). They would be good candidates for the detection of drifts (for B one expects 0.9~\\kms\\ in two years), but none are seen. They play a prominent role in the '1991/1992 peculiar phase' and their behaviour points to lifetimes $<$2 years, maybe even months (see discussion in Appendix \\ref{sdd_LineFitRes_Appendix}). The apparent constant velocity of the spectral components as shown in the U\\,Her's FVt-Plot (Fig. \\ref{fig:uher-fvt}) is therefore due to the superposition of clouds created with almost equal velocities and replacing each other every few years. The clouds should have systematic velocity drifts over typical maser cloud lifetimes, but these are not detectable in our single-dish spectra.\n\n\n\\subsection{Long-living regions \\label{sec:regions}}\nThe long-term maser brightness variations and the observation  that the clouds responsible for the strongest spectral components in U\\,Her are found in the same region of the maser shell over at least 6.5 years (almost 6 stellar pulsation cycles), indicate the presence of inhomogeneities in the otherwise (assumed) spherical stellar wind of Mira variables. One indicator for such inhomogeneities could be parts of the shell, where the conditions are more favourable for exciting the \\water\\ molecules ('long-living regions') than in others. \nWhile individual (maser) clouds would be clumps of material with enhanced density moving with the stellar wind, such regions remain stationary in location, at least as long as the wind is not disturbed at these locations. The individual clouds then light up, while they pass through the regions. \nThe existence of such regions would then naturally explain, why maser spectral components like component K in U\\,Her (see Sect. \\ref{region-lifetime}) regularly reappear over about 15 years at almost the same line-of-sight velocity, although different short-living maser clouds must have contributed.\n\n\n\\begin{table*}\n\\caption{\\water\\ maser luminosities, stellar luminosities, and mass-loss rates of the semiregular variable stars from Paper~II and the Mira variables U~Her and RR~Aql. Column \"D\" lists distances as given in Table~\\ref{centralcoords}, Paper I and II. Characteristic levels of \\water\\ maser brightness ($S(\\rm tot)$ = integrated fluxes) and maser luminosities (\\Lup  [L$_{\\odot}$]; $L_{\\rm p}$[photons s$^{-1}$]) are listed for the time range 1987-2015 for all sources except RX~Boo and SV~Peg (1987--2005; cf. Paper I). The definition of the levels (High, Mean, Low) is described in the text. Columns $\\log$\\,$L_{\\rm bol}$ and $\\log$\\,\\mdot\\ list stellar luminosities and mass-loss rates.} \n\\label{table:photon-luminosities}\n\\begin{center}\n\\begin{tabular}{lrr|c|rr|rr|rr|c|c}\n\\hline\\noalign{\\smallskip}\n\\multicolumn{1}{c}{Star} & \\multicolumn{1}{c}{Type} & \\multicolumn{1}{c|}{D} & log\\, \\Lup &\n\\multicolumn{6}{c|}{log\\,$S(\\rm tot)$ , log\\,$L_{\\rm p}$} & log\\,$L_{\\rm bol}$ & log\\,\\mdot \\\\[0.05cm]\n\\multicolumn{2}{c}{} & \\multicolumn{1}{c|}{[pc]} & [L$_{\\odot}$] &\n\\multicolumn{6}{c|}{[Jy~\\kms] , [s$^{-1}$]} & [L$_{\\odot}$]    &  [\\Myr]   \\\\\n&&&& \\multicolumn{2}{c|}{High} & \\multicolumn{2}{c|}{Mean}     & \\multicolumn{2}{c|}{Low} &&\\\\\n\\hline\\noalign{\\smallskip}\nR~Crt    & SRV &  236 & $-$4.88& 3.5    & 44.0& 2.9 & 43.4 &    2.3 &    42.8 &  $4.03\\pm0.10$ & $-5.46$ \\\\\nRT~Vir   & SRV &  226 & $-$5.16& 3.3    & 43.8& 2.8 & 43.3 &    2.3 &    42.8 &  $3.70\\pm0.09$ & $-6.05$ \\\\\nRX~Boo   & SRV &  136 & $-$6.23& 2.6    & 42.7& 1.9 & 41.9 &    1.5 &    41.5 &  $3.58\\pm0.11$ & $-6.12$ \\\\\nSV~Peg   & SRV &  333 & $-$6.42& 1.6    & 42.4& 0.9 & 41.7 & $<$0.8 & $<$41.6 &  $3.93\\pm0.20$ & $-6.04$ \\\\\nU~Her    &Mira &  266 & $-$5.77& 2.6    & 43.3    & 2.0    & 42.6    &    1.5 & 42.1   &  $3.71\\pm0.17$ & $-6.25$ \\\\\nRR~Aql   &Mira &  410 & $-$5.37& 2.7    & 43.7    & 2.1    & 43.1    &    1.4 & 42.4   &  $3.75\\pm0.15$ & $-5.84$ \\\\\n\\noalign{\\smallskip}\\hline\n\\end{tabular}\n\\end{center}\n\\end{table*}\n\n\nAlso in the SRVs RX\\,Boo, RT\\,Vir and R\\,Crt we have found evidence for the existence of long-living regions within the \\water\\ maser shell, in which preferred conditions for exciting the \\water\\ molecules exist.  In RX\\,Boo they manifested themselves as spatial asymmetries of the \\water\\ maser emission persistent for at least 11 years (Paper\\,I). For RT\\,Vir and R\\,Crt, the decade long variations of the \\water\\ maser brightness in particular velocity ranges, was attributed to such regions having possibly higher-than-average densities and being present in the \\water\\ maser shells for about two decades (Paper\\,II). Based on 3D models of \\cite{freytag23} (and preceding papers) we argued that such regions could be the remnants of large convective cells, which left the stars as part of the stellar winds. Having reached the \\water\\ maser shell they may have been inflated to sizes comparable to the width of the maser shells. They may provide the shell sectors with improved maser excitation conditions, in which clouds passing through are preferentially excited. Observationally it would lead to an asymmetric spatial maser distribution within an otherwise spherical symmetric \\water\\ maser shell. If the presence of (sub)-stellar companions plays a major role in shaping the stellar winds \\citep{decin20,gottlieb22}, their influence on the velocity field may be another factor to create long-living regions with improved excitation conditions. These regions could be in parts of the shell relatively far from companions, where the stellar wind is less disturbed. Lifetimes of the long-living regions would probably be of the order of the crossing time of material through the shell or of the orbital periods of the companions. \n\nWhile the spherical \\water\\ maser shell is a persistent feature with some variations of its boundaries, the maser clouds are a transient phenomenon with a lifetime of a few years. The regions, in which favourable excitation conditions for \\water\\ maser emission occur, are intermediate in the sense that they are part of the \\water\\ maser shell, but not lasting. They have long but limited lifetimes, because of the inhomogeneity of the stellar wind. Over time, such regions where maser clouds pass through are created at different locations within the shell. \n\n\\subsection{\\water\\ maser luminosities}\nIn Table~\\ref{table:photon-luminosities}, we give luminosity information on U~Her and RR~Aql, as well as those of the four SRVs treated in Paper I and II. For the details of how the luminosities were derived, we refer to Paper~II. In column 4, we give \\Lup, the potential maximum \\water\\ maser luminosity derived from the upper envelope, which represents the maximum output which the source could produce if all the velocity components we observe were to emit at their maximum level, at the same time and equally in all directions. The table also lists characteristic levels (high, mean, and low) of maser brightnesses, as given by integrated flux densities $S(\\rm tot)$ in Jy \\kms\\ (Cols. 5,7,9) and corresponding maser luminosities $L_{\\rm p}$ in photons per second (Cols. 6,8,10). The brightness of the mean level is the median of all integrated flux density measurements \nwhile the high and low level are represented by the median of the seven highest and lowest integrated flux density measurements, respectively. \n\nTable~\\ref{table:photon-luminosities} shows that the mean maser luminosities of U\\,Her and RR\\,Aql (as well as \\Lup) are in the range of luminosities shown by the SRVs. The ratio between the high and low level of the two Mira variables is 15--20, which is slightly higher than for the four SRVs with ratios $>$6--16 ($>$35--45 between \\Lup\\ and the low levels of $L_{\\rm p}$ compared to $>$25--55 of the SRVs).\n\nAs in Paper~II, we compared the \\water\\ maser photon luminosities with the bolometric luminosities and mass-loss rates of the stars. The bolometric luminosities $L_{\\rm bol}$ (Col. 11) were determined from bolometric fluxes, as described in \\cite{jimenez15}, and the distances listed in Table~\\ref{table:photon-luminosities}. The mass-loss rates \\mdot\\ (Col. 12) were taken from \\cite{loup93} (U~Her) and \\cite{danilovich15} (RR~Aql) scaled to the distances used here. All mass-loss rates come from measurements of the CO molecular line and are estimated to have an error of a factor of 3 (= 0.5 dex) prior to the uncertainties introduced by the distances. We note however, that due to a general underestimate of the terminal stellar wind velocities in the past, these mass-loss rates could be systematically overestimated \\citep{gottlieb22}. The results are listed together with those of the SRVs (see Paper~II) in the last two columns of Table~5.\n\nWe find that the two Mira variables have stellar luminosities and mass-loss rates within the range shown by the SRVs. Not surprisingly also the \\water\\ maser photon luminosities fit well within the range of those of the SRVs. The stars selected by us for \\water\\ maser monitoring all showed apparently bright masers since their discovery. Except for SV~Peg, all of them appear among the 10\\% brightest galactic \\water\\ masers in late-type stars known by the year 2000 on the sky north of $\\delta > -30^\\circ$ \\citep{valdettaro01}. Besides a few Red Supergiants this list of late-type stars is composed of optically bright Mira and SR variables.  It is therefore obvious that the stars from our monitoring program represent only the upper end of the photon luminosity distribution of \\water\\ masers of Mira and SR variables in the solar neighbourhood.\n\n\\subsection{\\label{sdd-cse-asym} Constraints on the CSE standard model}  \nThe analysis and discussion of the \\water\\ maser properties of U\\,Her and RR\\,Aql are based on the standard model, which assumes the presence of a spherical \\water\\ maser shell within a circumstellar envelope continuously fed by a homogeneous spherically outflowing stellar wind with smoothly increasing velocities. The stellar pulsations naturally lead to maser brightness variations and connected with this to variations of the velocity range over which emission is detected above the sensitivity limit.  The boundaries of the maser shell are, strictly speaking, boundaries for luminous maser sites, and may change over time due to evolution of the excitation conditions and changes in beaming directions. The time coverage of the maser observations since their discovery is still too short to understand the origins of the long-term variations, but they indicate that there are regions of improved excitation conditions within the maser shell, as discussed in Sect \\ref{sec:regions}, and hence inhomogeneities in the  spherical outflow.\n\nThe filling factor of detectable maser clouds within the maser shell is rather small \\citep{richards12} so that asymmetries seen in maser maps can be due either to accidentally emerging illumination variations, or is another indication that the underlying stellar wind itself is inhomogeneous. 3D models of the formation of dust-driven winds in AGB stars \nby \\cite{freytag23}, indicate that the stellar wind starts clumpy above the photosphere. It is not clear how much of the clumpiness is still present at the location of the \\water\\ maser shell (10--30 AU from the star for SRV and Mira variables), but the related velocity variations of the wind due to shock waves generated close to the photosphere may have become negligible at these distances \\citep{bladh19}. Smoothly increasing outflow velocities described by a velocity law with continuous acceleration and approaching a constant outflow at large distances, as for example expressed by Eq. (\\ref{eq:exp_model}), may therefore still be applicable. \n\nHowever, new observations of several molecular species and transitions in the CSEs of several AGB stars and RSGs challenge the validity of such a law \\citep{gottlieb22}. While these authors observe an overall increase of outflow velocities with radial distances, the scatter  is not compatible with a smooth velocity law. The scatter is ascribed to the presence of a (sub)stellar companion which disturbs the velocity field. In addition, they find that the final expansion velocities are up to a factor of two higher than previously adopted. For U\\,Her this is $V_{\\rm exp} = 19.7$ \\kms\\ compared to 13.1 \\kms\\ as adopted by us (Table \\ref{centralcoords}). RR\\,Aql was not observed. Their outflow velocity law for U\\,Her is  $V(r) = V_0 + (V_{\\rm exp}-V_0)  \\cdot  (1- r_0/r)^\\beta$, where $r_0 = 2.6$ R$_*$, $V_0\\sim4.4$ \\kms\\ and $\\beta=2.00\\pm0.54$. Parameter $r_0$ is the dust condensation radius, where the wind starts, and $V_0$ is the velocity at this radius. This law is considered an approximation and is given for distances in the CSE between $\\sim$2.6 and 500 R$_*$ ($\\sim4 - 700$ AU). \n\nAdopting this law in our model of the \\water\\ maser shell (Sect. \\ref{sec:shell-model}), the larger final expansion velocity leads to an increase of the acceleration in particular in the inner shell, where the \\water\\ masers are located. For example at radial distance r=18 AU the increase is from 0.5 to 1~\\kms yr$^{-1}$. Correspondingly the crossing time and the lifetimes derived for individual maser clouds are decreased by a factor of $\\sim$1.8.\n\nAs long as acceleration is present in the \\water\\ maser shell, the absence of velocity drifts of the \\water\\ maser spectral components constrains the lifetimes of the maser emitting clouds. A lifetime larger than the monitoring period of 22 years, would require a rather small acceleration $a < 0.023/ \\sin \\theta$~\\kms yr$^{-1}$ to avoid line-of-sight velocity shifts $|\\Delta V_{los}| > 0.5$ \\kms\\ (see Sect.~\\ref{region-lifetime-1}). \nThis would need almost a suspension of the acceleration within the \\water\\ maser shell (i.e. a step-wise increase of the velocity curve as discussed by \\cite{Decin15} in the case of the carbon star IRC+10216) or an approach to the final expansion velocity already at the inner maser shell radius. We adopted the latter explanation for the SRV RT\\,Vir, where we observed a maser feature showing a constant velocity within $< 0.06$ \\kms\\ over 7.5 years. We suggested the maser-emitting cloud to move in the outer part of the shell, where the final expansion velocity could have been reached already. The alternative, maser clouds with short lifetimes created again and again, was considered as less likely because of the very small velocity scatter (Paper II). \n\nSo far, a step-wise increase of the velocity curve has not been considered to explain the observed velocities in the \\water\\ maser shell. \nThe model presented in Sect. \\ref{sec:shell-model} for U\\,Her is strictly constrained only by the outflow velocity $V_{\\rm out} = 9.6$ \\kms\\ at a radial distance of 23.9 AU. Forcing the adopted velocity law to start at the photosphere ($r_0=1.4$ AU, $V_0 = 0$ \\kms) is a rather coarse assumption given that in U\\,Her's CSE, \\water\\ line emission from the (0,2,0) $6_{52} - 7_{43}$ transition at 268 GHz line was detected over a velocity range $-24.2$ to $-4.5$ \\kms\\  by \\cite{baudry23} within a projected distance of $\\sim$3.2 AU from the center of the star. This implies gas motions with outflow and infall of up to 10 \\kms\\ in the inner shell, and a smooth velocity law may not be applicable within several stellar radii from the star.\n\nIn principle, the outflow velocity could reach $V_{\\rm out} = 9.6$ \\kms\\ at the inner maser shell boundary, stay constant up to the outer maser shell boundary, and increase further beyond. \nIn the case of a step-wise increase of the velocity curve, a section with constant velocity of 9.6 \\kms\\ will have improved velocity coherence, and naturally delineate the shell, where \\water\\ maser emission preferentially occurs. The boundaries at $\\sim5$ and $\\sim30$ AU would be determined by the loss of velocity coherence outside the section with constant outflow velocity. However, marginal acceleration would favour radial amplification paths and one would expect double-peaked maser profiles strongest close to $\\pm V_{\\rm out}$ (in our case $\\pm9.6$ \\kms) as seen for 1612 MHz OH masers, which are excited at much larger distances from the star, where the outflow velocity also in the standard model is almost constant. This is however not observed, as \\water\\ maser profiles of SR- and Mira variables generally peak close to the stellar velocity \\citep{takaba94}, meaning that tangential maser amplifications paths are preferred to radial ones. \\cite{richards12} also argue against an absence of acceleration within the \\water\\ maser shell, as they found that in the shells of all observed AGB and RSG stars the outflow velocity increases by a factor of two. Following the numerous indications that the velocity field of the stellar winds is likely disturbed over a great part of the CSE, we consider therefore an almost constant velocity over several AU delineating the \\water\\ maser shell unlikely. \n\n\\section{Conclusions \\label{sec:conclusions}}\nWe analysed the properties of the \\water\\ maser emission of two Mira variables U~Her and RR~Aql, which both show maser emission spread in velocity over several \\kms, with usually bright individual features ($>100$ Jy). Their masers are among the strongest \\water\\ masers seen in late-type stars on the sky.\n\nThe variability of the maser emission is dominated by regular brightness variations synchronised with the stellar pulsation. As observed generally in Mira variables, the maser brightness variations are delayed relative to the optical variations, in the case of U\\,Her and RR\\,Aql by two and nearly three months, respectively. We attribute the cause of this phase-lag to the influence of absorption by molecular bands on the optical light curve. Superposed on the regular variations are brightness fluctuations on shorter time scales, which are due to secular variations of individual spectral components. Also on longer time scales variations are seen of the average brightness level in both stars. The brightness levels in the blue- and red-shifted velocity ranges show (on these time scales) variations that are independent from each other. This indicates that the stellar wind is inhomogeneous, which leads to varying excitation conditions in the front and rear cap of the \\water\\ maser shell. This change of excitation conditions can lead to a completely different spectral profile, where in the case of U\\,Her the usually dominating profile peak at -15 \\kms\\ was exceeded for 1.5 years by peaks at $< -18$ \\kms\\ ('1991/1992 peculiar phase'). Apart from that, the \\water\\ maser variability patterns of U\\,Her and RR\\,Aql are remarkably similar.\n\nBased on our VLA observations 1990-1992, we find that the \\water\\ masers of U\\,Her were located in a spherical shell with a size of 11-25 AU. Additional weak maser emission was found inside and outside these boundaries. Comparing this result with interferometric observations from the literature obtained at other epochs shows some variations of the size, but in general the strongest emission comes from radial distances of $\\sim15-20$ AU. With a radius of $\\sim$21 AU reported for RR\\,Aql's \\water\\ maser shell, also the shell dimensions corroborate the similarity of the \\water\\ maser properties between U\\,Her and RR\\,Aql.\n\nThe absence of velocity drifts strongly argues for short lifetimes of maser clouds (about $<4$ years). The location of the main emission features of U~Her in the same part of the \\water\\ maser shell over at least 6.5 years and the regular reappearance of emission at the same velocity (see component K in U~Her), support the idea of there being long-living regions, sections of the maser shell, with favourable excitation conditions which last over time scales longer than the stellar period and the lifetime of individual maser clouds.\n\nThe \\water\\ maser luminosities of the two Mira variables are within the range of luminosities derived in Paper\\,I and II for SRV's. They have variation properties very similar to the SRV's, except for their periodic variations as a consequence of the stellar pulsation (via pumping variations).\n\nVariability is present on several timescales, so that the conclusions drawn from our monitoring program lasting more than two decades are strictly valid only for the scales covered. For U\\,Her, the length of the monitoring program is about twice the time needed for the stellar wind to cross the \\water\\ maser shell. Variations in the mass-loss rates on longer timescales (hundreds to thousands of years) may lead to a loss of U~Her's and RR~Aql's prominent status among the strongest circumstellar \\water\\ masers observed from Earth. They may even join the much larger group of Mira variables which are currently not detected with \\water\\ masers, while in other Mira variables \\water\\ maser emission may brighten and/or be beamed into our line of sight. \n\nWith the validity of a smooth acceleration of the stellar wind put into question recently, the advantage of using \\water\\ masers to trace the velocity field of the inner wind region of AGB stars ($r\\la20$  R$_*$) will gain importance.\n\n\\begin{acknowledgements}\nThe Medicina 32-m data presented here are part of a long-term monitoring program, which concerned both late-type stars and star-forming regions. Thanks to those who helped with the observations.\nWe are grateful to the staff at Medicina observatory for their expert assistence and technical problem-solving. The Medicina radio telescope is funded by the Ministry of University and Research (MUR) and is operated as National Facility by the National Institute for Astrophysics (INAF). This research is partly based on observations with the 100-m telescope of the MPIfR (Max-Planck-Institut für Radioastronomie) at Effelsberg, and the VLA (Very Large Array). \nThe VLA is operated by the National Radio Astronomy Observatory, which is a facility of the National Science Foundation operated under cooperative agreement by Associated Universities, Inc. This research has made use of the SIMBAD database and the VizieR catalogue access tool, operated at CDS, Strasbourg, France, and of NASA's Astrophysics Data System. For data reduction and the preparation of figures GILDAS software available at www.iram.fr/IRAMFR/GILDAS was used.\nWe acknowledge with gratitude the variable star observations from the AAVSO International Database contributed by observers worldwide and used in this research.\nThe spectra are available in fits-format at CDS; also the VLA data cubes can be downloaded from there.\n\\end{acknowledgements}\n\n\\bibliographystyle{aa}\n\n\n\n\n\\Online\n\n\\appendix\n\n% <BEGIN_FILE: appendix.tex>\n\\section{\\label{sdd_LineFitRes_Appendix} U Her Spectral line fitting results}\nThe maser features identified between 1987 and 2015 in the individual spectra are listed in Tables \\ref{tab:compUHerB-E} and \\ref{tab:compUHerG-M}, where they were assigned to different spectral components separated through Gaussian fits. For each spectrum we give the Gregorian and Julian dates, its \\textsl{rms} noise level in Jansky, the integrated flux $S$(tot) in Jy\\,\\kms, the phase of the optical lightcurve $\\varphi_s$, and \nthe velocity ($V_{\\rm p}$) and peak flux density ($S_{\\rm p}$) for each component. Flux density values marked by ``:'' and upper limits were measured with the cursor on the computer screen. Velocity values marked by ``:'' are uncertain.\n$\\varphi_{\\rm s}$ was calculated for each observing date using the optical period and a reference epoch for the optical maxima TJD$_{max}$ (i.e. $\\varphi_{\\rm s} = 0$), as given in Table \\ref{centralcoords}. In the following we discuss in detail the variations in flux density and velocity of the spectral components, for the main velocity ranges discussed in the main body of the paper.\n\n\\subsection{The $V_{\\rm los}<-18$ \\kms\\ velocity range}\nThe most blue-shifted emission observed by us in 1987 -- 2015 was detected at $-23.8$ \\kms\\ in the Oct.\\,1991 VLA observation (spatial component A1 in Table \\ref{tab:components}). The emission was weak ($\\approx$0.4 Jy/beam) and not seen in the single-dish spectra of this epoch. Emission at velocities $<-22$ \\kms\\ was seen in the single dish spectra only occasionally and was blended with the stronger spectral component B. The most blue-shifted emission detected in Effelsberg and Medicina was seen in 1995 with a peak at $-22$ \\kms and extending down to $-23.3$ \\kms, while the star was increasing its optical brightness ($0.7 < \\varphi_{\\rm s} < 1.0$). The emission was on the $\\approx$1 Jy level. We conclude therefore that in the velocity range $-24 < V_{\\rm los} < -22$ \\kms\\ (the location of a putative spectral component A) maser emission was probably present, although below our detection limits most of the time.\n\nIn addition to this low-level emission we identified three spectral components B--D at velocities $V_{\\rm los}<-18$ \\kms. The emission in the $-22< V_{\\rm los} <-20$ \\kms\\ region (spectral component B) was seen only until 1996 and was in general about a factor of 10--100 weaker than emission at higher velocities. Only during a very brief period (the '1991/1992 peculiar phase') in Oct./Nov.\\,1991 (TJD = 8959) component B (then at $-20.6$ \\kms) was the strongest of all spectral components (Fig.~\\ref{fig:uher_sel}). The component was not detected in May 1991 and declined by a factor of $\\sim15$ until January 1992, constraining the duration of the '1991/1992 peculiar phase' to a few months.\n\nDuring the '1991/1992 peculiar phase', emission at $-20< V_{\\rm los} <-19$ \\kms\\ (spectral component C) increased in intensity after May 1991 (TJD $\\sim 8400$), reached its peak in Jan.\\,1992  (TJD $\\sim 8640$) and faded to the level seen before and afterwards until March 1992 (TJD $\\sim 8700$).  A similar short dominance was seen in Sep.\\,1996 (TJD = 10353), where spectral velocity component \\Done\\ ($V_{\\rm los} = -18.8$ \\kms) was the strongest feature in the profile. The brightenings seen in spectral components B, C, and \\Done\\ lasted typically 6 to 12 months.  \n\nThe emission between $V_{\\rm los} = -19$ and $-18$ \\kms\\ was dominated by spectral components \\Done\\ (strong in 3/1995 -- 2/1997, TJD = 9790 -- 10486) and \\Dtwo, which often were difficult to separate in velocity space. Therefore Tables~\\ref{tab:compUHerB-E}  and \\ref{tab:compUHerG-M} usually list only one of both components depending on the peak velocity provided by the Gaussian fit, although most probably both components where present. In February 1992 (TJD = 8682; $\\varphi_{\\rm s} = 0.97$) component \\Dtwo\\ reached the strongest flux density ($>300$ Jy) ever observed between 1987 and 2015 for an individual spectral feature. Until October 1992 it decreased to a level of $\\sim10$ Jy and remained on that level until it disappeared after 2001. \n\n\\subsection{The $-18 < V_{\\rm los} <-16$ \\kms\\ velocity range}\nOften blended with spectral components \\Dtwo\\ at $-18.3$ \\kms\\ and \\Gone\\ at $-15.5$ \\kms\\ the emission in between was almost always detected. Peak velocities of the Gaussian fits varied between $-17.5$ and $-16.5$ \\kms, indicating that more than a single emission feature contributed. As these features in general could not be decomposed, they are listed together in \nTable~\\ref{tab:compUHerB-E} as spectral component E.  In the interferometric maps 1990 -- 1992 we identified five spatial components E1 -- E5 (Table \\ref{tab:components}).\n\n\\subsection{The $-16 < V_{\\rm los} <-14$ \\kms\\ velocity range}\nThe dominant \\water\\ maser emission stemmed almost continuously from velocities $-16 < V_{\\rm los} <-14$ \\kms. Within this velocity range two or more spectral features contributed. Due to the variations of the relative intensities, the central velocities of the Gaussian fits drifted. Usually two features could be identified, which we designated spectral components \\Gone\\ and \\Gtwo\\ in \nTable~\\ref{tab:compUHerG-M}. In 1990, at the beginning of the systematic monitoring program two features with $>200$ Jy at $-15.3$ (\\Gone) and $-14.7$ (\\Gtwo) \\kms\\ dominated the profile, with component \\Gtwo\\ being the stronger one. This changed after 1991 and lasted until 2006, when again component \\Gtwo\\ became stronger than \\Gone. This behaviour is the cause for the apparent curvature of the ridge of the emission in the FVt-plot (Fig.~\\ref{fig:uher-fvt}) within the velocity interval $-16 < V_{\\rm los} <-14$ \\kms. In 2007 -- 2011 (TJD $\\ga 14000$) the evidence that \\Gone\\ is a blend of maser lines is particularly strong, as the peak velocity varied in the interval $-16.0 \\le V_{los} \\le -14.8$ and showed shifts between the extremes of this interval within four months (August - December 2007).\n\n\n\\subsection{The $V_{\\rm los} >-14$ \\kms\\ velocity range}\nIn this fourth velocity range only weaker emission was present which was assigned to spectral components I--M in Table~\\ref{tab:compUHerG-M}. Spectral component I was centered most of the time at $-12.9\\pm0.1$ \\kms. Only between 1993 and 1995 (TJD $\\sim8700 - 10200$) the center was at $-13.5\\pm0.2$ \\kms. After 1995 the component surpassed the detection limit only close to the maxima of the stellar lightcurve. The last four years after 2007 (TJD $>14450$) it was not detectable anymore, but re-appeared in 2015 during an optical maximum (Fig.~\\ref{fig:uher_all}). \n\nIn the period 1995 -- 2010 ($9700<$ TJD $<11700$) emission was detected at $\\sim-11.0$ \\kms\\ (component K), often at phases when also component I was detected and similar in strength (Fig.~\\ref{fig:uher-fvt}). At velocities $\\sim-10$ \\kms, in 1990/1991 (TJD $<8400$) emission with flux densities of a few Jansky was present at $-10.2$ \\kms. Later in a short phase of about 6 months in 2009/2010 emission was detected at at $-9.7$ \\kms\\ (Table~\\ref{tab:compUHerG-M}). Due to their proximity in velocity we assigned to them a common spectral component L. Components I, K and L are responsible for the distinguished feature in the FVt-plot (Fig.~\\ref{fig:uher-fvt}) at $\\sim-10$ \\kms\\ at TJD $\\sim15100-15300$.\n\nAlso seen for few months only (Oct. 1990 -- May 1991; Table~\\ref{tab:compUHerG-M}) was spectral component M, the most red-shifted component in the \\water\\ maser profile. It appeared at $\\sim-8$ \\kms\\ with flux densities $\\sim$1 Jy and is marginally visible at TJD $\\sim8300$ in the FVt-plot (Fig.~\\ref{fig:uher-fvt}). \\newline\n \n\n\\newpage\n\\onecolumn\n\n\\begin{sidewaystable}\n\\caption{Maser spectral components B -- E of U\\,Her}\n\\begin{tabular}{rrrrrrrrrrrrrrr}\n\\label{tab:compUHerB-E} \n           &         &         &          &  $    $ &\\multicolumn{2}{c}{B}&\\multicolumn{2}{c}{C}&\\multicolumn{2}{c}{\\Done}&\\multicolumn{2}{c}{\\Dtwo}&\\multicolumn{2}{c}{E} \\\\[0.1cm]\nDate & TJD & rms & $S$(tot) & $\\varphi_{\\rm s}$ &\\multicolumn{10}{c}{\\hrulefill\\ $V_{\\rm los}$\\gm $S_{\\rm p}$ \\hrulefill}  \\\\[0.1cm]\n           &         &  [Jy]   &[Jy*\\kms] &         & \\multicolumn{10}{c}{\\hrulefill\\ [km\\,s$^{-1}$, Jy] \\hrulefill} \\\\[0.1cm]\n\\noalign{\\smallskip}\\hline\\noalign{\\smallskip}\n26.03.87  &     6881  &  3.17  &  22   & 0.53   &  $-$ & $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n31.03.87  &     6886  &  2.49  &  27   & 0.54   &  $-$ & $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n17.06.87  &     6964  &  3.13  &  40   & 0.73   &  $-$ & $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n05.09.87  &     7044  &  5.14  & 113   & 0.93   &  $-$ & $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$ &  $-17.3$  &  24.3 \\\\\n17.02.90  &     7940  &  0.12  &  433  &  0.14  &  $-21.3$ & 3.8 & $-20$ & $<2$ & $-$ & $-$  &  $-18.0$  &  73.7 &  $-$  &  $-$ \\\\\n31.03.90  &     7982  &  0.17  &  395  &  0.24  &  $-21.2$ & 1.9 & $-20$ & $<2$ & $-$ & $-$  &  $-18.0$  &  57.6 &  $-$  &  $-$ \\\\\n24.04.90  &     8006  &  2.78  &  429  &  0.30  &  $-$ & $-$ & $-$ & $-$ & $-$ & $-$  &  $-18.1$  &  44.8 &  $-$  &  $-$ \\\\\n12.05.90  &     8024  &  0.29  &  294  &  0.35  &  $-21.0$ & $<1$ & $-20.0$ & $<1$ & $-$ & $-$  &  $-18.1$  &  23.9 &  $-$  &  $-$ \\\\\n21.10.90  &     8186  &  0.14  &  122  &  0.75  &  $-21.0$ & 0.5 & $-20.0$ & 0.5 & $-$ & $-$  &  $-18.1$  &  17.8 &  $-16.5$  &  9.4 \\\\\n24.10.90  &     8189  &  1.80  &  97  &  0.76  &  $-$ & $-$ & $-$ & $-$ & $-$ & $-$  &  $-18.3$  &  21.5 &  $-16.5$  &  $<9$ \\\\\n18.01.91  &     8275  &  1.76  &  271  &  0.97  &  $-21.3$ & 4.9 & $-$ & $-$ & $-$ & $-$  &  $-18.1$  &  85.5 &  $-17.1$  &  25.7 \\\\\n31.03.91  &     8347  &  0.11  &  267  &  0.15  &  $-21.2$ & 3.6 & $-20.3$ & 2.6 & $-$ & $-$  &  $-18.2$  &  76.7 &  $-17.2$  &  32.1 \\\\\n01.05.91  &     8378  &  0.23  &  257  &  0.22  &  $-21.2$ & 7.0 & $-20.2$ & 2.9 & $-$ & $-$  &  $-18.1$  &  76.2 &  $-17.2$  &  32.7 \\\\\n17.05.91  &     8394  &  2.14  &  150  &  0.26  &  $-$ & $-$ & $-$ & $-$ & $-$ & $-$  &  $-18.1$  &  50.6 &  $-17.1$  &  21.3 \\\\\n31.05.91  &     8408  &  3.83  &  238  &  0.30  &  $-$ & $-$ & $-$ & $-$ & $-$ & $-$  &  $-18.2$  &  63.1 &  $-16.9$  &  25.3 \\\\\n27.10.91  &     8557  &  0.83  &  170  &  0.66  &  $-20.6$ & 53.7 & $-19.5$ & 31.3 & $-$ & $-$  &  $-18.0$  &  51.8 &  $-$  &  $-$ \\\\\n02.11.91  &     8563  &  1.52  &  175  &  0.68  &  $-20.6$ & 71.0 & $-19.5$ & 29.0 & $-$ & $-$  &  $-18.0$  &  54.5 &  $-$  &  $-$ \\\\\n11.01.92  &     8633  &  0.74  &  275  &  0.85  &  $-21.5$ & 4.2 & $-19.5$ & 51.1 & $-$ & $-$  &  $-18.3$  &  107.0 &  $-17.3$  &  $<55$ \\\\\n18.01.92  &     8640  &  0.18  &  480  &  0.87  &  $-21.3$ & 8.7 & $-19.5$ & 90.6 & $-$ & $-$  &  $-18.2$  &  197.2 &  $-17.3$  &  $<95$ \\\\\n05.02.92  &     8658  &  1.27  &  242  &  0.91  &  $-21.6$ & 5.4 & $-19.5$ & $<22$ & $-$ & $-$  &  $-18.4$  &  116.0 &  $-17.3$  &  $<45$ \\\\\n29.02.92  &     8682  &  0.13  &  632  &  0.97  &  $-21.3$ & 20.5 & $-19.8$ & 20.3 & $-$ & $-$  &  $-18.3$  &  304.0 &  $-17.3$  &  $<120$ \\\\\n18.04.92  &     8731  &  1.49  &  386  &  0.09  &  $-21.5$ & 14.2 & $-19.5$ & $<23$ & $-$ & $-$  &  $-18.3$  &  175.0 &  $-17.3$  &  $<65$ \\\\\n05.07.92  &     8809  &  0.17  &  237  &  0.29  &  $-21.3$ & 4.6 & $-19.7$ & 5.5 & $-$ & $-$  &  $-18.3$  &  66.8 &  $-17.5$  &  41.6 \\\\\n01.09.92  &     8867  &  0.16  &  69  &  0.43  &  $-$ & $-$ & $-$ & $-$ & $-$ & $-$  &  $-18.3$  &  8.0 &  $-17.3$  &  8.5 \\\\\n23.10.92  &     8919  &  1.25  &  88  &  0.56  &  $-$ & $-$ & $-$ & $-$ & $-$ & $-$  &  $-18.2$  &  $<9$ &  $-17.2$  &  $<10$ \\\\\n22.12.92  &     8979  &  0.22  &  41  &  0.71  &  $-$ & $-$ & $-$ & $-$ & $-$ & $-$  &  $-18.3$  &  4.2 &  $-17.4$  &  5.9 \\\\\n26.01.93  &     9014  &  0.66  &  63  &  0.79  &  $-$ & $-$ & $-$ & $-$ & $-$ & $-$  &  $-18.3$  &  4.0 &  $-17.2$  &  9.8 \\\\\n20.04.93  &     9098  &  1.07  &  77  &  0.00  &  $-21.8$ & 3.1 & $-$ & $-$ & $-18.9$  &  7.2  &  $-$ & $-$ &  $-17.6$  &  14.3 \\\\\n21.04.93  &     9099  &  0.15  &  145  &  0.00  &  $-21.5$ & 4.1 & $-19.8$ & 2.0 & $-$ & $-$  &  $-18.4$  &  15.2 &  $-17.4$  &  24.5 \\\\\n13.05.93  &     9121  &  1.17  &  89  &  0.06  &  $-21.6$ & 2.5 & $-$ & $-$ & $-$ & $-$  &  $-18.5$  &  9.3 &  $-17.2$  &  14.2 \\\\\n03.11.93  &     9295  &  0.71  &  82  &  0.49  &  $-$ & $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$ &  $-17.0$  &  $<6$ \\\\\n30.11.93  &     9322  &  0.38  &  53  &  0.55  &  $-$ & $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$ &  $-17.0$  &  $<5$ \\\\\n08.03.94  &     9420  &  0.21  &  92  &  0.80  &  $-20.9$ & 1.5 & $-19.9$ & 2.9 & $-$ & $-$  &  $-18.5$  &  4.9 &  $-16.9$  &  9.4 \\\\\n16.04.94  &     9459  &  1.18  &  45  &  0.89  &  $-$ & $-$ & $-19.8$ & 2.7 & $-$ & $-$  &  $-18.4$  &  3.3 &  $-17.2$  &  5.3 \\\\\n08.09.94  &     9604  &  1.51  &  120  &  0.25  &  $-$ & $-$ & $-19.4$ & 11.6 & $-$ & $-$  &  $-$  &  $-$ &  $-17.0$  &  12.4 \\\\\n28.10.94  &     9654  &  1.23  &  117  &  0.37  &  $-$ & $-$ & $-19.9$ & 8.1 & $-$ & $-$  &  $-$  &  $-$ &  $-17.4$  &  $<11$ \\\\\n18.01.95  &     9736  &  1.08  &  54  &  0.58  &  $-$ & $-$ & $-19.6$ & 3.6 & $-$ & $-$  &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n13.03.95  &     9790  &  0.05  &  113  &  0.71  &  $-20.7$ & 2.1 & $-19.7$ & 8.8 & $-19.0$ & 5.5  &  $-18.4$  &  5.1 &  $-17.2$  &  12.0 \\\\\n27.03.95  &     9804  &  0.29  &  126  &  0.74  &  $-20.8$ & 1.9 & $-19.7$ & 7.4 & $-19.1$ & 9.4  &  $-18.3$  &  4.5 &  $-17.2$  &  6.1 \\\\\n03.06.95  &     9872  &  0.24  &  235  &  0.91  &  $-20.8$ & 3.0 & $-19.6$ & 14.6 & $-19.0$ & 17.8  &  $-18.3$  &  13.2 &  $-17.2$  &  32.8 \\\\\n23.06.95  &     9892  &  0.24  &  301  &  0.96  &  $-20.8$ & 3.4 & $-19.6$ & 18.2 & $-19.0$ & 21.9  &  $-18.3$  &  15.7 &  $-17.1$  &  45.6 \\\\\n\\noalign{\\smallskip}\\hline\n\\end{tabular}\n\\end{sidewaystable}\n\n\\addtocounter{table}{-1}\n\n\\newpage\n\n\\begin{sidewaystable}\n\\caption{Maser spectral components B -- E of U\\,Her (continued)}\n\\begin{tabular}{rrrrrrrrrrrrrrr}\n\\label{tab:compUHerB-E} \n           &         &         &          &  $    $ &\\multicolumn{2}{c}{B}&\\multicolumn{2}{c}{C}&\\multicolumn{2}{c}{\\Done}&\\multicolumn{2}{c}{\\Dtwo}&\\multicolumn{2}{c}{E} \\\\[0.1cm]\nDate & TJD & rms & $S$(tot) & $\\varphi_{\\rm s}$ &\\multicolumn{10}{c}{\\hrulefill\\ $V_{\\rm los}$\\gm $S_{\\rm p}$ \\hrulefill}  \\\\[0.1cm]\n           &         &  [Jy]   &[Jy*\\kms] &         & \\multicolumn{10}{c}{\\hrulefill\\ [km\\,s$^{-1}$, Jy] \\hrulefill} \\\\[0.1cm]\n\\noalign{\\smallskip}\\hline\\noalign{\\smallskip}\n17.09.95  &     9978  &  0.97  &  328  &  0.17  &  $-$ & $-$ & $-$ & $-$ & $-19.1$ & 21.7  &  $-18.2$  &  $<16$ &  $-17.1$  &  59.2 \\\\\n23.01.96  &    10106  &  1.31  &  122  &  0.49  &  $-$ & $-$ & $-$ & $-$ & $-18.9$  &  8.9  &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n01.03.96  &    10144  &  0.93  &  60  &  0.58  &  $-$ & $-$ & $-$ & $-$ & $-18.6$  &  5.3  &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n02.04.96  &    10176  &  1.75  &  100  &  0.66  &  $-$ & $-$ & $-$ & $-$ & $-18.9$  &  12.1  &  $-$  &  $-$ &  $-17.2$  &  5.1: \\\\\n26.09.96  &    10353  &  0.33  &  273  &  0.10  &  $-20.8$ & 5.2 & $-$ & $-$ & $-18.8$  &  74.3  &  $-$  &  $-$ &  $-16.9$  &  14.1 \\\\\n06.02.97  &    10486  &  0.69  &  136  &  0.43  &  $-$ & $-$ & $-$ & $-$ & $-18.8$  &  5.3  &  $-$  &  $-$ &  $-16.5$  &  5.9 \\\\\n12.03.97  &    10520  &  0.53  &  106  &  0.51  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $<$3 &  $-16.4$  &  7.0 \\\\\n14.03.97  &    10522  &  0.14  &  118  &  0.52  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-18.2$  &  2.8 &  $-16.5$  &  5.8 \\\\\n03.05.97  &    10572  &  0.74  &  73  &  0.64  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $<$3 &  $-16.9$  &  3.8 \\\\\n23.10.97  &    10745  &  0.57  &  99  &  0.07  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-18.7$  &  8.2 &  $-17.0$  &  12.1 \\\\\n16.12.97  &    10799  &  0.47  &  171  &  0.20  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-18.7$  &  9.4 &  $-16.8$  &  22.6 \\\\\n29.01.98  &    10843  &  0.55  &  126  &  0.31  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-18.7$  &  4.5 &  $-16.8$  &  11.3 \\\\\n20.03.98  &    10893  &  0.47  &  128  &  0.43  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-18.6$  &  2.7 &  $-16.7$  &  8.0 \\\\\n07.04.98  &    10911  &  0.79  &  105  &  0.48  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-18.8$  &  3.0: &  $-16.7$  &  5.4 \\\\\n11.05.98  &    10945  &  0.93  &  78  &  0.56  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-18.2$  &  2.2 &  $-16.6$  &  4.9 \\\\\n12.12.98  &    11160  &  0.45  &  151  &  0.09  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-18.5$  &  10.8 &  $-17.1$  &  16.0 \\\\\n19.01.99  &    11198  &  0.74  &  132  &  0.19  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-18.6$  &  7.5 &  $-16.7$  &  13.4 \\\\\n18.03.99  &    11256  &  0.43  &  121  &  0.33  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-18.4$  &  6.2 &  $-16.9$  &  9.7 \\\\\n12.05.99  &    11311  &  0.75  &  115  &  0.46  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $<$5 &  $-16.5$  &  7.7 \\\\\n31.07.99  &    11391  &  0.20  &  76  &  0.66  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-18.2$  &  2.7 &  $-16.7$  &  4.0 \\\\\n29.10.99  &    11481  &  1.66  &  143  &  0.88  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-18.5$  &  7.6 &  $-16.9$  &  15.7 \\\\\n28.12.99  &    11541  &  0.30  &  198  &  0.03  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-18.3$  &  9.8 &  $-17.0$  &  31.8 \\\\\n15.01.00  &    11559  &  0.80  &  260  &  0.08  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-18.4$  &  11.4 &  $-17.0$  &  41.6 \\\\\n05.04.00  &    11640  &  0.92  &  275  &  0.28  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-18.5$  &  7.0 &  $-17.0$  &  34.9 \\\\\n27.10.00  &    11845  &  0.82  &  93  &  0.78  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $<$3 &  $-16.4$  &  7.1: \\\\\n18.12.00  &    11897  &  0.73  &  72  &  0.91  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-16.8$  &  6.1 \\\\\n27.01.01  &    11937  &  0.77  &  86  &  0.01  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-18.5$  &  5.8 &  $-17.1$  &  9.9 \\\\\n19.04.01  &    12019  &  1.27  &  118  &  0.21  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-18.5$  &  5.9 &  $-17.3$  &  14.3 \\\\\n03.05.01  &    12033  &  1.85  &  89  &  0.25  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-18.6$  &  6.0 &  $-17.2$  &  11.0 \\\\\n19.09.01  &    12172  &  0.41  &  51  &  0.59  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n24.10.01  &    12207  &  1.02  &  52  &  0.68  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-16.4$: &  4.1 \\\\\n28.01.02  &    12303  &  0.86  &  46  &  0.91  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-17.1$  &  6.4 \\\\\n20.03.02  &    12354  &  0.89  &  63  &  0.04  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-17.0$  &  14.4 \\\\\n24.04.02  &    12389  &  1.05  &  96  &  0.13  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-17.1$  &  21.2 \\\\\n20.06.02  &    12446  &  0.44  &  107  &  0.27  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-18.2$  &  1.6 &  $-17.0$  &  15.7 \\\\\n26.06.02  &    12452  &  1.21  &  88  &  0.28  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-17.0$  &  12.8 \\\\\n01.10.02  &    12549  &  0.69  &  66  &  0.52  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n24.10.02  &    12572  &  0.88  &  66  &  0.58  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-17.0$  &  4.5 \\\\\n19.12.02  &    12628  &  0.77  &  75  &  0.72  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-16.8$  &  7.6 \\\\\n14.01.03  &    12654  &  1.36  &  80  &  0.78  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-17.0$  &  7.2 \\\\\n02.04.03  &    12732  &  0.67  &  132  &  0.97  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-18.0$  &  4.6 &  $-17.1$  &  16.6 \\\\\n\\noalign{\\smallskip}\\hline\n\\end{tabular}\n\\end{sidewaystable}\n\n\\addtocounter{table}{-1}\n\n\\newpage\n\n\\begin{sidewaystable}\n\\caption{Maser spectral components B -- E of U\\,Her (continued)}\n\\begin{tabular}{rrrrrrrrrrrrrrr}\n\\label{tab:compUHerB-E} \n           &         &         &          &  $    $ &\\multicolumn{2}{c}{B}&\\multicolumn{2}{c}{C}&\\multicolumn{2}{c}{\\Done}&\\multicolumn{2}{c}{\\Dtwo}&\\multicolumn{2}{c}{E} \\\\[0.1cm]\nDate & TJD & rms & $S$(tot) & $\\varphi_{\\rm s}$ &\\multicolumn{10}{c}{\\hrulefill\\ $V_{\\rm los}$\\gm $S_{\\rm p}$ \\hrulefill}  \\\\[0.1cm]\n           &         &  [Jy]   &[Jy*\\kms] &         & \\multicolumn{10}{c}{\\hrulefill\\ [km\\,s$^{-1}$, Jy] \\hrulefill} \\\\[0.1cm]\n\\noalign{\\smallskip}\\hline\\noalign{\\smallskip}\n19.11.03  &    12963  &  1.85  &  70  &  0.54  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n24.01.04  &    13029  &  0.66  &  52  &  0.72  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-16.7$:  &  \\\\\n31.03.04  &    13096  &  0.83  &  60  &  0.87  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-16.9$  &  5.7 \\\\\n11.05.04  &    13137  &  0.45  &  71  &  0.97  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-17.0$  &  10.0 \\\\\n18.06.04  &    13175  &  0.57  &  103  &  0.07  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-17.1$  &  14.2 \\\\\n17.09.04  &    13266  &  0.94  &  81  &  0.29  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-17.0$  &  9.7 \\\\\n18.12.04  &    13358  &  1.44  &  39  &  0.52  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n12.01.05  &    13383  &  1.17  &  40  &  0.58  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n15.02.05  &    13417  &  0.81  &  37  &  0.66  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-17.0$  &  2.6 \\\\\n13.04.05  &    13474  &  1.31  &  44  &  0.80  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-17.0$  &  3.8 \\\\\n21.06.05  &    13543  &  0.71  &  63  &  0.98  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-17.1$  &  5.9 \\\\\n11.07.05  &    13563  &  1.18  &  58  &  0.02  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-17.1$  &  5.8 \\\\\n23.11.05  &    13698  &  0.87  &  67  &  0.36  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-17.1$  &  3.8 \\\\\n14.02.06  &    13781  &  1.36  &  40  &  0.56  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-17.1$  &  2.2 \\\\\n07.04.06  &    13833  &  1.44  &  45  &  0.69  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n05.07.06  &    13922  &  0.84  &  53  &  0.91  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n01.09.06  &    13980  &  1.24  &  98  &  0.05  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-17.3$  &  4.3 \\\\\n17.10.06  &    14026  &  0.54  &  136  &  0.17  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-17.1$  &  6.1 \\\\\n01.12.06  &    14071  &  1.21  &  100  &  0.28  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-17.4$  &  3.5 \\\\\n17.01.07  &    14118  &  1.44  &  68  &  0.40  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n23.02.07  &    14155  &  0.99  &  47  &  0.49  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n10.04.07  &    14201  &  1.55  &  61  &  0.60  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n28.06.07  &    14280  &  0.86  &  87  &  0.80  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n24.07.07  &    14306  &  1.60  &  89  &  0.86  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n24.08.07  &    14337  &  0.98  &  130  &  0.94  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n15.10.07  &    14389  &  0.87  &  229  &  0.06  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n28.11.07  &    14433  &  0.70  &  121  &  0.17  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-$  &  $<$5 \\\\\n18.12.07  &    14453  &  0.85  &  107  &  0.22  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n29.01.08  &    14495  &  1.12  &  103  &  0.33  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n31.03.08  &    14557  &  1.43  &  80  &  0.48  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n13.05.08  &    14600  &  1.22  &  74  &  0.59  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n19.06.08  &    14637  &  1.49  &  55  &  0.68  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-17.6$  &  6.4 \\\\\n15.07.08  &    14663  &  0.59  &  49  &  0.74  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-17.7$  &  12.1 \\\\\n12.12.08  &    14813  &  1.07  &  28  &  0.11  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-17.4$  &  8.5 \\\\\n03.04.09  &    14925  &  0.64  &  54  &  0.39  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n13.05.09  &    14965  &  0.70  &  57  &  0.49  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-$  &  $<$2 \\\\\n23.09.09  &    15098  &  0.55  &  34  &  0.81  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n17.11.09  &    15153  &  0.94  &  63  &  0.95  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n09.12.09  &    15175  &  0.43  &  76  &  0.00  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n19.01.10  &    15216  &  0.21  &  80  &  0.11  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-17.2$  &  2.1 \\\\\n02.03.10  &    15258  &  0.70  &  61  &  0.21  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-17.2$  &  2.5 \\\\\n\\noalign{\\smallskip}\\hline\n\\end{tabular}\n\\end{sidewaystable}\n\n\\addtocounter{table}{-1}\n\n\\newpage\n\n\\begin{sidewaystable}\n\\caption{Maser spectral components B -- E of U\\,Her (continued)}\n\\begin{tabular}{rrrrrrrrrrrrrrr}\n\\label{tab:compUHerB-E} \n           &         &         &          &  $    $ &\\multicolumn{2}{c}{B}&\\multicolumn{2}{c}{C}&\\multicolumn{2}{c}{\\Done}&\\multicolumn{2}{c}{\\Dtwo}&\\multicolumn{2}{c}{E} \\\\[0.1cm]\nDate & TJD & rms & $S$(tot) & $\\varphi_{\\rm s}$ &\\multicolumn{10}{c}{\\hrulefill\\ $V_{\\rm los}$\\gm $S_{\\rm p}$ \\hrulefill}  \\\\[0.1cm]\n           &         &  [Jy]   &[Jy*\\kms] &         & \\multicolumn{10}{c}{\\hrulefill\\ [km\\,s$^{-1}$, Jy] \\hrulefill} \\\\[0.1cm]\n\\noalign{\\smallskip}\\hline\\noalign{\\smallskip}\n07.04.10  &    15294  &  0.65  &  60  &  0.30  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-17.3$  &  2.2 \\\\\n12.05.10  &    15329  &  0.54  &  73  &  0.39  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-17.2$  &  2.3 \\\\\n08.12.10  &    15539  &  0.73  &  29  &  0.90  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n22.02.11  &    15615  &  0.49  &  34  &  0.09  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n20.03.11  &    15641  &  0.62  &  33  &  0.16  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n\n24.02.15  &    17078  &  0.61  & 116  &  0.70  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-17.5$  &  7.0 \\\\\n27.05.15  &    17170  &  1.01  & 143  &  0.93  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n06.07.15  &    17210  &  0.61  &  95  &  0.03  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-17.1$  &  4.5 \\\\\n08.09.15  &    17274  &  0.33  & 139  &  0.19  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-16.9$  & 11.7 \\\\\n12.10.15  &    17308  &  0.47  & 144  &  0.27  &  $-$ & $-$ & $-$ & $-$ & $-$  &  $-$  &  $-$  &  $-$ &  $-17.1$  & 10.0 \\\\\n\\noalign{\\smallskip}\\hline\\\n\\end{tabular}\n\\end{sidewaystable}\n\n\\newpage\n\n\\begin{sidewaystable}\n\\caption{Maser spectral components G -- M of U\\,Her}\n\\begin{tabular}{rrrrrrrrrrrrrrrrr}\n\\label{tab:compUHerG-M} \n           &         &         &          &  $    $ &\\multicolumn{2}{c}{\\Gone}&\\multicolumn{2}{c}{\\Gtwo}&\\multicolumn{2}{c}{I}&\\multicolumn{2}{c}{K}&\\multicolumn{2}{c}{L}&\\multicolumn{2}{c}{M} \\\\[0.1cm]\nDate & TJD & rms & $S$(tot) & $\\varphi_{\\rm s}$ &\\multicolumn{12}{c}{\\hrulefill\\ $V_{\\rm los}$\\gm $S_{\\rm p}$ \\hrulefill}  \\\\[0.1cm]\n           &         &  [Jy]   &[Jy*\\kms] &         & \\multicolumn{12}{c}{\\hrulefill\\ [km\\,s$^{-1}$, Jy] \\hrulefill} \\\\[0.1cm]\n\\noalign{\\smallskip}\\hline\\noalign{\\smallskip}\n26.03.87  &     6881  &  3.17  &  22   & 0.53   &   $-$ & $-$ &             $-14.6$  &   18.7 & $-$ & $-$  &  $-$  &  $-$ &  $-$  &  $-$ &\t$-$\t&\t$-$\t\\\\\n31.03.87  &     6886  &  2.49  &  27   & 0.54   &   $-$ & $-$ &             $-14.5$  &   27.3 & $-$ & $-$  &  $-$  &  $-$ &  $-$  &  $-$ &\t$-$\t&\t$-$\t\\\\\n17.06.87  &     6964  &  3.13  &  40   & 0.73   &   $-$ & $-$ &             $-14.4$  &   33.6 & $-$ & $-$  &  $-$  &  $-$ &  $-$  &  $-$ &\t$-$\t&\t$-$\t\\\\\n05.09.87  &     7044  &  5.14  & 113   & 0.93   &   $-16$ & $<20$ &         $-14.3$  &   62.9 & $-$ & $-$  &  $-$  &  $-$ &  $-$  &  $-$ &\t$-$\t&\t$-$\t\\\\\n17.02.90  &     7940  &  0.12  &  433  &  0.14  & \t$-15.3$\t & \t184.6\t & \t$-14.6$\t & \t277.1\t & \t$-12.9$\t & \t3.0\t & \t$-$\t & \t$-$\t& \t$-10.2$\t&\t2.0\t&\t$-$\t&\t$-$\t \\\\\n31.03.90  &     7982  &  0.17  &  395  &  0.24  & \t$-15.2$\t & \t$<200$\t & \t$-14.6$\t & \t266.8\t & \t$-12.7$\t & \t3.1\t & \t$-$\t & \t$-$\t& \t$-10.2$\t&\t2.1\t&\t$-$\t&\t$-$\t \\\\\n24.04.90  &     8006  &  2.78  &  429  &  0.30  & \t$-15.0$\t & \t284.5\t & \t$-$\t     & $-$\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t& \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n12.05.90  &     8024  &  0.29  &  294  &  0.35  & \t$-14.9$\t & \t201.9\t & \t$-$\t     & $-$\t & \t$-12.9$\t     & \t$<3$\t & \t$-$\t & \t$-$\t& \t$-10.2$\t&\t1.9\t&\t$-$\t&\t$-$\t \\\\\n21.10.90  &     8186  &  0.14  &  122  &  0.75  & \t$-15.3$\t & \t$<45$\t & \t$-14.8$\t & \t68.5\t & \t$-13.1$\t & \t5.5\t & \t$-$\t & \t$-$\t& \t$-10.2$\t&\t1.5\t&\t$-7.5$\t&\t1.1\t \\\\\n24.10.90  &     8189  &  1.80  &   97  &  0.76  & \t$-15.0$\t & \t$<45$\t & \t$-14.8$\t & \t56.2\t & \t$-13.1$\t & \t$<6$\t & \t$-$\t & \t$-$\t& \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n18.01.91  &     8275  &  1.76  &  271  &  0.97  & \t$-$\t & \t$-$\t & \t$-14.6$\t & \t94.6\t & \t$-13.0$\t & \t$<17$\t & \t$-$\t & \t$-$\t& \t$-10.2$\t&\t$8.0$\t&\t$-8.1$\t&\t$<4$\t \\\\\n31.03.91  &     8347  &  0.11  &  267  &  0.15  & \t$-$\t & \t$-$\t & \t$-14.8$\t & \t80.5\t & \t$-13.0$\t & \t$<13$\t & \t$-$\t & \t$-$\t& \t$-10.2$\t&\t4.1\t&\t$-8.0$\t&\t1.2\t \\\\\n01.05.91  &     8378  &  0.23  &  257  &  0.22  & \t$-15.3$\t & \t$<50$\t & \t$-14.8$\t & \t73.2\t & \t$-13.0$\t & \t$<13$\t & \t$-$\t & \t$-$\t& \t$-10.2$\t&\t2.9\t&\t$-8.0$\t&\t0.7\t \\\\\n17.05.91  &     8394  &  2.14  &  150  &  0.26  & \t$-$\t & \t$-$\t & \t$-14.9$\t & \t43.7\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t& \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n31.05.91  &     8408  &  3.83  &  238  &  0.30  & \t$-$\t & \t$-$\t & \t$-15.0$\t & \t66.5\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t& \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n27.10.91  &     8557  &  0.83  &  170  &  0.66  & \t$-15.6$\t & \t16.9\t & \t$-15.0$\t & \t17.2\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t& \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n02.11.91  &     8563  &  1.52  &  175  &  0.68  & \t$-15.4$\t & \t18.8\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t& \t$-$\t&\t$-$\t& \t$-$\t& \t$-$\t \\\\\n11.01.92  &     8633  &  0.74  &  275  &  0.85  & \t$-15.8$\t & \t36.7\t & \t$-15.2$\t & \t34.5\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t& \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n18.01.92  &     8640  &  0.18  &  480  &  0.87  & \t$-15.8$\t & \t64.1\t & \t$-15.2$\t & \t60.5\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t& \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n05.02.92  &     8658  &  1.27  &  242  &  0.91  & \t$-15.7$\t & \t35.8\t & \t$-15.1$\t & \t35.8\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t& \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n29.02.92  &     8682  &  0.13  &  632  &  0.97  & \t$-15.8$\t & \t99.9\t & \t$-15.2$\t & \t101.0\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t& \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n18.04.92  &     8731  &  1.49  &  386  &  0.09  & \t$-15.6$\t & \t$<72$\t & \t$-15.1$\t & \t82.6\t & \t$-13.4$\t & \t5.1\t & \t$-$\t & \t$-$\t& \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n05.07.92  &     8809  &  0.17  &  237  &  0.29  & \t$-15.2$\t & \t91.3\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t& \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n01.09.92  &     8867  &  0.16  &   69  &  0.43  & \t$-15.3$\t & \t47.0\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t& \t$-10.2$\t&\t0.4:\t&\t$-$\t&\t$-$\t \\\\\n23.10.92  &     8919  &  1.25  &   88  &  0.56  & \t$-15.4$\t & \t58.0\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t& \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n22.12.92  &     8979  &  0.22  &   41  &  0.71  & \t$-15.2$\t & \t24.0\t & \t$-$\t & \t$-$\t & \t$-13.5$\t & \t$<1.5$\t & \t$-$\t & \t$-$\t& \t$-$\t&\t$-$\t&\t$-7.5$\t&\t0.6:\t \\\\\n26.01.93  &     9014  &  0.66  &   63  &  0.79  & \t$-15.4$\t & \t39.0\t & \t$-$\t & \t$-$\t & \t$-13.5$\t & \t2.7\t & \t$-$\t & \t$-$\t& \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n20.04.93  &     9098  &  1.07  &   77  &  0.00  & \t$-15.6$\t & \t31.7\t & \t$-$\t & \t$-$\t & \t$-13.6$\t & \t7.6\t & \t$-$\t & \t$-$\t& \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n21.04.93  &     9099  &  0.15  &  145  &  0.00  & \t$-15.3$\t & \t49.8\t & \t$-$\t & \t$-$\t & \t$-13.4$\t & \t13.1\t & \t$-$\t & \t$-$\t& \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n13.05.93  &     9121  &  1.17  &   89  &  0.06  & \t$-15.4$\t & \t34.0\t & \t$-$\t & \t$-$\t & \t$-13.3$\t & \t8.4\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n03.11.93  &     9295  &  0.71  &   82  &  0.49  & \t$-15.4$\t & \t66.4\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n30.11.93  &     9322  &  0.38  &   53  &  0.55  & \t$-15.4$\t & \t38.3\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n08.03.94  &     9420  &  0.21  &   92  &  0.80  & \t$-15.4$\t & \t53.0\t & \t$-$\t & \t$-$\t & \t$-13.7$\t & \t1.9\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n16.04.94  &     9459  &  1.18  &   45  &  0.89  & \t$-15.5$\t & \t19.1\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n08.09.94  &     9604  &  1.51  &  120  &  0.25  & \t$-15.4$\t & \t67.0\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n28.10.94  &     9654  &  1.23  &  117  &  0.37  & \t$-15.5$\t & \t68.7\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n18.01.95  &     9736  &  1.08  &   54  &  0.58  & \t$-15.5$\t & \t40.0\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n13.03.95  &     9790  &  0.05  &  113  &  0.71  & \t$-15.5$\t & \t56.3\t & \t$-$\t & \t$-$\t & \t$-13.3$\t & \t5.0\t & \t$-10.8$\t & \t1.2\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n27.03.95  &     9804  &  0.29  &  126  &  0.74  & \t$-15.6$\t & \t63.2\t & \t$-$\t & \t$-$\t & \t$-13.4$\t & \t5.5\t & \t$-10.8$\t & \t1.4\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n03.06.95  &     9872  &  0.24  &  235  &  0.91  & \t$-15.6$\t & \t96.8\t & \t$-$\t & \t$-$\t & \t$-13.5$\t & \t9.5\t & \t$-10.9$\t & \t2.4\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n23.06.95  &     9892  &  0.24  &  301  &  0.96  & \t$-15.6$\t & \t117.6\t & \t$-$\t & \t$-$\t & \t$-13.5$\t & \t14.9\t & \t$-10.8$\t & \t2.7\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n\\noalign{\\smallskip}\\hline\n\\end{tabular}\n\\end{sidewaystable}\n\n\\addtocounter{table}{-1}\n\n\\newpage\n\n\\begin{sidewaystable}\n\\caption{Maser spectral components G -- M of U\\,Her (continued)}\n\\begin{tabular}{rrrrrrrrrrrrrrrrr}\n\\label{tab:compUHerG-M} \n           &         &         &          &  $    $ &\\multicolumn{2}{c}{\\Gone}&\\multicolumn{2}{c}{\\Gtwo}&\\multicolumn{2}{c}{I}&\\multicolumn{2}{c}{K}&\\multicolumn{2}{c}{L}&\\multicolumn{2}{c}{M} \\\\[0.1cm]\nDate & TJD & rms & $S$(tot) & $\\varphi_{\\rm s}$ &\\multicolumn{12}{c}{\\hrulefill\\ $V_{\\rm los}$\\gm $S_{\\rm p}$ \\hrulefill}  \\\\[0.1cm]\n           &         &  [Jy]   &[Jy*\\kms] &         & \\multicolumn{12}{c}{\\hrulefill\\ [km\\,s$^{-1}$, Jy] \\hrulefill} \\\\[0.1cm]\n\\noalign{\\smallskip}\\hline\\noalign{\\smallskip}\n17.09.95  &     9978  &  0.97  &  328  &  0.17  & \t$-15.6$\t & \t126.7\t & \t$-$\t & \t$-$\t & \t$-13.5$\t & \t$<22$\t & \t$-10.8$: & \t7.9\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n23.01.96  &    10106  &  1.31  &  122  &  0.49  & \t$-15.5$\t & \t70.5\t & \t$-14.6$\t & \t14.5\t & \t$-13.4$\t & \t4.4\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n01.03.96  &    10144  &  0.93  &   60  &  0.58  & \t$-15.5$\t & \t34.2\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n02.04.96  &    10176  &  1.75  &  100  &  0.66  & \t$-15.3$\t & \t49.9\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n26.09.96  &    10353  &  0.33  &  273  &  0.10  & \t$-15.3$\t & \t74.0\t & \t$-$\t & \t$-$\t & \t$-12.8$\t & \t13.7\t & \t$-10.9$\t & \t10.1\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n06.02.97  &    10486  &  0.69  &  136  &  0.43  & \t$-15.5$\t & \t86.6\t & \t$-14.4$\t & \t37.0\t & \t$-12.8$\t & \t2.8\t & \t$-10.7$\t & \t1.7\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n12.03.97  &    10520  &  0.53  &  106  &  0.51  & \t$-15.5$\t & \t69.1\t & \t$-14.4$\t & \t35.9\t & \t$-$\t & \t$-$\t & \t$-10.8$\t & \t2.9\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n14.03.97  &    10522  &  0.14  &  118  &  0.52  & \t$-15.5$\t & \t77.0\t & \t$-14.4$\t & \t39.2\t & \t$-13.0$\t & \t1.8\t & \t$-11.0$\t & \t1.5\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n03.05.97  &    10572  &  0.74  &   73  &  0.64  & \t$-15.5$\t & \t41.4\t & \t$-14.3$\t & \t28.9\t & \t$-12.8$\t & \t2.4\t & \t$-$\t & \t$<$2\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n23.10.97  &    10745  &  0.57  &   99  &  0.07  & \t$-15.7$\t & \t19.4\t & \t$-14.7$\t & \t14.1\t & \t$-12.8$\t & \t13.1\t & \t$-10.9$\t & \t10.5\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n16.12.97  &    10799  &  0.47  &  171  &  0.20  & \t$-15.6$\t & \t44.2\t & \t$-14.8$\t & \t32.3\t & \t$-12.8$\t & \t17.5\t & \t$-10.9$\t & \t13.7\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n29.01.98  &    10843  &  0.55  &  126  &  0.31  & \t$-15.5$\t & \t62.2\t & \t$-14.5$\t & \t25.9\t & \t$-12.8$\t & \t8.8\t & \t$-11.1$\t & \t6.2\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n20.03.98  &    10893  &  0.47  &  128  &  0.43  & \t$-15.5$\t & \t90.4\t & \t$-14.5$\t & \t33.4\t & \t$-13.1$\t & \t2.8\t & \t$-10.8$\t & \t2.1\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n07.04.98  &    10911  &  0.79  &  105  &  0.48  & \t$-15.5$\t & \t79.0\t & \t$-14.5$\t & \t27.7\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n11.05.98  &    10945  &  0.93  &   78  &  0.56  & \t$-15.5$\t & \t62.9\t & \t$-14.5$\t & \t21.5\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n12.12.98  &    11160  &  0.45  &  151  &  0.09  & \t$-15.7$\t & \t54.2\t & \t$-14.6$\t & \t23.5\t & \t$-13.2$\t & \t30.6\t & \t$-10.9$\t & \t5.2\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n19.01.99  &    11198  &  0.74  &  132  &  0.19  & \t$-15.7$\t & \t65.0\t & \t$-14.6$\t & \t23.7\t & \t$-13.1$\t & \t13.3\t & \t$-10.8$\t & \t5.6\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n18.03.99  &    11256  &  0.43  &  121  &  0.33  & \t$-15.6$\t & \t68.0\t & \t$-14.5$\t & \t27.2\t & \t$-13.0$\t & \t7.4\t & \t$-$\t & \t$<$3\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n12.05.99  &    11311  &  0.75  &  115  &  0.46  & \t$-15.5$\t & \t87.9\t & \t$-14.4$\t & \t25.3\t & \t$-$\t & \t$<$5\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n31.07.99  &    11391  &  0.20  &   76  &  0.66  & \t$-15.6$\t & \t59.6\t & \t$-14.5$\t & \t16.1\t & \t$-13.0$\t & \t2.2\t & \t$-11.0$\t & \t0.9\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n29.10.99  &    11481  &  1.66  &  143  &  0.88  & \t$-15.6$\t & \t93.4\t & \t$-14.5$\t & \t25.7\t & \t$-12.9$\t & \t8.6\t & \t$-$\t & \t$<$4\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n28.12.99  &    11541  &  0.30  &  198  &  0.03  & \t$-15.7$\t & \t107.3\t & \t$-14.5$\t & \t26.0\t & \t$-12.9$\t & \t14.7\t & \t$-11.0$\t & \t7.9\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n15.01.00  &    11559  &  0.80  &  260  &  0.08  & \t$-15.7$\t & \t142.3\t & \t$-14.7$\t & \t32.3\t & \t$-12.8$\t & \t21.1\t & \t$-10.9$\t & \t10.9\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n05.04.00  &    11640  &  0.92  &  275  &  0.28  & \t$-15.6$\t & \t172.7\t & \t$-14.6$\t & \t41.1\t & \t$-12.9$\t & \t19.4\t & \t$-10.8$\t & \t8.3\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n27.10.00  &    11845  &  0.82  &   93  &  0.78  & \t$-15.5$\t & \t81.9\t & \t$-14.5$\t & \t15.0\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n18.12.00  &    11897  &  0.73  &   72  &  0.91  & \t$-15.6$\t & \t53.1\t & \t$-14.5$\t & \t11.2\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n27.01.01  &    11937  &  0.77  &   86  &  0.01  & \t$-15.6$\t & \t50.7\t & \t$-14.5$\t & \t11.3\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n19.04.01  &    12019  &  1.27  &  118  &  0.21  & \t$-15.6$\t & \t76.0\t & \t$-14.4$\t & \t12.6\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n03.05.01  &    12033  &  1.85  &   89  &  0.25  & \t$-15.6$\t & \t64.4\t & \t$-14.5$\t & \t9.7\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n19.09.01  &    12172  &  0.41  &   51  &  0.59  & \t$-15.4$\t & \t54.7\t & \t$-14.6$\t & \t7.2\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n24.10.01  &    12207  &  1.02  &   52  &  0.68  & \t$-15.5$\t & \t48.5\t & \t$-14.5$\t & \t7.2\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n28.01.02  &    12303  &  0.86  &   46  &  0.91  & \t$-15.5$\t & \t27.3\t & \t$-14.3$\t & \t8.2\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n20.03.02  &    12354  &  0.89  &   63  &  0.04  & \t$-15.7$\t & \t19.2\t & \t$-14.6$\t & \t16.3\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n24.04.02  &    12389  &  1.05  &   96  &  0.13  & \t$-15.6$\t & \t31.6\t & \t$-14.5$\t & \t23.1\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n20.06.02  &    12446  &  0.44  &  107  &  0.27  & \t$-15.5$\t & \t57.6\t & \t$-14.5$\t & \t23.6\t & \t$-12.7$\t & \t3.4\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n26.06.02  &    12452  &  1.21  &   88  &  0.28  & \t$-15.5$\t & \t52.8\t & \t$-14.5$\t & \t17.8\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n01.10.02  &    12549  &  0.69  &   66  &  0.52  & \t$-15.4$\t & \t60.2\t & \t$-14.7$\t & \t$<$10\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n24.10.02  &    12572  &  0.88  &   66  &  0.58  & \t$-15.5$\t & \t50.4\t & \t$-14.5$\t & \t10.9\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n19.12.02  &    12628  &  0.77  &   75  &  0.72  & \t$-15.6$\t & \t44.5\t & \t$-14.6$\t & \t23.9\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n14.01.03  &    12654  &  1.36  &   80  &  0.78  & \t$-15.6$\t & \t41.1\t & \t$-14.5$\t & \t22.1\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n02.04.03  &    12732  &  0.67  &  132  &  0.97  & \t$-15.7$\t & \t39.0\t & \t$-14.5$\t & \t44.6\t & \t$-12.8$\t & \t3.6\t & \t$-11.2$\t & \t2.0:\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n\\noalign{\\smallskip}\\hline\n\\end{tabular}\n\\end{sidewaystable}\n\n\\addtocounter{table}{-1}\n\n\\newpage\n\n\\begin{sidewaystable}\n\\caption{Maser spectral components G -- M of U\\,Her (continued)}\n\\begin{tabular}{rrrrrrrrrrrrrrrrr}\n\\label{tab:compUHerG-M} \n           &         &         &          &  $    $ &\\multicolumn{2}{c}{\\Gone}&\\multicolumn{2}{c}{\\Gtwo}&\\multicolumn{2}{c}{I}&\\multicolumn{2}{c}{K}&\\multicolumn{2}{c}{L}&\\multicolumn{2}{c}{M} \\\\[0.1cm]\nDate & TJD & rms & $S$(tot) & $\\varphi_{\\rm s}$ &\\multicolumn{12}{c}{\\hrulefill\\ $V_{\\rm los}$\\gm $S_{\\rm p}$ \\hrulefill}  \\\\[0.1cm]\n           &         &  [Jy]   &[Jy*\\kms] &         & \\multicolumn{12}{c}{\\hrulefill\\ [km\\,s$^{-1}$, Jy] \\hrulefill} \\\\[0.1cm]\n\\noalign{\\smallskip}\\hline\\noalign{\\smallskip}\n19.11.03  &    12963  &  1.85  &   70  &  0.54  & \t$-15.5$\t & \t74.0\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n24.01.04  &    13029  &  0.66  &   52  &  0.72  &       $-15.5$\t & \t50.9\t & \t$-14.5$\t & \t5.9\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n31.03.04  &    13096  &  0.83  &   60  &  0.87  & \t$-15.5$\t & \t43.4\t & \t$-14.6$\t & \t11.6\t & \t$-12.8$\t & \t4.7\t & \t$-10.9$\t & \t3.5:\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n11.05.04  &    13137  &  0.45  &   71  &  0.97  & \t$-15.5$\t & \t33.7\t & \t$-14.8$\t & \t21.8\t & \t$-12.9$\t & \t7.9\t & \t$-11.0$\t & \t4.7\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n18.06.04  &    13175  &  0.57  &  103  &  0.07  & \t$-15.5$\t & \t29.4\t & \t$-14.6$\t & \t32.3\t & \t$-12.9$\t & \t12.6\t & \t$-11.0$\t & \t8.5\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n17.09.04  &    13266  &  0.94  &   81  &  0.29  & \t$-15.5$\t & \t32.5\t & \t$-14.7$\t & \t20.8\t & \t$-12.9$\t & \t5.3\t & \t$-10.9$\t & \t5.3\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n18.12.04  &    13358  &  1.44  &   39  &  0.52  & \t$-15.4$\t & \t31.0\t & \t$-$\t & \t$<$10\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n12.01.05  &    13383  &  1.17  &   40  &  0.58  & \t$-15.4$\t & \t34.0\t & \t$-$\t & \t$<$10\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n15.02.05  &    13417  &  0.81  &   37  &  0.66  & \t$-15.5$\t & \t30.4\t & \t$-14.7$\t & \t7.3\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n13.04.05  &    13474  &  1.31  &   44  &  0.80  & \t$-15.5$\t & \t34.5\t & \t$-14.5$\t & \t9.4\t & \t$-$\t & \t$-$\t & \t$-10.9$\t & \t2.9\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n21.06.05  &    13543  &  0.71  &   63  &  0.98  & \t$-15.5$\t & \t23.3\t & \t$-14.4$\t & \t21.0\t & \t$-13.0$\t & \t4.4\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n11.07.05  &    13563  &  1.18  &   58  &  0.02  & \t$-15.5$\t & \t16.3\t & \t$-14.7$\t & \t24.6\t & \t$-13.1$\t & \t3.0\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n23.11.05  &    13698  &  0.87  &   67  &  0.36  & \t$-15.4$\t & \t38.6\t & \t$-14.6$\t & \t29.0\t & \t$-12.8$\t & \t3.4\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n14.02.06  &    13781  &  1.36  &   40  &  0.56  & \t$-15.4$\t & \t31.5\t & \t$-14.4$\t & \t6.7\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n07.04.06  &    13833  &  1.44  &   45  &  0.69  & \t$-15.4$\t & \t26.8\t & \t$-$\t & \t$<$15\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n05.07.06  &    13922  &  0.84  &   53  &  0.91  & \t$-15.4$\t & \t17.8\t & \t$-14.4$\t & \t20.9\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n01.09.06  &    13980  &  1.24  &   98  &  0.05  & \t$-$\t & \t$-$\t & \t$-14.4$\t & \t50.5\t & \t$-$\t & \t$-$\t & \t$-11.1$\t & \t3.9\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n17.10.06  &    14026  &  0.54  &  136  &  0.17  & \t$-$\t & \t$-$\t & \t$-14.4$\t & \t63.4\t & \t$-$\t & \t$-$\t & \t$-11.1$\t & \t4.3\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n01.12.06  &    14071  &  1.21  &  100  &  0.28  & \t$-$\t & \t$-$\t & \t$-14.6$\t & \t41.2\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n17.01.07  &    14118  &  1.44  &   68  &  0.40  & \t$-15.5$\t & \t16.6\t & \t$-14.4$\t & \t28.4\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n23.02.07  &    14155  &  0.99  &   47  &  0.49  & \t$-$\t & \t$-$\t & \t$-14.8$\t & \t22.3\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n10.04.07  &    14201  &  1.55  &   61  &  0.60  & \t$-15.4$\t & \t20.2\t & \t$-14.3$\t & \t31.9\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n28.06.07  &    14280  &  0.86  &   87  &  0.80  & \t$-15.6$\t & \t21.0\t & \t$-14.2$\t & \t46.5\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$<$3\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n24.07.07  &    14306  &  1.60  &   89  &  0.86  & \t$-15.9$\t & \t25.3\t & \t$-14.2$\t & \t46.6\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&       $-$\t&\t$-$\t \\\\\n24.08.07  &    14337  &  0.98  &  130  &  0.94  & \t$-16.0$\t & \t52.6\t & \t$-14.2$\t & \t57.4\t & \t$-12.9$: & \t5.7\t & \t$-10.9$\t & \t6.6\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n15.10.07  &    14389  &  0.87  &  229  &  0.06  & \t$-15.9$\t & \t84.1\t & \t$-14.3$\t & \t77.1\t & \t$-12.9$\t & \t11.2\t & \t$-10.9$\t & \t20.5\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n28.11.07  &    14433  &  0.70  &  121  &  0.17  & \t$-15.4$\t & \t25.2\t & \t$-14.3$\t & \t53.1\t & \t$-12.9$: & \t6.5\t & \t$-10.8$\t & \t12.0\t&\t$-9.5$:\t&\t2.9\t&\t$-$\t&\t$-$\t \\\\\n18.12.07  &    14453  &  0.85  &  107  &  0.22  & \t$-14.8$\t & \t28.5\t & \t$-14.3$\t & \t36.1\t & \t$-$\t & \t$-$\t & \t$-11.0$\t & \t7.4\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n29.01.08  &    14495  &  1.12  &  103  &  0.33  & \t$-14.9$\t & \t29.0\t & \t$-14.3$\t & \t46.0\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n31.03.08  &    14557  &  1.43  &   80  &  0.48  & \t$-14.9$\t & \t23.5\t & \t$-14.4$\t & \t51.5\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n13.05.08  &    14600  &  1.22  &   74  &  0.59  & \t$-14.9$\t & \t24.0\t & \t$-14.3$\t & \t32.0\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n19.06.08  &    14637  &  1.49  &   55  &  0.68  & \t$-15.4$\t & \t14.5\t & \t$-14.4$\t & \t35.4\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n15.07.08  &    14663  &  0.59  &   49  &  0.74  & \t$-14.9$\t & \t14.0\t & \t$-14.3$\t & \t13.4\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n12.12.08  &    14813  &  1.07  &   28  &  0.11  & \t$-15.0$\t & \t6.5\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n03.04.09  &    14925  &  0.64  &   54  &  0.39  & \t$-15.6$\t & \t13.7\t & \t$-14.5$\t & \t32.4\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$<3$\t&\t$-$\t&\t$<2$\t&\t$-$\t&\t$-$\t \\\\\n13.05.09  &    14965  &  0.70  &   57  &  0.49  & \t$-15.7$\t & \t15.0\t & \t$-14.6$\t & \t34.5\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$<3$\t&\t$-$\t&\t$<2$\t&\t$-$\t&\t$-$\t \\\\\n23.09.09  &    15098  &  0.55  &   34  &  0.81  & \t$-15.7$\t & \t13.5\t & \t$-14.5$\t & \t9.7\t & \t$-$\t & \t$-$\t & \t$-11.1$\t & \t2.8\t&\t$-9.8$\t&\t2.1\t&\t$-$\t&\t$-$\t \\\\\n17.11.09  &    15153  &  0.94  &   63  &  0.95  & \t$-15.8$\t & \t22.8\t & \t$-14.6$\t & \t8.2\t & \t$-$\t & \t$-$\t & \t$-10.5$\t & \t21.8\t&\t$-9.7$\t&\t13.9\t&\t$-$\t&\t$-$\t \\\\\n09.12.09  &    15175  &  0.43  &   76  &  0.00  & \t$-15.8$\t & \t21.5\t & \t$-15.3$\t & \t13.0\t & \t$-$\t & \t$-$\t & \t$-10.7$\t & \t10.5\t&\t$-9.7$\t&\t22.2\t&\t$-$\t&\t$-$\t \\\\\n19.01.10  &    15216  &  0.21  &   80  &  0.11  & \t$-15.8$\t & \t37.4\t & \t$-14.5$\t & \t13.1\t & \t$-$\t & \t$-$\t & \t$-10.8$\t & \t6.3\t&\t$-9.7$\t&\t16.4\t&\t$-$\t&\t$-$\t \\\\\n02.03.10  &    15258  &  0.70  &   61  &  0.21  & \t$-15.8$\t & \t28.1\t & \t$-14.5$\t & \t18.7\t & \t$-$\t & \t$-$\t & \t$-10.9$\t & \t3.1\t&\t$-9.5$:\t&\t3.6\t&\t$-$\t&\t$-$\t \\\\\n\\noalign{\\smallskip}\\hline\n\\end{tabular}\n\\end{sidewaystable}\n\n\\addtocounter{table}{-1}\n\n\\newpage\n\n\\begin{sidewaystable}\n\\caption{Maser spectral components G -- M of U\\,Her (continued)}\n\\begin{tabular}{rrrrrrrrrrrrrrrrr}\n\\label{tab:compUHerG-M} \n           &         &         &          &  $    $ &\\multicolumn{2}{c}{\\Gone}&\\multicolumn{2}{c}{\\Gtwo}&\\multicolumn{2}{c}{I}&\\multicolumn{2}{c}{K}&\\multicolumn{2}{c}{L}&\\multicolumn{2}{c}{M} \\\\[0.1cm]\nDate & TJD & rms & $S$(tot) & $\\varphi_{\\rm s}$ &\\multicolumn{12}{c}{\\hrulefill\\ $V_{\\rm los}$\\gm $S_{\\rm p}$ \\hrulefill}  \\\\[0.1cm]\n           &         &  [Jy]   &[Jy*\\kms] &         & \\multicolumn{12}{c}{\\hrulefill\\ [km\\,s$^{-1}$, Jy] \\hrulefill} \\\\[0.1cm]\n\\noalign{\\smallskip}\\hline\\noalign{\\smallskip}\n07.04.10  &    15294  &  0.65  &   60  &  0.30  & \t$-15.7$\t & \t22.3\t & \t$-14.5$\t & \t30.9\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n12.05.10  &    15329  &  0.54  &   73  &  0.39  & \t$-15.3$\t & \t24.5\t & \t$-14.4$\t & \t39.8\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n08.12.10  &    15539  &  0.73  &   29  &  0.90  & \t$-15.4$\t & \t12.7\t & \t$-14.3$\t & \t15.6\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n22.02.11  &    15615  &  0.49  &   34  &  0.09  & \t$-15.1$\t & \t12.6\t & \t$-14.4$\t & \t12.5\t & \t$-$\t & \t$-$\t & \t$-$\t & \t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t&\t$-$\t \\\\\n20.03.11  &    15641  &  0.62  &   33  &  0.16  & \t$-15.4$\t & \t12.4\t & \t$-14.4$\t & \t19.8\t & \t$-$\t    &  $-$  &  $-$  &  $-$ &  $-$  &  $-$ &  $-$  &\t$-$\t \\\\\n24.02.15  &    17078  &  0.61  & 116  &  0.70   &   $-16.1$  &  29.3     & $-14.9$   &  83.7     & $-$      &  $-$  &  $-$  &  $-$ &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n27.05.15  &    17170  &  1.01  & 143  &  0.93   &   $-16.0$  &  19.3     & $-14.8$   & 139.1     & $-12.9$  &  4.7  &  $-$  &  $-$ &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n06.07.15  &    17210  &  0.61  &  95  &  0.03   &   $-15.8$  &  18.5     & $-14.8$   &  74.8     & $-13.0$  &  4.4  &  $-$  &  $-$ &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n08.09.15  &    17274  &  0.33  & 139  &  0.19   &   $-16.0$  &  55.1     & $-14.9$   &  61.1     & $-12.8$  &  5.5  &  $-$  &  $-$ &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n12.10.15  &    17308  &  0.47  & 144  &  0.27   &   $-15.9$  &  91.6     & $-14.8$   &  50.9     & $-13.0$  &  3.6  &  $-$  &  $-$ &  $-$  &  $-$ &  $-$  &  $-$ \\\\\n\\noalign{\\smallskip}\\hline\\\n\\end{tabular}\n\\end{sidewaystable}\n\n\\twocolumn\n\n\\newpage\n\n\n\\section{\\label{sdd_LineFitRes_Appendix_RRAql} RR Aql Spectral line fitting results}\nThe maser features identified between 1987 and 2015 in individual spectra are listed in Table~ \\ref{tab:compRRAql}, where they were assigned to five different spectral components labeled A--E. The corresponding Gaussian fits were made using the original spectral resolution of the individual spectra.  The meaning of the columns in Table \\ref{tab:compRRAql} are as in Tables \\ref{tab:compUHerB-E} and \\ref{tab:compUHerG-M}.  In the following we discuss the variations in flux density and velocity of the spectral components.\n\n\\subsection{The $V_{\\rm los} < 28$ \\kms\\ velocity range}\nThe spectral component, which was almost always detectable in this velocity range is component A centered on $V_{\\rm los} = 27$ \\kms. It could be identified during the entire monitoring period 1990 -- 2011, except for a few months in 2005 and 2007. During these months emission was present at this velocity, but was too weak to be identified by the Gaussian fitting procedure as a separate maser feature. The same applies for 2015, when the velocity of component A was within the blue wing of the maser profile. Component A occasionally became the strongest feature in the spectral profile. If it reached high brightness levels ($>$150 Jy), this occurred ususally around the maximum of the visual light curve $0.0 \\le \\varphi_{\\rm s} \\le 0.3$. \n\nBetween 1990 and the beginning of 1994 spectral component B, centered on $V_{\\rm los} = 28$ \\kms, was the strongest component in the profiles (see the 1991 spectrum in Fig.~\\ref{fig:rraql_sel}). With brightness levels surpassing 400 Jy (0.2 < $\\varphi_{\\rm s} <0.4$), it reached also the highest brightness among all spectral components between 1987 and 2015. In 1994, emission at this velocity became weaker. After March 1995, blending by the neighbouring spectral components A and C became so severe that component B could not be identified unambiguously with the Gaussian fitting procedure anymore. After the observing gap 2011 -- 2015 component B was the dominating spectral component again, but with a slightly higher velocity of $V_{\\rm los} = 28.5$ \\kms. \n\nAt velocities $V_{\\rm los} < 27$ \\kms\\ the emission was always a factor of $\\ge10$ weaker than spectral components A or B. No convincing spectral components covering more than a few spectra could be identified with the Gaussian fitting procedure. \n\n\\subsection{The $28 < V_{\\rm los} \\le 30$ \\kms\\ velocity range}\nThis velocity range includes the stellar radial velocity $V_{\\ast} = 28.5$ \\kms\\ (see Table \\ref{centralcoords}). The prominent feature in this velocity range is spectral component C at $V_{\\rm los} = 29$ \\kms. \nIt was always present, even in 2015, when the emission at\n29 \\kms\\ was blended with the single feature seen at $V_{\\rm los} = 28.5$ \\kms\\, and which we assigned to spectral component B. Component C was the strongest feature in the years 2000--2009. In 2010 and 2011 it had a similar brightness as component A, resulting sometimes in characteristic double-peaked profiles (cf. in Fig.~\\ref{fig:rraql_all} in the Appendix).\n\nFor about a year in 1994/1995 we could identify a component with the Gaussian fits with a velocity of $V_{\\rm los} = 29.8$ \\kms. This component D is blended at other times with the stronger components C and E. Although emission is always present at this velocity, a separate feature could not be identified unambiguously with the Gaussian fit procedure. Component D is therefore similar to component B. When they are significantly weaker than their neighbouring components (or maybe even absent) they could not be identified due to blending in velocity space. \n\n\\subsection{The $V_{\\rm los} > 30$ \\kms\\ velocity range}\nVelocities $V_{\\rm los} > 30$ \\kms\\ are represented by component E at $V_{\\rm los} = 30.5$ \\kms. In general the emission is significantly weaker than those of spectral components A, B, and C. In only one occasion component E was found to be the strongest: In our first (isolated) spectrum taken 12.6.1987 and reported also by \\cite{comoretto90}, this feature reached a peak flux density $\\sim60$ Jy.\nThe maximum brightness observed during the whole monitoring time was in December 2004 / January 2005 with a flux density $\\sim$65 Jy. This indicates that in 1987 the other components were unusually weak, rather than that component E was particularly bright. The component E emission was prominent in the FVt-plot and could be identified with the Gaussian fit procedure  mostly at phases after the visual maximum, when the star was bright. After May 2009 and including 2015, the spectral component disappeared from the spectra, except between March and May 2010 and in September/October 2015 (at phase $\\varphi_{\\rm s} = 0.0\\pm0.1$ in both cases), when emission $<5$ Jy was observed at its velocity.\n\n\n\\newpage\n\\onecolumn\n\n\\begin{sidewaystable}\n\\caption{Maser spectral components of RR\\,Aql}\n\\label{tab:compRRAql} \n\\begin{tabular}{rrrrrrrrrrrrrrr}\n           &         &         &          &  $    $ &\\multicolumn{2}{c}{A}&\\multicolumn{2}{c}{B}&\\multicolumn{2}{c}{C}&\\multicolumn{2}{c}{D}&\\multicolumn{2}{c}{E} \\\\[0.1cm]\nDate & TJD & rms & $S$ & $\\varphi_s$ &\\multicolumn{10}{c}{\\hrulefill\\ $V_{\\rm los}$\\gm $S_{\\rm p}$ \\hrulefill}  \\\\[0.1cm]\n           &         &  [Jy]   &[Jy*\\kms] &         & \\multicolumn{10}{c}{\\hrulefill\\ [km\\,s$^{-1}$, Jy] \\hrulefill} \\\\[0.1cm]\n\\noalign{\\smallskip}\\hline\\noalign{\\smallskip}\n   12.06.87&     6959&     3.18&       153&     0.18&  --      &  --      &      28.6&      40.8&  --      &  --      &  --      &  --      &      31.1&      57.8\\\\\n   17.02.90&     7940&     0.99&        56&     0.63&  --      &  --      &      27.9&      17.4&      29.1&      31.5&  --      &  --      &      30.6&      12.9\\\\\n   24.04.90&     8006&     3.29&        62&     0.80&  --      &  --      &      27.9&  $<$19.6 &      28.7&      31.5&  --      &  --      &      30.5&  $$<$$4.0\\\\\n   21.10.90&     8186&     0.16&        67&     0.25&      26.7&       5.3&      27.9&      38.6&      29.2&      18.5&  --      &  --      &      30.6&       2.4\\\\\n   24.10.90&     8189&     2.12&       118&     0.26&      26.7&  $<$8.0  &      27.9&      77.3&      29.1&      36.3&  --      &  --      &  --      &  --      \\\\\n   19.01.91&     8276&     1.90&       106&     0.47&      26.8&  $<$4.1  &      27.9&     102.0&      29.0&      28.9&  --      &  --      &  --      &  --      \\\\\n   01.05.91&     8378&     0.25&       201&     0.73&      26.9&       9.2&      28.0&     199.9&      29.0&      30.1&  --      &  --      &  --      &  --      \\\\\n   06.05.91&     8383&     2.79&        83&     0.74&      26.8&  $<$9.0  &      27.9&      75.0&      29.0&  $<$15.0 &  --      &  --      &  --      &  --      \\\\\n   25.10.91&     8555&     1.30&       614&     0.17&      26.7&     191.7&      27.8&     264.2&      29.1&      26.7&  --      &  --      &      30.3&      18.3\\\\\n   18.01.92&     8640&     0.24&       629&     0.38&      26.9&      77.9&      27.9&     466.3&      29.3&     109.9&  --      &  --      &      30.4&  $<$15.0 \\\\\n   29.02.92&     8682&     0.14&       378&     0.49&      27.1&      29.2&      27.9&     289.5&      29.2&      97.3&  --      &  --      &      30.5&       7.4\\\\\n   19.04.92&     8732&     2.52&       161&     0.61&      26.9&  $<$20.0 &      28.0&     124.6&      29.3&      37.3&  --      &  --      &      30.5&  $<$7.0  \\\\\n   06.07.92&     8810&     0.18&        80&     0.81&      26.7&       9.0&      27.9&      46.2&      29.3&      10.4&  --      &  --      &      30.5&       1.9\\\\\n   02.09.92&     8868&     0.25&       400&     0.95&      27.1&     161.3&      28.2&     133.2&      29.4&      35.7&  --      &  --      &      30.6&      10.0\\\\\n   16.10.92&     8912&     2.00&       464&     0.06&      26.9&     133.2&      28.3&     192.0&      29.3&      67.6&  --      &  --      &      30.2&      21.4\\\\\n   22.12.92&     8979&     0.26&       476&     0.23&      27.3&      39.8&      28.3&     419.0&      29.3&      88.8&  --      &  --      &      30.4&      12.5\\\\\n   26.01.93&     9014&     0.78&       221&     0.32&      27.2&  $<$53.0 &      28.0&     107.8&      29.2&     103.3&  --      &  --      &      30.3&      10.0\\\\\n   21.04.93&     9099&     1.06&        78&     0.53&      27.1&  $<$17.3 &      27.9&      38.5&      29.2&      40.2&  --      &  --      &      30.4&  $<$3.0  \\\\\n   21.04.93&     9099&     0.17&       171&     0.53&      27.1&       9.6&      27.9&      81.1&      29.2&      85.2&  --      &  --      &      30.5&       3.5\\\\\n   02.11.93&     9294&     1.10&       114&     0.02&      27.0&       4.8&      28.1&      57.4&      29.0&      48.9&  --      &  --      &      30.4&       6.7\\\\\n   30.11.93&     9322&     0.74&       126&     0.09&      26.9&       4.9&      28.2&      74.0&      29.2&      42.0&  --      &  --      &      30.3&       8.6\\\\\n   08.03.94&     9420&     0.18&       202&     0.33&      27.0&       6.0&      28.0&     111.8&      29.0&     106.2&      29.8&       9.3&      30.3&       7.9\\\\\n   16.04.94&     9459&     1.45&       147&     0.43&      27.1&  $<$3.0  &      28.0&      71.1&      29.0&      86.6&      29.7&  $<$22.0 &      30.4&  $<$3.0  \\\\\n   18.01.95&     9736&     0.86&       121&     0.12&      27.2&      21.3&      28.1&      45.3&      29.0&      48.4&      29.7&      12.5&      30.5&      30.3\\\\\n   09.03.95&     9787&     0.11&       196&     0.25&      27.2&      22.1&      28.1&      82.6&      29.0&      95.2&      29.7&      23.5&      30.5&      32.2\\\\\n   03.06.95&     9872&     0.18&       248&     0.46&      26.7&     132.0&  --      &  --      &      28.7&     121.4&      29.8&       6.8&      30.4&       7.9\\\\\n   24.06.95&     9893&     0.25&       162&     0.52&      26.8&      31.7&  --      &  --      &      28.7&     106.0&      29.9&       8.3&      30.5&       5.0\\\\\n   03.09.96&    10330&     0.18&        79&     0.61&      27.6&       8.0&  --      &  --      &      28.8&      59.6&  --      &  --      &      30.5&  $<$1.5  \\\\\n   25.06.97&    10625&     0.39&       189&     0.35&      26.9&      40.4&  --      &  --      &      29.1&      42.1&  --      &  --      &      30.5&      16.1\\\\\n   20.12.00&    11899&     1.15&       116&     0.53&      27.0&      10.5&  --      &  --      &      28.6&      94.5&  --      &  --      &      30.2&       6.8\\\\\n   18.09.01&    12171&     0.96&       425&     0.21&      26.8&     189.1&  --      &  --      &      28.6&      88.3&  --      &  --      &      30.6&      22.0\\\\\n   24.10.01&    12207&     1.91&       231&     0.30&      26.9&      97.5&  --      &  --      &      28.6&      59.6&  --      &  --      &      30.5&      10.9\\\\\n   23.03.02&    12357&     1.95&        44&     0.68&      27.1&       9.4&  --      &  --      &      28.6&      29.7&  --      &  --      &      30.5&  $<$4.0  \\\\          \n   26.04.02&    12391&     1.18&        28&     0.76&      27.0&  $<$4.5  &  --      &  --      &      28.6&      17.4&  --      &  --      &      30.5&  $<$2.0  \\\\          \n   02.10.02&    12550&     1.84&       177&     0.16&      26.9&      50.6&  --      &  --      &      28.6&      51.5&  --      &  --      &      30.6&      28.1\\\\\n   29.10.02&    12577&     1.61&       148&     0.23&      27.0&      42.9&  --      &  --      &      28.7&      46.8&  --      &  --      &      30.5&      23.6\\\\\n   19.12.02&    12628&     1.26&       136&     0.35&      27.1&      40.2&  --      &  --      &      28.7&      54.6&  --      &  --      &      30.3&      14.6\\\\\n   14.01.03&    12654&     2.04&        77&     0.42&      27.1&      21.0&  --      &  --      &      28.8&      32.5&  --      &  --      &      30.4&  $<$8.0  \\\\\n   04.04.03&    12734&     1.60&        29&     0.62&      27.3&       6.3&  --      &  --      &      28.8&      18.6&  --      &  --      &      30.4&  $<$3.0  \\\\\n   18.11.03&    12962&     1.35&       100&     0.19&      26.8&      33.2&  --      &  --      &      28.7&      41.0&  --      &  --      &      30.5&      14.4\\\\\n   02.04.04&    13098&     1.27&        57&     0.53&      27.1&      10.3&  --      &  --      &      28.9&      31.5&  --      &  --      &  --      &  --      \\\\\n\\noalign{\\smallskip}\\hline\n\\end{tabular}\n\\end{sidewaystable}\n\n\\addtocounter{table}{-1}\n\n\\newpage\n\n\n\n\\begin{sidewaystable}\n\\caption{Maser spectral components of RR\\,Aql (continued)}\n\\label{tab:compRRAql} \n\\begin{tabular}{rrrrrrrrrrrrrrr}\n           &         &         &          &  $    $ &\\multicolumn{2}{c}{A}&\\multicolumn{2}{c}{B}&\\multicolumn{2}{c}{C}&\\multicolumn{2}{c}{D}&\\multicolumn{2}{c}{E} \\\\[0.1cm]\nDate & TJD & rms & $S$ & $\\varphi_s$ &\\multicolumn{10}{c}{\\hrulefill\\ $V_{\\rm los}$\\gm $S_{\\rm p}$ \\hrulefill}  \\\\[0.1cm]\n           &         &  [Jy]   &[Jy*\\kms] &         & \\multicolumn{10}{c}{\\hrulefill\\ [km\\,s$^{-1}$, Jy] \\hrulefill} \\\\[0.1cm]\n\\noalign{\\smallskip}\\hline\\noalign{\\smallskip}\n   13.05.04&    13139&     1.47&        32&     0.63&      27.1&       3.9&  --      &  --      &      28.9&      23.7&  --      &  --      &  --      &  --      \\\\\n   19.06.04&    13176&     2.39&        56&     0.72&      27.1&  $<$5.0  &  --      &  --      &      28.9&      53.5&  --      &  --      &  --      &  --      \\\\\n   15.09.04&    13264&     2.39&       299&     0.94&      27.2&      14.2&  --      &  --      &      28.9&     337.1&  --      &  --      &      30.7&      52.7\\\\\n   20.12.04&    13360&     1.11&       386&     0.18&      27.3&      33.9&  --      &  --      &      28.9&     386.9&  --      &  --      &      30.6&      66.1\\\\\n   12.01.05&    13383&     1.30&       372&     0.24&      27.4&      34.5&  --      &  --      &      28.9&     374.7&  --      &  --      &      30.6&      62.6\\\\\n   15.02.05&    13417&     1.01&       254&     0.32&      27.3&      25.5&  --      &  --      &      28.8&     261.0&  --      &  --      &      30.7&      40.8\\\\\n   14.04.05&    13475&     1.43&       143&     0.47&  --      &  --      &  --      &  --      &      28.9&     162.0&  --      &  --      &  --      &  --      \\\\\n   21.06.05&    13543&     2.34&        68&     0.64&  --      &  --      &  --      &  --      &      28.9&      57.9&  --      &  --      &  --      &  --      \\\\\n   12.07.05&    13564&     3.16&        69&     0.69&  --      &  --      &  --      &  --      &      28.9&      67.8&  --      &  --      &  --      &  --      \\\\\n   22.11.05&    13697&     1.52&       257&     0.02&      27.0&      50.8&  --      &  --      &      28.9&     263.6&  --      &  --      &  --      &  --      \\\\\n   15.02.06&    13782&     2.01&       312&     0.24&      27.1&      60.5&  --      &  --      &      28.9&     282.8&  --      &  --      &  --      &  --      \\\\\n   09.04.06&    13835&     2.21&       174&     0.37&      27.2&      32.0&  --      &  --      &      28.9&     135.7&  --      &  --      &  --      &  --      \\\\\n   07.07.06&    13924&     2.34&        34&     0.59&      27.0&       5.9&  --      &  --      &      29.1&      26.0&  --      &  --      &  --      &  --      \\\\\n   16.10.06&    14025&     2.13&        37&     0.85&      27.3&       9.7&  --      &  --      &      29.0&      24.9&  --      &  --      &  --      &  --      \\\\\n   30.11.06&    14070&     1.16&       119&     0.96&      27.2&      18.4&  --      &  --      &      28.9&      69.4&  --      &  --      &      30.4&      15.4\\\\\n   16.01.07&    14117&     1.26&       236&     0.07&      27.0&      41.7&  --      &  --      &      28.9&     186.1&  --      &  --      &      30.8&      35.4\\\\\n   22.02.07&    14154&     1.36&       236&     0.17&      27.0&      44.6&  --      &  --      &      28.9&     173.8&  --      &  --      &      30.8&      32.5\\\\\n   11.04.07&    14202&     1.48&       167&     0.29&      27.1&      30.8&  --      &  --      &      28.9&     116.1&  --      &  --      &      30.7&      16.9\\\\\n   27.06.07&    14279&     1.77&        66&     0.48&      27.2&      12.2&  --      &  --      &      29.1&      44.7&  --      &  --      &  --      &  --      \\\\\n   24.07.07&    14306&     1.63&        39&     0.55&  --      &  --      &  --      &  --      &      29.2&      23.4&  --      &  --      &  --      &  --      \\\\\n   23.08.07&    14336&     1.73&        29&     0.62&  --      &  --      &  --      &  --      &      29.2&      17.9&  --      &  --      &  --      &  --      \\\\\n   16.10.07&    14390&     1.21&        29&     0.76&  --      &  --      &  --      &  --      &      29.2&      18.1&  --      &  --      &  --      &  --      \\\\\n   26.11.07&    14431&     1.47&        68&     0.86&      27.0&      11.1&  --      &  --      &      29.2&      45.4&  --      &  --      &  --      &  --      \\\\\n   19.12.07&    14454&     1.06&       116&     0.92&      26.9&      24.3&  --      &  --      &      29.1&      71.3&  --      &  --      &  --      &  --      \\\\\n   28.01.08&    14494&     1.40&       212&     0.02&      26.8&      50.7&  --      &  --      &      29.1&     119.9&  --      &  --      &      30.7&      14.4\\\\\n   31.03.08&    14557&     1.33&       215&     0.18&      26.9&      54.2&  --      &  --      &      29.0&     114.6&  --      &  --      &      30.7&      13.8\\\\\n   13.05.08&    14600&     1.56&       184&     0.28&      27.0&      43.5&  --      &  --      &      29.1&      97.0&  --      &  --      &      30.6&       9.4\\\\\n   20.06.08&    14638&     2.04&       113&     0.38&      27.1&      27.8&  --      &  --      &      29.1&      57.3&  --      &  --      &      30.7&       7.0\\\\\n   15.07.08&    14663&     1.32&        60&     0.44&      27.3&      14.4&  --      &  --      &      29.1&      28.9&  --      &  --      &      30.7&  $<$3.0  \\\\\n   12.12.08&    14813&     1.18&        61&     0.82&      26.8&      19.9&  --      &  --      &      29.1&      18.4&  --      &  --      &      30.8&       5.0\\\\\n   03.04.09&    14925&     0.88&       206&     0.09&      26.7&      78.0&  --      &  --      &      28.9&      46.2&  --      &  --      &      30.9&      14.5\\\\\n   15.05.09&    14967&     1.22&       175&     0.20&      26.8&      60.8&  --      &  --      &      28.8&      40.7&  --      &  --      &      30.9&      13.8\\\\\n   22.09.09&    15097&     1.46&        35&     0.52&      27.2&  $<$10.0 &  --      &  --      &      28.5&      13.0&  --      &  --      &  --      &  --      \\\\\n   18.11.09&    15154&     0.91&        15&     0.67&      27.4&       3.3&  --      &  --      &      29.1&       6.7&  --      &  --      &  --      &  --      \\\\\n   20.12.09&    15186&     0.67&        19&     0.75&      27.7&       4.9&  --      &  --      &      29.4&       8.4&  --      &  --      &  --      &  --      \\\\\n   19.01.10&    15216&     1.01&        25&     0.82&      27.2&       7.1&  --      &  --      &      29.2&       8.4&  --      &  --      &  --      &  --      \\\\\n   01.03.10&    15257&     0.69&        54&     0.93&      27.0&      15.1&  --      &  --      &      29.1&      17.9&  --      &  --      &  --      &  --      \\\\\n   07.04.10&    15294&     0.74&        84&     0.02&      26.9&      30.0&  --      &  --      &      29.1&      26.3&  --      &  --      &  --      &  --      \\\\\n   14.05.10&    15331&     0.96&       110&     0.11&      26.9&      39.1&  --      &  --      &      29.0&      32.5&  --      &  --      &  --      &  --      \\\\\n   08.12.10&    15539&     1.43&        16&     0.63&      27.5&  $<$4.0  &  --      &  --      &      28.8&       8.3&  --      &  --      &  --      &  --      \\\\\n   19.01.11&    15581&     1.53&        25&     0.73&      28.2&       4.4&  --      &  --      &      29.2&      10.5&  --      &  --      &  --      &  --      \\\\\n\\noalign{\\smallskip}\\hline\n\\end{tabular}\n\\end{sidewaystable}\n\n\\addtocounter{table}{-1}\n\n\\newpage\n\n\n\\begin{sidewaystable}\n\\caption{Maser spectral components of RR\\,Aql (continued)}\n\\label{tab:compRRAql} \n\\begin{tabular}{rrrrrrrrrrrrrrr}\n           &         &         &          &  $    $ &\\multicolumn{2}{c}{A}&\\multicolumn{2}{c}{B}&\\multicolumn{2}{c}{C}&\\multicolumn{2}{c}{D}&\\multicolumn{2}{c}{E} \\\\[0.1cm]\nDate & TJD & rms & $S$ & $\\varphi_s$ &\\multicolumn{10}{c}{\\hrulefill\\ $V_{\\rm los}$\\gm $S_{\\rm p}$ \\hrulefill}  \\\\[0.1cm]\n           &         &  [Jy]   &[Jy*\\kms] &         & \\multicolumn{10}{c}{\\hrulefill\\ [km\\,s$^{-1}$, Jy] \\hrulefill} \\\\[0.1cm]\n\\noalign{\\smallskip}\\hline\\noalign{\\smallskip}\n   22.02.11&    15615&     0.64&        29&     0.82&      27.5&       6.7&  --      &  --      &      29.1&      11.7&  --      &  --      &  --      &  --      \\\\\n   20.03.11&    15641&     0.70&        55&     0.89&      27.4&      16.3&  --      &  --      &      29.1&      18.3&  --      &  --      &  --      &  --      \\\\\n   24.02.15&    17078&     0.72&        53&     0.48&  --      &  --      &      28.2&      33.1&  --      &  --      &  --      &  --      &  --      &  --      \\\\\n   28.05.15&    17171&     0.89&        25&     0.71&  --      &  --      &      28.5&      16.9&  --      &  --      &  --      &  --      &  --      &  --      \\\\\n   06.07.15&    17210&     0.77&        42&     0.81&  --      &  --      &      28.5&      23.7&  --      &  --      &  --      &  --      &  --      &  --      \\\\\n   08.09.15&    17274&     0.37&       124&     0.97&  --      &  --      &      28.5&      75.9&  --      &  --      &  --      &  --      &  --      &  --      \\\\\n   12.10.15&    17308&     0.54&       148&     0.05&  --      &  --      &      28.5&      97.0&  --      &  --      &  --      &  --      &  --      &  --      \\\\\n\\noalign{\\smallskip}\\hline\\\n\\end{tabular}\n\\end{sidewaystable}\n\n\\twocolumn\n\n\\newpage\n\n\n\\section{all maser spectra}\nIn this section we show all H$_2$O maser spectra of our targets. There are more spectra here than in the FVt-plots, because in those plots (apart from having averaged spectra taken within 4 days from one another) we have tried to avoid large gaps between observations, in order not to have to interpolate over large time intervals. \n\n\\begin{figure*}\n\\resizebox{18cm}{!}{\n\\includegraphics{uher-v2_plots_01.jpg}}\n\\caption{All H$_2$O maser spectra of U~Her. The observing date (top left) and TJD (top right) are indicated for each spectrum.}\n\\label{fig:uher_all}\n\\end{figure*}\n\n\\addtocounter{figure}{-1}\n\n\\begin{figure*}\n\\resizebox{18cm}{!}{\n\\includegraphics{uher-v2_plots_02.jpg}}\n\\caption{{\\it U~Her, continued}}\n\\label{fig:uher_all}\n\\end{figure*}\n\n\\addtocounter{figure}{-1}\n\n\\begin{figure*}\n\\resizebox{18cm}{!}{\n\\includegraphics{uher-v2_plots_03.jpg}}\n\\caption{{\\it U~Her, continued}}\n\\label{fig:uher_all}\n\\end{figure*}\n\n\\addtocounter{figure}{-1}\n\n\\begin{figure*}\n\\resizebox{18cm}{!}{\n\\includegraphics{uher-v2_plots_04.jpg}}\n\\caption{{\\it U~Her, continued}}\n\\label{fig:uher_all}\n\\end{figure*}\n\n\\addtocounter{figure}{-1}\n\n\\begin{figure*}\n\\resizebox{18cm}{!}{\n\\includegraphics{uher-v2_plots_05.jpg}}\n\\caption{{\\it U~Her, continued}}\n\\label{fig:uher_all}\n\\end{figure*}\n\n\\addtocounter{figure}{-1}\n\n\\begin{figure*}\n\\resizebox{18cm}{!}{\n\\includegraphics{uher-v2_plots_06.jpg}}\n\\caption{{\\it U~Her, continued}}\n\\label{fig:uher_all}\n\\end{figure*}\n\n\\addtocounter{figure}{-1}\n\n\\begin{figure*}\n\\resizebox{18cm}{!}{\n\\includegraphics{uher-v2_plots_07.jpg}}\n\\caption{{\\it U~Her, continued}}\n\\label{fig:uher_all}\n\\end{figure*}\n\n\\addtocounter{figure}{-1}\n\n\\begin{figure*}\n\\resizebox{18cm}{!}{\n\\includegraphics{uher-v2_plots_08.jpg}}\n\\caption{{\\it U~Her, continued}}\n\\label{fig:uher_all}\n\\end{figure*}\n\n\\clearpage\n\n\n\\begin{figure*}\n\\resizebox{18cm}{!}{\n\\includegraphics{rraql-v3_plots_01.jpg}}\n\\caption{All H$_2$O maser spectra of RR~Aql. The observing date (top left) and TJD (top right) are indicated for each spectrum.}\n\\label{fig:rraql_all}\n\\end{figure*}\n\n\\addtocounter{figure}{-1}\n\n\\begin{figure*}\n\\resizebox{18cm}{!}{\n\\includegraphics{rraql-v3_plots_02.jpg}}\n\\caption{{\\it RR~Aql, continued}}\n\\label{fig:rraql_all}\n\\end{figure*}\n\n\\addtocounter{figure}{-1}\n\n\\begin{figure*}\n\\resizebox{18cm}{!}{\n\\includegraphics{rraql-v3_plots_03.jpg}}\n\\caption{{\\it RR~Aql, continued}}\n\\label{fig:rraql_all}\n\\end{figure*}\n\n\\addtocounter{figure}{-1}\n\n\\begin{figure*}\n\\resizebox{18cm}{!}{\n\\includegraphics{rraql-v3_plots_04.jpg}}\n\\caption{{\\it RR~Aql, continued}}\n\\label{fig:rraql_all}\n\\end{figure*}\n\n\\addtocounter{figure}{-1}\n\n\\begin{figure*}\n\\resizebox{18cm}{!}{\n\\includegraphics{rraql-v3_plots_05.jpg}}\n\\caption{{\\it RR~Aql, continued}}\n\\label{fig:rraql_all}\n\\end{figure*}\n\n\n\n\n% <END_FILE: appendix.tex>\n\n\n\\end{document}\n\n\\typeout{get arXiv to do 4 passes: Label(s) may have changed. Rerun}\n\n\n\n\n\n\n"}
{"paper_id": "2403-00536", "version": "2403-00536v1", "root_file_path": "d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\data_raw\\2403-00536\\tex\\2403-00536v1\\arxiv-dynamic-knapsack.tex", "metadata": {"total_length": 219369, "merged_count": 1, "merged_files": ["arxiv-dynamic-knapsack.tex"], "missing_files": []}, "content": "\\documentclass[a4paper,cleveref, autoref,english,thm-restate]{lipics-v2021}\n\n\n\n\n\n\n\n\n\n\n\n\n\\pdfoutput=1 \n\\hideLIPIcs  \n\\nolinenumbers\n\n\n\n\\bibliographystyle{plainurl}\n\n\\title{Approximating the Geometric Knapsack Problem \\\\ in Near-Linear Time and Dynamically} \n\n\\titlerunning{Approximating the geometric knapsack problem in near-linear time and\n\tdynamically} \n\n\\author{Moritz Buchem}{Technische Universität München, Munich, Germany}{}{}{}\n\n\\author{Paul Deuker}{Technische Universität München, Munich, Germany}{}{}{}\n\\author{Andreas Wiese}{Technische Universität München, Munich, Germany}{}{}{}\n\n\n\\authorrunning{M. Buchem, P. Deuker, A. Wiese} \n\n\\Copyright{Moritz Buchem, Paul Deuker, Andreas Wiese} \n\n\\ccsdesc[500]{Theory of computation~Packing and covering problems} \n\n\\keywords{Geometric packing, approximation algorithms, dynamic algorithms} \n\n\\category{} \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\EventEditors{Wolfgang Mulzer and Jeff M. Phillips}\n\\EventNoEds{2}\n\\EventLongTitle{40th International Symposium on Computational Geometry (SoCG 2024)}\n\\EventShortTitle{SoCG 2024}\n\\EventAcronym{SoCG}\n\\EventYear{2024}\n\\EventDate{June 11-14, 2024}\n\\EventLocation{Athens, Greece}\n\\EventLogo{socg-logo}\n\\SeriesVolume{293}\n\\ArticleNo{XX}     \n\n\n\\begin{document}\n\t\n\t\\global\\long\\def\\OPT{\\mathrm{OPT}}\n\t\\global\\long\\def\\I{\\mathcal{I}}\n\t\\global\\long\\def\\H{\\mathcal{H}}\n\t\n\t\\global\\long\\def\\N{\\mathcal{N}^{*}}\n\t\n\t\\global\\long\\def\\V{\\mathcal{V}}\n\t\\global\\long\\def\\S{\\mathcal{S}}\n\t\\global\\long\\def\\L{\\mathcal{L}}\n\t\n\t\\global\\long\\def\\B{\\mathcal{B}}\n\t\n\t\\global\\long\\def\\Q{\\mathcal{Q}}\n\t\n\t\\global\\long\\def\\ALG{\\mathrm{ALG}}\n\t\n\t\\global\\long\\def\\S{\\mathcal{S}}\n\t\\global\\long\\def\\T{\\mathcal{T}}\n\n\t\\global\\long\\def\\P{\\mathcal{P}}\n\t\\global\\long\\def\\el{\\epsilon_{\\mathrm{large}}}\n\t\\global\\long\\def\\es{\\epsilon_{\\mathrm{small}}}\n\n\\maketitle\n\n\n\\begin{abstract}\nOne important goal in algorithm design is determining\nthe best running time for solving a problem (approximately). For some problems, we\nknow the optimal running time, assuming certain conditional lower\nbounds. In this paper, we study the $d$-dimensional geometric knapsack\nproblem in which we are far from this level of understanding. We are given a set of weighted\n$d$-dimensional geometric items like squares, rectangles, or hypercubes\nand a knapsack which is a square or a (hyper-)cube. Our goal is to\nselect a subset of the given items that fit non-overlappingly inside\nthe knapsack, maximizing the total profit of the packed items. We make a significant step towards determining the best running time for solving these problems approximately by presenting approximation algorithms whose running times are near-linear, i.e.,\n$O(n\\cdot\\mathrm{poly}(\\log n))$, for any constant $d$ and any parameter~$\\epsilon>0$ (the exponent of $\\log n$ depends on $d$ and $1/\\epsilon$)\n\nIn the case of (hyper)-cubes, we present a $(1+\\epsilon)$-approximation algorithm. This improves drastically upon the currently best known algorithm which is a $(1+\\epsilon)$-approximation algorithm with a running time of $n^{O_{\\epsilon,d}(1)}$ where the exponent of $n$ depends exponentially on $1/\\epsilon$\nand $d$. In particular, our algorithm is an efficient polynomial time approximation scheme (EPTAS). Moreover, we present a $(2+\\epsilon)$-approximation algorithm for rectangles in the setting without rotations and a $(\\frac{17}{9}+\\epsilon)\\approx 1.89$-approximation algorithm if we allow rotations by 90 degrees.\nThe best known polynomial time algorithms for this setting have approximation ratios of $\\frac{17}{9}+\\epsilon$\nand $1.5+\\epsilon$, respectively, and running times in which the exponent of $n$ depends exponentially on $1/\\epsilon$.\nIn addition, we give dynamic algorithms with polylogarithmic query and update times, having the same approximation guarantees as our other algorithms above.\n\nKey to our results is a new family of structured packings which we call \\emph{easily guessable packings}. They are flexible enough to guarantee the existence of profitable solutions while providing enough structure so that we can compute these solutions very quickly.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\end{abstract}\n\n\\section{Introduction}\n\n\\textsc{Knapsack} is a fundamental problem in combinatorial\noptimization. We are given a knapsack with a specified capacity $W$ and\na set of $n$ items, each of them characterized by its size $s_i$ and its profit $p_i$.\nThe goal is to compute a set of items that fits into the knapsack,\nmaximizing the total profit. \\textsc{Knapsack} is very well understood:\n\nthere is an FPTAS for the\nproblem with a running time of only $\\tilde{O}(n+(1/\\epsilon)^{2.2})$\n\\cite{deng2023approximating} with an asymptotically almost matching conditional lower bound\nof $(n+1/\\epsilon)^{2-o(1)}$~\\cite{cygan2019problems,knnemann_et_al:LIPIcs:2017:7468}.\nEven more, there are dynamic algorithms for\n\\textsc{Knapsack} that can maintain $(1+\\epsilon)$-approximate\nsolutions in polylogarithmic update time whenever an item is inserted to or removed\nfrom the input~\\cite{eberle_et_al:LIPIcs.FSTTCS.2021.18,henzinger_et_al:LIPIcs.STACS.2023.36}.\nIt is an important goal in algorithm design to determine\nthe best possible running time to solve (or approximate) a problem. Besides \\textsc{Knapsack}, there are\nalso many other problems for which we have (almost) matching upper and lower bounds, e.g.,\ncomputing the Fréchet distance~\\cite{BringmannFrechet}, Least Common Subsequence~\\cite{AbboudLCS,bringmann2015quadratic}, Negative Triangles~\\cite{williams2010subcubic}, or\nGraph Diameter~\\cite{roditty2013fast}.\n\n\n\n\nA natural generalization of \\textsc{Knapsack} is the \\emph{$d$-dimensional\n\tgeometric knapsack }problem in which the items are geometric objects\nlike squares, rectangles, or hypercubes. Like in \\textsc{Knapsack,}\nwe want to select a subset of the given items, but now we require\nin addition that they are placed non-overlappingly inside the knapsack,\nwhich we assume to be a square or a (hyper-)cube. The problem is motivated\nby many practical applications like placing advertisements on a board\nor a website, cutting pieces out of raw material like wood or metal,\nor loading cargo into a ship or a truck. Formally, we assume that\nwe are given an integer $N$ such that our knapsack is an axis-parallel\nsquare (if $d=2$) or a (hyper-)cube (if $d\\ge3$) where each edge\nhas length $N$. Also, we are given a set of items $\\I$ where each\nitem $i\\in\\I$ is a $d$-dimensional (hyper-)cube or a $d$-dimensional\n(hyper-)cuboid with axis-parallel edges and a given profit.\n\nUnfortunately, our understanding of the \\emph{$d$-dimensional geometric\n\tknapsack }problem falls short in comparison to our understanding of \\textsc{Knapsack}.\nThere is a polynomial time $(1+\\epsilon)$-approximation algorithm\nfor each $\\epsilon>0$ if all input items are (hyper-)cubes, due to Jansen, Khan, Lira, and Sreenivas~\\cite{jansen2022ptas}.\nIn the running time of this algorithm, the exponent\nof $n$ depends exponentially on $d$ and $1/\\epsilon$.\nHowever, there is no (conditional) lower bound known that justifies this. From\nall we know, it might still be possible to obtain a better running\ntime of the form $f(\\epsilon,d)n^{O(1)}$, for which the exponent\nof $n$ depends neither on $\\epsilon$ nor on $d$, but it is only\na small constant like 2 or even~1. Note that there are problems for\nwhich we know conditional running time lower bounds that rule this\nout, e.g., lower bounds of $\\Omega(n^{2})$ or $\\Omega(n^{3})$, based\non assumptions like the (Strong) Exponential Time Hypothesis or the\n3-SUM conjecture (see, e.g., \\cite{AbboudLCS,AbboundBringmannFischer2023,bringman2018multivariate, BringmannFrechet,bringmann2015quadratic} and references therein).\nFor example, for the Graph Diameter problem there is a lower bound of $\\Omega(m^{2})$, with $m$ being the number of edges of the given graph, for computing\na better approximation ratio than $3/2$~\\cite{roditty2013fast}.\nHowever, no such lower bounds are known for $d$-dimensional geometric knapsack.\n\nFor the special case of squares, i.e., $d=2$, there is a $(1+\\epsilon)$-approximation\nknown with a running time of the form $f(\\epsilon)n^{O(1)}$ due to Heydrich and Wiese~\\cite{heydrich2019faster}.\nHowever, even in this result the running time is much slower than\nlinear time since the algorithm uses an initial guessing step with\n$\\Omega(n)$ options and for each of these option solves several linear\nprograms of size $\\Omega(n)$ each. Furthermore, there is no dynamic\nalgorithm known for \\emph{$d$}-dimensional geometric knapsack, not\neven for the special case of squares (i.e., if $d=2$).\n\nIf we allow more general shapes than squares, cubes, and hypercubes,\nwe understand the problem even less. For two-dimensional axis-parallel\nrectangles, the best known polynomial time approximation algorithm is\ndue to Gálvez, Grandoni, Ingala, Heydrich, Khan, and Wiese~\\cite{galvez2021approximating}, having an approximation\nratio of~$1.89+\\epsilon$. If it is allowed to rotate rectangles by 90 degrees,\nthen a $(1.5+\\epsilon)$-approximation algorithm is known~\\cite{galvez2021approximating}. The problem\nis not known to be $\\mathsf{APX}$-hard, so it may even admit a PTAS.\nAlso in the mentioned results, the exponent of $n$ in the running\ntime depends exponentially on $1/\\epsilon$. However, we do not know any lower\nbound of the needed running time to solve the problem.\nThus, it might well be possible that we can achieve these\nor similar results in a running time that is much faster, e.g., $O(n\\cdot\\mathrm{poly}(\\log n))$.\nAlso, it is open whether a dynamic algorithm exists for the problem.\n\n\n\\subsection{Our contribution}\n\nOur first result is a $(1+\\epsilon)$-approximation algorithm for\\emph{\n}the\\emph{ $d$}-dimensional geometric knapsack problem for squares,\ncubes, and hypercubes with a running time that is near-linear, i.e.,\n$O(n\\cdot\\mathrm{poly}(\\log n))$ for any constant $d$ and $\\epsilon>0$, where the exponent of $\\log n$ depends on exponentially on $d$ as well as $1/\\epsilon$. In particular, this drastically improves the exponent of $n$ in the\nrunning time in~\\cite{jansen2022ptas} from a value that is exponential in the dimension\n$d$ and $1/\\epsilon$ to only 1, which is in particular completely\nindependent of $d$ and $1/\\epsilon$. This even implies that our algorithm is an \\emph{efficient} polynomial time approximation scheme (EPTAS) \\footnote{Note that\nthere exists a function $f$ such that\nfor any $n$ and $k$ we have that\n\t\t$(\\log n)^k \\le f(k) \\cdot n^{O(1)}$.}. Thus, up to polylogarithmic\nfactors, we obtain the fastest possible running time of a PTAS for\nany fixed $d$ and $\\epsilon$. Note that, for constant $d$, this yields a distinction\nto problems for which there are lower bounds of $\\Omega(n^{2})$\nor $\\Omega(n^{3})$ based on (S)ETH or other hypotheses, e.g., \\cite{AbboudLCS,AbboundBringmannFischer2023,bringman2018multivariate, BringmannFrechet,bringmann2015quadratic, roditty2013fast}.\n\nFor the case of rectangles, we present a $(2+\\epsilon)$-approximation\nalgorithm with a running time of $O(n\\cdot\\mathrm{poly}(\\log n))$\nfor any constant $\\epsilon>0$. If it is allowed to rotate the rectangles\nby 90 degrees, we obtain an approximation ratio of $\\frac{17}{9}+\\epsilon$\nwith the same running time bound. Thus, our algorithms are much faster\nthan the best known polynomial time algorithms for the problem~\\cite{galvez2021approximating};\nin their running times, the exponent of $n$ depends exponentially\non $1/\\epsilon$ while our exponent is only 1. Although we need\nmuch less running time, our approximation factors are not much higher than their\nratios of $1.89+\\epsilon$ and $1.5+\\epsilon$, respectively.\n\nMoreover, we present the first dynamic algorithms for $d$-dimensional\ngeometric knapsack with hypercubes and  rectangles with and without rotations by 90 degrees. These algorithms maintain solutions with the same approximation guarantees as stated above,\nwith poly-logarithmic worst-case query and update times. In comparison, note that there are problems\n\nfor which there are polynomial conditional lower bounds for\nthe update and query time for dynamic algorithms~\\cite{henzinger2015unifying}.\n\n\nWe remark that our algorithms maintain \\emph{implicit} solutions in the sense that after each update operation, the answers to all query operations refer to the same fixed approximate solution. Note that after adding or removing a single item (e.g., a very large but very profitable item), it can be necessary to change $\\Omega(n)$ items in the current solution in order to maintain a bounded approximation guarantee. Therefore, it is impossible to maintain explicit solutions with our update and query times.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Techniques}\nThe known algorithms for the\\emph{ $d$}-dimensional geometric knapsack\nproblem for squares, cubes, hypercubes, and rectangles are based on\nthe existence of structured packings into $O_{\\epsilon,d}(1)$ boxes\n(i.e., a constant number of boxes for each fixed $\\epsilon$ and $d$).\nIn these algorithms, one first guesses these boxes (i.e., enumerates\nall possibilities) which already yields a running time bound of\n$n^{O_{\\epsilon,d}(1)}$. It is not clear how to make use of these\nstructured packings without guessing the boxes first.\n\nInstead, we use a different type of structured packings which we call \\emph{easily guessable packings}. They are\n\n\\begin{enumerate}[(i)]\n\t\\item flexible enough so that they allow for very profitable solutions, and\n\t\\item structured enough so that we can compute these solutions very fast.\n\\end{enumerate}\nIn these packings, each box is specified by some parameters (e.g. height and width) and we can guess \\emph{all but at most one} parameter for each box in time $O(\\mathrm{poly}(\\log n))$. In contrast, in the previous\nresults, $n^{\\Omega(1)}$ time is needed already for one single parameter.\nHowever, for each box, there may still be one parameter whose value\nwe have not guessed yet. To determine them, we use an important property of our\neasily guessable packings. There is a partition of the input items\nand a partition of the boxes for which we have not yet guessed all parameters. For each resulting subset of items,\nall items of this set can be assigned only to boxes in one specific set of boxes in the partition.\nMoreover, all boxes in the latter set have a certain (identical) value for the remaining\n(not yet guessed) parameter.\n\n\n\n\nThis allows us to adapt the indirect guessing\nframework from~\\cite{heydrich2019faster} to guess the remaining parameter approximately for each box\nstep by step, while losing only a factor of $1+\\epsilon$ in our profit, compared\nto guessing it exactly in time $n^{\\Omega(1)}$.\n\nNote that for the case of rectangles without rotations there is a threshold of 2 since there are no structured packings with a better ratio using only constantly many\nboxes~\\cite{galvez2021approximating}. Thus, it is unclear how to improve our the approximation factor for this case to, e.g., $2-\\delta$ for~$\\delta>0$.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease note that due to space limitations, most proofs were moved to the appendix. \n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Other related work}\n\nPrior to the results for two-dimensional geometric knapsack for rectangles\nmentioned above, a polynomial time $(2+\\epsilon)$-approximation algorithm\nwas presented by Jansen and Zhang~\\cite{jansen2004rectangle} in which the\nexponent of $n$ in the running time is exponential in $1/\\epsilon$.\nFor the special case of unweighted rectangles, the same authors gave\na faster $(2+\\epsilon)$-approximation algorithm with a running time\nof $O(n^{1/\\epsilon+1})$~\\cite{jansen2004maximizing}. If\none allows pseudo-polynomial running time instead of polynomial running\ntime, there is also a $(4/3+\\epsilon)$-approximation algorithm in the setting of weighted rectangles known\ndue to Gálvez, Grandoni, Khan, Ramírez-Romero, and Wiese~\\cite{grandoni2021improved}, having a running\ntime of $(nN)^{O_{\\epsilon}(1)}$. Also here, the exponent of $nN$\nis exponential in $1/\\epsilon$. In addition, there is a $(1+\\epsilon)$-approximation\nalgorithm by Adamaszek and Wiese~\\cite{adamaszek2014quasi} with\nquasi-polynomial running time for any constant $\\epsilon>0$, assuming\nquasi-polynomially bounded input data. If the input objects are triangles\nthat can be freely rotated, there is a polynomial time $O(1)$-approximation\nalgorithm due to Merino and Wiese~\\cite{merino_et_al2020}.\n\nAnother way to generalize \\textsc{Knapsack} is to allow several knapsacks\ninto which the items can be packed, possibly with different capacities.\nThis generalization still admits $(1+\\epsilon)$-approximation algorithms\nwith a running time of $n^{O_{\\epsilon}(1)}$~\\cite{chekuri2005polynomial,kellerer1999polynomial},\nand even with a running time of the form $f(\\epsilon)n^{O(1)}$ for\nsome function $f$~\\cite{jansen2010parameterized,jansen2012fast}.\nFurthermore, there is a dynamic algorithm known for the problem\nwith polylogarithmic update time which also achieves\nan approximation ratio of $1+\\epsilon$~\\cite{eberle_et_al:LIPIcs.FSTTCS.2021.18}.\n\n\n\n\n\\section{\\label{sec:hypercubes}Algorithms for $d$-dimensional hypercubes}\n\nIn this section we present our PTAS and dynamic algorithm for the geometric knapsack problem with $d$-dimensional hypercubes. Let $\\epsilon>0$ and assume w.l.o.g.~that $1/\\epsilon\\in\\mathbb{N}$\nand $\\epsilon<1/2^{d+2}$; we assume that $d\\in \\mathbb{N}$ is a constant. We are given a set of $n$ items $\\I$ where each item $i \\in \\I$ is a hypercube with\nside length $s_i \\in \\mathbb{N}$ in each dimension and profit $p_i \\in \\mathbb{N}$, and an integer $N$ such that the knapsack has side length $N$ (in\neach dimension). In the following, we present a simplified version of our algorithm with a running time of $O(n\\cdot\\mathrm{poly}(\\log N))$. In Appendix~\\ref{app:cubes} we describe how to improve\nits running time to $O(n\\cdot\\mathrm{poly}(\\log n))$ using more involved technical ideas.\n\nIn our algorithms, we use a special data structure to store our items~$\\I$. This data structure will allow us to quickly update the input and obtain important\ninformation about the items in $\\I$. It is defined in the following lemma, which can be proven using standard data structures for range counting/reporting for points in two dimensions (corresponding to side length and profit of each item)~\\cite{lee1984computational}. The details can be found in Appendix~\\ref{app:data_struc}.\n\n\n\n\n\n\n\n\n\\begin{lemma}\n\t\\label{lem:data-structure}There is a data structure for the items\n\t$\\I$ that allows the following operations:\n\t\\begin{itemize}\n\t\t\\item Insertion and deletion of an item in time $O(\\log^2 n)$.\n\t\t\\item Given four values $a,b,c,d \\in \\mathbb{N}$, return the cardinality of the set $\\I(a,b,c,d):= \\{i \\in I: a\\leq s_i \\leq b, c\\leq p_i \\leq d\\}$ in time $O(\\log n)$.\n\t\t\\item Given four values $a,b,c,d \\in \\mathbb{N}$,  return the set $\\I(a,b,c,d):= \\{i \\in I: a\\leq s_i \\leq b, c\\leq p_i \\leq d\\}$ time $O(\\log n +|\\I(a,b,c,d)|)$. \n\t\t\n\t\t\n\t\t\n\t\\end{itemize}\n\\end{lemma}\n\nIn the following, we will refer to this data structure as our \\emph{item\n\tdata structure}. Observe that we can insert all given items $\\I$\ninto it in time $O(n\\log^2 n)$.\nAdditionally, we use balanced binary search trees (see e.g.~\\cite{guibas1978dichromatic}) to store the set of item side lengths and profits; an item can be inserted, deleted, and queried in time $O(\\log n)$ in each of these search trees.\n\n\n\n\n\\subsection{Easily guessable packing of hypercubes}\\label{sec:hypercubes_strucpack}\n\nOur algorithm is based on a structured packing into $O_{\\epsilon,d}(1)$ boxes, i.e., $d$-dimensional hypercuboids, such that the profit of\nthe packed items is at least $(1-O(\\epsilon))\\OPT$, where $\\OPT$ denotes the optimal solution of the given instance. These boxes are packed non-overlappingly inside the knapsack and each item is packed into one of these boxes (see Figure~\\ref{fig:packboxes}). Intuitively, in our algorithm we will guess the sizes of these boxes and compute an assignment of items into the boxes. Our boxes are designed such that we can make these guesses quickly and such that it is easy to place all items that were assigned to each box.\n\n\n\\begin{figure}[h!]\n\t\\begin{minipage}{.45\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[scale = 0.63, page = 9]{figures/Squares2D.pdf}\n\t\t\\subcaption{Packing of items into single boxes}\n\t\t\\label{fig:packboxes_boxes}\n\t\\end{minipage}\n\t\\begin{minipage}{.45\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=0.8\\linewidth,page = 7]{figures/Squares2D.pdf}\n\t\t\\subcaption{Packing of boxes into knapsack} \n\t\t\\label{fig:packboxes_full}\n\t\\end{minipage}\n\t\\caption{Visualization of packing using boxes}\n\t\\label{fig:packboxes}\n\\end{figure}\n\nIn~\\cite{jansen2022ptas}, a structured\npacking was presented that lead to a PTAS with a running time of $n^{O_{\\epsilon,d}(1)}$ for our problem. They use two specific types of boxes. In the formal definition of those, we need the volume and surface of a $d$-dimensional box $B$ which we define as $\\mathrm{VOL}_d(B) := \\prod_{d'=1}^d \\ell_{d'}(B)$ and $\\mathrm{SURF}_d(B) := 2\\sum_{d'=1}^d\\mathrm{VOL}_d(B)/\\ell_{d'}(B)$, respectively.\n\\begin{definition}[$\\mathcal{V}$-boxes and $\\mathcal{N}$-boxes~\\cite{jansen2022ptas}]\n\t\\label{def:jansen_boxes}\n\tLet $B$ be a $d$-dimensional hypercuboid, $\\I$ be a set of items packed in it, and let $\\hat{s}$ be an upper bound on the side lengths of the items in $\\I$. We say that $B$ is\n\t\\begin{itemize}\n\t \\item a $\\mathcal{V}$-box if its side lengths can be written as $n_1\\hat{s},n_2\\hat{s},\\dots,n_d\\hat{s}$ where $n_1,n_2,\\dots,n_d \\in \\mathbb{N}_{+}$ and the volume of the packed items is at most $\\mathrm{VOL}_d(B)-\\hat{s}\\frac{\\mathrm{SURF}_d(B)}{2}$,\n\t \\item an $\\mathcal{N}$-box if its side lengths can be written as $n_1\\hat{s},n_2\\hat{s},\\dots,n_d\\hat{s}$ where $n_1,n_2,\\dots,n_d \\in \\mathbb{N}_{+}$ and the number of packed items is at most $\\prod_{i=1}^d n_i$.\n\t\\end{itemize}\n\\end{definition}\n\n\nThese boxes and the structured packing using them, however, is not enough for our purposes since each parameter must be guessed. It is unclear how to do this faster than\nin time~$n^{\\Omega(1)}$, already for one single parameter. Therefore, we refine the packing presented in~\\cite{jansen2022ptas} by adapting the type of boxes to include additional technical properties.\n\n\n\n\n\n\n\n\nOur first type of boxes are $\\N$-boxes which are a refinement of the aforementioned $\\mathcal{N}$-boxes. For each $\\N$-box $B$ we specify two additional parameters $s_{\\min}(B)$ and $s_{\\max}(B)$ and require for each item $\\in \\I$ packed in $B$ that $s_{\\min}(B)\\le s_i \\le  s_{\\max}(B)$. These values $s_{\\min}(B)$ and $s_{\\max}(B)$ will help us in our computation later. Intuitively, the box $B$ is partitioned into a $d$-dimensional grid with spacing $s_{\\max}$, such that each item in $B$ is placed inside one of these grid cells (similar as the $\\N$-boxes), see Figure~\\ref{fig:origpack_Nmain}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{definition}[$\\N$-box]\nLet $B$ be a $d$-dimensional hypercuboid with given values $s_{\\min}(B)\\ge~0$ and\n$s_{\\max}(B)\\ge~0$\nand suppose we are given\na packing of items $\\I'\\subseteq\\I$ inside $B$.\n\n\n\nWe say that $B$ is an $\\N$\\emph{-box}\nif $s_{\\min}(B)\\le\\min_{i\\in\\I'}s_{i}$ and\n$s_{\\max}(B)\\ge\\max_{i\\in\\I'}s_{i}$\nand\nif its side lengths can be written as $n_{1}(B)s_{\\max}(B),\\dots,n_{d}(B)s_{\\max}(B)$\nwith $n_{1}(B),\\dots,n_{d}(B)\\in\\mathbb{N}$ such that~$|\\I'|\\le\\prod_{i=1}^{d}n_{i}(B)$.\n\\end{definition}\nOur second type of boxes are $\\S$-boxes. Intuitively, an $\\S$-box $B$ contains only items that are very small in each dimension compared to $B$. For technical reasons, we require that the packed items do not use the full volume of $B$, even\nif we increase each item size to the next larger power of $1+\\epsilon$. This will allow us to compute our solution efficiently since we may\nround up item sizes and profits to powers of $1+\\epsilon$.\n\nThroughout this paper, for any  $x > 0$ we define $\\lceil x\\rceil_{1+\\epsilon}$\n\nto be the smallest power of $1+\\epsilon$ that is larger\nthan~$x$ rounded down. Similarly, we define $\\lfloor x\\rfloor_{1+\\epsilon}$ to be the largest power of $1+\\epsilon$ that is smaller than $x$ rounded up.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{definition}[$\\S$-boxes]\nLet $B$ be a $d$-dimensional hypercuboid with side length $\\ell_{d'}(B)\\in\\mathbb{N}_{0}$\nfor each $d'\\in[d]$ and suppose we are given a packing of items $\\I'\\subseteq\\I$\ninside $B$. We say that $B$ is a $\\S$\\emph{-box} if for each item\n$i\\in\\I'$ we have $s_{i}\\leq\\epsilon\\min_{d'\\in[d]}\\ell_{d'}(B)$\nand additionally $\\sum_{i\\in\\I'}\\lceil s_{i}\\rceil_{1+\\epsilon}^{d}\\leq(1-2d\\cdot\\epsilon)\\mathrm{VOL}_d(B)$.\n\\end{definition}\n\\begin{figure}\n\t\\begin{minipage}{.4\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=0.9\\linewidth, page = 1]{figures/Squares2D.pdf}\n\t\t\\subcaption{Sorted packing of an $\\N$-box: each item is packed into a single grid cell.}\n\t\t\\label{fig:origpack_Nmain}\n\t\\end{minipage}\n\t\\hspace*{1cm}\n\t\\begin{minipage}{.4\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=0.9\\linewidth,page = 2]{figures/Squares2D.pdf}\n\t\t\\subcaption{NFDH packing of an $\\S$-box: items are packed greedily into shelves.}\n\t\t\\label{fig:origpack_Smain}\n\t\\end{minipage}\n\t\\caption{Visualization of an $\\N$- and an $\\S$-box for $d=2$}\n\t\\label{fig:origpackmain}\n\\end{figure}\n\n\n\nFor an $\\S$-box $B$, we can show that all items in\n\n$\\I'$\n\ncan be packed inside $B$ using the Next-Fit-Decreasing-Height (NFDH) algorithm. NFDH is a greedy algorithm that orders\nthe items non-increasingly by their sizes and then packs them greedily\nin this order into shelves within the box $B$ (see Figure~\\ref{fig:origpack_Smain} and\nAppendix~\\ref{app:cubes} for details).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{lemma}[implied by Lemma 4 in~\\cite{harren2009approximation}]\n\\label{lem:NFDH}Let $B$ be a box and let $\\I'$ be a set of items\nsuch that $s_{i}\\leq2\\epsilon\\ell_{\\min}(B)$ for each $i\\in\\I'$\nand $\\sum_{i\\in\\I'}\\lceil s_{i}\\rceil_{1+\\epsilon}^{d}\\leq\\mathrm{VOL}_{d}(B)-2(d-1)\\cdot\\epsilon\\mathrm{VOL}_{d}(B)$.\n\nThen, NFDH finds a packing of $\\I'$ into $B$.\n\n\t\n\\end{lemma}\nWe now prove that there is always a $(1+O(\\epsilon))$-approximate \\emph{easily guessable packing} using $O_{\\epsilon,d}(1)$ boxes, each of them being an $\\N$- or an $\\S$-box. In order to do so we refine the structured packing in~\\cite{jansen2022ptas} which uses $\\mathcal{N}$- and $\\V$-boxes.\nThe key additional property is that there are $O_\\epsilon(1)$ values $k_{1},k_{2},\\dots,k_{r}$ such that each $\\N$-box $B$ contains\nonly items whose respective sizes are within the interval $(k_{j-1},k_{j}]$\nfor some $j$. In particular, we have that $s_{\\min}(B)=k_{j-1}$ and $s_{\\max}(B)=k_{j}$ and the values $k_{1},k_{2},\\dots,k_{r}$ yield a partition\nof the items in $\\I$ of the form $(k_{j-1},k_{j}]$. This will allows us to apply an indirect guessing framework later to guess the values $k_{1},k_{2},\\dots,k_{r}$ quickly.\nAll remaining parameters of the boxes  can be easily guessed in time $O(\\log N)$ each (and there are only $O_{\\epsilon,d}(1)$ parameters in total).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{lemma}[Near-optimal packing of hypercubes]\n\\label{lem:struc_hypercubes} For any instance $\\I$ of the $d$-dimensional\nhypercube knapsack problem and any $\\epsilon<1/2^{d+2}$, there exists\na packing with the following properties: \n\\begin{enumerate}[i)]\n\t\\item It consists of $\\N$- and $\\S$-boxes whose total number is bounded\n\tby a value $C_{\\mathrm{boxes}}(\\epsilon,d)$ depending only on $\\epsilon$\n\tand $d$. \n\t\\item There exist values $k_{1},k_{2},\\dots,k_{r}\\in\\mathbb{Z}_{\\geq0}$\n\twith $r\\in O_{\\epsilon,d}(1)$ such that if $B$ is an $\\N$-box there\n\texists a value $j_{B}\\in\\{1,2,\\dots,r\\}$ such that $s_{\\max}(B)=k_{j_{B}}$\n\tand $s_{\\min}(B)=k_{j_{B}-1}$ with $k_{0}:=0$. \n\t\\item For each $\\S$-box $B$ and each $d' \\in [d]$ we have that $\\ell_{d'}(B) = \\lfloor x\\rfloor_{1+\\epsilon}$ for some $x\\in [N]$.\n\t\\item For each $\\N$-box $B$ and each $d' \\in [d]$ we have that $n_{d'}(B) = \\lfloor n'\\rfloor_{1+\\epsilon}$ for some $n'\\in [n]$ if $n_{d'}(B) > 1/\\epsilon$.\n\n\t\\item The total profit of the packing is at least $(1-2^{O(d)}\\epsilon)\\OPT$.\n\t\n\t\n\t\n\\end{enumerate}\n\\end{lemma}\n\\begin{proof}[Proof sketch.]\nWe start with the structured packing due to Jansen, Khan, Lira and Sreenivas~\\cite{jansen2022ptas} which consists of $O_{\\epsilon,d}(1)$ many $\\mathcal{N}$- and $\\V$-boxes and items with a total profit of at least~$(1-2^{d+2}\\epsilon)\\OPT$. We modify this packing as follows. First, we consider the $\\V$-boxes and split each of them into at most $O_{\\epsilon,d}(1)$ many $\\N$- and $\\S$-boxes. We lose at most a factor of $1+\\epsilon$ of our profit in this step.\nFor each resulting $\\N$-box $B$, we define\n\n\n$s_{\\max}(B)$ to be the maximum size of an item packed into $B$.\n\nThis yields a packing satisfying property i).\n\nIn order to satisfy property ii), we first introduce a value $k_j$ for each distinct value $s_{\\max}(B)$ of the $\\N$-boxes constructed so far. We order the resulting values $k_1,...,k_r$ increasingly.\nThen, we split the $\\N$-boxes into smaller boxes such that for each resulting box $B$, the range of item sizes of $B$ is contained in $(k_{j-1},k_{j}]$ for some $j$; hence, we define $s_{\\min}(B):=k_{j-1}$. We ensure that the number of boxes increases by at most a factor of $O_{\\epsilon,d}(1)$ in this step.\n\n\nNext, we modify the packing to satisfy properties iii) and iv).\nFor each $\\N$-box $B$ and each dimension $d'\\in [d]$, we round $n_{d'}(B)$ down to $\\lfloor n_{d'}(B) \\rfloor_{1+\\epsilon}$. This decreases the maximum number of items we can pack into $B$ by at most a factor of $(1+\\epsilon)^d=1+O(\\epsilon)$; hence, also our profit reduces by at most this factor. Finally, for each $\\S$-box $B$ we round down each side length\n$\\ell_{d'}(B)$ to $\\lfloor \\ell_{d'}(B)\\rfloor_{1+\\epsilon}$.\n\nSince all items in $B$ are small compared to each side length of~$B$, our profit reduces by at most a factor of $1+\\epsilon$.\n\n\n\\end{proof}\n\n\\subsection{\\label{subsec:Computing-packing}Computing a packing}\nIn this section we discuss how to compute our packing. Intuitively, we try to guess the packing due to Lemma~\\ref{lem:struc_hypercubes}. We assume that all input items $\\I$ are stored in our item data structure (see to Lemma~\\ref{lem:data-structure}) and we discard all items with profit less than $\\epsilon \\frac{p_{\\max}}{n}$ where $p_{\\max} :=\\max_{i\\in \\I} p_i$. The total profit of the discarded items is at most $n\\cdot \\epsilon \\frac{p_{\\max}}{n} \\le \\epsilon \\OPT$.\n\nLet $\\B$ denote the set of boxes due to Lemma~\\ref{lem:struc_hypercubes}. In our algorithm, we first guess all parameters of the boxes in $\\B$ apart from the values $k_1,\\dots,k_r$. We then modify the indirect guessing framework introduced by Heydrich and Wiese~\\cite{heydrich2019faster} to approximately compute the values $k_1,\\dots,k_r$. Our computed values might be imprecise in the sense that they yield a solution whose profit could be by a factor $1+\\epsilon$ lower than the solution due to Lemma~\\ref{lem:struc_hypercubes}. However, our resulting boxes are guaranteed to fit inside of the knapsack.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsubsection{Guessing basic quantities}\nWe start by guessing the number of $\\N$- and $\\S$-boxes, respectively. Then, for each $\\N$-box $B\\in\\B$ we guess\n\\begin{itemize}\n\t\\item the value $j_{B}$ indicating that $s_{\\max}(B)=k_{j_{B}}$ and $s_{\\min}(B)=k_{j_{B}-1}$; note that for $j_{B}$ there are only $O_{\\epsilon,d}(1)$ possibilities,\n\t\\item the value $n_{d'}(B)$ for each dimension $d' \\in [d]$; for each value $n_{d'}(B)$ there are only $O(\\log n)$ possibilities.\n\\end{itemize}\nFor each $\\S$-box $B\\in\\B$, we guess $\\ell_{d'}(B)$ for each $d'\\in[d]$. Note that also here, for each of these values there are only $O(\\log N)$ possibilities.\nWe denote by the \\emph{basic quantities} all these guessed values for all boxes in $\\B$.\n\n\n\n\n\n\n\n\n\n\\begin{lemma}\\label{lem:hypcub_guessing_basic}\n\tIn time $(\\log N)^{O_{\\epsilon,d}(1)}$ we can guess all basic quantities.\n\n\n\n\n\\end{lemma}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe do not know the values $k_{1},k_{2},\\dots,k_{r}$. However, recall that they yield a partition of $\\I$ with a set $\\I_{j}:=\\{i\\in\\I:s_{i}\\in(k_{j-1},k_{j}]\\}$\nfor each $j\\in[r]$. We next guess additional quantities which provide us with helpful information for the next step of our algorithm, the indirect guessing framework. First, we guess approximately the profit that each set $\\I_{j}$ contributes\nto $\\OPT$. Formally, for each $j\\in[r]$ we guess $\\hat{p}(j):=\\left\\lfloor p(\\I_{j}\\cap\\OPT)\\right\\rfloor _{1+\\epsilon}$\nif $p(\\I_{j}\\cap\\OPT)\\ge\\frac{\\epsilon}{r}\\OPT$ and $\\hat{p}(j):=0$\notherwise.  Since $\\OPT\\in[p_{\\max},n\\cdot p_{\\max})$, we have that $\\hat{p}_{j}\\in\\{0\\}\\cup[\\frac{\\epsilon}{r}p_{\\max},n\\cdot p_{\\max})$; hence, there are\nonly $O(\\log N)$ possibilities for $\\hat{p}_{j}$.\n\n\\begin{lemma}\\label{lem:hypcub_guessing_pjs}\n\tThe value $\\hat{p}_j$ for each $j\\in[r]$ can be guessed in time $(\\log n)^{O_{\\epsilon,d}(1)}$ and they satisfy $\\sum_{j=1}^{r}\\hat{p}(j)\\ge(1-O(\\epsilon))\\OPT$.\n\\end{lemma}\n\n\n\n\n\n\n\n\n\n\n\n\nObserve that each $\\N$-box $B\\in\\B$ can contain only items from\n$\\I_{j_{B}}$. However, each $\\S$-box $B\\in\\B$ might contain items\nfrom more than one set $\\I_{j}$. For each $\\S$-box $B\\in\\B$ and\neach set $\\I_{j}$, we guess approximately the fraction of the volume\n$B$ that is occupied by items from $\\I_{j}$. Formally, for each\nsuch pair we define the value $a_{B,j}:=\\frac{\\sum_{i\\in\\I(B)\\cap\\I_{j}}\\lceil s_{i}\\rceil_{1+\\epsilon}^{d}}{\\mathrm{VOL}_d(B)}$\nand guess the value $\\hat{a}_{B,j}:=\\left\\lceil \\frac{a_{B,j}}{\\epsilon/r}\\right\\rceil \\epsilon/r$,\ni.e., the value $a_{B,j}$ rounded up to the next larger integral\nmultiple of $\\epsilon/r$. Note that for each value $\\hat{a}_{B,j}$\nthere are only $O_{\\epsilon,d}(1)$ possibilities, and that there\nare only $O_{\\epsilon,d}(1)$ such values that we need to guess.\n\\begin{lemma}\\label{lem:hypcub_guessing_abjs}\n\tFor each $\\S$-box $B \\in \\B$ and each $j\\in[r]$ the value $\\hat{a}_{B,j}$\n\tcan be guessed in time~$(\\log n)^{O_{\\epsilon,d}(1)}$. Moreover, for each $\\S$-box $B \\in \\B$ we have that $\\sum_{j=1}^r \\hat{a}_{B,j} \\mathrm{VOL}_d(B) \\leq (1-2d\\cdot\\epsilon)\\mathrm{VOL}_d(B) + \\epsilon \\mathrm{VOL}_d(B)$.\n\\end{lemma}\n\n\n\n\n\n\n\\subsubsection{Indirect guessing}\nThe next step of our algorithm is to determine the values $k_{1},k_{2},\\dots,k_{r}$. Unfortunately, we cannot guess them directly in polylogarithmic time,\nsince there are $N$ options for each of them. In contrast to the other guessed quantities, it is not sufficient to allow only powers of $1+\\epsilon$ for each of them. If we choose a value for some $k_j$ that is only a little bit too large, then our boxes might not fit into the knapsack anymore, since for each $\\N$-box $B$ we have that $s_{\\max}(B)=k_{j_B}$ and the side length of $B$ in each dimension $d'$ equals $n_{d'}(B) s_{\\max}(B)$. On the other hand, if we define some value $k_j$ only a little bit too small, then we might not be able to assign items with enough profit to $B$, e.g., if $B$ contains only one item which does not fit anymore if we choose $k_j$ too small. Hence, we need a different approach to determine the values $k_{1},k_{2},\\dots,k_{r}$. To this end, we modify the \\emph{indirect guessing framework} introduced in~\\cite{heydrich2019faster} to fit our purposes.\n\nThe main idea is to compute values $\\tilde{k}_0, \\tilde{k}_{1},\\tilde{k}_{2},\\dots,\\tilde{k}_{r}$\nthat we use instead of the values $k_{0},k_{1},k_{2},\\dots,k_{r}$.\nThey yield a partition of $\\I$ into sets $\\tilde{\\I}_{j}:=\\{i\\in\\I:s_{i}\\in(\\tilde{k}_{j-1},\\tilde{k}_{j}]\\}$.\nIntuitively, for each $j$ we want to pack items from $\\tilde{\\I}_{j}$\ninto the space that is used by items in $\\I_{j}$ in the packing from\nLemma~\\ref{lem:struc_hypercubes}. We will choose the values $\\tilde{k}_{1},\\tilde{k}_{2},\\dots,\\tilde{k}_{r}$ such that in this way, we obtain almost the same profit as the packing of Lemma~\\ref{lem:struc_hypercubes}.\n\nFurthermore, we ensure that $\\tilde{k}_{j}\\le k_{j}$ for each $j\\in[r]$. This implies that the side lengths of the resulting $\\N$-boxes are at most the side lengths of the $\\N$-boxes due to Lemma~\\ref{lem:struc_hypercubes} and, therefore, the guessed $\\N$-boxes can be feasibly packed into the knapsack.\n\n\n\n\n\n\nFormally, we perform $r$ iterations, one for each value $k_j$. We define $\\tilde{k}_{0}:=0$. Suppose\ninductively that we have determined $\\ell$ values $\\tilde{k}_{1},\\tilde{k}_{2},\\dots,\\tilde{k}_{\\ell}$\nalready for some $\\ell\\in\\{0,1,...,r-1\\}$ such that $\\tilde{k}_{\\ell}\\le k_{\\ell}$.\nWe want to compute $\\tilde{k}_{\\ell+1}$ such that  $\\tilde{k}_{\\ell+1}\\le k_{\\ell+1}$. To this end, we do a binary search on the set $S:=\\{s_{i}:i\\in\\I\\wedge \\tilde{k}_{\\ell}<s_{i}\\}$, using our item data structure. Our first candidate value is the median of $S$\nwhich we can find in time $O(\\log n)$ via our binary search tree for the item sizes. For each candidate value $s\\in S$, we estimate\nup to a factor of $1+\\epsilon$ the possible profit due to items in $\\tilde{\\I}_{\\ell+1}$\nif we define $\\tilde{k}_{\\ell+1}:=s$.\nWe will describe later how we compute such an estimation. The objective is to find the smallest value $s \\in S$ such that the estimated profit is at least $(1+\\epsilon)^{-1}\\hat{p}(j)$. Hence, if for a specific guess $s$ our obtained profit due to $s$ is at least $(1+\\epsilon)^{-1}\\hat{p}(j)$, we restrict our set $S$ to $\\{s_{i}:i\\in\\I\\wedge \\tilde{k}_{\\ell} \\leq s_{i} \\leq s\\}$ and continue with the next iteration of the binary search.\nIf, however, the estimated profit due to $s$ is strictly less than $(1+\\epsilon)^{-1}\\hat{p}(j)$, this implies that $\\tilde{k}_{\\ell+1} > s$ since otherwise the set of items $\\tilde{\\I}_{\\ell+1}$ is a subset of the items considered for guess $s$ and cannot yield a larger profit. Hence, we restrict our set $S$ to $\\{s_{i}:i\\in\\I\\wedge s_{i}>s\\}$ and continue with the binary search. We stop when $|S|=1$.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe denote by $\\B_{\\N}$ and $\\B_{\\S}$ the set of $\\N$- and $\\S$-boxes in $\\B$ due to Lemma~\\ref{lem:struc_hypercubes}, respectively. Additionally, we denote by $\\B_{\\N}(\\ell+1)$ the set of $\\N$-boxes $B$ with $j_{B} = \\ell+1$ and\nby $\\B_{\\S}(\\ell+1)$ the set of $\\S$-boxes $B$ with $\\hat{a}_{B,\\ell+1}>0$. Note that those are the only boxes that are relevant for the current iteration in which we want to determine $k_{\\ell+1}$ (approximately).\nWe also define $\\B(\\ell+1) := \\B_{\\S}(\\ell+1) \\cup \\B_{\\N}(\\ell+1)$.\n\nWe describe now how we estimate the obtained profit for one specific candidate\nchoice of $s\\in S$.\nWe try to pack items from $\\tilde{\\I}_{\\ell+1}(s):=\\left\\{ i\\in\\I:s_{i}\\in(\\tilde{k}_{\\ell},s]\\right\\} $\ninto\n\\begin{itemize}\n\\item the $\\N$-boxes $\\B_{\\N}(\\ell+1)$ and\n\\item the $\\S$-boxes $\\B_{\\S}(\\ell+1)$, where for each $\\S$-box $\\B_{\\S}(\\ell+1)$, we use a volume\nof at most $\\hat{a}_{B,\\ell+1}\\cdot\\mathrm{VOL}(B)$ and ensure that we pack\nonly items $i\\in\\tilde{\\I}_{\\ell+1}(s)$ for which $s_{i}\\leq\\epsilon\\ell_{\\min}(B)$.\n\\end{itemize}\nWe solve this subproblem approximately via the following integer program\n$(\\mathrm{IP}(s))$. Intuitively,\nwe group items such that all items in the same group have the same size and profit, up to a factor of $1+\\epsilon$.\nFormally, we define\n\\begin{itemize}\n \\item a size class $\\Q_{t}=\\{i\\in \\tilde{\\I}_{\\ell+1}(s):s_{i}\\in[(1+\\epsilon)^{t},(1+\\epsilon)^{t+1})\\}$ for each $t\\in  \\mathcal{T}_Q =  \\{\\lfloor \\log_{1+\\epsilon}(\\tilde{k}_\\ell)\\rfloor,\\dots,\\lceil \\log_{1+\\epsilon}(s)\\rceil\\}$; we denote by $\\hat{s}(t):=(1+\\epsilon)^{t+1}$ the corresponding ``rounded'' size,\n \\item a profit class $\\P_{t'}=\\{i\\in \\tilde{\\I}_{\\ell+1}(s):p_{i}\\in[(1+\\epsilon)^{t'},(1+\\epsilon)^{t'+1})\\}$ for each  $t' \\in \\mathcal{T}_P =  \\{\\lfloor \\log_{1+\\epsilon} \\epsilon \\frac{p_{\\max}}{n}\\rfloor,\\dots, \\lceil \\log_{1+\\epsilon} p_{\\max}\\rceil\\}$; we denote by $\\hat{p}(t'):=(1+\\epsilon)^{t'+1}$ the corresponding ``rounded'' profit and\n \\item a set of pairs $\\mathcal{T}:=\\{(t,t') : t\\in \\mathcal{T}_Q \\wedge t'\\in \\mathcal{T}_P\\}$.\n\\end{itemize}\n\n\n\nOur subproblem is now equivalent (up to a factor of $1+\\epsilon$ in the profit) to choosing how many items from each group are packed into which box, while maximizing the total profit of these items, such that\n\\begin{itemize}\n\t\\item for each $\\N$-box $B \\in \\B_{\\N}(\\ell+1)$ the number of items packed into $B$ is at most the number of grid cells denoted by $n(B) := \\prod_{d'=1}^d \\hat{n}_{d'}(B)$,\n\t\\item for each $\\S$-box $B \\in \\B_{\\S}(\\ell+1)$ the total volume of items packed into $B$ does not exceed the designated volume for items in $\\I_{j}$ reserved in $B$ and\n\t\\item for each pair $(t,t')$ the number of items packed into all boxes is at most the number of available items in the corresponding group.\n\\end{itemize}\n\n\\begin{alignat*}{3}\n(\\mathrm{IP}(s))\\quad& \\text{max} \t& \\displaystyle \\sum_{(t,t') \\in \\mathcal{T}}\\sum_{B \\in \\B(\\ell+1)} x_{t,t',B} p(t')\t\t\t& \t\t\t& \\quad & \\\\\n& \\text{s.t.} & \\displaystyle\\sum_{(t,t') \\in \\mathcal{T}} x_{t,t',B} \t& \\leq n(B)\t& \t\t& \\forall B \\in \\B_{\\N}(\\ell+1) \\\\\n& & \\displaystyle\\sum_{(t,t') \\in \\mathcal{T}} x_{t,t',B}s(t)^d\t& \\leq a_{B,\\ell+1}\\mathrm{VOL}_d(B)\t& \t\t& \\forall B \\in \\B_{\\S}(\\ell+1) \\\\\n&\t\t\t\t& \\displaystyle\\sum_{B \\in \\B(\\ell+1)} x_{t,t',B}\t\t\t\t\t\t\t\t& \\leq n_{t,t'}\t& \t\t& \\forall (t,t') \\in \\mathcal{T} \\\\\n&\t\t\t\t& x_{t,t',B}\t\t\t\t\t\t\t\t& \\in \\mathbb{N}_{0}& \t\t&\\forall (t,t') \\in \\mathcal{T}, B \\in \\B(\\ell+1)\n\\end{alignat*}\n\nWe cannot solve $(\\mathrm{IP}(s))$ directly; however, we show that we can solve it\napproximately, losing only a factor of $1+\\epsilon$. We describe now how to do this in time $(\\log_{1+\\epsilon}(N))^{O_\\epsilon(1)}$. We start by guessing the\n$|\\B(\\ell+1)|\\cdot |\\T|/\\epsilon = O_{\\epsilon,d}(1)$\n most profitable items in an optimal solution of $(\\mathrm{IP}(s))$. To do this, we guess the profit type and size type of each of these items as well as which box they are packed in. For a single item this yields a total amount of $O_{\\epsilon,d}((\\log_{1+\\epsilon}(N))^2)$ many possibilities, and hence at most $(\\log_{1+\\epsilon}(N))^{O_\\epsilon(1)}$ possibilities overall. We adjust $(\\mathrm{IP}(s))$ accordingly (i.e., reduce the right-hand sides of our constraints) and solve the LP relaxation of the remaining problem in time $\\left( \\log_{1+\\epsilon}(N)\\right)^{O(1)}$, yielding a solution $x^*$. We round it by simply defining $\\bar{x}_{t,t',B}:=\\lfloor x^*_{t,t',B}\\rfloor$ for each $(t,t')\\in \\T$ and each $B\\in \\B(\\ell+1)$. This yields a solution consisting of the guessed items together with $\\bar{x}$, where $\\bar{x}$ represents the remaining items in our solution.\n Since we guessed the  $|\\B(\\ell+1)|\\cdot |\\T|/\\epsilon$ most profitable items before but there are only $|\\B(\\ell+1)|\\cdot |\\T|$ variables,\n we solve $(\\mathrm{IP}(s))$ up to a factor of $1+\\epsilon$.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{lemma}\\label{lem:cubes_IP_sol}\nThere is an algorithm with a running time of $(\\log_{1+\\epsilon}(N))^{O(1)}$\nthat computes a $(1+\\epsilon)$-approximate solution for \\textup{$(\\mathrm{IP}(s))$} for each $s$;\nwe denote by $q(s)$ the value of this solution. For two values $s,s'$\nwith $s\\le s'$ we have that $q(s)\\le q(s')$.\n\\end{lemma}\n\n\n\nAt the end of our binary search, we define $\\tilde{k}_{\\ell+1}$ to be the smallest value $s\\in S$ for which $q(s)\\ge (1-\\epsilon)\\hat{p}(\\ell+1)$. Let $x^*_{\\ell+1}$ denote the computed solution to $(\\mathrm{IP}(s))$ corresponding\nto $s=\\tilde{k}_{\\ell+1}$. Based on the inductive assumption $\\tilde{k}_{\\ell}\\le k_{\\ell}$ and our choice of $\\tilde{k}_{\\ell+1}$, we can show $\\tilde{k}_{\\ell+1}\\le k_{\\ell+1}$. This is crucial as $\\tilde{k}_{\\ell+1}$ determines the side lengths of an $\\N$-box $B$ with $j_B = \\ell+1$. Thus, in order to be able to pack our guessed boxes into the knapsack we must guarantee that these side lengths are not larger than the side lengths of the corresponding box in the packing underlying Lemma~\\ref{lem:struc_hypercubes}.\n\\begin{lemma}\\label{lem:cubes_induc_kr}\nWe have that $\\tilde{k}_{\\ell+1}\\le k_{\\ell+1}$.\n\\end{lemma}\nAfter completing the $r$ rounds of our indirect guessing framework, we have obtained values $\\tilde{k}_{1},\\tilde{k}_{2},\\dots,\\tilde{k}_{r}$ and $r$\nintegral solutions to $(\\mathrm{IP}(\\tilde{k}_{1})),...,(\\mathrm{IP}(\\tilde{k}_{r}))$.\nWe can construct a feasible solution $\\ALG$ by combining these solutions:\nfor each $j\\in[r]$ and each $\\N$-box $B\\in\\B$ we define $s_{\\max}(B):=\\tilde{k}_{j_{B}}$\nand we assign the items of each set $\\tilde{\\I}_{j}$ into\nthe boxes in $\\B$ according to our solution to $\\mathrm{IP}(\\tilde{k}_{j}))$.\n\nFor each $j \\in \\{1,\\dots,r\\}$, each size class $\\Q_t$, and each profit class $\\P_{t'}$, there is a certain number of items from the set\n$\\tilde{\\I}_{j} \\cap \\Q_t \\cap \\P_{t'}$ that our solution contains; we denote this number by $z_{j,t,t'}$. It is irrelevant which exact items from\nthis set we pick (up to a factor of $1+\\epsilon$ in the profit). Thus, we simply order the items from the set $\\tilde{\\I}_{j} \\cap \\Q_t \\cap \\P_{t'}$ in non-decreasing order of side lengths, select the first $z_{j,t,t'}$ items, and assign them to the corresponding boxes.\n\n\nFinally, we pack each $\\S$-box $B$ using the NFDH-algorithm (Lemma~\\ref{lem:NFDH}) and each $\\N$-box $B$ by placing each item into a single cell of the $d$-dimensional grid with side length $s_{\\max}(B)$.\n\n\n\n\n\nThis yields an algorithm with a running time of $n\\cdot(\\log^2 n)+(\\log N)^{O_{\\epsilon,d}(1)}$. In Appendix~\\ref{app:cubes} we explain how we can improve it to $n\\cdot(\\log^2 n)+(\\log n)^{O_{\\epsilon,d}(1)}$. Key to this is to guess the basic quantities and to solve each integer program $(\\mathrm{IP}(s))$ in time $(\\log n)^{O_{\\epsilon,d}(1)}$, using additional technical improvements such as restricting the range of the considered item sizes and profits\nwhen solving $(\\mathrm{IP}(s))$. We would like to remark that the exponent of $\\log n$ depends on the value $C_{\\mathrm{boxes}}(\\epsilon,d)$ in Lemma~\\ref{lem:struc_hypercubes} which in turn depends on the (not precisely specified) number of boxes used in the structured packing in~\\cite{jansen2022ptas}.\n\n\\begin{theorem}\\label{thm:cubes_ptas}\nThere is a $(1+\\epsilon)$-approximation algorithm for the $d$-dimensional\nhypercube knapsack problem with a running time of $n\\cdot(\\log^2 n)+(\\log n)^{O_{\\epsilon,d}(1)}$.\n\\end{theorem}\n\n\\subsection{Dynamic algorithm}\nThe algorithmic techniques above can be combined with our item data structure to derive a dynamic algorithm for the $d$-dimensional hypercube knapsack problem. Our algorithm supports the following operations:\n\\begin{enumerate}[(i)]\n\t\\item Insertion and deletion of an item into our data structure,\n\t\\item Output a $(1+\\epsilon)$-estimate of the value of the optimal solution, \n\t\\item Output a $(1+\\epsilon)$-approximate solution $\\ALG$ and\n\t\\item Query where a given item is contained in $\\ALG$.\n\\end{enumerate}\nFor operation (i) we simply add or delete an item from our item data structure (see Lemma~\\ref{lem:data-structure}) and our balanced binary search trees, which takes time $O(\\log^2 n)$.\nIn order to execute operation (ii), we run our algorithm described above, except for computing the explicit packing of the items in the end. Instead, we simply return the total profit of\nour solutions to the integer programs $(\\mathrm{IP}(s))$ that correspond the solution that we output at the end.\n This takes time $(\\log n)^{O_{\\epsilon,d}(1)}$ in total. If a $(1+\\epsilon)$-approximate solution $\\ALG$ is queried (operation (iii)), we also compute the exact set of items and their packing as described previously. Since their total number is $|\\ALG|$, we can compute and output $\\ALG$ in time $O(|\\ALG|\\cdot(\\log n))+(\\log n)^{O_{\\epsilon,d}(1)}$.\n\n Finally, if it is queried whether a given item $i \\in \\I$ is in contained in $\\ALG$ (operation (iv)), we determine the value  $j \\in \\{1,\\dots,r\\}$, the size class $\\Q_t$, and the profit class $\\P_{t'}$ for which $i\\in \\tilde{\\I}_{j} \\cap \\Q_t \\cap \\P_{t'}$. Recall that $z_{j,t,t'}$ denotes the total number of items from this set we select and we select the\n $z_{j,t,t'}$ items in this set of shortest side length. Hence, we output ``$i\\in\\ALG$'' if $i$ is among the shortest $z_{j,t,t'}$ items in $\\tilde{\\I}_{j} \\cap \\Q_t \\cap \\P_{t'}$, and ``$i\\notin\\ALG$'' otherwise. This ensures that we give consistent answers between consecutive updates of the set $\\I$.\n \n \n\n\n\n\n\n\\begin{theorem}\\label{thm:dyn_alg_cubes}\nThere is a dynamic algorithm for the $d$-dimensional hypercube knapsack\nproblem that supports the following operations:\n\\begin{enumerate}[(i)]\n\\item insertion or deletion of an item in time $O(\\log^2 n)$,\n\\item output a $(1+\\epsilon)$-estimate of the value of the optimal solution in time $(\\log n)^{O_{\\epsilon,d}(1)}$,\n\\item output a $(1+\\epsilon)$-approximate solution $\\ALG$ in time $O(|\\ALG|\\cdot(\\log n))+(\\log n)^{O_{\\epsilon,d}(1)}$,\n\\item query whether an item is contained in $\\ALG$ in time $(\\log n)^{O_{\\epsilon,d}(1)}$.\n\\end{enumerate}\n\\end{theorem}\n\n\n\\section{Algorithms for rectangles}\\label{sec:rec_main}\nIn this section we give an overview of our algorithms for two-dimensional knapsack for rectangles (see Appendix~\\ref{app:rec} for details).\nFirst, we classify items into four groups: we say that an item is \\emph{large} if it is large compared to edge length of the knapsack in both dimensions and \\emph{small} if it is small in both dimensions.\n\nAn item of relatively large width (height) and relatively small height (width) is referred to as \\emph{vertical} (\\emph{horizontal}). We construct \\emph{easily guessable packing} using four types of boxes.\nThe first type are \\emph{$\\L$-boxes} which contain only one large item each. Also, we use \\emph{$\\H$-boxes} inside which horizontal items are stacked on top of each other (see Figure~\\ref{fig:pack_VH}), and correspondingly \\emph{$\\V$-boxes} for vertical items.\n\nFinally, we use $\\S$-boxes which are defined in the same manner as in the case of (hyper-)cubes. \n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=.9\\linewidth,page = 3]{figures/figures_rectangles.pdf}\n\t\\caption{Visualization of packing of $\\L$-, $\\S$-,$\\H$- and $\\V$-box, respectively.}\n\t\\label{fig:pack_VH}\n\\end{figure}\nWe prove that there always exists a $(2+\\epsilon)$-approximate packing using these types of boxes.\n\\begin{lemma}[Informal]\n\tThere exists an easily guessable packing for packing rectangles into a two-dimensional knapsack with a profit of at least $(1/2-\\epsilon)\\OPT$.\n\\end{lemma}\nIn these packings, we can guess the height of each box (of each type) in $O_\\epsilon(1)$ time and the width of each $\\S$- and $\\V$-box in $O(\\mathrm{poly}(\\log n))$ time (and additionally some other basic quantities). Then, we apply the indirect guessing framework in order to determine the widths of the $\\L$- and $\\H$-boxes.\n\n\n\n\n\\begin{restatable}{theorem}{thmrecnr}\n\t\\label{thm:rec_nr}\n\tThere is a $(2+\\epsilon)$-approximation algorithm for the geometric\n\tknapsack problem for rectangles with a running time of $n\\cdot(\\log n)^{4}+(\\log n)^{O_{\\epsilon}(1)}$.\n\tAlso, there is a dynamic $(2+\\epsilon)$-approximation algorithm for\n\tthe problem which supports the following operations:\n\t\\begin{itemize}\n\t\t\\item insert or delete an item in time $O(\\log^4 n)$,\n\t\t\\item output a $(2+\\epsilon)$-estimate of the\n\t\tvalue of the optimal solution, or query whether an item is contained\n\t\tin $\\ALG$, in time $(\\log n)^{O_{\\epsilon}(1)}$, and\n\t\t\\item output a $(2+\\epsilon)$-approximate solution $\\ALG$ in time $O(|\\ALG|\\cdot(\\log n))+(\\log n)^{O_{\\epsilon}(1)}$.\n\t\\end{itemize}\n\\end{restatable}\n\nA natural open question is to improve our approximation ratio to $2-\\delta$ for some constant~$\\delta>0$. This seems difficult since there is provably no corresponding structured packing with only $O_\\epsilon(1)$ boxes \\cite{galvez2021approximating}. The known polynomial time $(17/9+\\epsilon)$-approximation uses an L-shaped container which is packed by a DP with a running time of \n$n^{\\Omega_\\epsilon(1)}$~\\cite{galvez2021approximating}. It is not clear how to improve this to near-linear running time.\n\nHowever, \nif we are allowed to rotate the rectangles by 90 degrees, then it is possible to construct an\n\neasily guessable packing with $O_\\epsilon(1)$ boxes and an approximation ratio of only $17/9+\\epsilon$. We use here that we have more freedom to modify the optimal packing by rotating some of its items.\n\n\\begin{lemma}[Informal]\n\tIf we are allowed to rotate the input rectangles, there exists an easily guessable packing into a two-dimensional knapsack with a profit of at least $(9/17-\\epsilon)\\OPT$.\n\\end{lemma}\n\nOn the other hand, it becomes harder to compute a solution that corresonds to our easily guessable packing. The reason is that a horizontal or vertical item can now be assigned to an $\\H$- \\emph{or} to a $\\V$-box. This is particularly problematic since these two types of boxes are not treated symmetrically. Like before, we can guess the height of each box in time $O_\\epsilon(1)$. However, for $\\V$-boxes this yields a different kind of restriction than for $\\H$-boxes.\n\n\n\n\\begin{restatable}{theorem}{theoremrecrot}\n\t\\label{thm:rectangles_rot}\n\tThere is a $(17/9+\\epsilon)$-approximation algorithm for the geometric\n\tknapsack problem for rectangles with rotations with a running time\n\tof $n\\cdot(\\log n)^{4}+(\\log n)^{O_{\\epsilon}(1)}$. Also, there\n\tis a dynamic $(17/9+\\epsilon)$-approximation algorithm for the problem\n\twhich supports the following operations:\n\t\\begin{itemize}\n\t\t\\item insert or delete an item in time $O(\\log^4 n)$,\n\t\t\\item output a $(17/9+\\epsilon)$-estimate of the value of the optimal solution, or query whether an item is contained\n\t\tin $\\ALG$, in time $(\\log n)^{O_{\\epsilon}(1)}$,\n\t\t\\item output a $(17/9+\\epsilon)$-approximate solution $\\ALG$ in time $|\\ALG|\\cdot(\\log n)^{3}+(\\log n)^{O_{\\epsilon}(1)}$.\n\t\\end{itemize}\n\\end{restatable}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\appendix\n\n\n\\section{Details of Section~\\ref{sec:hypercubes}}\\label{app:cubes}\nIn the following we present the technical details underyling our algorithms presented in Section~\\ref{sec:hypercubes}.\n\\subsection{Preliminaries}\nBefore going into more details, we need to introduce some important notation.\nAs defined earlier, for each item $i \\in \\I$, the side length rounded to the next larger power of $1+\\epsilon$ is denoted by $\\lceil s_i \\rceil_{1+\\epsilon} := (1+\\epsilon)^{\\lceil \\log_{1+\\epsilon}(s_i)\\rceil}$.\n\nWe denote by $s_{\\max}(\\I) := \\max_{i \\in \\I}s_i$, $s_{\\min}(\\I) := \\min_{i \\in \\I}s_i$ the largest and smallest side lengths of the items in $\\I$, respectively. Furthermore, the total volume of the items in $\\I$ is denoted by $\\mathrm{VOL}_d(\\I) := \\sum_{i \\in \\I}s_i^d$ and the total volume of the rounded items by  $\\lceil \\mathrm{VOL}_d(\\I)\\rceil_{1+\\epsilon} := \\sum_{i \\in \\I}\\lceil s_i\\rceil^d_{1+\\epsilon}$. For a $d$-dimensional hypercuboid $B$, we denote its side lengths by $\\ell_{d'}(B)$ for $d'=1,\\dots,d$ and the smallest side length is $\\ell_{\\min}(B) := \\min_{d'=1,\\dots,d}\\ell_{d'}(B)$. The volume and surface of $B$ are defined as  $\\mathrm{VOL}_d(B) := \\prod_{d'=1}^d \\ell_{d'}(B)$ and $\\mathrm{SURF}_d(B) := 2\\sum_{d'=1}^d\\mathrm{VOL}_d(B)/\\ell_{d'}(B)$. We refer to a $d$-dimensional hypercuboid as a \\emph{box}.\n\nFurthermore, we make use of the Next-Fit-Decreasing-Height (NFDH) algorithm when we show the existence of a\n\nstructured near-optimal solution, as well as in our algorithm itself. Following {the} existing literature (see e.g.~\\cite{bansal2006bin, galvez2021approximating,harren2009approximation,jansen2022ptas}), we describe NFDH in $d$-dimensions in an inductive manner. Suppose that we defined already how to\n\npack items in $d-1$ dimensions using NFDH. Let $B$ be a $d$-dimensional hypercuboid and $\\I$ be a set of items sorted in non-increasing order of heights (in the case of hypercubes this is equivalent to the side length). Consider the largest item in $\\I$, with side length $s_{\\max}(\\I)$; this item defines the length of the first shelf of our packing in dimension~$d$, i.e., the first shelf is a sub-box of $B$ with side lengths $\\ell_1(B),\\dots,\\ell_{d-1}(B), s_{\\max}(\\I)$. We then apply the $d-1$-dimensional NFDH algorithm using the set $\\I$ and the first shelf. Afterwards, let $P$ be the set of items packed into the first shelf, then we run $d$-dimensional NFDH with $B'$ being a box of side lengths $\\ell_1(B),\\dots,\\ell_{d-1}(B),\\ell_d(B)-s_{\\max}(\\I)$ and $\\I' = \\I \\setminus P$. We repeat this procedure until no more items can be packed.\nFinally, consider the (base) case that we run NFDH for $d=1$.\n\nHere, we add items until the next item does not fit. This item will then define the length of the new shelf in the second dimension. In this paper, we assume w.l.o.g. that before packing items into a $d$-dimensional hypercuboid we sort the dimensions such that $\\ell_d(B) \\leq \\dots \\leq \\ell_1(B)$. The running time of NFDH in $d$-dimensions is in $O(n \\log n)$~\\cite{bansal2006bin}. See Figure~\\ref{fig:origpack_S} for a visualization of a packing constructed by NFDH for $d=2$.\n\n\nSince the structured packing in~\\cite{jansen2022ptas} is the starting point of our new structured packing, we include the result for completeness. The structured packing used in~\\cite{jansen2022ptas} is based on $\\V$- and $\\N$-boxes as defined in Definition~\\ref{def:jansen_boxes}. Based on these two types of boxes, the following structural result is given in~\\cite{jansen2022ptas}.\n\\begin{theorem}[Theorem 7 of Jansen et al.~\\cite{jansen2022ptas}]\\label{thm:dD_cubes_jansen}\n\tFor any instance $\\I$ of the $d$-dimensional hypercube knapsack problem and any $\\epsilon < 1/2^{d+2}$, there exists a packing with the following properties:\n\t\\begin{enumerate}\n\t\t\\item It consists of $\\mathcal{N}$- and $\\mathcal{V}$-boxes whose total number is bounded by a constant $C_{boxes}(d,\\epsilon)$, which depends only on $\\epsilon$ and $d$. \n\t\t\\item The number of items in the packing that are not packed in these boxes is bounded by a constant $C_{large}(d,\\epsilon)$, which depends only on $\\epsilon$ and $d$.\n\t\t\\item The total profit of the packing is at least $(1-2^{d+2}\\epsilon)OPT(\\I)$, where $OPT(\\I)$ is the profit of an optimal packing for instance $\\I$.\n\t\\end{enumerate}\n\\end{theorem}\n\n\n\\subsection{Structured packing}\n\nWe now derive the structural packing of Lemma~\\ref{lem:struc_hypercubes}. Our structured packing uses $\\N$- and $\\S$-boxes as defined in Section~\\ref{sec:hypercubes}. See Figure~\\ref{fig:origpack} for a visualization of these boxes and packings.\n\\begin{figure}[h!]\n\t\\begin{minipage}{.4\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=.4\\linewidth, page = 1]{figures/Squares2D.pdf}\n\t\t\\subcaption{Sorted packing of $\\N$-box}\n\t\t\\label{fig:origpack_N}\n\t\\end{minipage}\n\t\\begin{minipage}{.4\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=.4\\linewidth,page = 2]{figures/Squares2D.pdf}\n \t\t\\subcaption{\\textit{NFDH} packing of $\\S$-box} \n\t\t\\label{fig:origpack_S}\n\t\\end{minipage}\n\t\\caption{Visualization of $\\N$- and $\\S$-box for $d=2$}\n\t\\label{fig:origpack}\n\\end{figure}\n\n\nWe first prove a few auxilliary results. We start by showing that if all items packed into a box $B$ are small compared the box itself, then using an argumentation via a linear program (LP) we can select a subset of the items packed into $B$, such that for this selected subset of items the box $B$ is an $\\S$-box.\n\n\n\n\n\\begin{lemma}\\label{lem:vstarbox_selection}\n\tLet $B$ be a $d$-dimensional hypercuboid and $\\I$ be the set of items packed into $B$ such that $s_i \\leq \\epsilon \\ell_{\\min}(B)$ for all items $i\\in \\I$. Then, there exists a subset of items $\\I'\\subseteq \\I$ such that $p(\\I') \\geq (1-O(\\epsilon))p(\\I)$ and $\\sum_{i \\in \\I'} \\lceil s_i \\rceil^d_{1+\\epsilon} \\leq \\mathrm{VOL}_d(B)-2\\cdot d \\cdot \\epsilon \\mathrm{VOL}_d(B)$ with $\\epsilon \\in (0,1/2^{d+2})$.\n\\end{lemma}\n\\begin{proof}\n\tTo prove this statement, we consider the one dimensional knapsack problem with capacity $\\mathrm{VOL}_d(B)-(2d+1)\\cdot\\epsilon \\mathrm{VOL}_d(B)$ and item set $\\I$ where we define the item size as $\\lceil s_i \\rceil_{1+\\epsilon}^d$. For this problem, we consider the following LP relaxation:\n\n\t\\begin{alignat*}{3}\n\t\t& \\text{minimize} & \\sum_{j=1}^{m} &x_ip_i& \\\\\n\t\t& \\text{subject to} \\quad& \\sum_{i \\in \\I}&x_{i}\\lceil s_i \\rceil_{1+\\epsilon}^d& \\leq \\mathrm{VOL}_d(B)-(2d+1)\\cdot\\epsilon \\mathrm{VOL}_d(B) \\\\\n\t\t&&& x_{i} \\geq 0, &  \\forall i \\in \\I\n\t\t\\\\\n\t\t&&& x_{i} \\leq 1, &  \\forall i \\in \\I\n\t\\end{alignat*}\n\n\tConsider the solution $\\hat{x}_i := (1-(2d+1)\\cdot\\epsilon)(1+\\epsilon)^{-d}$ for all $i\\in \\I(B)$. Clearly, $\\hat{x_i} \\leq 1$ and $\\hat{x_i} \\geq 0$ for each $i \\in \\I$. Furthermore, we have\n\t\\begin{align*}\n\t\t\\sum_{i \\in \\I}\\hat{x}_{i}\\lceil s_i \\rceil_{1+\\epsilon}^d & = (1-(2d+1)\\cdot\\epsilon)(1+\\epsilon)^{-d}\\sum_{i \\in \\I}\\lceil s_i \\rceil_{1+\\epsilon}^d \\\\\n\t\t& \\leq (1-(2d+1)\\cdot\\epsilon)(1+\\epsilon)^{-d}\\sum_{i \\in \\I} (1+\\epsilon)^ds_i^d \\\\\n\t\t& = (1-(2d+1)\\cdot\\epsilon)\\sum_{i \\in \\I} s_i^d \\\\\n\t\t& \\leq  (1-(2d+1)\\cdot\\epsilon)\\mathrm{VOL}_d(B).\n\t\\end{align*}\n\tHere, the last inequality follows from the fact that the total volume of items packed into $B$ is at most the volume of $B$ itself. Thus, $\\hat{x}$ is feasible and yields a profit of at least $(1-(2d+1)\\cdot\\epsilon)(1+\\epsilon)^{-d}p(B) \\geq (1-O(\\epsilon))p(B)$. Let $x^*$ be an optimal extreme point solution. Then, $x^*$ yields a profit of at least $(1-O(\\epsilon))p(B)$. Furthermore, due to the rank lemma (see e.g.~\\cite{lau2011iterative}), we know that there is at most one fractional variable in the support of $x^*$. We now obtain $\\I'$ by taking all items\n\t$i \\in \\I$ for which $x^*_i = 1$ and, additionally, the unique item $i' \\in \\I(B)$ for which $0< x^*_{i'} < 1$ if such an item exists. Using the argumentation above, we know that $p(\\I') \\geq (1-O_\\epsilon(1))p(B)$. Furthermore, as item $i'$ has side length at most $\\epsilon \\ell_{\\min}(B)$, if we round up its side length to the next larger power of $1+\\epsilon$, its volume is still bounded by\n\t\n\t\\[(1+\\epsilon)^d\\epsilon^d\\mathrm{VOL}_d(B).\\]\n\tSince $\\epsilon < 1/2^{d+2}$, we have that \n\t\\[(1+\\epsilon)^d\\epsilon^d < \\epsilon.\\]\n\tTherefore, the total volume of the rounded items in $\\I'$ is at most \\[\\mathrm{VOL}_d(B)-2d \\cdot \\epsilon \\mathrm{VOL}_d(B). \\]\n\tThis concludes the proof. \n\\end{proof}\n\nThe next two lemmas show that\nif a box $B$ is packed using NFDH, then we can split it\n\ninto $O_{\\epsilon,d}(1)$ many $\\N$-boxes if the side lengths of the items originally packed into $B$ fall within a certain range.\n\n\\begin{lemma}\\label{lem:aux_rangebox_single}\n\tLet $B$ be a $d$-dimensional hypercuboid and $\\hat{\\I}$ be a set of items such that\n\t\\begin{itemize}\n\t\t\\item all items in $\\hat{\\I}$ can be packed into $B$ using $d$-dimensional NFDH and\n\t\t\\item for all $i \\in \\hat{\\I}$ it holds that $s_i  \\in ((1+\\epsilon)^\\alpha,(1+\\epsilon)^{\\alpha+1}]$ for a fixed $\\epsilon >0$ and $\\alpha > 0$.\n\t\\end{itemize}\n\tThen, $B$ can be transformed into at most $1/\\epsilon^d$ many $\\N$-boxes such that a subset of items $\\hat{\\I}' \\subseteq \\hat{\\I}$ can be packed into these $\\N$-boxes with $p(\\hat{\\I}') \\geq (1-2\\epsilon)^d p (\\hat{\\I})$.\n\\end{lemma}\n\\begin{proof}\n\tWe prove this statement in an inductive manner. First, let $d=1$ and consider the following cases:\n\t\\begin{itemize}\n\t\t\\item \\textbf{Case 1:} Suppose $\\ell_1(B)/(1+\\epsilon)^\\alpha \\geq 1/\\epsilon$. The number of items packed into $B$ is at most \n\t\t\\[\\frac{\\ell_1(B)}{(1+\\epsilon)^\\alpha}.\\]\n\t\tWe transform $B$ into a $\\N$-box $B_1$ with $s_{\\max}(B)= (1+\\epsilon)^{\\alpha + 1}$ and $n_1= \\left\\lfloor \\frac{\\ell_1(B)}{s_{\\max}(\\hat{\\I})} \\right\\rfloor$. The number of items we can pack into $B_1$ is\n\t\t\\[n_1 \\geq \\frac{\\ell_1(B)}{(1+\\epsilon)^{\\alpha+1}} -1 \\geq \\frac{\\ell_1(B)}{(1+\\epsilon)^{\\alpha}}\\left(1-\\frac{\\epsilon}{1+\\epsilon}-\\epsilon\\right) \\geq (1-2\\epsilon)\\frac{\\ell_1(B)}{(1+\\epsilon)^{\\alpha}}.\\]\n\t\tThus, we can pack all but a $2\\epsilon$-fraction of the items in $\\hat{\\I}$ into $B_1$ which means that by packing the most profitable items into $B_1$ we lose a profit of at most $2\\epsilon p(\\hat{\\I})$.\n\t\t\\item \\textbf{Case 2:} Suppose $\\ell_1(B)/(1+\\epsilon)^\\alpha < 1/\\epsilon$. Then, the number of items packed into $B$ is $|\\hat{\\I}| < 1/\\epsilon$. We transform $B$ into $|\\hat{\\I}|$ many $\\N$-boxes where each item is packed into its own $\\N$-box. Since, we do not lose any items this way we also do not lose any profit.\n\t\\end{itemize}\n\tNow, assume the statement is true for $d-1$ and let $B$ be a $d$-dimensional hypercuboid. Again, we may distinguish two cases.\n\t\\begin{itemize}\n\t\t\\item \\textbf{Case 1:} Suppose $\\ell_d(B)/(1+\\epsilon)^\\alpha \\geq 1/\\epsilon$. As we use the sorted variant of NFDH this implies that $\\ell_{d'}(B)/(1+\\epsilon)^\\alpha \\geq 1/\\epsilon$ for all $d'=1,\\dots,d$. We now transform $B$ into a single $\\N$-box $B'$ with $s_{\\max}(B') = s_{\\max}(\\hat{\\I})$ and $n_{d'}= \\lfloor \\ell_{d'}(B)/s_{\\max}(B') \\rfloor$. Observe that for $d'=1,\\dots,d$ we have\n\t\t\\[n_{d'} \\geq \\frac{\\ell_{d'}(B)}{(1+\\epsilon)^{\\alpha+1}} -1 \\geq \\frac{\\ell_{d'}(B)}{(1+\\epsilon)^{\\alpha}}\\left(1-\\frac{\\epsilon}{1+\\epsilon}-\\epsilon\\right) \\geq (1-2\\epsilon)\\frac{\\ell_{d'}(B)}{(1+\\epsilon)^{\\alpha}}.\\]\n\t\tTherefore, the number of items we can pack into $B'$ is at least \n\t\t\\[\\prod_{d'=1}^d n_{d'} \\geq (1-2\\epsilon)^d\\prod_{d'=1}^d \\frac{\\ell_{d'}(B)}{(1+\\epsilon)^{\\alpha}}.\\]\n\t\tThe number of items packed into $B$ is at most\n\t\t\\[\\prod_{d'=1}^d \\frac{\\ell_{d'}(B)}{(1+\\epsilon)^{\\alpha}}.\\]\n\t\tTherefore, we keep a $(1-2\\epsilon)^d$-fraction of the items and by packing the items into $B'$ in non-increasing order of profits, we have $p(\\hat{\\I}') \\geq (1-2\\epsilon)^dp(\\hat{\\I})$.\n\t\t\\item \\textbf{Case 2:} Suppose $\\ell_d(B)/(1+\\epsilon)^\\alpha < 1/\\epsilon$. Then, we know that NFDH uses less than $1/\\epsilon$ shelves in the $d$-th dimension of the packing of $B$. We may consider each of these shelves as a $d-1$ dimensional box, which, by induction can be transformed into at most $1/\\epsilon^{d-1}$ many $d-1$-dimensional $\\N$-boxes. By setting $n_d = 1$ for each of these, we create at most $1/\\epsilon^d$ many $d$-dimensional $\\N$-boxes. The guarantee of the remaining profit follows from the fact that the guarantee holds for each of the $\\N$-boxes individually.\n\t\\end{itemize}\n\tThis concludes the proof.\n\\end{proof}\nWe now prove a more general statement. Consider a box that is packed using NFDH such that the side length of the \\emph{smallest} item packed into it is at least an $\\epsilon$-fraction of the side length of the \\emph{largest} item packed into it. We show that it can be transformed into $O_{\\epsilon,d}(1)$ many $\\N$-boxes.\n\n\n\\begin{lemma}\\label{lem:aux_rangebox_multi}\n\tLet $B$ be a $d$-dimensional hypercuboid and $\\hat{\\I}$ a set of items such that $\\hat{\\I}$ can be packed into $B$ using NFDH and $s_{\\min}(\\hat{\\I}) \\geq \\epsilon s_{\\max}(\\hat{\\I})$. Then, $B$ can be transformed into at most $\\left(1/\\epsilon \\right)^{d+2}$ many $\\N$-boxes such that a subset $\\hat{\\I}' \\subseteq \\hat{\\I}$ of items can be packed into them with $p(\\hat{\\I}') \\geq (1-O(\\epsilon))p(\\hat{\\I})$.\n\\end{lemma}\n\\begin{proof}\n\tAgain we prove this statement by induction. First, consider the one-dimensional setting ($d=1$). We group the item sizes into intervals $((1+\\epsilon)^\\alpha,(1+\\epsilon)^{\\alpha+1}]$ with $\\alpha = \\alpha_1,\\dots, \\alpha_2$. Observe that $\\alpha_1 =\\lfloor \\log_{1+\\epsilon}(s_{\\min}(\\I))\\rfloor$ and $\\alpha_2 = \\lceil \\log_{1+\\epsilon}(s_{\\min}(\\I))\\rceil$ and, therefore, there are at most $O(1/\\epsilon)$ such intervals. Next, we split $B$ into sub-boxes such that each sub-box contains only items from one of these intervals. For each of these sub-boxes we can apply Lemma~\\ref{lem:aux_rangebox_single}. This gives a total of at most $O(1/\\epsilon^2)$ many $\\N$-boxes. Since in the transformation underlying Lemma~\\ref{lem:aux_rangebox_single} we lose at most an $\\epsilon$-fraction of the profit in each sub-box of $B$, in total we lose at most an $\\epsilon$-fraction of the profit packed into $B$.\n\t\n\tSuppose now that the statement is true for $d-1$-dimensional hypercuboids and consider a $d$-dimensional hypercuboid $B$. We proceed in a similar fashion as above. Again, we group item sizes into intervals $((1+\\epsilon)^\\alpha,(1+\\epsilon)^{\\alpha+1}]$ with $\\alpha = \\alpha_1,\\dots, \\alpha_2$. We split $B$ into several sub-boxes as follows. For each $\\alpha \\in \\{\\alpha_1 ,...,\\alpha_2 \\}$, we consider all shelves of $B$ in which every item size is in the interval $((1+\\epsilon)^\\alpha,(1+\\epsilon)^{\\alpha+1}]$ and call the sub-box formed by these shelves $B(\\alpha)$. Observe that there might be shelves of $B$ which contain items from multiple intervals. Each such shelf forms its own sub-box, we denote by $C(\\alpha)$ the sub-box of this type in which the smallest item contained in it has a side length in $((1+\\epsilon)^\\alpha,(1+\\epsilon)^{\\alpha+1}]$. The number of sub-boxes we find in this way is at most $O(1/\\epsilon)$. We now argue how to treat each of these sub-boxes.\n\t\n\tFirst, consider a sub-box $B(\\alpha)$ and let $\\I(\\alpha)$ be the set of items packed into $B(\\alpha)$. By Lemma~\\ref{lem:aux_rangebox_single} we know that can be split into at most $O(1/\\epsilon^d)$ many $\\N$-boxes containing a subset $\\I'(\\alpha) \\subseteq \\I(\\alpha)$ of items with a total profit of at least $(1-2\\epsilon)^dp(\\I(\\alpha))$. \n\t\n\tNext, consider a sub-box $C(\\alpha)$. Since $C(\\alpha)$ contains only one shelf in dimension $d$, we can use an inductive argument here by applying the fact that $C(\\alpha)$ in $d-1$ dimensions can be split into $O(1/\\epsilon^{d+1})$ many $\\N$-boxes such that a subset $\\I'(C(\\alpha)) \\subseteq \\I(C(\\alpha))$ of items can be packed into them with $p(\\I'(C(\\alpha))) \\geq (1-O(\\epsilon))p(\\I(C(\\alpha)))$. These boxes can be transformed into $\\N$-boxes in $d$-dimensions by setting $n_d=1$ for each of them.\n\t\n\tFinally, since the profit guarantee holds for each sub-box separately, we know that the total profit packed into these sub-boxes is at least $(1-O(\\epsilon))p(\\I)$ and the total number of $\\N$-boxes is at most $O(1/\\epsilon^{d+2})$.\n\\end{proof}\n\nNext, we show that if $d=1$, any box which is packed using NFDH can be transformed into at most $O_\\epsilon(1)$ many $\\N$- and $V$- boxes.\n\\begin{lemma}\\label{lem:1DCubes_transform}\n\tLet $B_0$ be a $1$-dimensional box and let $\\hat{\\I}$ be the set of items packed into $B_0$ using NFDH and $\\epsilon < 1/2^{3}$ be given. Then, $B_0$ can be split into $O(1/\\epsilon)$ many $\\S$- and $\\N$-boxes such that a subset $\\hat{\\I}' \\subseteq \\hat{\\I}$ can be packed into them with $p(\\hat{\\I}') \\geq (1-O(\\epsilon))p(\\hat{\\I})$.\n\\end{lemma}\n\\begin{proof}\n\tLet $\\ell_1(B_0)$ be the side length of $B_0$. \n\tLet $G_1$ be the group of items of size strictly larger than $\\epsilon \\ell_1(B_0)$. If $p(G_1) \\leq \\epsilon p(\\hat{\\I})$, let $\\hat{\\I}' := \\hat{\\I} \\setminus G_1$. Otherwise, let $B_1$ be the sub-box of $B_0$ containing all items of size at most $\\epsilon \\ell_1(B_0)$. We now repeat the procedure with $G_2$ being all items packed into $B_1$ of size strictly larger than $\\epsilon \\ell_1(B_1)$. Again, if $p(G_2) \\leq \\epsilon p(\\hat{\\I})$, we are done. Otherwise, we continue. This gives us boxes $B_0,\\dots,B_g$ and $G_1,\\dots G_{g},G_{g+1}$ such that:\n\t\\begin{itemize}\n\t\t\\item All items packed into $B_g$ have side lengths at most $\\epsilon \\ell_{\\min}(B_g)$.\n\t\t\\item For each $G_\\gamma$ with $\\gamma = 1,\\dots,g$, we have that the profit of items packed into $G_\\gamma$ is strictly more than $\\epsilon p(B_0)$ and the side lengths of items packed into $G_\\gamma$ are at most $\\ell_{\\min}(B_{\\gamma-1})$ and at least $\\epsilon \\ell_{\\min}(B_{\\gamma-1})$.\n\t\t\\item For $G_{g+1}$ we have that $p(G_{g+1})\\leq \\epsilon p(\\hat{\\I})$.\n\t\\end{itemize}\n\tThe second property implies that $g\\leq 1/\\epsilon$. We discard all items contained in $G_{g+1}$.To transform $B_g$ into a $\\S$-box we apply Lemma~\\ref{lem:vstarbox_selection}. For each $\\gamma = 1,\\dots,g$, we apply Lemma~\\ref{lem:aux_rangebox_multi} to transform $G_\\gamma$ into $1/\\epsilon$ many $\\N$-boxes.\n\t\n\\end{proof}\n\nWe now use Lemmas~\\ref{lem:vstarbox_selection}~-~\\ref{lem:1DCubes_transform}, to show that for any dimension $d$, a $d$-dimensional box which is packed using NFDH can be split into at most $O_{\\epsilon,d}(1)$ many $\\N$- and $\\S$-boxes.\n\\begin{lemma}\\label{lem:Cubes_transform}\nLet $B_0$ be a $d$-dimensional box and let $\\hat{\\I}$ be the set of items packed into $B_0$ using NFDH and $\\epsilon < 1/2^{d+2}$ be given. Then, $B_0$ can be split into $O(1/\\epsilon^{d+2})$ many $\\S$- and $\\N$-boxes such that a subset $\\hat{\\I}' \\subseteq \\hat{\\I}$ can be packed into them with $p(\\hat{\\I} ') \\geq (1-O(\\epsilon))p(\\hat{\\I})$.\n\\end{lemma}\n\\begin{proof}\nWe prove this statement by induction. The base case of $d=1$ is given by Lemma~\\ref{lem:1DCubes_transform}. Now, suppose that the statement is true for $d-1$ dimensions. To prove it for $d$ dimensions, we start by splitting $B_0$ into a constant number of smaller boxes such that these can later be transformed into $\\S$-boxes and $\\N$-boxes. This splitting procedure is visualized in Figure~\\ref{fig:sq2D_splitgamma} (for $d=2$) and works as follows. Let $L_1,L_2,\\dots,L_m$ be the shelves of the $d$-dimensional NFDH packing of $B_0$ and denote by $h(L_j)$ the height of shelf $L_j$ defined by the side length of the largest item packed into shelf $L_j$. By definition of NFDH, the shelves are sorted in non-increasing order of heights. Let $B_1$ be the collection of shelves of $B_0$ of height at most $\\epsilon \\ell_{\\min}(B_0)$ and $G_1$ be the shelves of height strictly larger than $\\epsilon \\ell_{\\min}(B_0)$. If the profit of items packed into $G_1$ is at most $\\epsilon p(B_0)$, we delete all items packed into $G_1$ such that all remaining items packed into $B_0$ have side lengths at most $\\epsilon \\ell_{\\min}(B_0)$ and proceed by transforming $B_0$ into a $\\S$-box (see below). If, however, the profit of items packed into $G_1$ is more than $\\epsilon p(B_0)$, we split $B_0$ into $B_1$ and $G_1$ and repeat the procedure with $B_1$. We continue the procedure (including the final deletion step) until we have a collection of boxes $B_0,B_1,\\dots,B_g, G_1,\\dots,G_g, G_{g+1}$ with the following properties:\n\\begin{itemize}\n\t\\item All items packed into $B_g$ have side lengths at most $\\epsilon \\ell_{\\min}(B_g)$.\n\t\\item For each $G_\\gamma$ with $\\gamma = 1,\\dots,g$, we have that the profit of items packed into $G_\\gamma$ is strictly more than $\\epsilon p(B_0)$, the height of every shelf of $G_\\gamma$ is at least $\\epsilon \\ell_{\\min}(B_{\\gamma-1})$ and the side lengths of items packed into $G_\\gamma$ are at most $\\ell_{\\min}(B_{\\gamma-1})$ and at least $\\epsilon \\ell_{\\min}(B_{\\gamma-1})$ except for some items packed into the top shelf.\n\t\\item For $G_{g+1}$ we have that $p(G_{g+1})\\leq \\epsilon p(\\hat{\\I})$.\n\\end{itemize}\nIn the following, we use the boxes $B_g, G_1,\\dots,G_g$ to construct $\\N$- and $\\S$-boxes, while boxes $B_1,\\dots,B_{g-1}$ are crucial for the analysis and all items in $G_{g+1}$ are discarded. The second property above implies that $g \\leq 1/\\epsilon$ and since we delete only items in the final iteration of this procedure we lose a total profit of at most $\\epsilon p(B_0)$.\n\n\t\nTherefore, it remains to show how to transform $B_g$ into a $\\S$-box and how to transform each $G_\\gamma$ into a constant number of $\\N$- and $\\S$-boxes. First, consider $B_g$ and let $\\I(B_g)$ be the remaining items packed into $B_g$ after deleting all items in $G_{g+1}$.\nTo finalize the transformation of $B_g$ into a $\\S$-box, we use Lemma~\\ref{lem:vstarbox_selection} to find a subset of items $\\I'(B_g) \\subseteq \\I(B_g)$ such that $\\sum_{i \\in \\I'(B_g)} \\lceil s_i \\rceil^d_{1+\\epsilon} \\leq \\mathrm{VOL}_d(B_g)-2\\cdot d \\cdot \\epsilon \\mathrm{VOL}_d(B_g)$ and the lost profit is at most $O(\\epsilon)p(B_g)$.\n\n\t\n\t\n\\begin{figure}[h!]\n\t\\centering\n\t\\includegraphics[scale = 0.425,page = 3]{figures/Squares2D.pdf}\n\t\\caption{Iteration $\\gamma$ of the splitting procedure}\n\t\\label{fig:sq2D_splitgamma}\n\\end{figure}\n\n\nNext, consider some box $G_\\gamma$ with $\\gamma =1,\\dots,g$. We transform $G_\\gamma$ into a constant number of $\\S$- and $\\N$-boxes. To do so, we first split $G_\\gamma$ into at most $d$ sub-boxes as follows: The sub-box $G_\\gamma(1)$ contains all shelves in dimension $d$ of $G_\\gamma$ such that for every item packed into these shelves has side length at least $\\epsilon \\ell_{\\min}(B_{\\gamma-1})$ and at most $\\ell_{\\min}(B_{\\gamma-1})$. If\n$G_\\gamma(1)$ contains all shelves in dimension $d$ of $G_\\gamma$ we are done. Otherwise the last shelf of $G_\\gamma$ is not contained in $G_\\gamma(1)$. So we define $G_\\gamma(2)$  as the sub-box containing all shelves in dimension $d-1$ of the last shelf in dimension $d$ of $G_\\gamma$ such that every item packed into these shelves has side length at least $\\epsilon^2 \\ell_{\\min}(B_{\\gamma-1})$ and at most $\\ell_{\\min}(B_{\\gamma-1})$.\nIf all other shelves in dimension $d-1$ of the last shelf of $G_\\gamma$ in dimension $d$ contain items of size at most $\\epsilon^2\\ell_{\\min}(B_{\\gamma-1})$ we define an additional sub-box containing these items and are done. Otherwise, we proceed in a similar fashion in dimension $d-2$. Repeating this procedure leads to at most $r\\leq d+1$ sub-boxes which we now transform into $\\N$- and $\\S$-boxes as follows.\n\nWe first consider box $G_\\gamma(1)$ which contains items with side lengths of at least $\\epsilon \\ell_{\\min}(B_{\\gamma-1})$ and at most $\\ell_{\\min}(B_{\\gamma-1})$. We split $G_\\gamma(1)$ into at most $O_{\\epsilon,1}(1)$ many $\\N$-boxes losing a total profit of at most\n$O(\\epsilon)p(G_\\gamma(1)).$\nUsing Lemma~\\ref{lem:aux_rangebox_multi} we do the same for all sub-boxes  $G_\\gamma(2),\\dots,G_\\gamma(r-1)$ we created for which the items have side lengths at least $\\epsilon^2  \\ell_{min}(B_{\\gamma-1})$. For each $r' =2,\\dots,r-1$, we lose a total profit of at most \n$O(\\epsilon)p(G_\\gamma(r')).$\n\nFinally, consider $G_\\gamma(r)$ which contains items of side lengths less than $\\epsilon^2 \\ell_{min}(B_{\\gamma-1})$. Observe that since $G_\\gamma(r)$ is a sub-box of the last shelf (in dimension $d$) packed into $G_\\gamma$, we know that $\\ell_d(G_\\gamma(r)) \\geq \\epsilon \\ell_{\\min}(B_{\\gamma-1})$. We now use an inductive argument to show that $G_\\gamma(r)$ can be split into $O_{\\epsilon,d}$ many $\\N$- and $\\S$-boxes. To this end, we ignore dimension $d$ and apply the inductive step to $G_\\gamma(r)$ in dimensions $1$ to $d-1$. Let $B'$ be a resulting $\\S$-box with the properties that\n\\begin{itemize}\n\t\\item for every item $i \\in \\I(B')$, $s_i \\leq \\epsilon \\ell_{\\min}(B')$ {and}\n\t\\item the total volume of the rounded up items packed into $B'$ is\n\t\\[\\sum_{i \\in \\I} \\lceil s_i \\rceil^{d-1}_{1+\\epsilon} \\leq \\mathrm{VOL}_{d-1}(B')-2\\cdot (d-1)\\cdot \\epsilon \\mathrm{VOL}_{d-1}(B').\\]\n\\end{itemize}\nWe now consider what happens when we take into account the $d$-th dimension of $B'$. Since $\\ell_d(B') = \\ell_d(G_\\gamma(r)) \\geq \\epsilon \\ell_{\\min}(B_{\\gamma-1})$ and every item packed into $B'$ has side length at most $\\epsilon^2 \\ell_{\\min}(B_{\\gamma-1})$, and $B'$ still satisfies the first property in $d$ dimensions. Furthermore, the total volume of the items packed into $B'$ rounded up in $d$ dimensions is at most\n\\[\\sum_{i \\in \\I} \\lceil s_i \\rceil^{d}_{1+\\epsilon} \\leq (1+\\epsilon)\\epsilon^2 \\ell_{\\min}(B_{\\gamma-1}) \\mathrm{VOL_{d-1}(B')} \\leq (1+\\epsilon)\\epsilon \\mathrm{VOL_{d}(B')} < 2\\epsilon\\mathrm{VOL_{d}(B')}.\\]\nSince $\\epsilon < 1/2^{d+2}$, this implies that \n\\[\\sum_{i \\in \\I} \\lceil s_i \\rceil^{d}_{1+\\epsilon} < \\mathrm{VOL}_d(B')-2\\cdot d \\cdot \\epsilon \\mathrm{VOL}_d(B').\\]\nHence, $B'$ is a $d$-dimensional $\\S$-box. \n\nNext, let $B''$ be a $d-1$-dimensional $\\N$-box resulting from the transformation of $G_\\gamma(r)$ with $s_{\\max}(B'')$ being the side length of the largest item packed into $B''$. If we take $\\ell_d(B'') = s_{\\max}(B'')$ and $n_d(B'')= 1$, then $B''$ is a $\\N$-box in $d$-dimensions. Observe that $s_{\\max}(B'') < \\ell_d(G_\\gamma(r))$ which implies that its feasible to extend $B''$ into the $d$-th dimension this way.\n\nTherefore, by induction we know that each of these sub-boxes can be transformed into $O(1/\\epsilon^{d+1})$ many $\\N$- and $\\S$-boxes. In the end, this whole procedure leads to $O(1/\\epsilon^{d+2})$ many $\\N$- and $\\S$-boxes constructed out of the original box $B_0$ with a profit of at least $(1-\\epsilon)p(B_0)$.\n\\end{proof}\n\nWe will now prove Lemma~\\ref{lem:struc_hypercubes} using Lemma~\\ref{lem:Cubes_transform}.\n\\begin{proof}[Proof of Lemma~\\ref{lem:struc_hypercubes}]\nConsider an instance $\\I$ and let $\\epsilon < 1/2^{d+2}$. We start with the structural packing proven by Jansen et al.~\\cite{jansen2022ptas} (see Theorem~\\ref{thm:dD_cubes_jansen}). Let $n_b = C_{boxes}(d,\\epsilon) + C_{large}(d,\\epsilon)$ be the total number of boxes and large items (which we will treat as ${\\mathcal{N}}$-boxes) used in this packing and denote these boxes by $B_1,B_2,\\dots,B_{n_b}$. By Lemma~\\ref{lem:Cubes_transform} we can transform any ${\\V}$-box $B_h$ into $O_{\\epsilon,d}(1)$ many $\\S$- and $\\N$-boxes\n{and pack items from $B_h$ with a}\nprofit of at least $(1-O(\\epsilon))p(B_h)$. Thus, the total profit is at least $(1-O(\\epsilon))\\sum_{i=1}^{n_b} p(B_i) \\geq (1-O(\\epsilon))(1-2^{d+2}\\epsilon)OPT(\\I) \\geq (1-{2^{O(d)}}\\epsilon)OPT(\\I)$. {Denote by $n'_{b}$ the number of boxes after these transformations.}\n\nTo finish the proof, we still need to show that there exist values $k_1,k_2,\\dots, k_r \\in \\mathbb{Z}_{\\geq 0}$ with $r \\in O_{\\epsilon,d}(1)$ such that if $B$ is a $\\N$-box there exists a $j \\in \\{1,2,\\dots,r\\}$ such that $s_{\\max}(B) \\leq k_{j}$ and $s_{\\min}(B) \\geq k_{j-1}$.\n\nTo this end, we are interested in the distinct values of $s_{\\max}(B_i)$. Let $r \\leq n'_b$ be the number of $\\N$-boxes with distinct values of $s_{\\max}(B_i)$. Clearly, $r \\in O_{\\epsilon,d}(1)$. Let $B_1,\\dots,B_{n_b'}$ be $\\N$-boxes with distinct $s_{\\max}$-values such that $s_{\\max}(B_i) > s_{\\max}(B_{i+1})$ for every $i=1,\\dots,r-1$. Let $k_i = s_{\\max}(B_i)$ for $i=1,\\dots,r$ and $k_0 = 0$.\n\n\nWe can now split any $\\N$-box $B$ into a constant number of $\\N$-boxes such that each of these $\\N$-boxes contains items of size $s_i \\in (k_{j-1},k_j]$ for some $j=1,\\dots, r$. To do so, let $G_j(B)$ be all shelves in dimension $d$ of $B$ containing items of size $s_i \\in (k_{j-1},k_j]$ for each $j=1,\\dots, r$. For each $j = 1,\\dots,r$, $G_j(B)$ is a $\\N$-box. Now, consider shelves in dimension $d$ of $B$ which contain items from more than one of the intervals. For such a shelf, we repeat the procedure above in dimension $d-1$ and set $n_d = 1$ for each of the resulting $\\N$-boxes. Note that since we may have to repeat this procedure in each dimension and these shelves contain at most items in at most $r$ different intervals, the total number of $\\N$-boxes formed from $B$ is at most $r^d$. We repeat this procedure for each original $\\N$-box and end up with $O_{\\epsilon,d}(1)$ many new $\\N$-boxes since $r\\in O_{\\epsilon,d}(1)$. Hence, we have constructed a packing satisfying properties i), ii) and v). It remains to modify the packing such that it also satisfies properties iii) and iv) while maintaining the other properties. First, consider a $\\V$-box $B$. We now round down $\\ell_{d'}(B)$ to $\\lfloor \\ell_{d'}(B) \\rfloor_{1+\\epsilon}$. Observe, that in this way we lose at most a factor of $(1+\\epsilon)^d$ of the volume of $B$.  Applying a similar LP-argument as used in the proof of Lemma~\\ref{lem:vstarbox_selection}, we can find a subset of the items packed into $B$ that still can be packed into $B$ with the new side lengths such that the remaining profit is at least a factor of $(1+\\epsilon)^d$ of the original profit in $B$. Applying this to all $\\V$-boxes leads to a packing satisfying property iii). Next, observe that for an $\\N$-box $B$ for which $n_{d'}(B) > 1/\\epsilon$, rounding  $n_{d'}(B)$ to an integer power of $(1+\\epsilon)$ implies that the number of items that we can pack into $B$ decreases by at most a factor of $(1+\\epsilon)$. Thus, doing this in all dimensions leads to a packing satisfying property iv) while only losing a factor of  $(1+O(\\epsilon))$ of the profit.\n\\end{proof}\n\n\\subsection{Computing a packing}\nIn the following, we {describe} details of Section~\\ref{subsec:Computing-packing} {that were omitted above}. First, we prove that we can guess the side lengths of all $\\S$-boxes in time $(\\log_{1+\\epsilon} n)^{O_{\\epsilon,d}(1)}$. For the remainder of this subsection,  we disregard all items with profit less than $\\epsilon/n \\cdot p_{\\max}$ while losing only an $\\epsilon$-fraction of the optimal profit as $p_{\\max} \\leq \\OPT$. We next show how to strengthen the statement of Lemma~\\ref{lem:hypcub_guessing_basic} by reducing the total number of guesses to $\\left( \\log_{1+\\epsilon}(n)\\right)^{O_{\\epsilon,d}(1)}$ such that all basic quantities can be guessed in time $(\\log n)^{O_{\\epsilon,d}(1)}$.\n\n\\begin{lemma}\nLet $\\B_\\S$ be the set of $\\S$-boxes in the packing due to Lemma~\\ref{lem:struc_hypercubes}. By losing a factor $(1+\\epsilon)^d$ in the profit of this packing, we can guess values $ \\ell_{d'}(B) $ for all boxes $B \\in \\B_{{S}}$ and for all $d'=1,\\dots, d$ in time $(\\log_{1+\\epsilon} n)^{O_{\\epsilon,d}(1)}$,\n\tsuch that we can reduce the side lengths of each box $B \\in \\B_{{S}}$ to $ \\ell_{1}(B) ,...,\n\t \\ell_{d}(B) $.\n\\end{lemma}\n\n\\begin{proof}\n{It is straightforward to guess values the $ \\ell_{1}(B) ,..., \\ell_{d}(B) $ in time\n\t$(\\log_{1+\\epsilon} N)^{O_{\\epsilon,d}(1)}$. To decrease the number of guesses to $(\\log_{1+\\epsilon} n)^{O_{\\epsilon,d}(1)}$}, we will make use of the following useful fact{:} let $B$ be a $\\S$-box, then we know that $\\ell_{\\min}(B) \\geq \\frac{1}{\\epsilon}s_{\\max}(\\I(B))$.\n\n{Now, let $B^*$ be an $\\S$-box in $\\B_{\\S}$ such that $s_{\\max}(\\I(B^*)) = \\max_{B \\in \\B_\\S}\\{s_{\\max}(\\I(B))\\}$. We will now find the side length of the largest item packed into $B^*$ which will give us an estimate on the side lengths of $B^*$. We do this by first guessing the profit class of this item. By our preprocessing step of the profits, this can be done in time $O(\\log_{1+\\epsilon}n)$. Once we have the profit class of this item, we do the following. Losing only a factor of $1+\\epsilon$, we can guess the number of items of this profit class in our structured packing in time $O(\\log_{1+\\epsilon}n)$. To be more precise, we guess how many items of this profit class are packed into each of the boxes. Let $t$ be the guessed profit class and let $n_{t,B}$ denote the guessed number of items of class $t$ packed into box $B$. We can find all values $n_{t,B}$ in time {$(\\log_{1+\\epsilon} n)^{O_{\\epsilon,d}(1)}$}. We define $n_t := \\sum_{B \\in \\B}n_{t,B}$ and using our item data structure and a binary search over all possible side lengths using the balanced binary search tree data structure which stores the side lengths, we find the $n_t$-th smallest item of profit class $t$ in time $O(\\log^3 n)$. For this item, we guess the box in which it must be packed in time $O_{\\epsilon,d}(1)$. We know that it can only be packed into an $\\N$-box or $B^*$ since using that $\\ell_{min}(B) \\geq \\frac{1}{\\epsilon}s_{\\max}(B)$ we have a contradiction for the definition of $B^*$. Hence, if the guess is $B^*$ {we are done since we obtain a value} $\\hat{s}$ giving the size of the largest item packed into $B^*$. If the guessed box $B$, however, is an $\\N$-box, then w.l.o.g. we may assume that this item and {also} the $n_{t,B}-1$ smaller items {of the same profit class} are all packed into $B$ and we update the value of $n_t$ to $n_t-n_{t,B}$. Repeating this procedure leads to a total amount of $O_{\\epsilon,d}(1)$ iterations since there are at most $O_{\\epsilon,d}(1)$ many $\\N$-boxes. Thus, we can find the value $s_{\\max}(B^*)$ in time {$(\\log_{1+\\epsilon} n)^{O_{\\epsilon,d}(1)}$}. We know that $\\ell_{d'}(B^*) \\in [\\frac{1}{\\epsilon}s_{\\max}(B^*),ns_{\\max}(B^*)]$ for all $d' = 1,\\dots,d$. Thus, we can guess all values $ \\ell_{d'}(B^*)$ in time {$(\\log_{1+\\epsilon} n)^{O_{\\epsilon,d}(1)}$}.\n}\n\n\n\n\n\nNext, we argue that for any other $\\S$-box $B$ we either have $s_{\\max}(B) \\geq \\frac{s_{\\max}(B^*)}{n^{1/d}}$ or all items inside $B$ can be moved to $B^*$,\n{while losing only a factor of $1+\\epsilon$ in the profit of the items packed inside $B^*$}.\nConsider all $\\S$-boxes in $\\B$ such that for each of them the largest item packed into them has size strictly less than\n\\[\n\\frac{s_{\\max}(B^*)}{n^{1/d}}.\n\\]\nSince there are at most $n$ such items and by definition of $s_{\\max}(B^*)$, we know that their total volume is at most\n\\[\nn \\cdot \\left(\\frac{s_{\\max}(B^*)}{n^{1/d}}\\right)^d \\leq n \\cdot \\left(\\frac{\\epsilon \\ell_{\\min}(B^*)}{n^{1/d}}\\right)^d \\leq \\epsilon^d \\mathrm{VOL}_d(B^*).\n\\]\nThus, by {reserving this volume in $B^*$ to pack all these items, we may assume that there are no such boxes.}\n\nFrom this it follows that for any other $\\S$-box $B$, we have\n\\[\ns_{\\max}(B^*) \\geq s_{\\max}(B) \\geq \\frac{s_{\\max}(B^*)}{n^{1/d}}.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nThis implies that for each $d'=1,\\dots,d$ we have\n\\[\n\\frac{s_{\\max}(B^*)}{\\epsilon \\cdot n^{1/d}}\\leq \\ell_{d'}(B) \\leq \\epsilon \\cdot n^2\\cdot s_{\\max}(B*).\n\\]\nTherefore, we can guess the value $ \\ell_{d'}(B)$ using at most $O(\\log_{1+\\epsilon}n)$ guesses. Thus, we can guess the side lengths of all $\\S$-boxes in time $\\log_{1+\\epsilon}^{O_{\\epsilon,d}(1)}n$ losing only a factor of $(1-O(\\epsilon))$ of the profit.\n\nIt remains to be shown that by reducing the side lengths of each box $B \\in \\B_{\\S}$ to $ \\ell_{1}(B) ,..., \\ell_{d}(B) $ we only lose a factor of $(1+\\epsilon)^d$. Hereto, consider some box $B \\in \\B_\\S$ with the set of items $\\I(B)$ packed into it. We know that by reducing the side lengths as described above we reduce the volume by at most $(1+\\epsilon)^{-d}$. We now show that there exists a subset of $\\I(B)$ such that the total profit of the items is at least\n\t\\[\n\t(1+\\epsilon)^{-d}p(\\I(B)).\n\t\\]\n\tWe know that for $B$ and $\\I(B)$ we have that $s_i \\leq \\epsilon \\ell_{\\min}(B)$ for all $i \\in \\I(B)$ and furthermore\n\t\\[\\sum_{i\\in\\I(B)}\\lceil s_{i}\\rceil_{1+\\epsilon}^{d}\\leq(1-2d\\cdot\\epsilon)\\mathrm{VOL}(B).\\]\n\tWe will now use a similar argument as in the proof of Lemma~\\ref{lem:vstarbox_selection} considering the following LP:\n\n\t\\begin{alignat*}{3}\n\t\t& \\text{minimize} & \\sum_{j=1}^{m} &x_ip_i& \\\\\n\t\t& \\text{subject to} \\quad& \\sum_{i \\in \\Q}&x_{i}\\lceil s_i \\rceil_{1+\\epsilon}^d& \\leq (1+\\epsilon)^{-d}(1-2d\\cdot \\epsilon) \\mathrm{VOL}_d(B) \\\\\n\t\t&&& x_{i} \\geq 0, &  \\forall i \\in \\I(B)\n\t\t\\\\\n\t\t&&& x_{i} \\leq 1, &  \\forall i \\in \\I(B)\n\t\\end{alignat*}\n\n\tConsider the following solution $\\hat{x}_i := (1+\\epsilon)^{-d}$. Clearly, this solution is feasible. Furthermore, observe that $s_i \\leq \\epsilon  \\ell_{\\min}(B) $ and $\\lceil s_i \\rceil_{1+\\epsilon}^d \\leq 2\\epsilon \\prod_{d'=1}^d  \\ell_{\\min}(B) $. Let $x^*$ be the optimal solution to the LP. Then, by the rank lemma~\\cite{lau2011iterative}, we know that there is at most one item $i' \\in \\I(B)$ for which $0 < x^*_i < 1$. We now define the set of items packed into $B$ with reduced side lengths as $\\I'(B) :=\\{i\\in \\I(B): x^*_i > 0\\}$. By the candidate solution $\\hat{x}$ we know that the obtained profit is at least $(1+\\epsilon)^{-d}p(\\I(B))$. Furthermore, we have the following\n\t\\[\\sum_{i\\in\\I(B')}\\lceil s_{i}\\rceil_{1+\\epsilon}^{d}\\leq(1+\\epsilon)^{-d}(1-2d\\cdot \\epsilon) \\mathrm{VOL}_d(B) + 2\\epsilon\\prod_{d'=1}^d  \\ell_{\\min}(B) .\\]\n\tBy Lemma~\\ref{lem:NFDH}, we know that $\\I'(B)$ can be packed into $B$ with reduced side lengths using NFDH.\n\\end{proof}\n\nThe proofs of Lemmas~\\ref{lem:hypcub_guessing_pjs} and~\\ref{lem:hypcub_guessing_abjs} follow from the arguments in Section~\\ref{sec:hypercubes}. Therefore, all basic quantities can be guessed in time $(\\log n)^{O_{\\epsilon,d}(1)}$.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe will now move on to the details of our indirect guessing framework. Recall that for each $\\ell \\in \\{1,...,r\\}$, we are {concerned} with finding an estimate of $k_{\\ell+1}$, denoted by $\\tilde{k}_{\\ell+1}$, assuming that we have already found {estimates} $\\tilde{k}_{1},\\dots, \\tilde{k}_{\\ell}$.\nConsider a candidate $s$ for $\\tilde{k}_{\\ell+1}$.\nWe will first show how to reduce the number of variables considered in $(\\mathrm{IP}(s))$ by only losing a factor of $1+\\epsilon$ of the profit. To this end, we denote by $(\\mathrm{LP}(s))$ the linear relaxation of $(\\mathrm{IP}(s))$.\n\n\\begin{lemma}\\label{lem:hypcub_reducevar}\nGiven a set of guessed boxes $\\B$ and the set of input items $\\tilde{\\I}_{\\ell+1}(s)$, there exists a subset of items of $\\tilde{\\I}_{\\ell+1}(s)$ such that there are at most $O_\\epsilon(\\log n)$ many size and profit classes. The optimal solution to $(\\mathrm{LP}(s))$ considering only this subset yields a profit of at least {a $(1-\\epsilon)$-fraction} of the profit of the optimal solution to $(\\mathrm{LP}(s))$ considering all items in $\\tilde{\\I}_{\\ell+1}(s)$.\n\\end{lemma}\n\\begin{proof}\nRecall that we disregarded all items with profit less than $\\epsilon/n \\cdot p_{\\max}$.\n\nThis implies that for all $i \\in \\tilde{\\I}_{\\ell+1}(s)$, we have that $p_i \\in [\\epsilon/n \\cdot p_{\\max}, p_{\\max}]$ and {there are only} $O_\\epsilon(\\log n)$ profit classes.\nIn order to restrict the number of size classes consider the following two linear programs. The first one is $(\\mathrm{LP}(s))$ and the second one is $(\\mathrm{LP}(s))$ after disregarding every item $i \\in \\tilde{\\I}_{\\ell+1}(s)$\n{that is \\emph{small}, i.e., for which}\n$s_i \\leq \\frac{\\epsilon^{1/d} \\mathrm{v}^{1/d}}{n^{1/d}}$ where $\\mathrm{v}$ is the maximum volume of the guessed $\\S$-boxes. We denote the latter linear program by $(\\mathrm{LP}'(s))$. We will argue later that we can select all small items.\n\n\\begin{alignat*}{3}\n\t(\\mathrm{LP}(s))\\quad& \\text{max} \t& \\displaystyle \\sum_{(t,t') \\in \\mathcal{T}}\\sum_{B \\in \\B(\\ell+1)} x_{t,t',B} p(t')\t\t\t& \t\t\t& \\quad & \\\\\n\t& \\text{s.t.} & \\displaystyle\\sum_{(t,t') \\in \\mathcal{T}} x_{t,t',B} \t& \\leq n(B)\t& \t\t& \\forall B \\in \\B_{\\N}(\\ell+1) \\\\\n\t& & \\displaystyle\\sum_{(t,t') \\in \\mathcal{T}} x_{t,t',B}s(t)^d\t& \\leq a_{B,\\ell+1}\\mathrm{VOL}_d(B)\t& \t\t& \\forall B \\in \\B_{\\S}(\\ell+1) \\\\\n\t&\t\t\t\t& \\displaystyle\\sum_{B \\in \\B(\\ell+1)} x_{t,t',B}\t\t\t\t\t\t\t\t& \\leq n_{t,t'}\t& \t\t& \\forall (t,t') \\in \\mathcal{T} \\\\\n\t&\t\t\t\t& x_{t,t',B}\t\t\t\t\t\t\t\t& \\geq 0& \t\t&\\forall (t,t') \\in \\mathcal{T}, B \\in \\B(\\ell+1)\n\\end{alignat*}\n\n\\begin{alignat*}{3}\n\t(\\mathrm{LP}'(s))\\quad& \\text{max} \t& \\displaystyle \\sum_{(t,t') \\in \\mathcal{T}}\\sum_{B \\in \\B(\\ell+1)} x_{t,t',B} p(t')\t\t\t& \t\t\t& \\quad & \\\\\n\t& \\text{s.t.} & \\displaystyle\\sum_{(t,t') \\in \\mathcal{T}} x_{t,t',B} \t& \\leq n(B)\t& \t\t& \\forall B \\in \\B_{\\N}(\\ell+1) \\\\\n\t& & \\displaystyle\\sum_{(t,t') \\in \\mathcal{T}} x_{t,t',B}s(t)^d\t& \\leq a_{B,\\ell+1}(1-\\epsilon)\\mathrm{VOL}_d(B)\t& \t\t& \\forall B \\in \\B_{\\S}(\\ell+1) \\\\\n\t&\t\t\t\t& \\displaystyle\\sum_{B \\in \\B(\\ell+1)} x_{t,t',B}\t\t\t\t\t\t\t\t& \\leq n_{t,t'}\t& \t\t& \\forall (t,t') \\in \\mathcal{T} \\\\\n\t&\t\t\t\t& x_{t,t',B}\t\t\t\t\t\t\t\t& \\geq 0& \t\t&\\forall (t,t') \\in \\mathcal{T}, B \\in \\B(\\ell+1)\n\\end{alignat*}\n\nLet $x^*$ be an optimal solution to $(\\mathrm{LP}(s))$, then $\\hat{x}:= (1-\\epsilon)x^*$ is a feasible solution to $(\\mathrm{LP}'(s))$ with a profit of $(1-\\epsilon)$ times the profit of $x^*$. Adding the small items will only increase the profit. We can add all small items since their total volume will be at most $\\epsilon \\mathrm{v}$ such that they can be packed into the $\\S$-box with largest volume.\n\\end{proof}\nWe continue by presenting the proof of Lemma~\\ref{lem:cubes_IP_sol}.\n\\begin{proof}[Proof of Lemma~\\ref{lem:cubes_IP_sol}]\nWe first show how to find a $(1-\\epsilon)$-approximate integral solution to $(\\mathrm{IP}(s))$. Recall that $(\\mathrm{IP}(s))$ is defined as follows.\n\n\\begin{alignat*}{3}\n\t(\\mathrm{IP}(s))\\quad& \\text{max} \t& \\displaystyle \\sum_{(t,t') \\in \\mathcal{T}}\\sum_{B \\in \\B(\\ell+1)} x_{t,t',B} p(t')\t\t\t& \t\t\t& \\quad & \\\\\n\t& \\text{s.t.} & \\displaystyle\\sum_{(t,t') \\in \\mathcal{T}} x_{t,t',B} \t& \\leq n(B)\t& \t\t& \\forall B \\in \\B_{\\N}(\\ell+1) \\\\\n\t& & \\displaystyle\\sum_{(t,t') \\in \\mathcal{T}} x_{t,t',B}s(t)^d\t& \\leq a_{B,\\ell+1}\\mathrm{VOL}_d(B)\t& \t\t& \\forall B \\in \\B_{\\S}(\\ell+1) \\\\\n\t&\t\t\t\t& \\displaystyle\\sum_{B \\in \\B(\\ell+1)} x_{t,t',B}\t\t\t\t\t\t\t\t& \\leq n_{t,t'}\t& \t\t& \\forall (t,t') \\in \\mathcal{T} \\\\\n\t&\t\t\t\t& x_{t,t',B}\t\t\t\t\t\t\t\t& \\in \\mathbb{N}_{0}& \t\t&\\forall (t,t') \\in \\mathcal{T}, B \\in \\B(\\ell+1)\n\\end{alignat*}\n\nWe will now compute an approximate solution to $(\\mathrm{IP}(s))$. We start by guessing the $2C_{boxes}(\\epsilon,d)/\\epsilon$ most profitable items in the solution to $(\\mathrm{LP}'(s))$. Denote by $S_g$ the set of these items.\n\t\n\tWe obtain $S_g$ by guessing for each of the most profitable items the profit class it belongs to and then choose an item from the smallest size class for which at least one item of this profit class exists. Repeating this for each of the most profitable items gives a total of\n\t{$(\\log n)^{O_{\\epsilon,d}(1)}$}\n\t\n\tguesses. Furthermore, we need to guess the correct box for each of these items which can be done in time $O_{\\epsilon,d}(1)$. After obtaining $S_g$ in this way, we update the values of the right-hand side of the constraints in $(\\mathrm{IP}(s))$. Hereto, consider a box $B \\in \\B$. If $B$ is a $\\N$-box, we denote by $n^g(B)$ the number of guessed items packed into $B$. If $B$ is a $\\S$-box, we denote by $\\mathrm{VOL}_d^g(B)$ the total volume of the guessed items packed into $B$. Furthermore, for each pair $t,t'$, denote by $n_{t,t'}^g$ the number of items of size class $\\S_t$ and profit class $\\P_{t'}$ in $S_g$. Based on this we consider the following variant of $(\\mathrm{IP}(s))$ which we denote by $(\\mathrm{IP}_g(s))$.\n\t\n\t{\n\t\\small\n\t\\begin{alignat*}{3}\n\t\t(\\mathrm{IP}_g(s))\\quad& \\text{max} \t& \\displaystyle \\sum_{(t,t') \\in \\mathcal{T}}\\sum_{B \\in \\B(\\ell+1)} x_{t,t',B} p(t')\t\t\t& \t\t\t& \\quad & \\\\\n\t\t& \\text{s.t.} & \\displaystyle\\sum_{(t,t') \\in \\mathcal{T}} x_{t,t',B} \t& \\leq n(B)-n^g(B)\t& \t\t& \\forall B \\in \\B_{\\N}(\\ell+1) \\\\\n\t\t& & \\displaystyle\\sum_{(t,t') \\in \\mathcal{T}} x_{t,t',B}s(t)^d\t& \\leq a_{B,\\ell+1}\\mathrm{VOL}_d(B) - \\mathrm{VOL}_d^g(B)\t& \t\t& \\forall B \\in \\B_{\\S}(\\ell+1) \\\\\n\t\t&\t\t\t\t& \\displaystyle\\sum_{B \\in \\B(\\ell+1)} x_{t,t',B}\t\t\t\t\t\t\t\t& \\leq n_{t,t'} - n_{t,t'} ^g\t& \t\t& \\forall (t,t') \\in \\mathcal{T} \\\\\n\t\t&\t\t\t\t& x_{t,t',B}\t\t\t\t\t\t\t\t& \\in \\mathbb{N}_{0}& \t\t&\\forall (t,t') \\in \\mathcal{T}, B \\in \\B(\\ell+1)\n\t\\end{alignat*}}\n\n\tThe objective is now to find an approximate solution to $(\\mathrm{IP}_g(s))$ via its LP-relaxation. \n\t\n\t{\n\t\t\\small\n\t\\begin{alignat*}{3}\n\t(\\mathrm{LP}_g(s))\\quad& \\text{max} \t& \\displaystyle \\sum_{(t,t') \\in \\mathcal{T}}\\sum_{B \\in \\B(\\ell+1)} x_{t,t',B} p(t')\t\t\t& \t\t\t& \\quad & \\\\\n\t& \\text{s.t.} & \\displaystyle\\sum_{(t,t') \\in \\mathcal{T}} x_{t,t',B} \t& \\leq n(B)-n^g(B)\t& \t\t& \\forall B \\in \\B_{\\N}(\\ell+1) \\\\\n\t& & \\displaystyle\\sum_{(t,t') \\in \\mathcal{T}} x_{t,t',B}s(t)^d\t& \\leq a_{B,\\ell+1}\\mathrm{VOL}_d(B) - \\mathrm{VOL}_d^g(B)\t& \t\t& \\forall B \\in \\B_{\\S}(\\ell+1) \\\\\n\t&\t\t\t\t& \\displaystyle\\sum_{B \\in \\B(\\ell+1)} x_{t,t',B}\t\t\t\t\t\t\t\t& \\leq n_{t,t'} - n_{t,t'} ^g\t& \t\t& \\forall (t,t') \\in \\mathcal{T} \\\\\n\t&\t\t\t\t& x_{t,t',B}\t\t\t\t\t\t\t\t& \\geq 0& \t\t&\\forall (t,t') \\in \\mathcal{T}, B \\in \\B(\\ell+1)\n\\end{alignat*}\n}\n\n\tObserve, that we cannot solve $(\\mathrm{LP}_g(s))$ in time $\\mathrm{poly}(\\log n)$. However, we can apply Lemma~\\ref{lem:hypcub_reducevar} to $(\\mathrm{LP}_g(s))$ and restrict the number of size classes to be considered. This gives the following LP denoted by $(\\mathrm{LP}'_g(s))$. Here, $\\mathcal{T}$ is the set of pairs $(t,t')$ considering only the size classes that are left after applying Lemma~\\ref{lem:hypcub_reducevar}. By our assumption that we we only consider items with profit at least $\\frac{\\epsilon p_{\\max}}{n}$, we have that $|\\mathcal{T}| \\in O(\\log^2_{1+\\epsilon}n)$. Observe that an optimal fractional solution to $(\\mathrm{LP}'_g(s))$ yields at least $(1-\\epsilon)$ times an optimal fractional solution to $(\\mathrm{LP}_g(s))$.\n\t\n\t\\begin{alignat*}{3}\n\t(\\mathrm{LP}'_g(s))\\quad& \\text{max} \t& \\displaystyle \\sum_{(t,t') \\in \\mathcal{T}'}\\sum_{B \\in \\B(\\ell+1)} x_{t,t',B} p(t')\t\t\t& \t\t\t& \\quad & \\\\\n\t& \\text{s.t.} & \\displaystyle\\sum_{(t,t') \\in \\mathcal{T}'} x_{t,t',B} \t& \\leq \\leq n(B)-n^g(B)\t& \t\t& \\forall B \\in \\B_{\\N}(\\ell+1) \\\\\n\t& & \\displaystyle\\sum_{(t,t') \\in \\mathcal{T}'} x_{t,t',B}s(t)^d\t& \\leq a_{B,\\ell+1}\\mathrm{VOL}_d(B) - \\mathrm{VOL}_d^g(B)\t& \t\t& \\forall B \\in \\B_{\\S}(\\ell+1) \\\\\n\t&\t\t\t\t& \\displaystyle\\sum_{B \\in \\B(\\ell+1)} x_{t,t',B}\t\t\t\t\t\t\t\t& \\leq n_{t,t'} - n_{t,t'} ^g\t& \t\t& \\forall (t,t') \\in \\mathcal{T}' \\\\\n\t&\t\t\t\t& x_{t,t',B}\t\t\t\t\t\t\t\t& \\geq 0& \t\t&\\forall (t,t') \\in \\mathcal{T}', B \\in \\B(\\ell+1)\n\\end{alignat*}\n\n\tNow, we obtain an optimal fractional solution $S_f$ to $(\\mathrm{LP}'_g(s))$ in time $(\\log_{1+\\epsilon}(n))^{O(1)}$ (using the ellipsoid method or for example~\\cite{cohen2021solving}). Next, we argue that there are at most $2C_{boxes}(\\epsilon,d)$ many fractional non-zero variables.  By the rank lemma (see e.g.~\\cite{lau2011iterative}) we know for an optimal vertex solution $\\tilde{x}$ of $(\\mathrm{LP}'(s))$, the number of fractional non-zeros in $\\tilde{x}$ is upper bounded by the number of linearly independent tight constraints of $(\\mathrm{LP}'(s))$. We now argue that there can be at most $2C_{boxes}(\\epsilon,d)$ such constraints out of the   $O_{\\epsilon,d}(\\log^2 n)$ many. To this end, consider the third group of constraints\n\t\\begin{align*}\n\t\t\\displaystyle\\sum_{B \\in \\B} x_{t,t',B}\t\t\t\t\t\t\t\t& \\leq n_{t,t'}\t& \t\t& \\forall  t\\in \\{0,\\dots,\\lceil \\log_{1+\\epsilon}(s)\\rceil\\}, t'\\in \\{0,\\dots, \\log n\\}\n\t\\end{align*}\n\tSuppose there is at least one tight constraint of the third type. Observe, it holds that $n_{t,t'} \\in \\mathbb{N}$ and, therefore, if there is a fractional variable in a {tight} constraint of this type in an optimal vertex solution, there has to be at least one other fractional variable in this constraint such that their sum is an integer. Assume there are $k$ fractional variables in this constraint. Then, there must be $k-1$ constraints of either the first or second type that are tight. However, the number of constraints of the first two types is at most $C_{boxes}(\\epsilon,d)$. Therefore, the number of fractional variables is at most $2C_{boxes}(\\epsilon,d)$. \n\tWe now obtain a $(1+\\epsilon)$-approximate fractional solution to $(\\mathrm{IP}(s))$ by combining $S_g$ and all integer variables of $S_f$ together with all small items. Since $S_g$ contains $2C_{boxes}(\\epsilon,d)/\\epsilon$ many integer variables and $S_f$ contains at most $2C_{boxes}(\\epsilon,d)$ many fractional variables each having a smaller profit than the ones of $S_g$, we lose a total profit of at most $\\epsilon p(S_{g})$.\n\n\tFinally, we need to know the total profit of the small items with $s_i \\leq \\frac{\\epsilon^{1/d} \\mathrm{v}^{1/d}}{n^{1/d}}$\n\t{where $\\mathrm{v}$ is the maximum volume of the guessed $\\S$-boxes}.\n\tBy Lemma~\\ref{lem:data-structure}, this can be done in time $O_{\\epsilon}(\\log_{1+\\epsilon}^2 n)$ with one query for each profit class which counts the number of small items of this profit class. We can use this and the rounded profit of each profit class to compute a $(1+\\epsilon)$-estimate of the total profit of these items.\n\tFinally, consider two values $s\\leq s'$. Then, the approximate solution obtained to $(\\mathrm{IP}(s))$ is also feasible for $(\\mathrm{IP}(s'))$. Therefore, $q(s) \\leq q(s')$.\n\\end{proof}\nNext, we will prove Lemma~\\ref{lem:cubes_induc_kr}.\n\\begin{proof}[Proof of Lemma~\\ref{lem:cubes_induc_kr}]\nSince we solve $(\\mathrm{IP}(s))$ up to a factor of $1-\\epsilon$,\nwe have that $q(k_{\\ell+1})\\ge (1-\\epsilon)\\hat{p}(\\ell+1)$\nsince $\\tilde{k}_{\\ell}\\le k_{\\ell}$ and, therefore, $\\tilde{\\I}_{\\ell+1}(k_{\\ell})=\\left\\{ i\\in\\I:s_{i}\\in[\\tilde{k}_{\\ell},k_{\\ell})\\right\\} \\subseteq\\left\\{ i\\in\\I:s_{i}\\in[k_{\\ell},k_{\\ell})\\right\\} =\\I_{\\ell+1}$. Recall that we define $\\tilde{k}_{\\ell+1}$ to be the smallest value $s\\in S$ for which $q(s)\\ge (1-\\epsilon)\\hat{p}(\\ell+1)$. Since $q(k_{\\ell+1})\\ge (1-\\epsilon)\\hat{p}(\\ell+1)$ it must hold that  $\\tilde{k}_{\\ell+1}\\le k_{\\ell+1}$.\n\\end{proof}\nFinally, we present the proof of Theorem~\\ref{thm:cubes_ptas}.\n\\begin{proof}[Proof of Theorem~\\ref{thm:cubes_ptas}]\n\n\nOur approximation scheme proceeds in three stages:\n\\begin{enumerate}[(A)]\n\t\\item  \\textit{Guessing basic quantities:} The total number of guesses is $(\\log n)^{O_{\\epsilon,d}(1)}$. {Observe that due to our method of guessing the basic quantitities for each $\\N$-box, the number of items we can pack into it is at least $(1+\\epsilon)^{-d}$ times the number of items packed into it if these quantities are guessed exactly. Thus, our packing of $\\N$-boxes is a $(1+O(\\epsilon))$-approximate packing.}\n\t\\item \\textit{Indirect guesing framework:} For each guess we need $r \\in O_{\\epsilon,d}(1)$ iterations of the indirect guessing framework, leading to solutions $x^*(1),\\dots,x^*(r)$ to the LP-relaxations of $(\\mathrm{IP}(\\tilde{k}_{1})),...,(\\mathrm{IP}(\\tilde{k}_{r}))$ {after guessing the mentioned most profitable items}. This takes time\n\t\n\t{$(\\log n)^{O_{\\epsilon,d}(1)}$}.\n\tLet $\\hat{x}(1),\\dots,\\hat{x}(r)$ be the rounded solutions to $(\\mathrm{IP}(\\tilde{k}_{1})),...,(\\mathrm{IP}(\\tilde{k}_{r}))$\n\t\\item \\textit{Constructing final solution:} Observe that it is not relevant which item is assigned to which box but rather that each box is assigned the correct number of items of each combination of size class $\\Q_t$ and profit class $\\P_{t'}$ and we {obtain} at least the total profit of the combined solutions $\\hat{x}(1),\\dots,\\hat{x}(r)$. For each combination of size class $\\Q_t$ and profit class $\\P_{t'}$ we use Lemma~\\ref{lem:data-structure} to find the set of items with sizes of class $\\Q_t$ and profits of class $\\P_{t'}$ in time $O(n)$. Assigning the items one-by-one to boxes in $\\B$ in non-decreasing order of side lengths can be done in time $O(n\\log n)$. By making sure that each box $B$ receives the correct number of items for each pair  size class $\\Q_t$ and profit class $\\P_{t'}$ this guarantees a profit of at least {an $(1+\\epsilon)^{-1}$-fraction} of the profit due {to} $\\hat{x}(1),\\dots,\\hat{x}(r)$. Finally, it remains to find a packing of {the} items into boxes. Each $\\N$-box $B$ can be filled one-by-one in time $O(n)$ placing each item into one of the cells of the grid of $B$. For the $\\S$-boxes, we use NFDH which packs all of them in time $O(n\\log n)$~\\cite{bansal2006bin}. Due to the way we guess the basic quantities of our $\\S$-boxes we know that for any $\\S$-box $B$ we have that all items packed into it have side length at most $2\\epsilon \\ell_{\\min}(B)$ and the total volume of the items rounded up is at most $(1-2d\\epsilon)\\mathrm{VOL}_d(B)+\\epsilon \\mathrm{VOL}_d(B)$. Hence, we know that by Lemma~\\ref{lem:NFDH} all selected items can be packed into $B$\n\t{by NFDH}.\n\\end{enumerate}\n\nThis yields a total running time of $O(n \\log^2 n) + (\\log n)^{O_{\\epsilon,d}(1)}$, where the first term is due to insertion of all items into the item data structure and the balanced binary search trees. By the nature of our guessing and Lemma~\\ref{lem:struc_rectangles}, we know that the guessed boxes guarantee a profit of at least $(1-2^{O(d)}\\epsilon)OPT$. The indirect guessing scheme guarantees that for each set $\\I_j$ (with $j=1,\\dots,r$) we find a set of items yielding a profit of at least $(1-\\epsilon)\\hat{p}_j$ and by definition of $\\N$- and $\\S$-boxes all of these items can be packed.\n\\end{proof}\n\\subsection{Dynamic Algorithm}\\label{apx:cubes_dynamic}\nIn this section we present the proof of Theorem~\\ref{thm:dyn_alg_cubes}.\n\\begin{proof}[Proof of Theorem~\\ref{thm:dyn_alg_cubes}]\nIf an item $i$\nis added to $\\I$ or removed from $\\I$, then we update our item data\nstructure in time $O(\\log^2 n)$ (see Lemma~\\ref{lem:data-structure}). Additionally, insertion or deletion of the corresponding side length from the balanced binary search tree takes $O(\\log n)$~\\cite{guibas1978dichromatic}.\nIf an $(1+\\epsilon)$-estimate for $\\OPT$ is queried, we execute\nthe indirect guessing framework above \\emph{without }computing the\nprecise set of tasks in the computed solution. Instead, we compute\nonly the solutions $x^*(1),\\dots,x^*(r)$ to the respective instance of $(\\mathrm{LP}(s))$ {(after guessing\n\tthe most profitable items of $(\\mathrm{IP}(s))$)}.\nWe can do this in time $(\\log n)^{O_{\\epsilon,d}(1)}$. This suffices\nto estimate the profit of the computed solutions, and hence the profit\nof the optimal solution, up to a factor $(1+\\epsilon)$. \nSuppose that for an item $i\\in\\I$ it is queried whether $i$ is contained\nin the current solution. We compute the values $\\tilde{k}_{1},\\tilde{k}_{2},\\dots,\\tilde{k}_{r}$\nand the corresponding solutions to $(\\mathrm{IP}(\\tilde{k}_{1})),...,(\\mathrm{IP}(\\tilde{k}_{r}))$\nin time $(\\log n)^{O_{\\epsilon,d}(1)}$. Based on them, we specify implicitly\na fixed solution $\\ALG$ for which we answer whether $i\\in\\ALG$ or\n$i\\notin\\ALG$. We do this as follows. For each combination of a set\n$\\tilde{\\I}_{j}$, a size class $\\Q_{t}$, and a profit class $\\P_{t'}$\nwe consider the set $\\tilde{\\I}_{j}\\cap \\Q_{t}\\cap\\P_{t'}$. Based\non the rounded LP-solutions, we define a value $z_{j,t,t'}\\in\\mathbb{N}_{0}$\nthat defines how many items from $\\tilde{\\I}_{j}\\cap\\Q_{t}\\cap\\P_{t'}$\nare contained in $\\ALG$. Using the way we construct our packing, we know that we picked the items in non-decreasing order of side lengths. For an item $i\\in\\tilde{\\I}_{j}\\cap\\Q_{t}\\cap\\P_{t'}$,\nwe output ``$i\\in\\ALG$'' if $i$ is among the first $z_{j,t,t'}$\nitems, and ``$i\\notin\\ALG$'' otherwise. This can be done using a single query in time $O(\\log^2 n)$ to count the number of items in $\\tilde{\\I}_{j}\\cap \\Q_{t}\\cap\\P_{t'}$ with side length smaller than $s_i$. This ensures that we give\nconsistent answers between two consecutive updates of the set $\\I$.\nFinally, once all values $z_{j,t,t'}$ are computed as described above,\nwe can easily output the whole solution $\\ALG$ in time $|\\ALG|\\cdot(\\log n) +(\\log n)^{O_{\\epsilon}(1)}$\nif this is desired.\n\\end{proof}\n\n\\section{Details of Section~\\ref{sec:rec_main}}\\label{app:rec}\nIn this section we present the details of our (dynamic) algorithms for the two-dimensional geometric knapsack problem with rectangles. Recall that we are given a set $\\I$.  For an item $i\\in\\I$, denote by $h_{i}$ its height, by $w_{i}$ is width, by $p_i$ its profit and by $d_i := p_i/w_i$ its density. Our knapsack $K$ has side length $N$ in each dimension for a given integer $N$. We assume for the remainder that we are given a fixed constant $\\epsilon > 0$. Again, we\nstore our items $\\I$ in a suitable data structure based on range counting/reporting data structures for points in three dimensions (corresponding to item heights, widths, profits and densities)~\\cite{lee1984computational}. We refer to it\n\nas our \\emph{rectangle data structure}. See Appendix~\\ref{app:data_struc} for details.\n\n\n\n\n\\begin{lemma}\n\t\\label{lem:data-structure-rec}There is a data structure for the items\n\t$\\I$ that allows the following operations:\n\t\\begin{itemize}\n\t\t\\item Insertion and deletion of an item in time $O(\\log^4 n)$.\n\t\t\\item Given eight values $a,b,c,d,e,f,g,h \\in \\mathbb{N}$, return the cardinality of the set $\\I(a,b,c,d,e,f,g, h):= \\{i \\in I: a\\leq h_i \\leq b, c\\leq w_i \\leq d, e\\leq p_i \\leq f,g\\leq d_i \\leq h\\}$ in time $O(\\log^{3}n)$.\n\t\t\\item Given eight values $a,b,c,d,e,f,g,h \\in \\mathbb{N}$, return the total profit of items in the set $\\I(a,b,c,d,e,f,g, h):= \\{i \\in I: a\\leq h_i \\leq b, c\\leq w_i \\leq d, e\\leq p_i \\leq f,g\\leq d_i \\leq h\\}$ in time $O(\\log^{3}n)$.\n\t\t\\item Given eight values $a,b,c,d,e,f,g,h \\in \\mathbb{N}$, return the total width of items in the set $\\I(a,b,c,d,e,f,g, h):= \\{i \\in I: a\\leq h_i \\leq b, c\\leq w_i \\leq d, e\\leq p_i \\leq f,g\\leq d_i \\leq h\\}$ in time $O(\\log^{3}n)$.\n\t\t\\item Given eight values $a,b,c,d,e,f,g,h \\in \\mathbb{N}$,, return the set $\\I(a,b,c,d,e,f,g, h):= \\{i \\in I: a\\leq h_i \\leq b, c\\leq w_i \\leq d, e\\leq p_i \\leq f,g\\leq d_i \\leq h\\}$ in time $O(\\log^3 n +|\\I(a,b,c,d,e,f,g,h)|)$. \n\t\t\n\t\\end{itemize}\n\\end{lemma}\nFurthermore, we use balanced binary search trees (see e.g.~\\cite{guibas1978dichromatic}) to store the set of item densities, profits, widths and heights such that an item can be inserted, deleted, and queried in time $O(\\log n)$ in each of these additional data structures.\n\nWe distinguish our items $\\I$ into four types, depending on constants\n$\\el$ and $\\es$ with $1\\ge\\epsilon\\ge\\el\\ge\\es>0$\nsuch that $c(\\epsilon)\\es<\\el$\nfor a value $c(\\epsilon)$ (which we will define later).\n\n\nWe say that an item $i\\in\\I$ is\n\\begin{itemize}\n\\item \\emph{large }if $h(i)>\\el N$ and $w(i)>\\el N$,\n\\item \\emph{horizontal} if $w(i)>\\el N$ and $h(i)\\le\\es N$,\n\\item \\emph{vertical} if $h(i)>\\el N$ and $w(i)\\le\\es N$,\n\\item \\emph{small} if $h(i)<\\es N$ and $w(i)<\\es N$,\n\\item \\emph{intermediate }otherwise, i.e., $h(i)\\in(\\es N,\\el N]$ or $w(i)\\in(\\es N,\\el N]$. \n\\end{itemize}\nFollowing the ideas of~\\cite{grandoni2021improved}, we can guess $\\el$ and $\\es$ in $O_\\epsilon(1)$ time such that we may disregard all intermediate items and $\\el$ and $\\es$ are constants depending only on $\\epsilon$. We state the lemma here for completeness.\n\\begin{lemma}[\\cite{galvez2021approximating}]\\label{lem:rec_noint}\n\tFor any $\\epsilon \\geq 0$ and a positive increasing function $f(\\cdot)$ (with $f(x) > x$), there exist constant values $\\el$,~$\\es$ with $\\epsilon \\geq \\el \\geq f(\\es) \\in \\Omega_{\\epsilon}(1)$ and $\\es \\in \\Omega_\\epsilon(1)$ such that the total profit of intermediate rectangles is bounded by $\\epsilon p(\\OPT)$. Furthermore, the pair $(\\el,\\es)$ is one pair from a set of $O_\\epsilon(1)$ pairs and this set can be computed in polynomial time.\n\\end{lemma}\n\n\n\\begin{figure}[h!]\n\t\\centering\n\t\\includegraphics[width=.6\\linewidth,page = 3]{figures/figures_rectangles.pdf}\n\t\\caption{Visualization of $\\L$-, $\\S$-,$\\V$- and $\\H$-boxes}\n\t\\label{fig:pack_LSVH}\n\\end{figure}\nIn the remainder of this section, we use the following notions. Consider some set of items $\\I$. Denote by $h_{\\min}(\\I)$ ($w_{\\min}(\\I)$) and $h_{\\max}(\\I)$ ($w_{\\max}(\\I)$) the minimum and maximum height (width) among all items in $\\I$, respectively.\n\n\\subsection{No rotations}\\label{app:rec_nr}\nIn this subsection, we present the details for the setting without rotations leading to the proof of Theorem~\\ref{thm:rec_nr}. We first give a high level overview. Similarly as before, let $\\epsilon>0$ and we assume w.l.o.g.~that $1/\\epsilon\\in\\mathbb{N}$ and that $\\epsilon$ is sufficiently small.\n\n\n\\subsubsection{Structured packing of rectangles}\nWe use a packing based on rectangular boxes, similar to our packing\nin Section~\\ref{sec:hypercubes}. However, we distinguish the boxes\nnow into four different types (Figure~\\ref{fig:pack_LSVH} shows a visualization of these types). The first type are $\\L$-boxes which\ncontain only one large item each.\n\\begin{definition}[$\\L$-boxes]\nLet $B\\subseteq K$ be an axis-parallel rectangle and suppose we\nare given a packing of items inside $B$. We say that $B$ is an $\\L$\\emph{-box\n}if $B$ contains exactly one large item and no other items.\n\\end{definition}\nThe next types are $\\H$- and $\\V$-boxes inside which the items are\nintuitively horizontally and vertically stacked, respectively. See Figure~\\ref{fig:pack_VH} for a visualization of these boxes.\n\\begin{definition}[$\\H$- and $\\V$-boxes]\nLet $B$ be an axis-parallel rectangle and suppose we are given\na packing of items $\\I'\\subseteq\\I$ inside $B$. We say that $B$\nis a\n\\begin{itemize}\n\t\\item $\\H$\\emph{-box} if $\\I'$ contains only horizontal items which are\n\tstacked on top of each other inside $B$ and there are given values $w_{\\min}(B),w_{\\max}(B)$\n\tsuch that $w_{\\min}(B)\\le w_{i}\\le w_{\\max}(B)$ for each\n\t$i\\in\\I'$, \n\t\\item $\\V$\\emph{-box} if $\\I'$ contains only vertical items which are\n\tstacked one next to the other inside $B$\n\tand there are given values\n\t$h_{\\min}(B),h_{\\max}(B)$\n\tsuch that $h_{\\min}(B)\\le h_{i}\\le h_{\\max}(B)$ for each\n\t$i\\in\\I'$.\n\\end{itemize}\n\\end{definition}\nThe third type are $\\S$-boxes for which \n\n\neach item is small compared to the size of the box, and thus\nNFDH can pack these items, unless they use almost the entire volume\nof the box.\n\\begin{definition}[$\\S$-boxes]\nLet $B$ be an axis-parallel rectangle with height $h_{B}\\in\\mathbb{Z}_{\\ge0}$\nand width $w_{B}\\in\\mathbb{Z}_{\\ge0}$ and let $\\I'\\subseteq\\I$ be\na set of items packed into $B$. We say that $B$ is an $\\S$\\emph{-box}\nif for each item $i\\in\\I'$ we have $h_{i}\\le\\epsilon\\cdot h_{B}$\nand $w_{i}\\le\\epsilon\\cdot w_{B}$ and additionally $\\sum_{i\\in\\I'}\\lceil h_{i} \\rceil_{1+\\epsilon}\\cdot \\lceil w_{i}\\rceil_{1+\\epsilon}\\leq(1-3\\epsilon)h_{B}w_{B}$.\n\\end{definition}\nWe argue now that there always exists a $(2+\\epsilon)$-approximate\n\\emph{easily guessable packing} with $\\L$-, $\\H$-, $\\V$-, and $\\S$-boxes that is suitable\nfor our indirect guessing framework.\n\n\\begin{lemma}[Near-optimal packing of rectangles]\n\\label{lem:struc_rectangles} There exists a packing with the following\nproperties: \n\n\\begin{enumerate}[i)]\n\t\\item it consists of $\\L$-, $\\H$-, $\\V$- and $\\S$-boxes whose total\n\tnumber is bounded by a value $C_{\\mathrm{boxes}}(\\epsilon)$ depending\n\tonly on $\\epsilon$. \n\t\\item there is a universal set of values $U(\\epsilon$) with $|U(\\epsilon)|=O_{\\epsilon}(1)$\n\tsuch that for each box $B$ of the packing $h_{B}=\\alpha_{B}\\cdot N$\n\tfor some $\\alpha_{B}\\in U(\\epsilon)$,\n\t\\item there exist values $k_{0},k_{1},k_{2},\\dots,k_{r}\\in\\mathbb{Z}_{\\geq0}$\n\twith $k_{0}=0$ and $r\\in O_{\\epsilon}(1)$ and a value $j_{B}\\in\\{1,2,\\dots,r\\}$\n\tfor each $\\H$- or $\\V$-box $B$ such that\n\t\\begin{enumerate}\n\t\t\\item if $B$ is a $\\H$-box, then $w_{\\min}(B)=k_{j_{B}-1}$ and $w_{\\max}(B)=k_{j_{B}}$, \n\t\t\\item if $B$ is a $\\L$-box, then the unique item $i\\in\\I$ packed inside\n\t\t$B$ satisfies that $w_{i}=k_{j_{B}}$,\n\t\\end{enumerate}\n\t\\item let $h^{(1)},\\dots, h^{(c)}$ be the distinct heights of $\\V$-boxes in non-decreasing order such that for each $j = 1,\\dots,c$ we have that\n\t\\begin{enumerate}\n\t\t\\item each $\\V$-box of height $h^{(1)}$ only contains items of height at most $h^{(1)}$ and\n\t\t\\item each $\\V$-box of height $h^{(j)}$ only contains items of height at most $h^{(j)}$ and strictly larger than $h^{(j-1)}$ for $j=2,\\dots,c$\n\t\\end{enumerate}\n\t\\item the total profit of the packing is at least $(\\frac{1}{2}-O(\\epsilon))\\OPT$.\n\t\n\t\n\t\n\\end{enumerate}\n\\end{lemma}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo prove Lemma~\\ref{lem:struc_rectangles} we need the \\emph{resource augmentation packing lemma} introduced by Galvez et al.~\\cite{galvez2021approximating}. In~\\cite{galvez2021approximating}, three types of boxes are used. Let $B$ be a box with height $h(B)$ and width $w(B)$. Then, $B$ is a \\emph{type 1} box if all items in $B$ are stacked on top of each other in non-increasing order of widths. Analogously, $B$ is a \\emph{type 2} box if all items in $B$ are placed next to each other in non-increasing order of heights. Finally, we say $B$ is a \\emph{type 3} box if for every item $i \\in \\I$ packed into $B$ we have $h_i  \\leq \\epsilon h(B)$ and $w_i \\leq \\epsilon w(B)$. The resource augmentation packing lemma states the following.\n\\begin{lemma}[\\cite{galvez2021approximating}]\\label{lem:rec_rs_augment}\n\tLet ${\\hat{\\I}}$ be a collection of items that can be packed into a box $B$ with height $h(B)$ and width $w(B)$, and let $\\epsilon_{ra} > 0$ be a given constant. Then there exists a packing into sub-boxes of type 1, 2 and 3 of ${\\hat{\\I}}' \\subseteq {\\hat{\\I}}$ inside a box $B'$ with height $(1+\\epsilon_{ra})h(B)$ and width $w(B)$ such that:\n\t\\begin{itemize}\n\t\t\\item $p({\\hat{\\I}}') \\geq (1-O(\\epsilon_{ra}))p({\\hat{\\I}})$\n\t\t\\item The number of sub-boxes used in the packing is $O_{\\epsilon_{ra}}(1)$.\n\t\\end{itemize}\n\\end{lemma}\nObserve that the three types of boxes described above are different from our $\\H$-, $\\V$-, $\\L$- and $\\S$-boxes. But as it turns out in the upcoming proof of Lemma~\\ref{lem:struc_rectangles}, we can modify the packing into type 1,2 and 3 boxes into a packing using $\\H$-, $\\V$-, $\\L$- and $\\S$-boxe while keeping a $1-O(\\epsilon)$ fraction of the profit. \n\\begin{proof}[Proof of Lemma~\\ref{lem:struc_rectangles}]\n\tLet $\\OPT$ be the optimal packing of $\\I$ into $K$. We first apply a shifting argument to identify a horizontal strip\n\t$S:=[0,N)\\times[a,a+\\epsilon N)$ for\n\tsome $a\\in[0,(1-\\epsilon)N)$ inside the knapsack $K$ such that\n\t\\begin{itemize}\n\t\t\\item the items that are horizontal or small and that intersect with $S$,\n\t\ttogether with\n\t\t\\item the items that are large or vertical and for which at least one corner\n\t\tis contained in $S$\n\t\\end{itemize}\n\thave a total profit of at most $2\\epsilon\\cdot\\OPT$.\n\tTo {show} this we can consider values $a_1,\\dots, a_{1/\\epsilon}$ where $a_j = (j-1)\\epsilon$ as candidates for $a$ and pick one of them uniformly at random.\n\t\n\tSince $\\epsilon \\geq \\el > \\es$ we know that each horizontal and small item will intersect with at most two of the candidate strips. Hence, for each of them the probability of intersecting with the chosen strip is at most $2\\epsilon$. For large and vertical items each corner is contained in at most one of the candidate strips. Therefore, the probability\n\t{that a given large or vertical item is contained in the set of items defined above is at most}\n\t$\\epsilon$. Hence, the expected total profit of the two groups of items is at most $2\\epsilon OPT$ which implies that there must also exist one particular strip such that the total profit of the two groups of items is at most $2\\epsilon OPT$. We choose this strip and remove the items described above from $\\OPT$, losing in total a profit of\n\tat most $O(\\epsilon)\\OPT$.\n\t\n\t\n\tNow, consider the remaining packing. We partition the remaining items into\n\ttwo sets. The first set consists of all remaining items that are large\n\tor vertical and that intersect with $S$; we denote them by $\\I_{1}$.\n\tLet $\\I_{2}$ denote all other remaining items. \n\t\n\tIf $p(\\I_{1})\\ge p(\\I_{2})$ then $p(\\I_{1})\\ge(\\frac{1}{2}-O(\\epsilon))\\OPT$\n\tand we construct a packing for $\\I_{1}$ as follows. Note that each\n\titem $i\\in\\I_{1}$ must ``cross'' $S$ since no corner of $i$ is\n\tcontained in $S$. This implies that no two items are packed on top of each other and we can\n\t{imagine that the items are packed as in a one-dimensional knapsack.}\n\t\n\tBy re-arranging the items from left to right, first starting with the vertical items and then {continuing} with the large items, we create a single $\\V$-box and at most $O_\\epsilon(1)$ many $\\L$-boxes, each of height $N$, i.e., $\\alpha_B=1$ for each of them. This packing satisfies properties i), ii) and v). We will now show that it also satisfies property iii). To do this, let $B_1,\\dots, B_r$ be the $\\L$-boxes with distinct values of $w_{\\max}(\\cdot)$ such that $w_{\\max}(B_1) \\leq \\dots \\leq w_{\\max}(B_r)$ and {define} $k_j = w_{\\max}(B_j)$ for each $j=1,\\dots,r$. As the number of $\\L$-boxes in the packing is at most $O_\\epsilon(1)$, we also know that $r \\in O_\\epsilon(1)$. Finally, observe that we have only a single $\\V$-box which automatically satisfies property iv).\n\t\n\tIf $p(\\I_{1})<p(\\I_{2})$ then $p(\\I_{2})\\ge(\\frac{1}{2}-O(\\epsilon))\\OPT$\n\tand we construct a packing for $\\I_{2}$. Note that no item $i\\in\\I_{2}$\n\tintersects with $S$. Using the empty space $S$, we invoke the \\emph{resource augmentation packing lemma} (see Lemma~\\ref{lem:rec_rs_augment}) with $\\epsilon_{ra} = \\frac{\\epsilon-\\epsilon^2}{2-2\\epsilon} \\leq \\epsilon$ which allows us to find a packing of a subset of items $\\I'_{2}$ into $O_{\\epsilon}(1)$ boxes,\n\tusing only the area $[0,N]\\times[0,(1-\\epsilon/2-\\epsilon^2)N]$ inside $K$ such that $p(\\I'_2) \\geq (1-O(\\epsilon))p(\\I_2)$. The boxes used in this packing, however, are boxes of type 1, 2 or 3. We will now show how to change the packing into a packing using $\\H, \\V, \\L$ and $\\S$-boxes. Let $B$ be a box of the current packing. We will now make a case distinction.\n\t\n\t\\textbf{Case 1:} $B$ is a box of type 1 (all items in $B$ are stacked on top of each other in non-increasing order of heights). We first re-order the packing from bottom to top. First, we stack all horizontal items on top of each other. Then, we stack all large items on top of each other, followed by the vertical and, finally, the small items. Let $B_h$ be the sub-box of $B$ filled with horizontal items.\n\t{We define a $\\H$-box as a sub-box of $B$ for all these horizontal items.}\n\t\n\tFurthermore, since the height of $B$ is at most $(1-\\epsilon/2)N$, there can be at most $1/\\el$ many large and vertical items, respectively. For each large item,\n\t{we define an $\\L$-box as a sub-box of $B$ that contains only this large item.}\n\t\n\tSimilarly, {for each vertical item, we define a $\\V$-box that contains only this vertical item.}\n\t\n\tFinally, consider the area of $B$ containing all small items. We will take care of these items later.\n\t\n\t\\textbf{Case 2:} $B$ is a box of type 2 (all items in $B$ are stacked on next to each other in non-increasing order of widths). We first re-order the packing from left to right. First, we stack all vertical items on next to each other. Then, we stack all large items on next to each other, followed by the horizontal and, finally, the small items.\n\t{We define a $\\V$-box $B_v$ as a sub-box of $B$ that contains all vertical items in $B$.}\n\t\n\tFurthermore, since the width of $B$ is at most $N$, there can be at most $1/\\el$ many large and horizontal items, respectively. For each large item, {we define an $\\L$-box as a} the sub-box of $B$ containing only this item. Similarly,\n\t{for each horizontal item we define a $\\H$-box containing only this item.}\n\t\n\tFinally, consider the area of $B$ containing all small items. We will take care of these items later.\n\t\n\t\\textbf{Case 3:} $B$ is a box of type 3 (for every item $i \\in \\I(B)$ packed into $B$ we know that $h_i \\leq \\epsilon h_B$ and $w_i \\leq \\epsilon w_B$). We will now find a subset $\\I'(B) \\subseteq \\I(B)$ such that\n\t\\begin{itemize}\n\t\t\\item $p(\\I'(B)) \\geq (1-O(\\epsilon))p(\\I(B))$ and\n\t\t\\item $\\sum_{i\\in\\I'(B)}\\lceil h_{i} \\rceil_{1+\\epsilon}\\cdot \\lceil w_{i}\\rceil_{1+\\epsilon}\\leq(1-3\\epsilon)h_{B}w_{B}$.\n\t\\end{itemize}\n\tTo do this, consider the following LP-relaxation of the 1-D Knapsack problem with capacity $(1-4\\epsilon)h_{B}w_{B}$ and items $\\I(B)$, where the heigth and width are rounded up {to the next larger power of $1+\\epsilon$} and the item size is given by the area of the items.\n\t\n\t\\begin{alignat*}{3}\n\t\t& \\text{minimize} & \\sum_{i \\in \\I(B)} &x_ip_i& \\\\\n\t\t& \\text{subject to} \\quad& \\sum_{i\\in\\I(B)} &x_i \\lceil h_{i} \\rceil_{1+\\epsilon}\\cdot \\lceil w_{i}\\rceil_{1+\\epsilon}&\\leq(1-4\\epsilon)h_{B}w_{B} \\\\\n\t\t&&& x_{i} \\geq 0, &  \\forall i \\in \\I(B)\n\t\t\\\\\n\t\t&&& x_{i} \\leq 1, &  \\forall i \\in \\I(B)\n\t\\end{alignat*}\n\n\tObserve that $\\sum_{i\\in\\I(B)}\\lceil h_{i} \\rceil_{1+\\epsilon}\\cdot \\lceil w_{i}\\rceil_{1+\\epsilon}\\leq(1+\\epsilon)^2h_{B}w_{B}$. Consider the following LP-solution $\\hat{x}$ with $\\hat{x}_i := (1+\\epsilon)^{-2}(1-4\\epsilon)$\n\t{for each item $i \\in \\I(B)$}. This solution yields a profit of at least $(1+\\epsilon)^{-2}(1-4\\epsilon)p(\\I(B)) \\geq (1-O(\\epsilon))p(\\I(B))$ and is feasible due to the observation above. Let $x^*$ be the optimal {extreme point} solution to the LP. By the rank lemma~\\cite{lau2011iterative}, we know that there is at most one item $i' \\in \\I(B)$ such that $0< x^*_i < 1$. Let $\\I'(B):= \\{i \\in \\I(B): x^*_i > 0\\}$. Then, we have that $p(\\I'(B)) \\geq (1-O(\\epsilon))p(\\I(B))$ and since we only pick one item for which $x^*_i < 1$ and this item has height $h_i \\leq \\epsilon h_B$ and width $w_i \\leq \\epsilon w_B$, we have that $\\sum_{i\\in\\I'(B)}\\lceil h_{i}\\cdot w_{i}\\rceil_{1+\\epsilon}\\leq(1-3\\epsilon)h_{B}w_{B}$. Thus, $B$ with the set $\\I'(B)$ packed into $B$ is an $\\S$-box.\n\t\n\tLastly, consider all small items which were packed into boxes of type 1 or type 2 or that are now packed into $\\S$-boxes of width less than $\\es N$. We will argue that these items can be packed into a box of height $\\epsilon^2 N$ and width $N$ which can be placed at the top the free strip $S$. Observe that for each item $i\\in \\I$ in this group we have $h_i \\leq \\es N \\leq \\epsilon^3 N$ and $w_i \\leq \\es N \\leq \\epsilon N$ (by adequate choice of $f(\\cdot)$ when applying Lemma~\\ref{lem:rec_noint}).\n\t\n\tTherefore, the total area of the small items packed into a box of type 1 (or type 2) is at most $(1-\\epsilon)\\es N^2$. Since there are at most $C_{boxes}(\\epsilon)$ of these groups, the total area of all of them is at most $C_{boxes}(\\epsilon)(1-\\epsilon)\\es N^2 \\leq (1-\\epsilon)\\epsilon^2N^2$. Here, again we use that we choose an appropriate $f(\\cdot)$ when removing intermediate items. Observe that this is possible since $f(\\cdot)$ will only depend on $\\epsilon$. In fact, we choose $f(x):= \\frac{x}{c(\\epsilon)}$ for some constant $c(\\epsilon)\\geq C_{boxes}(\\epsilon)$. Using an LP-argument similar to the one used for the boxes of type 3, we can now transform this box into a single $\\S$-box. Note that if one of the $\\S$-boxes of width less than $\\es N$ also contained vertical items we may now treat this box as a $\\V$-box.\n\t\n\tRepeating these operations for each box of type 1, 2 or 3 results in a packing using $O_\\epsilon(1)$ many $\\H$-, $\\V$-, $\\L$- and $\\S$-boxes containing a total profit of at least $(1/2-O(\\epsilon))OPT(\\I)$. Hence, this packing satisfies properties i), ii) and v) of the lemma. \n\t\n\tWe now proceed, to make sure that the packing also satisfies property iv). Consider a $\\V$-box $B$ such that the property does not hold. Then, we can split $B$ into at most $C_{boxes}(\\epsilon)$ many $\\V$-boxes for which the property holds. To do this let $h^{(1)},\\dots,h^{(c)}$ be the distinct heigths of $\\V$-boxes in non-decreasing order.\n\tThen, let $B(h^{(1)})$ be the sub-area of $B$ containing items of height at most $h^{(1)}$, we now turn this sub-area into its own $\\V$-box. Similarly, for each $j=2,\\dots,c$ we look at the sub-area of $B$ containing items with heigth at most $h^{(j)}$ and strictly larger than $h^{(j-1)}$ and turn this sub-area into its own $\\V$-box. Since $c \\in O_\\epsilon(1)$ this\n\t{increases the number of boxes by at most a factor $O_\\epsilon(1)$.}\n\t\n\tFinally, we adapt the packing such that it also satisfies property iii). To this end, let $k_{1},\\dots,k_r$ with $r \\leq C_{boxes}(\\epsilon)$ be the distinct widths of $\\H$- and $\\L$-boxes in non-decreasing order and set $k_0 := 0$. Now, for each $\\H$-box $B$, we do the following. Let $B_j$ be the sub-box of $B$ containing only items with $w_i \\in (k_{j-1},k_j]$. Then, $B_j$ is a $\\H$-box as well. Repeating this for all horizontal boxes leads to at most $O_\\epsilon(1)$ many $\\H$-boxes satisfying property iii). For $\\L$-boxes we make sure that each $\\L$-box is exactly as wide as the item placed inside of it. Lastly, define $U(\\epsilon):=\\{i\\cdot \\frac{\\epsilon}{2(C_{boxes}(\\epsilon))}: i=1,\\dots, \\frac{2(C_{boxes}(\\epsilon))}{\\epsilon}\\}$, where $C_{boxes}(\\epsilon)$ is the number of boxes. For each box $B$, we round its height up to $\\lceil h_B \\cdot \\frac{\\epsilon}{2(C_{boxes}(\\epsilon)+1)} \\rceil N$. As there are at most $C_{boxes}(\\epsilon)$ boxes on top of each other and we still have a free space with height $\\epsilon/2 N$ at the top of $K$ the packing remains feasible after this rounding.\n\t\n\tThus, we derived two packings satisfying properties i), ii), iii) and iv) such that one of them also satisfies property v).\n\\end{proof}\n\n\\subsubsection{Computing a packing}\nWe now describe how to compute a packing based on Lemma~\\ref{lem:struc_rectangles}. We follow a similar structure as our algorithm for hypercubes. First, we guess some basic quantities of the packing.\nThen, we find an implicit packing of vertical items into $\\V$- and $\\S$-boxes as well an implicit packing of small items into $\\S$-boxes. Finally, we use {the} indirect guessing framework to find a packing of horizontal and large items into $\\S$-,$\\H$- and $\\L$-boxes. Let $\\B$ denote the set of boxes of the structured packing and $\\B_\\H$, $\\B_\\V$, $\\B_\\L$ and $\\B_\\S$ denote the set of $\\H$-, $\\V$-, $\\L$- and $\\S$-boxes in $\\B$, respectively. {In the following, we assume that\n\t$\\min\\{\\alpha_B: \\alpha_B \\in U(\\epsilon)\\} \\ge \\epsilon\\cdot\\epsilon_{\\mathrm{small}}$. Since $U(\\epsilon)$ is a universal set depending only on $\\epsilon$, we can ensure this by choosing $c(\\epsilon)$ accordingly.}\n\nFor the remainder of this section, we partition the set of items into profit classes, where a profit class is defined as $\\P_{t}:= \\{i \\in \\I: p_i \\in [(1+\\epsilon)^{t},(1+\\epsilon)^{t+1})]\\}$. Let $p_{\\min}$ and $p_{\\max}$ denote the smallest and largest profit of items in $\\I$. We may disregard all items with profit less than $\\epsilon \\frac{p_{\\max}}{n}$ (while only losing a profit of at most $\\epsilon p_{\\max} \\leq \\epsilon OPT$) such that $p_{\\min} \\geq \\epsilon \\frac{p_{\\max}}{n}$. Hence, $t \\in \\mathcal{T}_P:=\\{\\lfloor p_{\\min}\\rfloor_{1+\\epsilon}, \\dots,\\lceil p_{\\max}\\rceil_{1+\\epsilon}\\}$ with $|\\mathcal{T}_P| \\in O_\\epsilon(\\log n)$. Furthermore, we define $\\widehat{p}(t)  = (1+\\epsilon)^{t+1}$ {for each $t$ which is hence} the rounded profit of items in profit class $\\P_{t}$.\n\n\n\\paragraph{Guessing basic quantities.}\nFirst, we guess how many boxes of each type there are in $\\B$. This amounts to a total of $O_{\\epsilon}(1)$ many possibilities. For each of the boxes $B$ we guess its height. Since $|U(\\epsilon)|\\le O_\\epsilon(1)$ there are only $O_\\epsilon(1)$ possibilities for each of these heights. Note that each box has a height of at least $\\frac{\\el}{\\epsilon}N$.\n\nSince for a rectangle the height and the width can be different, guessing the widths of the $\\S$-boxes is more difficult than it was in the setting of hypercubes. We will explain later how we do this.\n\n\nWe do not know the values $k_{1},k_{2},\\dots,k_{r}$; however, we\nknow that they yield a partition of $\\I$ into sets $\\I_{j}:=\\{i\\in\\I:w_{i}\\in(k_{j-1},k_{j}]\\}$\nfor each $j\\in[r]$ where for convenience we define $k_{0}:=0$. We\nguess approximately the profit that each set $\\I_{j}$ contributes\nto $\\OPT$. Formally, for each $j\\in[r]$ we guess $\\hat{p}(j):=\\left\\lfloor p(\\I_{j}\\cap\\OPT)\\right\\rfloor _{1+\\epsilon}$\nif $p(\\I_{j}\\cap\\OPT)\\ge\\frac{\\epsilon}{r}\\OPT$ and $\\hat{p}(j):=0$\notherwise. Observe that for $\\hat{p}(j)$ there are at most $O_{\\epsilon,d}(\\log n)$\npossibilities since $\\OPT\\in[p_{\\max},n\\cdot p_{\\max})$ and hence\n$\\hat{p}_{j}\\in\\{0\\}\\cup[\\frac{\\epsilon}{r}p_{\\max},n\\cdot p_{\\max})$.\nAlso, one can show that $\\sum_{j=1}^{r}\\hat{p}(j)\\ge(1-O(\\epsilon))\\OPT$.\n\nNow, for each $\\H$- or $\\L$-box $B$, we guess the value $j_B$. This needs a total amount of $O_\\epsilon(1)$ many guesses. \nObserve that each $\\H$- or $\\L$-box $B\\in\\B$ can contain only items from\n$\\I_{j_{B}}$. However, each $\\S$-box $B\\in\\B$ might contain items\nfrom more than one set $\\I_{j}$. For each $\\S$-box $B\\in\\B$ and\neach set $\\I_{j}$, we guess approximately the fraction of the area of\n$B$ that is occupied by items from $\\I_{j}$. Formally, for each\nsuch pair we define the value $a_{B,j}:=\\frac{\\sum_{i\\in\\I(B)\\cap\\I_{j}}\\lceil h_{i}\\rceil_{1+\\epsilon}\\lceil w_{i}\\rceil_{1+\\epsilon}}{h_Bw_B}$\nand guess the value $\\hat{a}_{B,j}:=\\left\\lceil \\frac{a_{B,j}}{\\epsilon/(r+2)}\\right\\rceil \\epsilon/(r+2)$,\ni.e., the value $a_{B,j}$ rounded up to the next larger integral\nmultiple of $\\epsilon/r$. Note that for each value $\\hat{a}_{B,j}$\nthere are only $O_{\\epsilon}(1)$ possibilities, and that there\nare only $O_{\\epsilon}(1)$ such values. Additionally, $\\S$-boxes may contain small items and vertical items (which by definition are not part of any $\\I_j)$. Therefore, we also guess values $\\hat{a}^{\\mathrm{small}}_{B}$ and $\\hat{a}^{\\mathrm{vert}}_{B}$ defined similarly as above. For the correct guesses we have that $\\left(\\sum_{j=1}^r \\hat{a}_{B,j} + \\hat{a}^{\\mathrm{small}}_{B} + \\hat{a}^{\\mathrm{vert}}_{B} \\right)h_Bw_B   \\leq (1-3\\epsilon)h_Bw_B$ for each $\\S$-box $B$ to account for possibly needing more space later. Overall, the rounded values may lead to less profit than in the structured packing. However, we can still guarantee a profit of at least a $(1+O(\\epsilon))$ ratio of the profit achieved by the structured packing. \n\nWe now distinguish between two cases in order to {guess} the widths of $\\S$-boxes. We first consider the case that there is at least one $\\S$-box containing a horizontal or large item, i.e. $\\hat{a}_{B,j} > 0$ for some $B \\in \\B_\\S$ and some $j$. In this case we guess the width of this box {up to a factor of $1+\\epsilon$} in time $O_{\\epsilon}(1)$ since it is at most $N$ and at least $\\frac{\\el}{\\epsilon} N$. Let $w^*$ denote this width and consider all small items\n\n\nof width less than $\\frac{\\epsilon}{n}w^*$. Their total width is at most $\\epsilon w^*$ {and hence they} can be placed next to each other in a shelf of height $\\el N$ using a shifting argument we can replace other items in this box while losing only an $\\epsilon$-fraction of the profit. Thus, all remaining {small} items have width at least $\\frac{\\epsilon}{n}w^*$ implying that {for} each remaining $\\S$-box {its} width {is} in the interval $[\\frac{\\epsilon}{n}w^*,w^*]$; {hence, it } can be guessed in time $O(\\log_{1+\\epsilon} n)$ {up to a power of} $1+\\epsilon$. We will explain later how to handle the other case.\n\n\nNext, we will describe our indirect guessing framework which is used to pack horizontal and large items.\n\n\\paragraph{Indirect guessing framework: Packing horizontal and large items into $\\H$-, $\\L$- and $\\S$-boxes.}\nNext, we explain how to compute a packing of large and horizontal items into $\\H$-, $\\L$- and $\\S$-boxes. We proceed similarly to the indirect guessing framework used in the hypercube setting based on the framework introduced in~\\cite{heydrich2019faster}. We will describe the indirect guessing framework for the case that we guessed the approximate width of $\\S$-boxes already using the trick explained above. If this is not the case, we can ignore the $\\S$-boxes in the indirect guessing framework since they only contain small and vertical items; we will pack the latter items later.\n\nThe goal is to determine the values $k_{1},k_{2},\\dots,k_{r}$.\nUnfortunately, we cannot guess them directly in polylogarithmic time,\nsince there are $N$ options for each of them. Instead, we define $\\tilde{k}_{0}:=0$ and compute values $\\tilde{k}_{1},\\tilde{k}_{2},\\dots,\\tilde{k}_{r}$\nthat we use instead of the values $k_{0},k_{1},k_{2},\\dots,k_{r}$.\nThis yields a partition of $\\I$ into sets $\\tilde{\\I}_{j}:=\\{i\\in\\I:w_{i}\\in(\\tilde{k}_{j-1},\\tilde{k}_{j}]\\}$.\nIntuitively, for each $j$ we want to pack items from $\\tilde{\\I}_{j}$\ninto the space that is used by items in $\\I_{j}$ in the packing from\nLemma~\\ref{lem:struc_rectangles}. We will choose the values $\\tilde{k}_{1},\\tilde{k}_{2},\\dots,\\tilde{k}_{r}$\nsuch that in this way, we obtain almost the same profit. On the other\nhand, we will ensure that $\\tilde{k}_{j}\\le k_{j}$ for each $j\\in[r]$.\n\nWe work in $r$ iterations. We define $\\tilde{k}_{0}:=0$. Suppose\ninductively that we have determined $\\ell$ values $\\tilde{k}_{1},\\tilde{k}_{2},\\dots,\\tilde{k}_{\\ell}$\nalready for some $\\ell\\in\\{0,1,...,r-1\\}$ such that $\\tilde{k}_{\\ell}\\le k_{\\ell}$.\nWe want to compute $\\tilde{k}_{\\ell+1}$. We can assume w.l.o.g.~that\n$k_{\\ell+1}$ equals $w_{i}$ for some item $i\\in\\I$. We do binary\nsearch on the set $W(\\ell):=\\{w_{i}:i\\in\\I\\wedge w_{i}>\\tilde{k}_{\\ell}\\}$,\nusing our rectangle data structure. For each candidate value $w \\in W(\\ell)$,\nwe estimate the possible profit due to items in $\\tilde{\\I}_{\\ell+1}$\nif we define $\\tilde{k}_{\\ell+1}:=w$. We want to find such a value\n$w$ such that the obtained profit from the set $\\tilde{\\I}_{\\ell+1}$\nequals essentially $\\hat{p}(\\ell+1)$. In the following, we denote by $\\B_{\\H}$, $\\B_{\\L}$ and $\\B_{\\S}$, the set of $\\H$-, $\\L$- and $\\S$-boxes in $\\B$, respectively.\n\nWe describe now how we estimate the obtained profit for one specific\nchoice of $w\\in W(\\ell)$. We try to pack items from $\\tilde{\\I}_{\\ell+1}(w):=\\left\\{ i\\in\\I:w_{i}\\in(\\tilde{k}_{\\ell},w]\\right\\} $\ninto\n\\begin{itemize}\n\\item the $\\H$-boxes $B\\in\\B_{\\H}$ for which $j_{B}=\\ell+1$,\n\\item the $\\L$-boxes $B\\in\\B_{\\L}$ for which $j_{B}=\\ell+1$ and\n\\item the $\\S$-boxes, where for each $\\S$-box $B\\in\\B_{\\S}$, we use an area\nof $\\widehat{a}_{B,\\ell+1}\\cdot h_Bw_B$ and ensure that we pack\nonly items $i\\in\\tilde{\\I}_{\\ell+1}(w)$ for which $w_{i}\\leq 2\\epsilon \\widehat{w}_B$ and $h_i \\leq \\epsilon h_B$.\n\\end{itemize}\nWe solve this subproblem approximately via the following integer program\n$(\\mathrm{IP}(w))$. Intuitively, we treat items equally if they have\nalmost the same profit and almost the same height and width, up to a factor $1+\\epsilon$,\nrespectively. To this end, we use the notion of height and width classes.  Formally, we define a height class $H_{t'}=\\{i\\in \\tilde{\\I}_{\\ell+1}(w):h_{i}\\in[(1+\\epsilon)^{t'},(1+\\epsilon)^{t'+1})\\}$\nfor each $t'\\in \\mathcal{T}_H:= \\{\\lfloor \\log_{1+\\epsilon}(h_{\\min})\\rfloor,\\dots,\\lceil \\log_{1+\\epsilon}(h_{\\max})\\rceil\\}$. Furthermore, we define a width class $W_{t^{''}}=\\{i\\in \\tilde{\\I}_{\\ell+1}(w):w_{i}\\in[(1+\\epsilon)^{t^{''}},(1+\\epsilon)^{t^{''}+1})\\}$ for each  $t^{''} \\in \\mathcal{T}_W:= \\{\\lfloor \\log_{1+\\epsilon}(\\tilde{k}_\\ell)\\rfloor,\\dots,\\lceil \\log_{1+\\epsilon}(w)\\rceil\\}$.\nWe denote by $\\hat{h}(t'):=(1+\\epsilon)^{t'+1}$ and $\\hat{w}(t^{''}):=(1+\\epsilon)^{t^{''}+1}$ the rounded height and width, respectively. Furthermore, we define a set of triplets $\\mathcal{T}:=\\{(t,t',t''): t \\in \\mathcal{T}_P \\wedge t' \\in \\mathcal{T}_H \\wedge t^{''} \\in \\mathcal{T}_W\\}$. For each triplet $(t,t',t^{''}) \\in \\mathcal{T}$, $n_{t,t',t^{''}}$ denotes the number of items of profit class $\\P_t$, height class $H_{t'}$ and width class $W_{t^{''}}$. Losing only a factor of $(1+O(\\epsilon))$, the subproblem is equivalent to selecting how many items of each combination of profit, height and width class will be packed into each box which we can formulate as the following IP. We denote by $\\B_\\H(\\ell+1)$ the $\\H$-boxes for which $j_B = \\ell+1$ and by $\\B_\\L(\\ell+1)$ the $\\L$-boxes for which $j_B = \\ell+1$. Furthermore, by $\\B(\\ell+1)$ we denote the set of all relevant boxes for this iteration of the indirect guessing framework.\n\n\\begin{alignat*}{3}\n\t(\\mathrm{IP}(w))\\quad& \\text{max} \t& \\displaystyle \\sum_{(t,t',t^{''}) \\in \\mathcal{T}}\\sum_{B \\in \\B(\\ell+1)} x_{t,t',t''',B} p(t)\t\t\t& \t\t\t& \\quad & \\\\\n\t& \\text{s.t.} & \\displaystyle  \\sum_{(t,t',t^{''}) \\in \\mathcal{T}} x_{t,t',t'',B} \\hat{h}(t')\t& \\leq h_B& \t\t& \\forall B \\in \\B_{\\H}(\\ell+1) \\\\\n\t&  & \\displaystyle  \\sum_{(t,t',t^{''}) \\in \\mathcal{T}} x_{t,t',t'',B} \t& \\leq 1& \t\t& \\forall B \\in \\B_{\\L}(\\ell+1) \\\\\n\t& & \\displaystyle \\sum_{(t,t',t^{''}) \\in \\mathcal{T}} x_{t,t',t'',B} \\hat{h}(t')\\hat{w}(t'')\t& \\leq a_{B,\\ell+1}h_B\\widehat{w}_B\t& \t\t& \\forall B \\in \\B_{\\S}\\\\\n\t&\t\t\t\t& \\displaystyle\\sum_{B \\in \\B(\\ell+1)} x_{t,t',t'',B}\t\t\t\t\t\t\t\t& \\leq n_{t,t',t''}\t& \t\t& \\forall  (t,t',t^{''}) \\in \\mathcal{T} \\\\\n\t&\t\t\t\t& x_{t,t',t'',B}\t\t\t\t\t\t\t\t& \\in \\mathbb{N}_{0}& \t\t&\\forall (t,t',t^{''}) \\in \\mathcal{T}, B \\in \\B(\\ell+1)\\\\\n\\end{alignat*}\n\nWe are now faced with the same challenges as in the case of hypercubes. We cannot solve $(\\mathrm{IP}(w))$ or its LP-relaxation directly since we might have $\\mathrm{poly}(\\log N)$ many variables. However, we now show how to solve it approximately losing only a factor of $1+\\epsilon$. We remark that the computed solution is not an explicit packing since for each triplet $(t,t',t'')$ we compute how many items of profit class $\\P_t$, height class $H_{t'}$ and width class $W_{t^{''}}$ we want to select for each box but not the identities of the selected items. This would not be possible in time $O_{\\epsilon}(\\log(n))$ since the solution might consist of $\\Omega(n)$ items\n\n\\begin{lemma}\\label{lem:rec_IP_sol}\nThere is an algorithm with a running time of {$(\\log_{1+\\epsilon}(n))^{O(1)}$}\nthat computes an $(1{+}O(\\epsilon))$-approximate solution for \\textup{$(\\mathrm{IP}(w))$};\nwe denote by $q(w)$ the value of this solution. For two values $w,w'$\nwith $w\\le w'$ we have that $q(w)\\le q(w')$.\n\\end{lemma}\n\\begin{proof}\nWe will compute an approximate solution $(\\mathrm{IP}(w))$, by first guessing the $2C_{boxes}(\\epsilon)/\\epsilon$ most profitable items of set $\\tilde{\\I}_{\\ell+1}(w)$ in the solution.To do this, we again first guess the profit class of each of these items with a total number of $\\log_{1+\\epsilon}^{O_{\\epsilon}(1)}n$ many guesses. For each profit class $t$ this gives value $n_t^g \\in O_{\\epsilon}(1)$ indicating the number of items guessed from this profit class. Next, for each profit class we find the $n_t$ items of smallest height. We can do this in time $O(\\log^4 n)$ using our rectangle data structure (see Lemma~\\ref{lem:data-structure-rec}) and the balanced binary search tree used to store the distinct item heights. Observe that, we only lose a factor of $(1+\\epsilon)$ of the profit by taking these items. Now it remains to guess for each of these items which box it must be packed in which can be done in time $O_\\epsilon(1)$. We denote this guessed solution by $S^g$. Next, we need to update the right hand-sides of $(\\mathrm{IP}(w))$.  For an $\\H$-box $B$, let $h^g_B$ be the total height of all guessed items. Similarly, for an $\\S$-box $B$ let $\\mathrm{area}^g_B$ be the total area of the guessed items. For $\\L$-boxes, we denote by $n^g_B$ the number of items guessed for $B$ (which is either $0$ or $1$). Then, the following IP remains.\n\\begin{alignat*}{3}\n\t(\\mathrm{IP}'(w))\\quad& \\text{max} \t& \\displaystyle \\sum_{(t,t',t^{''}) \\in \\mathcal{T}}\\sum_{B \\in \\B(\\ell+1)} x_{t,t',t''',B} p(t)\t\t\t& \t\t\t& \\quad & \\\\\n\t& \\text{s.t.} & \\displaystyle  \\sum_{(t,t',t^{''}) \\in \\mathcal{T}} x_{t,t',t'',B} \\hat{h}(t')\t& \\leq h_B-h^{g}_B& \t\t& \\forall B \\in \\B_{\\H}(\\ell+1) \\\\\n\t&  & \\displaystyle  \\sum_{(t,t',t^{''}) \\in \\mathcal{T}} x_{t,t',t'',B} \t& \\leq 1-n^{g}_B& \t\t& \\forall B \\in \\B_{\\L}(\\ell+1) \\\\\n\t& & \\displaystyle \\sum_{(t,t',t^{''}) \\in \\mathcal{T}} x_{t,t',t'',B} \\hat{h}(t')\\hat{w}(t'')\t& \\leq a_{B,\\ell+1}h_B\\widehat{w}_B\t - \\mathrm{area}^{g}_B& \t\t& \\forall B \\in \\B_{\\S}\\\\\n\t&\t\t\t\t& \\displaystyle\\sum_{B \\in \\B(\\ell+1)} x_{t,t',t'',B}\t\t\t\t\t\t\t\t& \\leq n_{t,t',t''}\t& \t\t& \\forall  (t,t',t^{''}) \\in \\mathcal{T} \\\\\n\t&\t\t\t\t& x_{t,t',t'',B}\t\t\t\t\t\t\t\t& \\in \\mathbb{N}_{0}& \t\t&\\forall (t,t',t^{''}) \\in \\mathcal{T}, B \\in \\B(\\ell+1)\\\\\n\\end{alignat*}\n\nNext, we restrict the number of variables by restricting the number of height and width classes. {To this end, observe that since we only consider horizontal and large items, we know that there are $O_{\\epsilon}(1)$ many relevant width classes and for large items we also only have $O_{\\epsilon}(1)$ many height classes. Thus, we need to restrict the number of height classes for for horizontal items. Therefore, let $B \\in \\B_\\H$ be the horizontal box of maximum height, denoted by $h^*$.\n\t{Since the set $U(\\epsilon)$ is a universal set that depends only on $\\epsilon$ and we can require that $\\el$ is small\n\t\tcompared to the values in $U(\\epsilon)$, we can ensure that $B$ has a height\n\t\tof}\n\t\n\tat least $\\frac{\\el}{\\epsilon}N$. Thus, consider all horizontal items of height less than $\\frac{\\el}{n}N$. By reserving a space of of $\\el N$ in each of the horizontal boxes we may pack all these very thin items while losing only a factor of $1+\\epsilon$ in the profit.}\n\n\nIf we consider the appropriate height and width classes in $(\\mathrm{IP'}(w))$, we now have an IP with $\\log_{1+\\epsilon}^{O(1)} n$ many variables. Let $S^f(w)$ be an optimal solution to the LP-relaxation of this IP. By the rank lemma~\\cite{lau2011iterative} $S^f(w)$ has at most $2C_{boxes}(\\epsilon)$ many fractional non-zero variables and the profit corresponding to each of them is smaller than the profit of any item in the guessed solution $S^g(w)$. Thus, by rounding down these values we lose at most a profit of $\\epsilon(p(S^g(w)))$. Therefore, combining this rounded solution with $S^g(w)$ and all tiny items we discarded to reduce the number of variables, we can find an approximate solution to $(\\mathrm{IP}(w))$. Finally, consider two values $w\\leq w'$. Then, the approximate solution obtained to $(\\mathrm{IP}(w))$ is also feasible for $(\\mathrm{IP}(w'))$. Therefore, $q(w) \\leq q(w')$.\n\\end{proof}\n\nWe define $\\tilde{k}_{\\ell+1}$ to be the smallest value $w\\in W(\\ell)$\nfor which $q(w)\\ge (1-\\epsilon)\\hat{p}(\\ell+1)$. Based on this and the inductive assumption that $\\tilde{k}_{\\ell}\\le k_{\\ell}$ we can prove the following statement.\n\\begin{lemma}\\label{lem:rec_induc_kr}\nWe have that $\\tilde{k}_{\\ell+1}\\le k_{\\ell+1}$.\n\\end{lemma}\n{We run our indirect guessing framework for $O_{\\epsilon}(1)$ iterations, and hence obtain a packing of horizontal and large items into $\\H$-, $\\L$- (and possibly $\\S$-boxes).}\n\n\n\\paragraph{Packing vertical items into $\\V$- and $\\S$-boxes.}\n{Before explaining how to pack vertical and small items, we now show how to guess the width of $\\S$-boxes in case this was not done before the indirect guessing framework. To this end, we first guess the widths of $\\V$-boxes as follows. We start by guessing the $2C_{boxes}(\\epsilon)/\\epsilon$ most profitable vertical items in the packing (this will also be useful for charging profit of items we discard later). We do this following the same technique as before. First, we guess the profit type of each of these items in time $O(\\log_{1+\\epsilon}n)$ and {also the box that} it must be assigned to (this gives an indication on the height {of the item}). Then, for each profit class and each interval of heights $(h^{(j-1)},h^{(j)}]$ defined by the heights of $\\V$-boxes, we now know how many items to pick. Losing only a factor of $(1+\\epsilon)$ of the profit we may choose the items in non-decreasing order of widths. We next guess the total width and how it is split among the $\\V$-boxes. To do this, we again guess the profit type of the item packed into the remaining space of $\\V$-boxes whose width is maximal in time $O(\\log_{1+\\epsilon}n)$. Let $t$ be the this profit type and let $n_t^g(\\I_{\\V}(j))$ be the number of items from $\\I_{\\V}(j)$ of this profit type which should be in our final packing. We can guess this value as a power of $(1+\\epsilon)$ in time $O(\\log_{1+\\epsilon}n)$. By losing only a profit of $(1+\\epsilon)$ we can assume that we may choose the  $n_t^g(\\I_{\\V}(j))$ narrowest (lowest width) items of this profit class. Therefore, we find the width of the $n_t^g(\\I_{\\V}(j))$-th narrowest of these items in time $O(\\log^4 n)$;\nwe denote by $w^*$ its width.\n\n\nIf this item is packed into a $\\V$-box then we know that the total width of the remaining $\\V$-boxes is within the range $[w^*,nw^*]$ and can be guessed up to a factor of $(1+\\epsilon)$ in time $O(\\log_{1+\\epsilon}n)$. If, however, this item is packed into an $\\S$-box we may assume that all items of width {at most} $\\epsilon \\frac{w^*}{n}$ can also be packed into this $\\S$-box. Therefore, the total remaining width of the $\\V$-boxes must be in the range $[\\frac{\\epsilon}{n} w^*,nw^*]$ and can be guessed up to a factor of $(1+\\epsilon)$ in time $O(\\log_{1+\\epsilon}n)$. Now, using this guessed value of the total remaining width of the $\\V$-boxes, we can guess the {approximately the individual width $\\hat{w}_B$ of each $\\V$-box $B$\n\tin time $(\\log_{1+\\epsilon}n)^{O_\\epsilon(1)}$}. We are now ready to guess the widths of the $\\S$-boxes as powers of $(1+\\epsilon)$. Consider the $\\S$-box $B^*$ of maximum width.\n{Imagine that we make $B^*$ greedily wider as much as possible such that we possibly push other boxes on the left of $B^*$ to the left towards the left edge of the knapsack, and similarly we push boxes on the right of $B^*$ to the right towards the right edge of the knapsack. Once we cannot make $B^*$ wider anymore, there is a set of boxes\n\t$\\B'$ with $B^* \\in \\B'$ such that $\\sum_{B\\in \\B'}w_B =N$.\n\tLet $w(\\mathrm{fixed})$ be the total width of $\\L$-,$\\H$- and $\\V$-boxes in $\\B'$ for which we already know the widths and let $n_\\S$ the number of $\\S$-boxes in $\\B'$}. Then, we know that the width of $B^*$ is in the interval $[\\frac{N-w(\\mathrm{fixed})}{n_\\S},N-w(\\mathrm{fixed})]$ and hence it can be guessed in time $O(\\log_{1+\\epsilon}n)$ {up to a factor of $1+\\epsilon$}. Let $\\widehat{w}_{B^*}$ denote {the guessed width}. Then, by our assumption on the minimum height of the boxes, we know that all items of width less than $\\frac{\\epsilon}{n}\\widehat{w}_{B^*}$ have a total area of at most $\\epsilon$ times the area of $B^*$. Therefore, by reserving space for these items we can pack them all in $B^*$, {by removing some other items from $B^*$ such that we lose only a factor of $1+\\epsilon$ in the profit}. From this it follows that for all other $\\S$-boxes {$B'$, each item packed into $B'$} has width at least $\\frac{\\epsilon}{n}\\widehat{w}_{B^*}$ and at most $\\epsilon \\widehat{w}_{B^*}$ and, thus, the widths {of the other $\\S$-boxes} can be guessed in time $O(\\log_{1+\\epsilon}n)$.\n}\n\n\nLet $\\I_{\\V}$ denote the set of vertical items. It is important to observe that vertical items are only contained in $\\V$-boxes and $\\S$-boxes. We will now prove the following statement. Furthermore, observe that due to the structured packing and since we guessed the height of each $\\V$-box we know a specific range for the height of items which we may pack into each $\\V$-box. More specifically, let $\\B_{\\V}$ denote the set of $\\V$-boxes in $\\B$ and let $h^{(1)},\\dots,h^{(c)}$ be the set of distinct heights of boxes in $\\B_{\\V}$ in non-decreasing order and set $h_0:=0$. Then, we know that each $\\V$-box of heigth $h^{(j)}$ contains items with height in the interval $(h^{(j-1)},h^{(j)}]$ only. Denote by $\\B_{\\V}(j)$ the set of $\\V$-boxes of height $h_j$ and let $\\I_{\\V}(j):= \\{i \\in \\I_{\\V}: h_i \\in (h^{(j-1)},h^{(j)}]\\}$. For each $\\S$-box we can now guess a value $\\hat{a}^{\\mathrm{vert}}_{B,,j} := \\left\\lceil \\frac{a_{B,\\V,j}}{\\epsilon/(c)}\\right\\rceil \\epsilon/(c)$ where $a^{\\mathrm{vert}}_{B,j}:=\\frac{\\sum_{i\\in\\I(B)\\cap\\I_{\\V}(j)}\\lceil h_{i}\\rceil_{1+\\epsilon}\\lceil w_{i}\\rceil_{1+\\epsilon}}{h_Bw_B}$. We only consider guesses satisfying $\\sum_{j=1}^c \\hat{a}^{\\mathrm{vert}}_{B,j} = \\hat{a}^{\\mathrm{vert}}_{B}$.\nWe will now show how to find a packing of vertical items for some range $(h^{j-1},h^j]$. We then apply this procedure to each range individually.\n\\begin{lemma}\\label{lem:rec_lp_vert}\nThere is an algorithm with a running time of {$(\\log_{1+\\epsilon}n)^{O_{\\epsilon}(1)}$}\nthat computes an $(1{+}O(\\epsilon))$-approximate packing of items $\\I_{\\V}(j)$.\n\\end{lemma}\n\\begin{proof}\nWe have already guessed the widths of every box and the $2C_{boxes}(\\epsilon)/\\epsilon$ most profitable vertical items in the packing. We will proceed by finding an approximate solution for the remaining packing using similar ideas as before. First, we define a width class $W_{t'}:=\\{i\\in \\I_{\\V(j)}:w_{i}\\in[(1+\\epsilon)^{t'},(1+\\epsilon)^{t'+1})\\}$ with rounded-up width $\\widehat{w}(t') =(1+\\epsilon)^{t'+1}$. We denote by $\\mathrm{W}$ the set of all width classes and define $\\mathcal{T}_W :=\\{t': \\mathcal{W}_{t'} \\in \\mathrm{W}\\}$. For each pair $(t,t') \\in \\mathcal{T}:=\\{(t,t'):t \\in \\mathcal{T}_P \\wedge t' \\in \\mathcal{T}_W\\}$ we treat items of profit class $\\P_t$ and width class $W_{t'}$. Using similar arguments as before we can show that we can restrict ourselves to $O(\\log_{1+\\epsilon}n)$ many width classes by either reserving an $\\epsilon$-fraction of the width of the $\\V$-boxes or an $\\epsilon$-fraction of the area of $\\S$-boxes. Thus, to find the remaining packing of vertical items, we can consider the following LP which finds a fractional implicit packing of the remaining items from. An optimal solution to this LP again loses a factor of $1+\\epsilon$ of the profit and it leaves space to add all omitted items.\n{\n\t\\small\t\n\t\\begin{alignat*}{3}\n\t\t(\\mathrm{LP}(j))\\quad& \\text{max} \t& \\displaystyle \\sum_{(t,t') \\in \\mathcal{T}}\\sum_{B \\in  \\B_{\\V}(j)} x_{t,t',B} \\widehat{p}(t)\t\t\t& \t\t\t& \\quad & \\\\\n\t\t& \\text{s.t.} & \\displaystyle \\sum_{(t,t') \\in \\mathcal{T}} x_{t,t',B} \\widehat{w}(t')\t& \\leq (1-\\epsilon)\\widehat{w}_B& \t\t& \\forall B \\in \\B_{\\V}(j) \\\\\n\t\t& \\text{s.t.} & \\displaystyle \\sum_{(t,t') \\in \\mathcal{T}} x_{t,t',B} \\widehat{w}(t')\t& \\leq \\widehat{a}_{B,\\V,j}(1-\\epsilon)h_Bw_B - \\mathrm{area}(S^g(j))& \t\t& \\forall B \\in \\B_{\\S} \\\\\n\t\t&\t\t\t\t& \\displaystyle\\sum_{B \\in  \\B_{\\V}(j)} x_{t,t',B}\t\t\t\t\t\t\t\t& \\leq n_{t,t'}\t& \t\t& \\forall  (t,t') \\in \\mathcal{T}\\\\\n\t\t&\t\t\t\t& x_{t,t',B}\t\t\t\t\t\t\t\t& \\geq 0& \t\t&\\forall (t,t') \\in \\mathcal{T}, B \\in \\B_{\\V}(j)\\\\\n\t\\end{alignat*}\n}\n\nLet $S^f(j)$ be the optimal fractional solution to this remaining LP which we can find in time\n$(\\log_{1+\\epsilon}n)^{O_\\epsilon(1)}$~\\cite{cohen2021solving}. We now combine $S^g(j)$ and $S^f(j)$ to yield a feasible fractional solution to $\\mathrm{LP}(j))$.\n\n\nDue to the rank lemma~\\cite{lau2011iterative} {we have} at most $2C_{boxes}(\\epsilon)$ fractional non-zero variables; {each item corresponding to such a fractional variable has less profit than any previously guessed item.}\n\nTherefore, by rounding down these variables we only lose a profit of at most $\\epsilon p(S^g(j))$ (where $p(S^g(j))$ is the profit due to the guessed items). Finally, combining the integral solution with all tiny items that were discarded to restrict our necessary number of guesses, we find an implicit $(1+O(\\epsilon)$-{approximate}\npacking of vertical items of the set $\\I_{\\V}(j)$. It is important to note that using our rectangle data structure (see Lemma~\\ref{lem:data-structure-rec}.) we can find the number of tiny items as well as an $(1+\\epsilon)$-approximation of the total profit of tiny items in time $O(\\mathrm{poly}(\\log n))$.\n\\end{proof}\n\nAs there are at most $O_{\\epsilon}(1)$ many different values of $j$, we have the following result.\n\\begin{lemma}\\label{lem:rec_lp_vert_all}\nThere is an algorithm with a running time of {$(\\log_{1+\\epsilon}(n))^{O_{\\epsilon}(1)}$}\nthat computes an $(1{+}O(\\epsilon))$-approximate packing of the items $\\I_{\\V}$ into the guessed $\\V$-boxes $\\B_\\V$. \n\\end{lemma}\n\n\\paragraph{Packing small items into $\\S$-boxes.}\nAs small items are only packed into $\\S$-boxes, we may also pack them separately. Hereto, we use the same ideas as for the vertical items. Let $\\I_{s}$ denote the set of small items. {At this point, we already know the guessed width of the $\\S$-boxes as well as that all remaining small items have width at least $\\frac{\\epsilon}{n}w^*_{\\S}$ and at most $\\epsilon w^*_{\\S}$, where $w^*_{\\S}$ is the maximum width of all $\\S$-boxes. Thus, we can show the following result using the same techniques as before.}\n\\begin{lemma}\\label{lem:rec_lp_small}\nThere is an algorithm with a running time of {$(\\log_{1+\\epsilon}(n))^{O_{\\epsilon}(1)}$}\nthat computes an $(1{+}O(\\epsilon))$-approximate packing of the items $\\I_{s}$ into the guessed $\\S$-boxes $\\B_{\\S}$.\n\\end{lemma}\n\\begin{proof}\nAgain, we start by guessing the $2C_{boxes}(\\epsilon)/\\epsilon$ most profitable items from \n$\\I_{s}$ in our packing. We do this as follows. For each of these items we first guess the profit class which we can do in time $(\\log_{1+\\epsilon}n)^{O_\\epsilon(1)}$. For each profit class $\\P_t$ this gives us a value $n_t \\in O_{\\epsilon}(1)$. Now for each profit class we can find the $n_t$ items of lowest width in this profit class in time $O(\\log^4 n)$ using our rectangle data structure (see~Lemma~\\ref{lem:data-structure-rec}) in combination with the balanced binary search tree storing all possible width values. Note that we may assume that the structured packing also uses this set of items by only losing a factor of $(1+\\epsilon)^{-1}$. Finally, for each of these items we guess which box it must be assigned to which can be done in time $O_{\\epsilon}(1)$. This results in a preliminary packing $S^g$ of $C_{boxes}/\\epsilon$ items of the set $\\I_{\\S}$. For each $\\S$-box we also obtain a value $\\mathrm{area}(S^g)$ which is now already occupied.\n\nWe will now show how to compute the remaining packing. To this end, we again use the notion of width and height classes (of the small items now). Formally, we define a height class $H_{t'}=\\{i\\in \\I_s:h_{i}\\in[(1+\\epsilon)^{t'},(1+\\epsilon)^{t'+1})\\}$\nfor each $t'\\in \\mathcal{T}_H= \\{\\lfloor \\log_{1+\\epsilon}(h_{\\min}(\\I_s))\\rfloor,\\dots,\\lceil \\log_{1+\\epsilon}(h_{\\max}(\\I_s))\\rceil\\}$. Furthermore, we define a width class $W_{t^{''}}=\\{i\\in \\I_s:w_{i}\\in[(1+\\epsilon)^{t^{''}},(1+\\epsilon)^{t^{''}+1})\\}$ for each  $t^{''} \\in \\mathcal{T}_W = \\{\\lfloor \\log_{1+\\epsilon}(w_{\\min}(\\I_s))\\rfloor,\\dots,\\lceil \\log_{1+\\epsilon}(w_{\\max}(\\I_s))\\rceil\\}$.\nWe denote by $\\hat{h}(t'):=(1+\\epsilon)^{t'+1}$ and $\\hat{w}(t^{''}):=(1+\\epsilon)^{t^{''}+1}$ the rounded height and width, respectively.  For each triplet $(t,t',t^{''}) \\in \\mathcal{T}:=\\{(t,t',t^{''}):t \\in \\mathcal{T}_P \\wedge t' \\in \\mathcal{T}_H\\wedge t^{''} \\in \\mathcal{T}_W\\}$ we treat items of profit class $\\P_t$, height class $H_{t'}$ and width class $W_{t^{''}}$ as identical.\nLastly, we let $n_{t,t',t^{''}}$ denote the number of items of profit class $\\P_t$, height class $H_{t'}$ and width class $W_{t^{''}}$ for each triplet $(t,t',t^{''}) \\in \\mathcal{T}$. Consider $B \\in \\B_\\S$ such that $a^{\\mathrm{small}}_Bh_Bw_B$ is maximal. We already know that the number of width classes is $O(\\log_{1+\\epsilon}n)$. Hence, we need to restrict the number of height classes. Which can be done using the same techniques used to restrict the number of width classes by discarding some tiny items and reserving space for them in each of the $\\S$-boxes. For the remaining items we consider the following LP which gives a fractional $(1+O(\\epsilon))$-approximation for the remaining packing.\n\n{\n\\footnotesize\n\t\\begin{alignat*}{3}\n\t\t(\\mathrm{LP}(\\S))\\quad& \\text{max} \t& \\displaystyle \\sum_{(t,t',t^{''}) \\in \\mathcal{T}}\\sum_{B \\in \\B_\\S} x_{t,t',t''',B} p(t)\t\t\t& \t\t\t& \\quad & \\\\\n\t\t& &\\displaystyle\\sum_{(t,t',t^{''}) \\in \\mathcal{T}} x_{t,t',t'',B} \\hat{h}(t')\\hat{w}(t'')\t& \\leq (1-\\epsilon)a^{\\mathrm{small}}_{B}h_Bw_B - \\mathrm{area}(S^g)\t& \t\t& \\forall B \\in \\B_\\S \\\\\n\t\t&\t\t\t\t& \\displaystyle\\sum_{B \\in  \\B_\\S} x_{t,t',t'',B}\t\t\t\t\t\t\t\t& \\leq n_{t,t',t''}\t& \t\t& \\forall (t,t',t^{''}) \\in \\mathcal{T} \\\\\n\t\t&\t\t\t\t& x_{t,t',t'',B}\t\t\t\t\t\t\t\t& \\geq 0& \t\t&\\forall (t,t',t^{''}) \\in \\mathcal{T}, B \\in  \\B_\\S\\\\\n\t\\end{alignat*}\n}\n\nWe can now argue in the same direction as we did for the packing of vertical items. Let $S^f$ be the optimal fractional solution to this remaining LP which we can find in time $\\log_{1+\\epsilon}^{O_\\epsilon(1)}n$~\\cite{cohen2021solving}. We now combine $S^g$ and $S^f$ by rounding down all fractional non-zero variables which by the rank lemma~\\cite{lau2011iterative} are at {most} $C_{boxes}(\\epsilon)$ {many}.\nSince we have at least $2C_{boxes}(\\epsilon)/\\epsilon$ integral variables, due to our guessing scheme and the fact that each of the fractional variables\n{corresponds to a set of items for which each item is at most as profitable as each previously guessed item,}\n\nthe total profit of the fractional variables is at most $\\epsilon p(S^g)$ ({where $p(S^g)$ is the profit due to the guessed items)}.\nFinally, combining the integral solution with all tiny items that were discarded to restrict our necessary number of guesses, we find an implicit $(1+O(\\epsilon)$-{approximate} packing of small items.\nAgain, by using our rectangle data structure (see Lemma~\\ref{lem:data-structure-rec}) we can find the number of tiny items as well as an $(1+\\epsilon)$-approximate guess of the total profit of tiny items in time $\\mathrm{poly}(\\log n)$. \n\\end{proof}\n\nCombining the algorithms to compute implicit packings of small and vertical items with the indirect guessing framework, we can prove Theorem~\\ref{thm:rec_nr}.\n\n\n\\begin{proof}[Proof of Theorem~\\ref{thm:rec_nr}]\nWe first prove the first part of the theorem.\n\nObserve that since there is only a constant number of boxes, we can guess their packing into the knapsack in constant time, {once we have determined their heights and widths}.\nOur approximation algorithm proceeds in two stages.\n\\begin{enumerate}[(A)]\n\\item  \\textit{Guessing basic quantities:} The total number of guesses is $(\\log n )^{O_{\\epsilon}(1)}$. Additionally, since the number of boxes is a constant, we can guess {their relative arrangement in the knapsack}\n\nin constant time.\n\\item \\textit{Indirect guesing framework and construction of packing:} \n\\begin{enumerate}[(i)]\n\t\\item \\textit{Packing of horizontal and large items into $\\H$, $\\L$- and $\\S$-boxes}: We need $r \\in O_{\\epsilon,d}(1)$ iterations of the indirect guessing framework, leading to solutions $x^*(1),\\dots,x^*(r)$ to the LP-relaxations of $(\\mathrm{IP}(\\tilde{k}_{1})),...,(\\mathrm{IP}(\\tilde{k}_{r}))$. This takes time $(\\log_{1+\\epsilon}n)^{O_\\epsilon(1)}$.  Let $\\hat{x}(1),\\dots,\\hat{x}(r)$ be the rounded solutions to $(\\mathrm{IP}(\\tilde{k}_{1})),...,(\\mathrm{IP}(\\tilde{k}_{r}))$. These can be found in time $(\\log_{1+\\epsilon}(n))^{O_{\\epsilon}(1)}$ due to Lemma~\\ref{lem:rec_IP_sol}. In order to compute an explicit packing of $\\L$-boxes, observe that for each triplet $(t,t',t'')$ the preliminary packing only indicates the number of large items chosen of of profit class $\\P_t$, height class $H_{t'}$ and width class $W_{t''}$. Again, we may choose these items in non-increasing order of profits in time  $O_{\\epsilon}(n+\\log^3 n)$ and assign them to the correct $\\L$-boxes in time $O_\\epsilon(n)$. For $\\H$-boxes, we proceed similarly and stack items on top of each other in non-increasing order of widths. This can be done in time $O(n\\log n)$ for each box. Finally, for $\\S$-boxes, we need to remark that again we may choose any set of items such that for each triplet $(t,t',t'')$ the correct number of items is chosen. Thus, we again may choose the items in non-increasing order of profits and assign the correct number of items for each triplet to each box. This way we only lose a factor of $(1+\\epsilon)$ of the profit for each $\\S$-box and the selected items still satisfy the conditions necessary to pack them into $\\S$ using NFDH. \n\t\\item \\textit{Packing of vertical items:} \tHere, we compute packings of vertical items into $\\V$- and $\\S$-boxes using Lemma~\\ref{lem:rec_lp_vert_all}. For each combination of profit, width and height class, indicated by {a tuple} $(t,t',t'')$, this gives us a value $z_{t,t',t''}$ indicating the number of vertical items of profit class $\\P_t$, height class $H_{t'}$ and width class $W_{t''}$ we must choose for our packing. We may choose them in non-increasing order of profits. Afterwards, we must assign the correct number of items to each of the boxes. We may now construct the packing of vertical boxes by packing items from left to right in non-increasing order of heights. The correct set of items can be found in time $O(n+\\log^3 n)$ using our rectangle data structure. The packing of $\\V$-boxes can be done in time~$O(n)$.\n\t\\item \\textit{Packing of small items:} \tHere, we compute the packing of small items into $\\S$-boxes using Lemma~\\ref{lem:rec_lp_small}. Similar as before, all that is left is to choose the correct number of items from each profit class. Again we select the in non-increasing order of profits and assign the correct number to each box. This process can be done in time $O(n +\\log^3(n))$ using our rectangle data structure. Finally, we can pack each $\\S$-box in time $O(n \\log n)$ using NFDH.\n\\end{enumerate}\n\\end{enumerate}\n\nThis yields a total running time of $O(n\\cdot(\\log^4 n))+(\\log n)^{O_{\\epsilon}(1)}$ since all items can be inserted in time $O(n \\log^4 n)$ to initialize the data structure.\n\nNext, we will prove the second statement of the theorem regarding the dynamic algorithm. The insertion and deletion of items in time $O(\\log^4 n)$ is due to our rectangle data structure. To output a $(2+\\epsilon)$-approximate solution $|ALG|$ in time $O(|ALG|\\cdot(\\log n))+(\\log n)^{O_{\\epsilon}(1)}$, we use the algorithm described above with a refinement of the running time since we will choose at most $|ALG|$ items. If one queries for a $(2+\\epsilon)$-estimate of the optimal value, we invoke the algorithm above without the construction of the actual packings, i.e., we only compute the preliminary packings using profit, height and width classes, which can be done in time $(\\log n)^{O_{\\epsilon}(1)}$. Observe that for the tiny items we now need to compute an estimate of the total profit, which we can do in time $O_\\epsilon(\\log^3n)$ using our rectangle data structure\nby querying for the number of items and then multiplying this with the rounded profit. Finally, if one wishes to query whether an item $i \\in \\I$ is contained in the solution $ALG$, we compute the preliminary packings in time $(\\log n)^{O_{\\epsilon}(1)}$. Let $(t,t',t'')$ be such that $i$ is of profit type $\\P_t$, height type $H_{t'}$ and width type $W_{t''}$ and let $z_{t,t',t''}$ be the total number of items of these types in the preliminary packing. Note that the choice of the triplet $(t,t',t'')$ may depend on which type of item $i$ is.\nThen by the construction of our packing we only must check whether $i$ is among the first $z_{t,t',t''}$ when items are ordered in non-increasing order of profits. This can be done using our rectangle data structure by counting the number of items of classes $(t,t',t'')$ that have smaller profit than $i$ which we can do with a single range counting query in time $O(\\log n)$ If this value is at least $z_{t,t',t''}$, we answer the query with ``no\" and otherwise with ``yes''. This ensures that we give consistent answers between two consecutive updates of the set $\\I$.\n\\end{proof}\n\n\n\n\\subsection{Rotations allowed}\\label{app:rec_r}\nWe consider now the case where it is allowed to rotate the input rectangles\nby 90 degrees and first give a high-level overview of our results. For this case, we present even a $(17/9+\\epsilon)$-approximation.\nAs for the case without rotations, we argue that there is an \\emph{easily guessable packing}\nbased on $\\L$-, $\\H$-, $\\V$- and $\\S$-boxes.\nSince the items can be rotated, we allow now both horizontal and vertical items\nto be assigned to $\\H$- and $\\V$-boxes.\nThere might be\nbe a special (intuitively very large) $\\L$-box $B^{*}$ of width $N$.\nAlso, a special case arises when the packing consists of at most three items.\n\\begin{lemma}[Near-optimal packing of rectangles with rotations]\n\t\\label{lem:struc_rectangles-rotation} There exists a packing\n\twith a total profit of at least $(9/17-O(\\epsilon))\\OPT$,\n\twhich consists of at most three items or which satisfies all of\n\tthe following properties: \n\t\\begin{enumerate}[i)]\n\t\t\\item it consists of a set of boxes $\\B$ where $\\B$ is bounded\n\t\tby a value $C_{\\mathrm{boxes}}(\\epsilon)$ depending only on~$\\epsilon$,\n\t\t\\item each box in $\\B$ is a $\\L$-, $\\H$-, $\\V$- or $\\S$-box,\n\t\t\\item possibly, there is an $\\L$-box $B^{*}$ that is declared as \\emph{special} and that\n\t\tsatisfies\n\t\t\n\t\t\n\t\t$w_{B^{*}}=N$; if there is no special box we define for convenience $h_B^* :=0$,\n\t\t\\item there is a universal set of values $U(\\epsilon)$ with $|U(\\epsilon)|=O_{\\epsilon}(1)$,\n\t\tvalues $k_{0},k_{1},k_{2},\\dots,k_{r}\\in\\mathbb{Z}_{\\geq0}$ with\n\t\t$k_{0}=0$ and $r\\in O_{\\epsilon}(1)$, and a value $j_{B}\\in\\{1,2,\\dots,r\\}$\n\t\tfor each box $B\\in\\B\\setminus\\{B^{*}\\}$ such that\n\t\t\\begin{enumerate}\n\t\t\t\\item if $B$ is a $\\H$-box, then \n\t\t\t\n\t\t\t\t\n\t\t\t\t$h_{B}=\\alpha_{B}\\cdot (N-h_{B^*}) $ for some $\\alpha_{B}\\in U(\\epsilon)$,\n\t\t\t\t$w_{\\min}(B)=k_{j_{B}-1}$, and \\mbox{$w_{\\max}(B)=k_{j_{B}}$},\n\t\t\t\t\n\t\t\t\t\n\t\t\t\\item if $B$ is a $\\V$-box, then \n\t\t\t\n\t\t\t\t\n\t\t\t\t\n\t\t\t\t\n\t\t\t\t$h_{B}=\\alpha_{B}\\cdot (N-h_{B^*}) $ for some $\\alpha_{B}\\in U(\\epsilon)$, \n\t\t\t\t\n\t\t\t\t\n\t\t\t\t\n\t\t\t\\item if $B$ is an $\\L$-box and $B \\ne B^*$, then $h_{B}=\\alpha_{B}\\cdot (N-h_{B^*}) $ \n\t\t\tfor some $\\alpha_{B}\\in U(\\epsilon)$ and the unique item packed inside $B$ satisfies \\mbox{$w_{i}=k_{j_{B}}$}.\n\t\t\\end{enumerate}\n\t\t\\item let $h^{(1)},\\dots, h^{(c)}$ be the distinct heigths of $\\V$-boxes in non-decreasing order such that for each $j = 1,\\dots,c$ we have that\n\t\t\\begin{enumerate}\n\t\t\t\\item each $\\V$-box of height $h^{(1)}$ only contains items of heigth at most $h^{(1)}$ and\n\t\t\t\\item each $\\V$-box of height $h^{(j)}$ only contains items of height at most $h^{(j)}$ and strictly larger than $h^{(j-1)}$ for $j=2,\\dots,c$\n\t\t\t\\item for each $\\H$-box, $k_{j_B} \\leq h^{(j)}$ and $k_{j_B-1} \\geq h^{(j-1)}$ for some $j = 1,\\dots,c$,\n\t\t\\end{enumerate}\n\t\t\n\t\t\n\t\t\n\t\t\\item if $B$ is an $\\S$-box, then $w_B \\in \\Omega(\\es(N-h_{B^*}))$.\n\t\\end{enumerate}\n\\end{lemma}\n\nUsing Lemma~\\ref{lem:struc_rectangles-rotation},\nwe compute an $(17/9+O(\\epsilon))$-approximate packing using our indirect\nguessing framework. Our algorithm is technically more involved\nthan our algorithm in the setting without rotations. For example,\nto the  $\\H$- and $\\V$-boxes we can assign both horizontal and vertical items;\nfor such\n\ninput items, we do not know into which box type we need to assign it.\n\n\n\nAlso,\nthere is possibly the special $\\L$-box~$B^{*}$. For $B^{*}$ we\ncannot estimate its height via the\n\nvalues in $U(\\epsilon)$.\n\nInstead, we first estimate the sizes of all other boxes, guess their\narrangement inside of the knapsack, and then place $B^{*}$ into the\nremaining space. Finally, we devise an extra routine for the special case that the packing consists of at most three items.\n\n\\theoremrecrot*\n\n\nIn the following, we present the detailed proofs of Lemma~\\ref{lem:struc_rectangles-rotation} and Theorem~\\ref{thm:rectangles_rot}. To do this, we need some useful results from the literature. One of these results is the resource augmentation packing lemma (see Lemma~\\ref{lem:rec_rs_augment}) which we also used when rotations are not allowed. The next result we need is the so-called resource contraction lemma introduced by Galvez et al.~\\cite{galvez2021approximating}. We say that an item $i \\in \\I$ is \\emph{massive} if $h_i \\geq (1-\\epsilon)N$ and $w_i \\geq (1-\\epsilon N)$. The resource contraction lemma states the following in the absence of such a massive item.\n\n\\begin{lemma}[\\cite{galvez2021approximating}]\\label{lem:rec_resource_contr}\nIf a set of items $\\I$ does not contain a massive item and can be packed in a box of size $N \\times N$, then it is possible to pack a set $\\I'$ of profit at least $\\frac{1}{2}p(\\I)$ into a box of size $N \\times (1-\\frac{\\epsilon}{2})N$ (or into a box of size $(1-\\frac{\\epsilon}{2})N \\times N$) if rotations are allowed.\n\\end{lemma}\n\nThe third result we make use of later is due to Steinberg~\\cite{steinberg1997strip} and gives an area based packing guarantee of rectangles.\n\\begin{theorem}\nWe are given a set of rectangles $\\I$ and a box of size $w \\times h$. Let $h_{\\max}\\leq h$ and $w_{\\max}\\leq w$ denote be the maximum width and height among the items in $\\I$, respectively. Then all items in $\\I$ can be packed into the box if\n\\[\n2\\sum_{i \\in \\I}h_iw_i \\leq hw - \\max\\{2h_{\\max}-h,0\\}\\max\\{2w_{\\max}-w,0\\}.\n\\]\n\\end{theorem}\nWe will now prove Lemma~\\ref{lem:struc_rectangles-rotation}. Again, we use Lemma~\\ref{lem:rec_noint} such that $c(\\epsilon)\\cdot \\es \\leq \\epsilon^2$ for a large constant $c(\\epsilon) \\geq 1/\\epsilon$.\n\n\\begin{proof}[Proof of Lemma~\\ref{lem:struc_rectangles-rotation}]\nLet $\\OPT$ be an optimal packing. Assume first that there is \\emph{no massive item} in $\\OPT$. In the following, we will construct three candidate packings and later argue that one of them contains a profit of at least $(\\frac{7}{19}-O(\\epsilon))\\OPT$.\n\n\\textbf{Candidate packing $\\mathrm{A}$:} Consider a ring of width $\\frac{\\epsilon}{32}N-\\frac{\\epsilon^2}{64}N$ in the knapsack. Let $\\OPT_{ring}$ be the items contained in this ring and $\\OPT_{inner}$ be all other items. We now apply the resource contraction lemma to $\\OPT_{inner}$. More precisely, we use it to find a subset of items of $\\OPT_{inner}$ which can be packed into box of height $(1-\\epsilon/2)N$ and width $N$ such that their profit is at least $1/2p(\\OPT_{inner})$. Similar as in the case without rotation, we now use the resource augmentation lemma (Lemma~\\ref{lem:rec_rs_augment}) with $\\epsilon_{ra} = \\frac{\\epsilon - 2\\epsilon^2}{2-\\epsilon} \\leq \\epsilon$. This, allows us to find a packing with profit $(1/2-O(\\epsilon))\\OPT_{inner}$ inside a box of height $(1-\\epsilon/4-2\\epsilon^2)$ and width $N$. {The additional space will help us later to round up the heights of boxes and pack all items packed into $\\S$-boxes of width at most $\\es N$ into a single $\\S$-box at the top of the knapsack.} Using the same arguments as in the proof of Lemma~\\ref{lem:struc_rectangles} we can transform this packing into one using a constant number of $\\V$-, $\\L$-, $\\H$ and $\\S$-boxes satisfying the properties of our structured packing inside the area $[0,N]\\times [0,(1-\\epsilon/4)N]$. Now, we consider $\\OPT_{ring}$. Observe that the total area of the items packed into the outer ring is at most the area of the ring which is\n$\\frac{\\epsilon}{8}N^2 - \\frac{\\epsilon^2}{256}N^2.$ Furthermore, we can rotate all items such that $h_{\\max} \\leq \\frac{\\epsilon}{32}N-\\frac{\\epsilon^2}{64}N < \n\\frac{\\epsilon}{4}N-\\frac{\\epsilon^2}{128}N$ and $w_{\\max} \\leq N$. Therefore, by Steinberg's theorem all items which were packed into the outer ring can be packed into the area $[0,N]\\times [(1-\\epsilon/4)N,(1-\\epsilon^2/128)N]$. Finally, using the resource augmentation packing lemma with an appropriate choice of $\\epsilon_{ra}$ and our transformation described in the proof of Lemma~\\ref{lem:struc_rectangles}, we can find a packing of profit $(1-O(\\epsilon))\\OPT_{ring}$ using a constant number of $\\V$-, $\\L$-,$\\H$- and $\\S$-boxes satisfying properties i), ii), iii) and iv) of our structured packing. Observe that the $\\V$-boxes already satisfy property v). Thus, we only need to modify the $\\H$-boxes such that they satisfy property v) as well. To this end, we split each $\\H$-box into a constant number of $\\H$-boxes and update the $k_r$ values. To ensure property vi), as described in the proof of Lemma~\\ref{lem:struc_rectangles}, we consider all $\\S$-boxes of width less than $\\es N$. The total volume of these items is at most $C_{boxes}(\\epsilon) \\es N^2$ and, hence, by an appropriate choice of $\\es$ we know that these can be packed into the area at the top of the knapsack of height $\\epsilon^2 N$ such that this gives one additional $\\S$-box. In this way, we ensure that for each $\\S$-box its height and width are at least $\\es N$.\nThus, we find a packing with profit at least\n\\[\n\\left(1-O(\\epsilon)\\right)\\left(\\frac{1}{2}\\OPT_{inner}+\\OPT_{ring}\\right).\n\\]\n\n\\textbf{Candidate packing $\\mathrm{B}$:} {The reader may imagine that} $\\OPT_{ring} = 0$, implying that there are no items contained inside the outer ring of the knapsack. Let $S_{left}$, $S_{right}$, $S_{top}$ and $S_{bottom}$ be strips of width or height $\\frac{\\epsilon}{32}-\\frac{\\epsilon^2}{64}$ on the left, right, top and bottom of the knapsack, respectively. We first define crossing items which are items that touch both $S_{left}$ and $S_{right}$ or $S_{top}$ and $S_{bottom}$ {but which do not intersect with more than two of the strips}. We may assume w.l.o.g. that all crossing items are packed horizontally and we may move these items to the bottom of the knapsack. They can be packed into an $\\H$-box of width $N$ since there are no items packed to the left and right of them (recall that $\\OPT_{ring} = 0$).\n{Thus, the remaining items are packed into the area $[0,N] \\times [N',N]$ where $N'$ is the total height of the moved crossing items.\nAccordingly, the strip $S_{bottom}$ corresponds now to the area $[0,N] \\times [N',N' + (\\frac{\\epsilon}{32}-\\frac{\\epsilon^2}{64})\\cdot N]$, but it is still disjoint from $S_{top}$.}\n\nNow, we consider the remaining items such that each of them intersects at most two of the four strips and there at most {four} items that intersect two of the strips (at the corners defined by the intersections of the strip boundaries). Let $\\OPT_{corner}$ denote these corner items and $\\OPT_{rest}$ denote the items which intersect with at most one strip. We now choose one of the strips uniformly at random and delete all items intersecting it. The remaining items can now be packed into the knapsack with a free space of width (height) $\\epsilon/32 \\cdot N$\n{since we deleted the items intersecting one of the strips. This guarantees a}\n\nprofit of\n\\[\n\\frac{3}{4} \\OPT_{rest} + \\frac{1}{2}\\OPT_{corner}.\n\\]\nUsing the resource augmentation packing lemma (with an appropriate choice of $\\epsilon_{ra}$ {to enable us to round up the heights of the boxes\n{and still keep a strip of width $N$ and height $\\Omega(\\epsilon N)$ empty)}}\nwe transform\n\nthe resulting containers into $\\V$-, $\\L$-,$\\H$- and $\\S$-boxes results in a packing satisfying all properties of our structured packing except for property vi.). To ensure that property v) holds for all $\\H$-boxes we again split them according to the intervals given by the distinct heights of $\\V$-boxes. Our obtained profit is at least\n\\[\n\\left(1-O(\\epsilon)\\right) \\left(  \\frac{3}{4} \\OPT_{rest} + \\frac{1}{2}\\OPT_{corner} \\right).\n\\]\nAgain, in a final step, similar to what we described in the proof of Lemma~\\ref{lem:struc_rectangles}, we remove all $\\S$-boxes of height or width less than $\\es N$ and pack their items into a single $\\S$-box at the top of the knapsack of height $\\Omega(\\epsilon N)$.\n\n\\textbf{Candidate packing $\\mathrm{C}$:} {The reader may imagine that $\\OPT_{rest} = 0$ and that hence there are only the corner items}. Then, we either keep all of them or three out of four. This yields a profit of at least\n\\[\n\\frac{3}{4}\\OPT_{corner}.\n\\]\n\n\n\n\nWe now choose the best out of the three candidate packings. Observe that $\\OPT_{inner}+\\OPT_{ring}=1$ and $\\OPT_{rest}+\\OPT_{corner} = \\OPT_{inner}$.\n{Therefore, in any case we obtain a\n\nprofit of at least $(\\frac{7}{19}-O(\\epsilon))\\OPT$.}\n\nNext, assume that there is a single massive item in $\\OPT$. We now construct two candidate packings. In the following, we denote $\\OPT_{massive}$ as the solution containing only the massive item and $\\OPT_{rest}$ as the solution containing all other items.\n\n\\textbf{Candidate Packing $\\mathrm{Massive A}$}: In this packing, we take out the massive item. Observe that the remaining items (up to rotation) can be packed into a knapsack of width $N$ and height at most $4\\epsilon N$. Thus, we can apply the resource augmentation packing lemma to find a packing with profit at least\n\\[\n(1-O(\\epsilon))\\OPT_{rest}.\n\\]\nAccounting for the fact that we need to round up the height of boxes, we choose $\\epsilon_{ra}$ such that the resulting packing (before rounding) can be packed into \na knapsack of height $(1-(\\epsilon/2-2\\epsilon^2)N$ and width $N$. We can then use the same arguments as in the proof of Lemma~\\ref{lem:struc_rectangles} to find the desired packing. In addition to these arguments we use the final step to remove all small $\\S$-boxes similar to this procedure in \\textbf{Candidate Packing $\\mathrm{A}$} or  \\textbf{Candidate Packing $\\mathrm{B}$} and modify the $\\H$-boxes such that they satisfy property v).\n\n\\textbf{Candidate Packing $\\mathrm{Massive B}$}: Next, we find a packing which combines the massive item with a subset of the other items. Again let $S_{left}$, $S_{right}$, $S_{top}$ and $S_{bottom}$ be the strips of the knapsack on the left, right, top or bottom, respectively, which are not filled by the massive item. We will now construct a packing in a similar way as in~\\cite{galvez2021approximating} with slight modifications necessary to have a packing using our four types of boxes as well as taking into account the rounding of the height of each box. We consider the strip with the largest amount of profit among the four strips (note that strips may share some profit). But there is one strip such that the profit of all items contained inside this strip is at least\n\\[\n\\OPT_{strip} \\geq \\frac{1}{4}\\OPT_{rest}.\n\\]\nWe may assume w.l.o.g. that this is one of the {horizontal} strips ($S_{top}$ or $S_{bottom}$) as otherwise we may rotate the massive item and the chosen strip such that the strip is {horizontally} positioned {above or below} the massive item. In particular,\n{we can assume that}\nthe massive item $i$ is now packed into the area $[0,N]\\times [N-h_i,N]$ and the items that were previously packed into the chosen strip are packed into the area $[0,N]\\times [0,N]$. We now treat this area as a single knapsack and use the proof of Lemma~\\ref{lem:struc_rectangles} to find a packing into constantly many $\\V$-,$\\H$-,$\\L$- and $\\S$-boxes satisfying the properties of our structured packing such that the profit is at least\n\\[\n\\left(\\frac{1}{2}-O(\\epsilon)\\right) \\OPT_{strip} \\geq  \\left(\\frac{1}{8}-O(\\epsilon)\\right)\\OPT_{rest}.\n\\] \n{When invoking Lemma~\\ref{lem:struc_rectangles}, we make sure that the height of the resulting boxes are rounded up to values {of the form} $\\alpha_B (N-h_i)$; {also, we make sure that\nthere is enough space at the top of the knapsack to move all items in very small $\\S$-boxes (i.e., boxes $B $ with width\n$w_B \\in O({\\es(N-h_{B^*}}))$}\n\nthere.}\nHence, this packing in combination with the massive item which we place in a $\\L$-box of height $N$ yields a profit of at least\n\\[\n\\OPT_{massive}+\\left(\\frac{1}{8}-O(\\epsilon)\\right)\\OPT_{rest}.\n\\]\nUsing the fact that $\\OPT_{massive}+\\OPT_{rest} = 1$, chosing the most profitable of the two candidate packings yields a guarantee of approximately $(\\frac{8}{15}-O(\\epsilon))\\OPT$.\n\\end{proof}\n\n\\subsubsection{Computing a packing}\nIn the following, we explain how to compute a packing corresponding to Lemma~\\ref{lem:struc_rectangles-rotation}, {the goal being to prove} Theorem~\\ref{thm:rectangles_rot}. In the following, we assume w.l.o.g. (due to rotation) that there are no vertical items such that we only have small, horizontal and large items. A lot of the steps needed to compute our packing are similar to the ideas used in the non-rotational case. In the following, we will focus on the key differences.\nFirst, we will explain how to compute a packing of at most three items.\n\\begin{lemma}\nIf the structured packing with profit at least $(9/17-O(\\epsilon))\\OPT$ consists of at most $3$ items, it can be computed in time $(\\log_{1+\\epsilon}n)^{{O(1)}}$.\n\\end{lemma}\n\\begin{proof}\nLet $i_1, i _2$ and $i_3$ denote the items in the packing. {Since we have at most three items, one can show that\nthere is one item that we can we can assign to an $\\L$-box of height $N$ such that this box does not intersect any of the other two items.\nW.l.o.g. suppose that $i_1$ is this item.} Then, we guess in time $O(\\log_{1+\\epsilon}n)$ the profit type of $i_1$. Losing only a factor of $1+\\epsilon$ of the profit we now choose the item of this profit type with the smallest width or height (due to rotation) which we can find in time $O(\\log^3 n)$ using our rectangle data structure and a binary search. This will leave enough space for the other two items. For each of these items we can again guess the profit class in time $O(\\log_{1+\\epsilon}n)$ and the orientation of the item which will then indicate whether we take the item with the smallest width or smallest height in this profit class. Using our rectangle data structure we can do this in time $O(\\log^3 n)$.\n\\end{proof}\nWe now proceed with how to compute a packing corresponding to Lemma~\\ref{lem:struc_rectangles-rotation} if there are more than three items. In this case, we use the following structure resembling our results for hypercubes and rectangles without rotation. We first guess some basic quantities. Then, we pack the small items into $\\S$-boxes. Then, we use our indirect guessing framework to pack horizontal and large items into $\\H$-$\\V$-,$\\L$- and $\\S$-boxes.\n\n\\paragraph{Guessing basic quantities.} We start by guessing similar quantities as in the setting without rotation. For completeness, we give the details here. We again disregard all items with profit less than $\\epsilon \\frac{p_{\\max}}{n}$ such that we have $O(\\log_{1+\\epsilon}n)$ profit classes of the form $\\P_{t}:= \\{i \\in \\I: p_i \\in [(1+\\epsilon)^{t},(1+\\epsilon)^{t+1})]\\}$.\n\n\n\nNext, we guess how many boxes of each type there are in $\\B$. This amounts to a total of $O_{\\epsilon}(1)$ many possibilities. For each of the boxes $B$ we guess its height by guessing $\\alpha_B$, using that $|U(\\epsilon)|\\le O_{\\epsilon}(1)$.\n\n\n\nThe remaining quantities are the same as in the non-rotational setting with a few exceptions. We do not need to guess values $\\hat{a}_{B,\\V}$ for an $\\S$-box $B$ since we now treat all vertical items as horizontal items which will be packed in the indirect guessing framework. For each $\\S$-box $B\\in\\B$, we guess an estimate of the width $w_B$. Formally, for each $\\S$-box $B\\in\\B$, we guess the value $\\left\\lfloor w_B \\right\\rfloor _{1+\\epsilon}$. Observe that there are $O_\\epsilon(1)$ many since for each $\\S$-box we know that its width is in the interval $[\\es N, N]$ and $\\es$ is bounded from {below by some constant depending only on $\\epsilon$. Furthermore, consider the distinct heights of $\\V$-boxes denoted by $h^{(1)},\\dots, h^{(c)}$ and $h^{(0)}:=0$. For each range $(h^{(j-1)},h^{(j)}]$ we guess the total profit of horizontal items with width in this range used in $\\V$-boxes in the structured packing. More, precisely for each $j=1,\\dots,c$ we guess a value $\\hat{p}_{\\V}(j)$ as a power of $1+\\epsilon$ which we can do in time $O_\\epsilon(1)$.\n\n\\paragraph{Packing small items into $\\S$-boxes.} Observe that in contrast to the non-rotational setting, we now know the guessed widths of $\\S$-boxes in advance and can, therefore, start by packing small items into $\\S$-boxes. For this step, we proceed in a similar fashion as in the non-rotation case with the only exception that now a small item $i$ may be placed into an $\\S$-box $B$ if $w_i \\leq \\epsilon w_B$ and $h_i \\leq \\epsilon h_B$ or $h_i \\leq \\epsilon w_B$ and $w_i \\leq \\epsilon h_B$. Let $\\I_s$ be the set of small items and $\\B_\\S$ be the set of $\\S$-boxes. Again, we use the notion of height and width classes. We define a height class $H_{t'}=\\{i\\in \\I_s:h_{i}\\in[(1+\\epsilon)^{t'},(1+\\epsilon)^{t'+1})\\}$\nfor each $t'\\in \\mathcal{T}' = \\{\\lfloor \\log_{1+\\epsilon}(h_{\\min}(\\I_s))\\rfloor,\\dots,\\lceil \\log_{1+\\epsilon}(h_{\\max}(\\I_s))\\rceil\\}$. Furthermore, we define a width class $W_{t^{''}}=\\{i\\in \\I_s:w_{i}\\in[(1+\\epsilon)^{t^{''}},(1+\\epsilon)^{t^{''}+1})\\}$ for each  $t^{''} \\in \\mathcal{T}^{''} = \\{\\lfloor \\log_{1+\\epsilon}(w_{\\min}(\\I_s))\\rfloor,\\dots,\\lceil \\log_{1+\\epsilon}(w_{\\max}(\\I_s))\\rceil\\}$.\n\nWe denote by $\\hat{h}(t'):=(1+\\epsilon)^{t'+1}$ and $\\hat{w}(t^{''}):=(1+\\epsilon)^{t^{''}+1}$ the rounded height and width, respectively. Let $\\mathcal{T}:= \\{(t,t',t''): t \\in \\mathcal{T_P} \\wedge t' \\in \\mathcal{T}_H \\wedge t'' \\in \\mathcal{T}_W\\}$. We denote by $n_{t,t',t^{''}}$ the number of items corresponding to the tuple $(t,t',t^{''})$. Now, we compute a packing based on the following IP. We denote by {$\\I(B_\\S)$} the set of small items which may be packed into box $B$. In the following, IP we only consider variables for which the height class and width class is compatible with a box $B$. In particular, we only consider a variable $x_{t,t',t'',B}$ if $\\hat{h}_{t'} \\leq 2\\epsilon h_B$ and $\\hat{w}_{t''} \\leq 2\\epsilon w_B$ or $\\hat{h}_{t'} \\leq 2\\epsilon w_B$ and $\\hat{w}_{t''} \\leq 2\\epsilon h_B$.\n\n{\n\\small\n\\begin{alignat*}{3}\n(\\mathrm{IP}(\\S))\\quad& \\text{max} \t& \\displaystyle \\sum_{(t,t',t'') \\in \\mathcal{T}}\\sum_{B \\in \\B_\\S} x_{t,t',t'',B} p(t)\t\t\t& \t\t\t& \\quad & \\\\\n& &\\displaystyle \\sum_{(t,t',t'') \\in \\mathcal{T}} x_{t,t',t'',B} \\hat{h}(t')\\hat{w}(t'')\t& \\leq a^{\\mathrm{small}}_{B}h_Bw_B & \t\t& \\forall B \\in \\B_\\S(\\ell+1) \\\\\n&\t\t\t\t& \\displaystyle\\sum_{B \\in  \\B_\\S} x_{t,t',t'',B}\t\t\t\t\t\t\t\t& \\leq n_{t,t',t''}\t& \t\t& \\forall  (t,t',t'') \\in \\mathcal{T} \\\\\n&\t\t\t\t& x_{t,t',t'',B}\t\t\t\t\t\t\t\t& \\in \\mathbb{N}_{0}& \t\t&\\forall t  (t,t',t'') \\in \\mathcal{T}, B \\in  \\B_\\S\n\\end{alignat*}\n}\nWe now show how to find a $(1+O(\\epsilon))$-approximate implicit solution to the problem of packing small items into $\\S$-boxes using the same techniques as used in the non-rotational setting. More specifically, we first guess the most profitable items to gain a partial integral solution to $(\\mathrm{IP}(\\S))$. Then, we show how we can restrict the number of relevant width and heigth classes to $O(\\log_{1+\\epsilon}n)$ each such that we can solve the remaining LP in time $(\\log_{1+\\epsilon}n)^{O(1)}$~\\cite{cohen2021solving}. {Here, we again use the fact that all\n{$\\S$-boxes} are sufficiently high such that all small items whose height is much less than that of the {highest $\\S$-box} can be packed into this box, while only losing a {factor of $1+\\epsilon$ with respect to the\nprofit of a fractional packing of items into this box.}} Lastly, we take {an optimal solution to this LP} and round all fractional non-zero variables down such that in combination with the guessed solution and the discarded items we obtain a feasible solution to $(\\mathrm{IP}(\\S))$. Finally, we use the rank lemma~\\cite{lau2011iterative} to argue that {due to this we lose only a factor of $1+\\epsilon$} of the profit due the rounding. {Finally,} we add the profit of {the} tiny items {that we had} discarded to restrict the number of height and width classes.\n{Overall, our solution is a $(1+O(\\epsilon))$-approximate solution to $(\\mathrm{IP}(\\S))$.} \n\n\n\n\\begin{lemma}\\label{lem:rec_rot_lp_small}\nThere is an algorithm with a running time of {$(\\log_{1+\\epsilon}(n))^{O_{\\epsilon}(1)}$}\nthat computes an {implicit} $(1{+}O(\\epsilon))$-approximate solution to $(\\mathrm{IP}(\\S))$.\n\\end{lemma}\n\n\\paragraph{Indirect guessing framework to pack horizontal and large items.} In contrast to the setting without rotation, we now must consider all boxes when packing horizontal and large items since horizontal items may be packed into $\\H$-,$\\V$- and $\\S$-boxes while large items may be packed into $\\L$- and $\\S$-boxes. In particular, the fact that horizontal items may be packed into $\\H$- as well as $\\V$-boxes requires a more involved and technical procedure to compute a packing compared to the indirect guessing framework applied in the non-rotational setting. Hereto, we make use of property v) of our structured packing (Lemma~\\ref{lem:struc_rectangles-rotation}) which allows us to consider each interval $(h^{(j-1)},h^{(j)}]$, with $h^{(1)},\\dots, h^{(c)}$ being the distinct heights of $\\V$-boxes and $h^{(0)}:= 0$, separately in our indirect guessing framework.\n\nWe will now explain how our to use the indirect guessing framework for each of these intervals separately. To this end, consider the interval $(h^{(0)}, h^{(1)}]$. For all other intervals the procedure will be the same. Let $k_{1},k_{2},\\dots,k_{r^{(1)}}$ be the $k_r$-values within the interval $(h^{(0)}, h^{(1)}]$. The objective is again to determine these values in polylogarithmic time which cannot be achieved by guessing them {exactly} since there are $N$ options for each of them. A key difference between the rotational setting and the non-rotational setting is that $\\V$- and $\\H$-boxes must both be considered for horizontal items and, therefore, $\\V$-boxes are relevant for the indirect guessing framework. Since we do not know the widths of the $\\V$-boxes this requires a step before the indirect guessing framework.\n\nWe start by guessing the $C_{boxes}(\\epsilon)/\\epsilon$ most profitable horizontal items packed into the $\\V$-boxes of height $h^{(j)}$. To do this we guess the profit class for each of these items in time $\\log_{1+\\epsilon}^{O_{\\epsilon}(1)}n$. Let $n^g_{t}$ be the number of guessed items for each profit class. Using our rectangle data structure we consider the $n^g_{t}$ items of smallest width of profit class $t$ which can be found in time $O(\\log^4 n)$. For each of these items, we guess which box it is assigned to in the structured packing. If an item is assigned to a $\\V$-box, $\\H$-box or $\\S$-box in the structured packing, we also assign this item to this box. If, however, an item is not used in the structured packing then we assign it to a $\\V$-box losing only a factor of $(1+\\epsilon)$ of the profit while making sure that the picked item is at most as wide as the item used in the structured packing.\n\nAfter finding this partial packing, we distinguish between two cases based on the widths of $\\V$-boxes of height $h^{(j)}$. Let $h_{B}^{min}$ be the minimum height of all $\\H$-boxes relevant for the current iteration of the algorithm. Consider the $\\V$-box of height $h^{(j)}$ with largest width. If this is width at least $\\frac{\\epsilon}{C_{boxes}(\\epsilon)}h_{B}^{min}$, we can guess it as a power of $(1+\\epsilon)$ in time $O_{\\epsilon}(1)$ since  $h_{B}^{min} \\in \\Omega(\\es N)$. Let $B^*$ be this box and denote by $\\hat{w}_{B^*}$ the guessed width. We may assume now that all other $\\V$-boxes of height $h^{(j)}$ have width at least $\\frac{\\epsilon}{C_{boxes}(\\epsilon)}\\hat{w}_{B^*}$ by reserving an $\\epsilon$-fraction of the width of $B^*$ for all items packed into $\\V$-boxes of width less than $\\frac{\\epsilon}{C_{boxes}(\\epsilon)}\\hat{w}_{B^*}$. Thus, we can also guess the width of the remaining boxes each in time $O_\\epsilon(1)$. Now, suppose the maximum width of all $\\V$-boxes of height $h^{(j)}$ is less than $\\frac{\\epsilon}{C_{boxes}(\\epsilon)}h_{B}^{min}$. In this case, we reserve an $\\epsilon$-fraction in each $\\H$-box for all items packed into $\\V$-boxes which can also fit into $\\H$-boxes. Such that all remaining items packed into the $\\V$-boxes do not fit into any of the $\\H$-boxes and, therefore, we can pack the $\\V$-boxes after packing the $\\H$-boxes using our indirect guessing framework.\n\nWe now explain how to use the indirect guessing framework. We will assume here that we must also consider $\\V$-boxes with the guessed widths following the above procedure. Note that if we are in the second case described above this is not necessary and we will later describe how to compute a packing of the $\\V$-boxes in this case. We start by defining $\\tilde{k}_{0}:=0$ and will compute values $\\tilde{k}_{1},\\tilde{k}_{2},\\dots,\\tilde{k}_{r^{(1)}}$ to use instead of the values $k_{0},k_{1},k_{2},\\dots,k_{r^{(1)}}$. With these values we get a partition of $\\I$ into sets $\\tilde{\\I}_{j}:=\\{i\\in\\I:w_{i}\\in(\\tilde{k}_{j-1},\\tilde{k}_{j}]\\}$. Now, for each $j$ we want to pack items from $\\tilde{\\I}_{j}$ into the space reserved for these items in the packing from Lemma~\\ref{lem:struc_rectangles-rotation}.We will choose the values $\\tilde{k}_{1},\\tilde{k}_{2},\\dots,\\tilde{k}_{r^{(1)}}$ such that in this way, we obtain almost the same profit. On the other hand, we will ensure that $\\tilde{k}_{j}\\le k_{j}$ for each $j\\in[r^{(1)}]$.\n\nWe work in $r^{(1)}$ iterations. We define $\\tilde{k}_{0}:=0$. Suppose\ninductively that we have determined $\\ell$ values $\\tilde{k}_{1},\\tilde{k}_{2},\\dots,\\tilde{k}_{\\ell}$\nalready for some $\\ell\\in\\{0,1,...,r^{(1)}-1\\}$ such that $\\tilde{k}_{\\ell}\\le k_{\\ell}$.\nWe want to compute $\\tilde{k}_{\\ell+1}$. We can assume w.l.o.g.~that\n$k_{\\ell+1}$ equals $w_{i}$ for some item $i\\in\\I$. We do {a} binary\nsearch on the set $W(\\ell):=\\{w_{i}:i\\in\\I\\wedge w_{i}>\\tilde{k}_{\\ell}\\}$,\nusing our rectangle data structure. For each candidate value $w \\in W(\\ell)$,\nwe estimate the possible profit due to items in $\\tilde{\\I}_{\\ell+1}$\nif we define $\\tilde{k}_{\\ell+1}:=w$. We want to find such a value\n$w$ such that the obtained profit from the set $\\tilde{\\I}_{\\ell+1}$\nequals essentially $\\hat{p}(\\ell+1)$. In the following, we denote by $\\B_{\\H}(\\ell+1)$ the set of $\\H$-boxes for which $j_B = \\ell+1$, by $\\B_{\\V}(\\ell+1)$ the set of $\\V$-boxes for which $j_B = \\ell+1$, by $\\B_{\\V}(\\ell+1)$ the set of $\\V$-boxes for which $j_B = \\ell+1$ and by $\\B_\\S$ the set of $\\S$-boxes.\n\nWe describe now how we estimate the obtained profit for one specific\nchoice of $w\\in W(\\ell)$. We try to pack items from $\\tilde{\\I}_{\\ell+1}(w):=\\left\\{ i\\in\\I:w_{i}\\in(\\tilde{k}_{\\ell},w]\\right\\} $\ninto\n\\begin{itemize}\n\\item the $\\H$-boxes $B\\in\\B_{\\H}(\\ell+1)$, \n\\item the $\\V$-boxes $B\\in\\B_{\\V}(\\ell+1)$, \n\\item the $\\L$-boxes $B\\in\\B_{\\L}(\\ell+1)$ and \n\\item the $\\S$-boxes, where for each $\\S$-box $B\\in\\B_{\\S}$, we use an area\nof $\\widehat{a}_{B,\\ell+1}\\cdot h_Bw_B$ and ensure that we pack\nonly items $i\\in\\tilde{\\I}_{\\ell+1}(w)$ for which $w_{i}\\leq \\epsilon w_B$ and $h_i \\leq \\epsilon h_B$ or $h_{i}\\leq \\epsilon w_B$ and $w_i \\leq \\epsilon h_B$ (due to rotation).\n\\end{itemize}\nWe do this via an IP where we again use the notion of width and height types. Denote by $\\B(\\ell+1)$. Then, losing only a factor of $(1+\\epsilon)$, we can formulate the problem as the following IP. Note that for each box $B$ we must only consider height and width classes that are relevant, e.g. for large boxes only those corresponding to large items. Here, $\\hat{w}_B$ denote the guessed widths from the previous step. Furthermore, we assume that the right hand-side for $\\H$- and $\\S$-boxes already take into account that we potentially assigned items to these boxes in our partial solution above. We again use the notation of profit, width and height class introduced in the non-rotational setting.\n\n{\n\\small\n\\begin{alignat*}{3}\n(\\mathrm{IP}(w))\\quad& \\text{max} \t& \\displaystyle  \\sum_{(t,t',t'') \\in \\mathcal{T}}\\sum_{B \\in \\B(\\ell+1)} x_{t,t',t'',B} p(t)\t\t\t& \t\t\t& \\quad & \\\\\n& \\text{s.t.} & \\displaystyle \\sum_{(t,t',t'') \\in \\mathcal{T}} x_{t,t',t'',B} \\hat{h}(t')\t& \\leq h_B& \t\t& \\forall B \\in \\B_{\\H}(\\ell+1) \\\\\n& & \\displaystyle \\sum_{(t,t',t'') \\in \\mathcal{T}} x_{t,t',t'',B} \\hat{w}(t'')\t& \\leq \\hat{w}_B& \t\t& \\forall B \\in \\B_{\\V}(\\ell+1) \\\\\n&  & \\displaystyle  \\sum_{(t,t',t'') \\in \\mathcal{T}} x_{t,t',t'',B} \t& \\leq 1& \t\t& \\forall B \\in \\B_{\\L}(\\ell+1) \\\\\n& & \\displaystyle  \\sum_{(t,t',t'') \\in \\mathcal{T}} x_{t,t',t'',B} \\hat{h}(t')\\hat{w}(t'')\t& \\leq a_{B,\\ell+1}h_Bw_B\t& \t\t& \\forall B \\in \\B_{\\S}\\\\\n&\t\t\t\t& \\displaystyle\\sum_{B \\in \\B(\\ell+1)} x_{t,t',t'',B}\t\t\t\t\t\t\t\t& \\leq n_{t,t',t''}\t& \t\t& \\forall (t,t',t'') \\in \\mathcal{T} \\\\\n&\t\t\t\t& x_{t,t',t'',B}\t\t\t\t\t\t\t\t& \\in \\mathbb{N}_{0}& \t\t&\\forall (t,t',t'') \\in \\mathcal{T}, B \\in \\B(\\ell+1)\\\\\n\\end{alignat*}\n}\n\nWe now describe how to find an implicit $(1+O(\\epsilon))$-approximate solution to this IP following the same ideas as used in the setting of hypercubes and rectangles without rotations.\n\n\\begin{lemma}\\label{lem:rec_IP_sol_rot}\nThere is an algorithm with a running time of {$(\\log_{1+\\epsilon}(n))^{O(1)}$}\nthat computes an {implicit} $(1{+}O(\\epsilon))$-approximate solution for \\textup{$(\\mathrm{IP}(w))$};\nwe denote by $q(w)$ the value of this solution. For two values $w,w'$\nwith $w\\le w'$ we have that $q(w)\\le q(w')$.\n\\end{lemma}\n\\begin{proof}\nWe start by guessing the $C_{boxes}(\\epsilon)/\\epsilon$ most profitable large {and} horizontal items in the solution. First, consider the large items. For these, we first guess the profit, height and width class which can be done in {time} $O(\\log_{1+\\epsilon}n)$, $O_{\\epsilon}(1)$ and $O_{\\epsilon}(1)$, respectively. For each triplet $(t,t',t'')$ this gives us a value $n_{t,t',t''}^g$ indicating the number of items for this combination of classes. We now find the $n_{t,t',t''}^g$ items corresponding to $(t,t',t'')$ with lowest height. This can be done in time $O(\\log^2 n)$ using our rectangle data structure. For each of these items we guess which box it must be assigned to (among the $\\L$- and $\\S$-boxes) which takes another $O_{\\epsilon}(1)$guesses. Now, we do the same for horizontal items as follows. We guess the profit class of each of the $C_{boxes}(\\epsilon)/\\epsilon$ most profitable items as well as the width class which can both be done in $O(\\log_{1+\\epsilon}n)$. Then, for each combination of profit and width class we get a value $n_{t,t''}^g$ and look at the  $n_{t,t''}^g$ items of lowest height and for each of them guess which box it must be assigned to. Observe that in this way we make sure that we only lose a factor $1+\\epsilon$ of the profit while also making sure that the picked items are at most as high (in case of $\\H$-boxes) or at most as wide (in case of $\\V$-boxes) as the correct item used in the structured packing. This again takes time $O(\\log^2 n)$ using our rectangle data structure. After these procedures, we have a partial solution to $(\\mathrm{IP}(w))$ denoted by $S^g$. For each $\\L$-box, let $n^g_B$ be the number of items assigned to $B$ by $S^g$. For each $\\S$-box let $\\mathrm{area}^g_B$ be the total area of the items assigned to $B$ by $S^g$ (using the rounded width and height). Similarly, for $\\H$- and $\\V$-boxes let $h^g_B$ and $w^g_B$ be the height of the items assigned to $B$. Note that also for $\\V$-boxes we are interested in the height due to rotation. We now consider an LP-relaxation of $(\\mathrm{IP}(w))$ with updated right hand-sides.\n\n{\n\\footnotesize\n\\begin{alignat*}{3}\n(\\mathrm{LP}(w))\\quad& \\text{max} \t& \\displaystyle \\sum_{(t,t',t'') \\in \\mathcal{T}}\\sum_{B \\in \\B(\\ell+1)} x_{t,t',t''',B} p(t)\t\t\t& \t\t\t& \\quad & \\\\\n& \\text{s.t.} & \\displaystyle  \\sum_{(t,t',t'') \\in \\mathcal{T}} x_{t,t',t'',B} \\hat{h}(t')\t& \\leq h_B-h^g_B& \t\t& \\forall B \\in \\B_{\\H}(\\ell+1) \\\\\n& & \\displaystyle \\sum_{(t,t',t'') \\in \\mathcal{T}} x_{t,t',t'',B} \\hat{w}(t'')\t& \\leq \\hat{w}_B - w^g_B& \t\t& \\forall B \\in \\B_{\\V}(\\ell+1) \\\\\n&  & \\displaystyle  \\sum_{(t,t',t'') \\in \\mathcal{T}} x_{t,t',t'',B} \t& \\leq 1-n^g_B& \t\t& \\forall B \\in \\B_{\\L}(\\ell+1) \\\\\n& & \\displaystyle  \\sum_{(t,t',t'') \\in \\mathcal{T}} x_{t,t',t'',B} \\hat{h}(t')\\hat{w}(t'')\t& \\leq a_{B,\\ell+1}h_Bw_B - \\mathrm{area}^g_B\t& \t\t& \\forall B \\in \\B_{\\S}\\\\\n&\t\t\t\t& \\displaystyle\\sum_{B \\in \\B(\\ell+1)} x_{t,t',t'',B}\t\t\t\t\t\t\t\t& \\leq n_{t,t',t''}-n_{t,t',t''}^g& \t\t& \\forall   (t,t',t'') \\in \\mathcal{T} \\\\\n&\t\t\t\t& x_{t,t',t'',B}\t\t\t\t\t\t\t\t& \\geq 0& \t\t&\\forall  (t,t',t'') \\in \\mathcal{T}, B \\in \\B(\\ell+1)\n\\end{alignat*}\n}\nThe goal is now to solve and round $(\\mathrm{LP}(w))$ in polylogarithmic time. Observe that for large items we have at most $O_{\\epsilon}(1)$ height and width classes and for horizontal items we have at most $O_{\\epsilon}(1)$ width classes. Thus, we only need to restrict the number of height classes. To this end, let ${\\hat{b}}_{max}$ be the maximum value among all {values} $h_B-h_B^g$ and $\\widehat{w}_B$. We know that all horizontal items we consider must have height at most $\\hat{b}_{max}$.\n\nReserving a space of all items of height less than $\\frac{\\epsilon \\hat{b}_{\\max}}{n}$ we can restrict the number of height classes to $O(\\log_{1+\\epsilon}n)$ many. Hereto, we reduce the available height of all $\\H$-boxes and width of all $\\V$-boxes by a factor $1-\\epsilon$. Let $S^f$ be a fractional optimal solution to $(\\mathrm{LP}(w))$ taking into account the reduction of space for height and width boxes. {We can compute this solution in time\n$(\\log n)^{O_{\\epsilon}(1)}$~\\cite{cohen2021solving}}.\n{Due to the rank lemma, we know that we have at most $C_{boxes}(\\epsilon)$ fractional variables in our solution.\nDue to our guessed solution $S^g$, we know that}\n\nrounding down all fractional non-zero variables of $S^f$ {to the next smaller integer} leads to a loss of profit of at most $\\epsilon p(S^g)$. Thus, taking $S^g$ together with the integral variables of $S^f$ as well as the discarded items used to reduce the number of height classes for horizontal items, we obtain an $(1+O(\\epsilon))$-approximate solution to $(\\mathrm{IP}(w))$ in time $(\\log_{1+\\epsilon} n)^{O_\\epsilon(1)}$. The second part of the lemma follows from the fact that a solution for $w$ is also feasible for $w'$.\n\\end{proof}\nAgain, we define $\\tilde{k}_{\\ell+1}$ as the smallest value $w\\in W(\\ell)$\nfor which $q(w)\\ge (1-\\epsilon)\\hat{p}(\\ell+1)$. From  this and the inductive assumption that $\\tilde{k}_{\\ell}\\le k_{\\ell}$ the following statement holds. \n\\begin{lemma}\\label{lem:rec_induc_kr_rot}\nWe have that $\\tilde{k}_{\\ell+1}\\le k_{\\ell+1}$.\n\\end{lemma}\nWe now repeat the indirect guessing framework for each $j=1,\\dots, r^{(1)}$. Finally, it could be that we still need to compute the remaining implicit packing of $\\V$-boxes (in case we could not guess their widths before our indirect guessing framework). Here, we will use that we guessed the profit obtained by items packed into these boxes upfront. Recall, that this profit is given by $\\hat{p}_{\\V}(1)$ for the current range $(h^{(0)},h^{(1)}]$. To find this implicit packing, we will find the minimum total width necessary to achieve the desired profit. To this end, we use our rectangle data structure as well as the balanced binary search tree of item densities to find the smallest density $d^*$ such that packing all horizontal items with height in $(\\tilde{k}_{r^{(1)}},h^{(1)}]$ up to this density yield a total profit of at least  $\\hat{p}_{\\V}(1)$. This can be done in time $O(\\log^4 n)$. Hence, we now know the total width $w^*$ of all considered $\\V$-boxes (with height in $(h^{(0)},h^{(1)}]$) as well as an implicit packing stating that all horizontal items with height in $(\\tilde{k}_{r^{(1)}},h^{(1)}]$ and density of at most $d^*$ are packed. It remains to guess the width of each individual $\\V$-boxes as well as implicit packings of these boxes. In the analysis as well as the construction of the implicit packing of the individual boxes we assume w.l.o.g. that the items are packed into the auxilliary $\\V$-box of width $w^*$ in non-increasing order of profits. Let $B_1,\\dots,B_{\\ell}$ be the considered $\\V$-boxes. We start by guessing the values $\\hat{w}_B := \\lfloor \\frac{w_B}{\\frac{\\epsilon}{\\ell} w^*} \\rfloor \\frac{\\epsilon}{\\ell}w^*$ for each box $B$. First, observe that $\\sum_{j=1}^\\ell \\hat{w}_{B_j} \\geq (1-\\epsilon)w^*$ and since the items are packed in non-increasing order of profits all items that are packed into the width of  $\\sum_{j=1}^\\ell \\hat{w}_{B_j}$ yield a profit of at least $(1-\\epsilon)$. This packing might include a single fractional item. We now split this box into $B_1,\\dots,B_{\\ell}$ according to the guessed widths. This way, we lose at most one item per box. However, since initially (before the indirect guessing framework) we guess the $C_{boxes}(\\epsilon)/\\epsilon$ most profitable items packed into $\\V$-boxes and each of the items we cannot pick has a profit strictly less than these guessed items, the total profit we lose is at most an $\\epsilon$-fraction of  $\\hat{p}_{\\V}(1)$. Thus, we can construct an implicit packing of the individual boxes as follows. For box $B_1$, we use our rectangle data structure in combination with the binary search trees to find a value $p^{(1)}$ such that all items with profit at least $p$, density at most $d^*$ and height in $(\\tilde{k}_{r^{(1)}},h^{(1)}]$ have total width at most $\\hat{w}_{B_1}$ while increasing the profit would violate the width of the box. Then, we repeat this to find values $p^{(2)}, \\dots,p^{(\\ell)}$. This total procedure can be done in time $O_{\\epsilon}(\\log^4 n)$.\n\nRepeating this procedure for all intervals $(h^{(j-1)},h^{(j)}]$, gives a complete implicit packing in polylogarithmic time.\n\nFinally, let $i^*$ be the item that is packed in the box $B^*$ in the packing due to Lemma~\\ref{lem:struc_rectangles-rotation}. If there are at least $1/\\epsilon$ items from the same profit class as $i^*$ in the packing due to Lemma~\\ref{lem:struc_rectangles-rotation}, then we can simply omit $i^*$ while losing only a factor of $1+\\epsilon$ in the profit.\nOtherwise, we guess the profit class of $i^*$ and identify the $1/\\epsilon$ input items of this profit class of smallest width (assuming w.l.o.g. that in the input they are rotated such that their heights are not smaller than their widths).\nIf all of them have already been selected previously by our routines above, then we are done, losing at most a factor of $1+\\epsilon$.\nOtherwise, we can assume w.l.o.g. that $i^*$ is among these items, we guess $i^*$ in time $1/\\epsilon$, and place $i^*$\nin our packing.\n\n\nTogether with the algorithm to compute implicit packings of small items and the algorithm to compute implicit packings of horizontal and large items, we can prove Theorem~\\ref{thm:rectangles_rot}. Note that we again proceed in a similar fashion as in the proof of Theorem~\\ref{thm:rec_nr}.\n\n\\begin{proof}[Proof of Theorem~\\ref{thm:rectangles_rot}]\nWe first prove the first part of the theorem.\n\nObserve that since there is only a constant number of boxes, we can guess their {relative arrangement inside} the knapsack in constant time.\nOur approximation algorithm proceeds in three stages.\n\\begin{enumerate}[A]\n\\item  \\textit{Guessing basic quantities:} The total number of guesses is $(\\log n )^{O_{\\epsilon}(1)}$. Additionally, since the number of boxes is a constant, we can guess their {relative arrangement inside} in our knapsack in constant time.\n\\item \\textit{Indirect guesing framework and construction of packing:} \n\\begin{enumerate}[i]\n\\item \\textit{Packing of small items:} Here, we compute the implicit solution due to Lemma~\\ref{lem:rec_rot_lp_small}. This gives us for each profit, width and height class of small items {(indicated by a tuple $(t,t',t'')$)} a value $z_{t,t',t''}$ which tells us how many items of this class combination we must pack. Assigning these items in non-incresing order of profits to the $\\S$-boxes such that each $\\S$-box receives the correct number of items can be done in time $O(\\log^3(n) + n)$ using our rectangle data structure and the balanced binary search tree for the profits. Furthermore, we can find the set of tiny items which we discarded in time $O(\\log^2(n) + n)$, these will be assigned to the largest $\\S$-box.\n\\item \\textit{Packing of horizontal and large items:}  For each guess we need $r \\in O_{\\epsilon,d}(1)$ iterations of the indirect guessing framework, leading to solutions $x^*(1),\\dots,x^*(r)$ to the LP-relaxations of $(\\mathrm{IP}(\\tilde{k}_{1})),...,(\\mathrm{IP}(\\tilde{k}_{r}))$. This takes time $(\\log_{1+\\epsilon}n)^{O_\\epsilon(1)}$.  Let $\\hat{x}(1),\\dots,\\hat{x}(r)$ be the rounded solutions to $(\\mathrm{IP}(\\tilde{k}_{1})),...,(\\mathrm{IP}(\\tilde{k}_{r}))$.\nIn order to compute a packing of $\\L$-boxes, observe that for each triplet $(t,t',t'')$ the implicit packing indicates the number of large items chosen of of profit class $\\P_t$, height class $H_{t'}$ and width class $W_{t''}$. Again, we may choose these items in non-increasing order of profits in time  $O_{\\epsilon}(\\log^3 n + n)$ and assign them to the correct $\\L$-boxes in time $O_\\epsilon(n)$. For $\\H$- and $\\V$-boxes, we proceed similarly.  Finally, for $\\S$-boxes, we need to remark that again we may choose any set of items such that for each triplet $(t,t',t'')$ the correct number of items is chosen. Thus, we again may choose the items in non-increasing order of profits and assign the correct number of items for each triplet to each box. This way we only lose a factor of $1+\\epsilon$ of the profit for each $\\S$-box and the selected items still satisfy the conditions necessary to pack them into $\\S$ using NFDH. This takes time $O(n\\log n)$ for each box.\n\\end{enumerate}\n\\end{enumerate}\nThis yields a total running time of $O(n\\cdot(\\log n))+(\\log n)^{O_{\\epsilon}(1)}$.\n\nNext, we will prove the second statement of the theorem regarding the dynamic algorithm. The insertion and deletion of items in time $O(\\log^3 n)$ is due to our rectangle data structure. To output a $({17/9}+\\epsilon)$-approximate solution $|ALG|$ in time $O(|ALG|\\cdot(\\log n))+(\\log n)^{O_{\\epsilon}(1)}$, we use the algorithm described above with a refinement of the running time since we will choose at most $|ALG|$ items. If one queries for a $({17/9}+\\epsilon)$-estimate of the optimal {objective function} value, the algorithm above without the construction of the actual packings, i.e., only the computation of the implicit packings can be {executed} in time $(\\log n)^{O_{\\epsilon}(1)}$. Observe that for the tiny items we now need to compute an estimate of the total profit, which we can do in time $O_{\\epsilon}(\\log^3n)$ using our rectangle data structure\nby querying for the number of tiny items for each profit class and then multiplying this with the rounded profit.\n\nFinally, if one wishes to query whether an item $i \\in \\I$ is contained in the solution $ALG$, we compute the implicit solutions in time $(\\log n)^{O_{\\epsilon}(1)}$. Let $(t,t',t'')$ be such that $i$ is of profit type $\\P_t$, height type $H_{t'}$ and width type $W_{t''}$ and let $z_{t,t',t''}$ be the total number of items of these types in the implicit solution. Note that the choice of the triplet $(t,t',t'')$ may depend on {the type of item~$i$}.\nThen by the construction of our packing {it is sufficient to} check whether $i$ is among the first $z_{t,t',t''}$ {items} when items are ordered in non-increasing order of profits. This can be done using our rectangle data structure by counting the number of items of classes $(t,t',t'')$ that have smaller profit than $i$ which we can do with a single range counting query in time $O(\\log^2 n)$. If this value is at least $z_{t,t',t''}$, we answer the query with ``no\" and otherwise with ``yes''. This ensures that we give consistent answers between two consecutive updates of the set $\\I$.\n\n\\end{proof}\n\n\\section{Details of Data Structures}\\label{app:data_struc}\nAn important ingredient of our algorithms is the usage of range counting/reporting data structures. The goal is to construct a data structure whose input points are points in $\\mathbb{R}^d$ characterised by points $x_1,\\dots,x_n \\in \\mathbb{R}^d$ and weights $f_1,\\dots,f_n$. As can be found in the survey by Lee and Preparata~\\cite{lee1984computational} (and references therein), one can construct data structures which allow the following operations:\n\\begin{itemize}\n\t\\item Insertion and deletion of a new point in time $O(\\log^d n)$.\n\t\\item Given a hyperrectangle $A= [a_1,b_1] \\times \\dots \\times [a_d,b_d]$, all points $x_i \\in A$ can be reported in time $O(\\log^{d-1}n + k)$ where $k$ is the number of input points in $A$.\n\t\\item Given a hyperrectangle $A= [a_1,b_1] \\times \\dots \\times [a_d,b_d]$, the sum of the weights of all points $x_i \\in A$ can be reported in time $O(\\log^{d-1}n)$.\n\\end{itemize}\nTo construct the data structures underlying our algorithms, we proceed as follows. For the \\emph{item data structure} used in Section~\\ref{sec:hypercubes}, we use a $2$-dimensional data structure where each item is stored as a point $x_i \\in \\mathbb{R}^2$ and the first coordinate represents the side length of the item while the second coordinate represents the profit of the item. Furthermore, we set $f_i := 1$ for any item. This allows the operations described in Lemma~\\ref{lem:data-structure}. \n\nFor the \\emph{rectangle data structure}, we consider points in $\\mathbb{R}^4$ with a coordinate for the width, height, profit and density of each item. To allow our range counting queries, we use three data structures where one uses $f_i := 1$, one uses $f_i := p_i$ and the third uses $f_i := w_i$. This data structure allows all operations in the given query times mentioned in Lemma~\\ref{lem:data-structure-rec}.\n\n\n\\end{document}\n"}
{"paper_id": "2403-00537", "version": "2403-00537v1", "root_file_path": "d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\data_raw\\2403-00537\\tex\\2403-00537v1\\Paper.tex", "metadata": {"total_length": 38632, "merged_count": 1, "merged_files": ["Paper.tex"], "missing_files": []}, "content": "\n\n\n\n\\documentclass{article}\n\\usepackage{spconf,amsmath,graphicx}\n\\usepackage{mathrsfs,hyperref}\n\\usepackage{stackengine}\n\n\\usepackage{algorithm}\n\\usepackage{bm}\n\\usepackage{siunitx}\n\\usepackage{subcaption}\n\\usepackage{algpseudocode}\\usepackage{xspace}\n\\usepackage[export]{adjustbox}\n\\usepackage{xcolor}\n\\usepackage{makecell}\n\n\n\n\\makeatletter\n\\DeclareRobustCommand\\onedot{\\futurelet\\@let@token\\@onedot}\n\\def\\@onedot{\\ifx\\@let@token.\\else.\\null\\fi\\xspace}\n\n\\def\\eg{\\emph{e.g}\\onedot} \\def\\Eg{\\emph{E.g}\\onedot}\n\\def\\ie{\\emph{i.e}\\onedot} \\def\\Ie{\\emph{I.e}\\onedot}\n\\def\\cf{\\emph{c.f}\\onedot} \\def\\Cf{\\emph{C.f}\\onedot}\n\\def\\etc{\\emph{etc}\\onedot} \\def\\vs{\\emph{vs}\\onedot}\n\\def\\wrt{w.r.t\\onedot} \\def\\dof{d.o.f\\onedot}\n\\def\\etal{\\emph{et al}\\onedot}\n\\makeatother\n\n\n\\algnewcommand{\\LineComment}[1]{\\State \\(\\triangleright\\) #1}\n\n\n\n\n\\usepackage[capitalize]{cleveref}\n\\crefname{section}{Sec.}{Secs.}\n\\crefname{section}{Section}{Sections}\n\\crefname{table}{Table}{Tables}\n\\crefname{table}{Tab.}{Tabs.}\n\\crefname{algorithm}{Alg.}{Algs.}\n\n\n\n\n\n\n\\def\\x{{\\bm x}}\n\\def\\L{{\\cal L}}\n\n\n\n\\newcommand{\\cropimage}[1]\n{\\scalebox{1}[-1]{\\includegraphics[trim=60 70 0 75,clip,width=1.05\\linewidth]{#1}}}\n\\newcommand{\\insertwithsubimage}[2][200 120 190 180]\n{\\stackinset{l}{-0.1cm}{t}{-0.1cm}\n  {\\scalebox{1}[-1]{\\includegraphics[trim=#1,clip,width=1.3cm]{#2}}}\n  {\\cropimage{#2}}}\n\n\\newcommand{\\insertwithsubimagenew}[3][200 120 190 180]\n{\\stackinset{l}{-0.1cm}{t}{-0.1cm}\n  {\\scalebox{1}{\\includegraphics[trim=#1,clip,width=1.3cm]{#3}}}\n  {\\includegraphics[width=#2]{#3}}}\n\n\n\n\n\\title{A Modular and Robust Physics-Based Approach for Lensless Image Reconstruction}\n\n\n\n\\name{Yohann Perron$^*$, Eric Bezzam$^*$, Martin Vetterli\\thanks{\n$^*$These authors contributed equally to this work.\n\n\n}}\n\\address{Audiovisual Communications Laboratory\\\\\\'{E}cole Polytechnique F\\'{e}d\\'{e}rale de Lausanne (EPFL)}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{document}\n\n\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present a modular approach for reconstructing lensless measurements. It consists of three components: a newly-proposed pre-processor, a physics-based camera inverter to undo the multiplexing of lensless imaging, and a post-processor. The pre- and post-processors address noise and artifacts unique to lensless imaging before and after camera inversion respectively. By training the three components end-to-end, we obtain a \\SI{1.9}{\\decibel} increase in PSNR and a \\SI{14}{\\percent} relative improvement in a perceptual image metric (LPIPS) with respect to previously proposed physics-based methods. We also demonstrate how the proposed pre-processor provides more robustness to input noise, and how an auxiliary loss can improve interpretability.\n\\end{abstract}\n\n\\begin{keywords}\n  Lensless imaging, modular reconstruction, end-to-end optimization\n\\end{keywords}\n\n\\section{Introduction}\n\\label{sec:intro}\n\n\n\n\n\n\n\n\n\n\n\n\nLensless imaging systems are computational cameras that replace the lens with a typically thin phase or amplitude mask at a short distance from the sensor~\\cite{Boominathan:22}. This design reduces the constraints imposed by designing traditional lensed systems, allowing for a camera that can be light-weight, compact, and cheap. \nHowever, removing a traditional optical configuration means that a viewable image is not directly formed on the sensor, and that a computational algorithm is needed to reconstruct an image from the highly multiplexed and defocused measurement. \n\nWhen selecting the reconstruction algorithm, there is often a tradeoff between performance and computational complexity. Moreover, the availability of labeled data is a key requirement for approaches that tend towards deep learning. \nMonakhova \\etal~\\cite{Monakhova:19} studied a range of algorithms -- (1) ADMM~\\cite{ADMM} that incorporates a physical forward model through iterative reconstructions, (2) unrolling a few iterations of ADMM with learned hyperparameters~\\cite{lista}, (3) a U-Net~\\cite{Unet}, and (4) unrolled ADMM follow by a U-Net. \nThe latter three approaches are trained on lensless-lensed image pairs. \nThey found that incorporating physical information of the system can reduce the amount of data needed for robust performance, while also allowing for interpretability.\nSimilarly, Khan \\etal~\\cite{flatnet} train a camera inversion component that jointly learns the system response (for improved de-multiplexing of the camera's mask) and a neural network-based perceptual enhancement component.\n\nIn this paper, we propose a modular reconstruction approach for lensless imaging which consists of (1) a novel pre-processing block to better prepare the content before (2) camera inversion which accounts for the system's response to invert its multiplexing, and (3) a post-processing block to handle artifacts from the reconstruction algorithm, color correction, and additional enhancement.\nDuring training, we introduce an auxiliary loss from the camera inversion output to the total loss, to improve the interpretability of the intermediate outputs of our modular reconstruction approach.\n\nThe source code for training and applying both the baseline and proposed reconstruction approaches are available on GitHub as part of \\textit{LenslessPiCam}~\\cite{LenslessPiCam},\\footnote{Demo notebook: \\url{https://go.epfl.ch/lensless-modular}} which is a complete toolkit for lensless imaging hardware and software.\n\n\n\n\n\\section{Lensless Imaging}\n\\label{sec:related_works}\n\n\n\n\n\\subsection{Problem formulation}\n\\label{ssec:lensless_imaging}\nUnder the assumption that the imaging system is linear and shift invariant, the forward operator from a 2D scene $\\bm{X}$ to a 2D lensless measurement $\\bm{Y}$ can be written as:\n\\begin{equation}\n\\label{eq:forward}\n    \\bm{Y} = \\mathscr{C}(\\mathscr{P} * \\bm{X}),\n\\end{equation}\nwhere $\\mathscr{P}$ is the point spread function (PSF) of the system and $\\mathscr{C}$ is the cropping operator to the sensor size~\\cite{Diffuser3D}. \nIf the images are vectorized, namely $\\bm{x}=\\text{vec}(\\bm{X})$ and $\\bm{y}=\\text{vec}(\\bm{Y})$, the convolution and cropping operation can be viewed as a matrix multiplication with $P$ and $C$ respectively:\n$\\bm{y} = \\bm{C}\\bm{P} \\bm{x}$.\n\n\nUnder a Gaussian noise assumption, the maximum likelihood estimator of $\\bm{x}$ is given by minimizing the mean-squared error (MMSE) between the measurement $\\bm{y}$ and $\\bm{C}\\bm{P} \\bm{x}$.\n\n\n\n\nDue to the multiplexing characteristic of most lensless camera PSFs, this minimization problem is ill-posed and regularization is needed.\nA typical approach is to use a non-negativity and a sparsity constraint in the total variation space~\\cite{Diffuser3D,PhlatCam}, yielding the following optimization problem:\n\\begin{align}\n    \\label{eq:opt_reg}\n    \\hat{\\bm{x}} = \\arg \\min_{\\bm{x} \\ge 0} \\frac{1}{2} ||\\bm{y} - \\bm{C}\\bm{P}\\bm{x}||_2^2 + \\tau ||\\bm{\\Psi}\\bm{x})||_1,\n\\end{align}\nwhere $\\bm{\\Psi}$ computes finite differences along 2D.\n\\cref{eq:opt_reg} can be solved with an iterative optimization algorithm such as ADMM~\\cite{ADMM}, whose pseudocode is shown in \\cref{alg:ADMM}.\n\n\\begin{algorithm}[t!]\n  \\caption{ADMM pseudocode.}\n  \\label{alg:ADMM}\n  \\begin{algorithmic}[1]\n    \\Require{ $\\tau > 0$, $\\mu_1 > 0$, $\\mu_2 > 0$, $\\mu_3 > 0$}\n    \\For{$k = 1$ to $t$}\n    \\LineComment{Sparsifying soft-threshold}\n    \\State $u^{k+1} \\gets \\mathcal{T}_{\\tau / \\mu_2}\\left(\\bm{\\Psi} \\bm{x}^k+\\alpha_2^k / \\mu_2\\right)$  \n    \n    \\LineComment{Least-squares update}\n    \\State $v^{k+1} \\gets \\left(\\bm{C}^{\\bm{T}} \\bm{C}+\\mu_1 I\\right)^{-1}\\left(\\alpha_1^k+\\mu_1 \\bm{P} \\bm{x}^k+\\bm{C}^{\\bm{T}} \\bm{y}\\right)$ \\\n    \n    \\LineComment{Enforce non-negativity}\n    \\State $w^{k+1} \\gets \\max \\left(\\alpha_3^k / \\mu_3+\\bm{x}^k, 0\\right)$ \n    \n    \\State $r^k \\gets \\left(\\mu_3 w^{k+1}-\\alpha_3^k\\right)+\\boldsymbol{\\Psi}^{\\bm{T}}\\left(\\mu_2 u^{k+1}-\\alpha_2^k\\right)$  $ + \\bm{P}^{\\bm{T}}\\left(\\mu_1 v^{k+1}-\\alpha_1^k\\right)$\n    \\LineComment{Least-squares update}\n    \\State $\\bm{x}^{k+1} \\gets \\left(\\mu_1 \\bm{P}^{\\bm{T}} \\bm{P}+\\mu_2 \\boldsymbol{\\Psi}^{\\bm{T}} \\boldsymbol{\\Psi}+\\mu_3 I\\right)^{-1} r^k$ \n    \n    \\LineComment{Dual update for $v,u,w$}\n    \n    \\State $\\alpha_1^{k+1} \\gets\\alpha_1^k+\\mu_1\\left(\\bm{P} \\bm{x}^{k+1}-v^{k+1}\\right)$ \n    \n\n    \n    \\State $\\alpha_2^{k+1} \\gets \\alpha_2^k+\\mu_2\\left(\\bm{\\Psi} \\bm{x}^{k+1}-u^{k+1}\\right)$ \n    \n\n    \n    \\State $\\alpha_3^{k+1} \\gets \\alpha_3^k+\\mu_3\\left(\\bm{x}^{k+1}-w^{k+1}\\right)$ \n    \n    \\EndFor\n  \\end{algorithmic}\n\\end{algorithm}\n\nIn the spirit of deep learning, another approach is to collect a sufficiently large dataset of lensless-lensed pairs, and train a neural network, \\eg U-Net~\\cite{Unet}, to approximate the inverse mapping. \nHowever, this requires significant training resources and a large dataset to achieve good performance.\nAlternatively, there exist a range of method between traditional optimization and deep learning,\nthat can promote consistency with the measurements (\\eg by incorporating a forward model as in \\cref{eq:forward}) and can require less data. These methods are described below and also inspire the proposed method presented in~\\cref{ssec:proposed-method}.\n\n\\subsection{Unrolled reconstruction}\n\\label{ssec:unrolled}\nUnrolled algorithms are a small departure from pure optimization algorithms as \\cref{alg:ADMM} and a step towards data-driven approaches~\\cite{lista}. \nIn unrolled algorithms, a fixed number of iterations of a convex optimization approach are \\emph{unrolled} as layers of a neural network, with each layer $k$ having its own hyperparameters, \\eg $\\{\\tau^k$,$\\mu_1^k$,$\\mu_2^k$,$\\mu_3^k \\}$ for ADMM, and hyperparameters are trained end-to-end using backpropagation. Since the number of parameters is small, namely a few dozen, it is possible to train such techniques with a small dataset.\n\nWhile this approach does not have the same theoretical guarantees as classical convex optimization, they can converge faster and achieve better performance than heuristically selecting hyperparameters. Therefore, the cost of training can be offset by the gain in inference time. In the context of lensless imaging, an unrolled version of ADMM has been explored by Monakhova \\etal \\cite{Monakhova:19} with great success. They achieve similar results with 5 iterations of unrolled ADMM with learned hyperparameters as with 100 iterations with fixed manually-selected hyperparameters, \nand train with only $100$ examples.\nWith $23000$ training examples, they also explore adding a learned denoiser (U-Net of 10M parameters at the output), which produces their best results.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Proposed method}\n\\label{ssec:proposed-method}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.7\\linewidth]{figure/artifacts.png}\n    \\caption{Highlighting spatial artifacts of lensless image reconstruction, \\eg unrolled ADMM (20 iterations).}\n    \\label{fig:artifacts}\n\\end{figure}\n\n\\begin{figure}[t!]\n  \n    \\centering\n    \n    \n    \\includegraphics[width=\\linewidth]{figure/imaging_pipeline.png}\n    \n  \n  \\caption{Proposed lensless imaging pipeline.}\n  \\label{fig:denoise}\n\\end{figure}\n\nOur proposed imaging pipeline is shown in \\cref{fig:denoise}, which has (1) a novel pre-processor to reduce the measurement noise and better prepare the measurement for (2) the subsequent camera inversion and (3) a post-processor to reduce the artifacts from the reconstruction algorithm and perform additional enhancement. \n\nThe pre-processor works in a space with well-studied sensor noise sources, but due the multiplexing property of lensless cameras, the measurements lack any of the structure usually expected by most denoisers. \nOn the other hand, the post-processor works in the well-known space of viewable images, but the artifacts from the reconstruction algorithm are complex with very high spatial coherency and cannot be found in typical noisy natural images, \n\\eg prominent vertical and horizontal streaks as seen in the reconstruction in \\cref{fig:artifacts}.\n\nBoth processors have a very challenging task and are difficult to design, in particularly simultaneously. \nTo this end, our solution is to train both processors \\textit{and} the camera inversion end-to-end, such that the appropriate processing can be learned from the data rather than heuristically-designed processing. \nWhile previous work has demonstrated the effectiveness of using a reconstruction algorithm with a post-processor \\cite{Monakhova:19,flatnet}, our experiments reveal the additional benefit of incorporating a pre-processor. \n\nThere are several options for the camera inversion block that can cater to the requirements and constraints of the problem at hand (\\eg memory, speed, performance); these approaches include iterative algorithms (ADMM~\\cite{ADMM}/FISTA~\\cite{beck2009fast}), unrolling iterative algorithms~\\cite{Monakhova:19}, trainable camera inversion~\\cite{flatnet}, and Tikhonov regularization~\\cite{flatcam}.\n\n\nThe following sub-sections further describe the motivation behind the proposed components in our reconstruction approach and during training.\n\n\\subsection{Pre-processor}\n\nLensless imaging shares similar challenges to channel estimation and equalization in wireless communications, namely a PSF (similar to the channel) needs to be estimated and its multiplexing effect removed (similar to equalization) to form a viewable image.\nThis inverse problem can be very sensitive to noise in the input, \\eg simple zero-forcing (dividing by the channel) can amplify input noise at the output~\\cite{wireless2003}.\nMinimizing the mean-squared error (MMSE), similar to the formulation in~\\cref{eq:opt_reg}, tries to avoid such amplifications of input noise by minimizing the estimate noise instead of dividing by the channel (note in \\cref{alg:ADMM} that there is no division by the channel but rather by its Gram matrix at Line 10).\nHowever, this formulation assumes (1) linearity and (2) good knowledge of the channel.\nWhile both assumptions are typically made to simplify lensless imaging reconstruction, it is far from the case: estimating the PSF is difficult; shift invariance (\\ie infinite memory effect) is too simplistic of a model; and image sensors can have non-linear responses (especially when there is saturation).\nTherefore, the goal of the pre-processor is to map the lensless measurement to a space that is more suitable for the reconstruction algorithm, \\ie using non-linear operations to map the measurement to a space where the assumptions on linearity and knowledge of the channel better hold for the camera inversion step.\n\n\n\nThe pre-processor can also improve robustness to variations in input noise.\nIn \\cref{results:pre}, we perform an experiment to show the benefits of incorporating a pre-processor as the input signal-to-noise ratio (SNR) varies.\n\n\\subsection{Auxiliary loss}\n\nDuring training, deep neural networks can experience vanishing/exploding gradients at the intermediate layers. \nAn auxiliary loss, \\ie having an intermediate output redirected to the final backprogated loss, can lead to more stable training and allow for the training of deeper neural networks~\\cite{szegedy2015going}.\nFor our application, we redirect the output of the camera inversion block to the output loss, as this image could be discernible and a loss can be computed with the ground-truth lensed image. \nThe proposed auxiliary loss can also ensure more interpretable results, \\eg between the reconstruction algorithm and the post-processor, as shown in the experiment of~\\cref{results:res}.\n\n\\section{Experimental setup}\n\\label{sec:experiments}\n\n\\subsection{Dataset}\n\\label{ssec:dataset}\nWe use the DiffuserCam Lensless Mirflickr (DLM) dataset~\\cite{Monakhova:19}, which consists of 25000 pairs of standard (lensed) images and their associated lensless measurement with a diffuser-based lensless camera.\nThe dataset is collected by projecting an image on a computer monitor, and simultaneously capturing an image with both a lensed and a lensless camera.\nWe follow the train-test split suggested by the authors: the first 1000 image pairs are used as a test set, and the remaining 24000 pairs are used for training.\n\nIn our experiments, we downsample the images by $2\\times$ to a resolution of $240 \\times135 $ pixels. \n\n\n\n\n\n\\subsection{Training and evaluation}\n\\label{ssec:training}\nAll experiments are run on a Dell Precision 5820 Tower X-Series (08B1) machine with an Intel i9-10900X CPU and an NVIDIA RTX A5000 GPU. PyTorch~\\cite{Pytorch} is used for dataset preparation and training.\nAll models are trained for 25 epochs on the 24000 image pairs with a batch size of 8. The Adam optimizer~\\cite{Adam} is used with a learning rate of $10^{-4}$. The loss function is a sum of the MSE  and the LPIPS score (with VGG weights) \\cite{LPIPS} between the model output and the lensed image:\n\\begin{equation}\n    \\mathscr{L}\\left(\\bm{x},\\bm{\\hat{x}}\\right) = \\mathscr{L}_{\\text{MSE}}\\left(\\bm{x},\\bm{\\hat{x}}\\right) + \\mathscr{L}_{\\text{LPIPS}}\\left(\\bm{x},\\bm{\\hat{x}}\\right).\n\\end{equation}\nWith the proposed auxiliary loss with weighting $\\alpha > 0$, the backpropagated loss (when there is a post-processor) is:\n\\begin{equation}\n    \\label{eq:loss_res}\n    \\mathscr{L}_{\\text{res}}\\left(\\bm{x},\\bm{\\hat{x}},\\bm{\\hat{x}}_{\\text{inv}}\\right) = \\mathscr{L}\\left(\\bm{x},\\bm{\\hat{x}}\\right) + \\alpha \\hspace{0.2em} \\mathscr{L}\\left(\\bm{x},\\bm{\\hat{x}}_{\\text{inv}}\\right).\n\\end{equation}\nFor evaluation, we use peak signal-to-noise ratio (PSNR) and LPIPS. The former is measured in decibels (higher is better) while the latter is within $[0,1]$ and lower is better.\n\n\n\\subsection{Reconstruction algorithm and processors}\n\\label{ssec:model}\n\nUnless otherwise noted, all baseline and proposed approaches use ADMM for camera inversion (see \\cref{fig:denoise})  with different variations: 100 iterations with fixed hyperparameters (\\textit{ADMM100}), or unrolled with 20 iterations (\\textit{Unrolled20}).\n\nThe processor components are U-Nets with $d$ residual blocks between each down- and up-sampling layer. There are three down-sampling layers using strided convolutions with a stride of two, and three up-sampling layers using transposed convolutions. The number of channels is 64 for the first layer and is doubled at each down-sampling layer. \nAs processors, we either train the above U-Net architecture from scratch with $d=2$, or fine-tune DRUNet~\\cite{DruNet} (above architecture with $d=4$ that has been trained on degraded-clean image pairs).\n\n\nIn the results presented in \\cref{ssec:results}, we refer to these processors as \\textit{UNet2} and \\textit{DRUNet} respectively.\n\n\\section{Results}\n\\label{ssec:results}\n\nWe perform three experiments to demonstrate the benefits of the proposed techniques:\n\\begin{enumerate}\n    \\item In \\cref{results:pre}, we vary the input SNR to show the utility of the proposed pre-processor component.\n    \\item In \\cref{results:res}, we show how the auxiliary loss from the reconstruction output can improve interpretability.\n    \\item In \\cref{results:ablation}, we perform a comparison with baselines from previous work.\n\\end{enumerate}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Varying input signal-to-noise ratio}\n\\label{results:pre}\n\n\\begin{figure}[t!]\n    \\centering\n\t\\begin{subfigure}{0.49\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width=0.99\\linewidth]{figure/exp1_PSNR.png} \n\t\t\\caption{PSNR.}\n\t\t\\label{fig:exp1_PSNR}\n\t\\end{subfigure}\n\t\\begin{subfigure}{0.49\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width=0.99\\linewidth]{figure/exp1_LPIPS_Vgg.png}\n        \\caption{LPIPS.}\n\t\t\\label{fig:exp1_LPIPS_Vgg}\n\t\\end{subfigure}\n\t\\caption{Average and standard deviation (shaded) performance on test set for varying signal-to-noise ratio; ``orig'' (x-axis) means no simulated noise is added.}\n\t\\label{fig:vary_snr}\n\\end{figure}\n\n\n\\begin{figure}[t!]\n    \\centering\n\t\\begin{subfigure}{0.49\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width=0.99\\linewidth]{figure/exp2_PSNR.png} \n\t\t\\caption{PSNR.}\n\t\t\\label{fig:exp2_PSNR}\n\t\\end{subfigure}\n\t\\begin{subfigure}{0.49\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width=0.99\\linewidth]{figure/exp2_LPIPS_Vgg.png}\n        \\caption{LPIPS.}\n\t\t\\label{fig:exp2_LPIPS_Vgg}\n\t\\end{subfigure}\n\t\\caption{Average and standard deviation (shaded) performance on test set for varying auxiliary loss weight.}\n\t\\label{fig:vary_alpha}\n \\vspace{-0.4cm}\n\\end{figure}\n\n\n\n\\newcommand{\\figsizebench}{0.16}\n\\begin{figure*}[t!]\n\\centering\n\t\\begingroup\n\t\\renewcommand{\\arraystretch}{1} \n\t\\setlength{\\tabcolsep}{0.2em} \n\t\n\t\\begin{tabular}{ccccc}\n    \\makecell{Lensed\\\\(clean)} & \\makecell{Post-proc.\\\\ (\\SI{0}{\\decibel})} & \\makecell{Pre-proc.\\\\ (\\SI{0}{\\decibel})} & \\makecell{Post-proc.\\\\ (\\SI{20}{\\decibel})} & \\makecell{Pre-proc.\\\\ (\\SI{20}{\\decibel})}\\\\\n    \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp1/0_lensed.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp1/0_Unrolled20+UNet2_0db.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp1/0_UNet2+Unrolled20_0db.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp1/0_Unrolled20+UNet2_20db.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp1/0_UNet2+Unrolled20_20db.png} \\\\\n    \n    \\insertwithsubimagenew[98 72 90 35]{\\figsizebench\\linewidth}{figure/exp1/1_lensed.png} & \n    \\insertwithsubimagenew[98 72 90 35]{\\figsizebench\\linewidth}{figure/exp1/1_Unrolled20+UNet2_0db.png} & \n    \\insertwithsubimagenew[98 72 90 35]{\\figsizebench\\linewidth}{figure/exp1/1_UNet2+Unrolled20_0db.png} & \n    \\insertwithsubimagenew[98 72 90 35]{\\figsizebench\\linewidth}{figure/exp1/1_Unrolled20+UNet2_20db.png} & \n    \\insertwithsubimagenew[98 72 90 35]{\\figsizebench\\linewidth}{figure/exp1/1_UNet2+Unrolled20_20db.png}  \\\\\n    \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp1/3_lensed.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp1/3_Unrolled20+UNet2_0db.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp1/3_UNet2+Unrolled20_0db.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp1/3_Unrolled20+UNet2_20db.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp1/3_UNet2+Unrolled20_20db.png}  \\\\\n    \n    \\insertwithsubimagenew[73 70 115 37]{\\figsizebench\\linewidth}{figure/exp1/4_lensed.png} & \n    \\insertwithsubimagenew[73 70 115 37]{\\figsizebench\\linewidth}{figure/exp1/4_Unrolled20+UNet2_0db.png} & \n    \\insertwithsubimagenew[73 70 115 37]{\\figsizebench\\linewidth}{figure/exp1/4_UNet2+Unrolled20_0db.png} & \n    \\insertwithsubimagenew[73 70 115 37]{\\figsizebench\\linewidth}{figure/exp1/4_Unrolled20+UNet2_20db.png} & \n    \\insertwithsubimagenew[73 70 115 37]{\\figsizebench\\linewidth}{figure/exp1/4_UNet2+Unrolled20_20db.png}  \\\\\n    \n    \\insertwithsubimagenew[34 37 154 70]{\\figsizebench\\linewidth}{figure/exp1/8_lensed.png} & \n    \\insertwithsubimagenew[34 37 154 70]{\\figsizebench\\linewidth}{figure/exp1/8_Unrolled20+UNet2_0db.png} & \n    \\insertwithsubimagenew[34 37 154 70]{\\figsizebench\\linewidth}{figure/exp1/8_UNet2+Unrolled20_0db.png} & \n    \\insertwithsubimagenew[34 37 154 70]{\\figsizebench\\linewidth}{figure/exp1/8_Unrolled20+UNet2_20db.png} & \n    \\insertwithsubimagenew[34 37 154 70]{\\figsizebench\\linewidth}{figure/exp1/8_UNet2+Unrolled20_20db.png}  \\\\\n\t\\end{tabular}\n\t\n\t\\endgroup\n\t\\caption{Example outputs of varying signal-to-noise ratio.}\n  \\label{fig:exp1_compare}\n\\end{figure*}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\newcommand{\\figsizegen}{0.16}\n\\newcommand{\\newlinegen}{1pt}\n\\begin{figure*}[t!]\n\\centering\n\t\\begingroup\n\t\\renewcommand{\\arraystretch}{1} \n\t\\setlength{\\tabcolsep}{0.2em} \n\t\n\t\\begin{tabular}{ccccc}\n\t\t  \\SI{10}{\\decibel} & \\SI{15}{\\decibel} & \\textit{\\SI{20}{\\decibel}} & \\SI{25}{\\decibel} & \\SI{30}{\\decibel}\\\\\n\n    \\insertwithsubimagenew[108 57 80 50]{\\figsizegen\\linewidth}{figure/exp1/3_Unrolled20+UNet2_trainsnr20_testsnr10.png}\n    \n  & \\insertwithsubimagenew[108 57 80 50]{\\figsizegen\\linewidth}{figure/exp1/3_Unrolled20+UNet2_trainsnr20_testsnr15.png}\n  \n  &\\insertwithsubimagenew[108 57 80 50]{\\figsizegen\\linewidth}{figure/exp1/3_Unrolled20+UNet2_trainsnr20_testsnr20.png}\n  \n  & \\insertwithsubimagenew[108 57 80 50]{\\figsizegen\\linewidth}{figure/exp1/3_Unrolled20+UNet2_trainsnr20_testsnr25.png}\n  \n  & \\insertwithsubimagenew[108 57 80 50]{\\figsizegen\\linewidth}{figure/exp1/3_Unrolled20+UNet2_trainsnr20_testsnr30.png}\n  \n\\\\[\\newlinegen]\n        \\insertwithsubimagenew[108 57 80 50]{\\figsizegen\\linewidth}{figure/exp1/3_UNet2+Unrolled20_trainsnr20_testsnr10.png}\n\t\t\n  & \\insertwithsubimagenew[108 57 80 50]{\\figsizegen\\linewidth}{figure/exp1/3_UNet2+Unrolled20_trainsnr20_testsnr15.png}\n  \n  & \\insertwithsubimagenew[108 57 80 50]{\\figsizegen\\linewidth}{figure/exp1/3_UNet2+Unrolled20_trainsnr20_testsnr20.png}\n  \n  & \\insertwithsubimagenew[108 57 80 50]{\\figsizegen\\linewidth}{figure/exp1/3_UNet2+Unrolled20_trainsnr20_testsnr25.png}\n  \n  & \\insertwithsubimagenew[108 57 80 50]{\\figsizegen\\linewidth}{figure/exp1/3_UNet2+Unrolled20_trainsnr20_testsnr30.png}\n  \n\\\\\n\t\\end{tabular}\n\t\n\t\\endgroup\n\t\\caption{Applying \\textit{Unrolled20+UNet2} (top) and \\textit{UNet2+Unrolled20} (bottom), both trained at \\SI{20}{\\decibel}, to various noise levels.}\n  \\label{fig:gensnr}\n\\end{figure*}\n\n\nWith this experiment, we show the effectiveness and necessity of a pre-processor for removing noise prior to the camera inversion.\nTo this end, we add shot noise (\\ie Poisson distribution) to the DLM dataset at various SNRs (\\SI{0}{\\decibel}, \\SI{10}{\\decibel}, \\SI{20}{\\decibel}), and train the following models on the noisy DLM dataset: \\textit{Unrolled20}, \\textit{Unrolled20+UNet2} (just post-processor as in~\\cite{Monakhova:19}), \\textit{UNet2+Unrolled20} (just pre-processor).\n\n\\cref{fig:vary_snr} shows the PSNR and LPIPS of the three approaches as the input SNR varies.\nUsing a pre-processor (\\textit{UNet2+Unrolled20}) is more robust to variations in the input SNR, as its slope in \\cref{fig:vary_snr} is less steep.\nWhile the metrics for using a post-processor (\\textit{Unrolled20+UNet2}) are better, the reconstructions, as shown in~\\cref{fig:exp1_compare}, reveal the limitations of relying on these metrics for evaluating performance.\nWe observe that using a pre-processor (third and last column) is better at recovering finer details than the corresponding outputs when using a post-processor (second and fourth column).\nThe lower image quality metrics for the pre-processor are likely due to the grainier outputs and/or color differences with respect to the \\textit{Lensed} image.\nHowever, these can be easily remedied with the addition of a post-processor.\nIn other words, this experiment reveals the complementary nature of the pre- and post-processors: the pre-processor allows finer details to be recovered by the camera inversion, while the latter enhances the global quality of the image at the output of the camera inversion.\nWe also find that using the pre-processor is more robust to differences between the SNR at training and at inference, \\ie generalizing to unseen noise levels. \nAs seen in \\cref{fig:gensnr}, for both lower and higher SNRs (than used at training) the reconstruction is more robust with the pre-processor.\nIn the following experiments, no noise is added to the original DLM dataset.\n\n\n\n\n\n\n\n\n\n\n\\newcommand{\\figsizeaux}{0.17}\n\\newcommand{\\newlineaux}{18pt}\n\\begin{figure*}[t!]\n\\centering\n\t\\begingroup\n\t\\renewcommand{\\arraystretch}{1} \n\t\\setlength{\\tabcolsep}{0.1em} \n\t\n\t\\begin{tabular}{cccc|c}\n\t\t  & After pre-processor & After camera inversion & After post-processor & Lensless/ Lensed \\\\\n\n    $\\alpha=0$ \\hspace{0.5em} & \\includegraphics[width=\\figsizeaux\\linewidth,valign=m]{figure/exp2/res0_pre.png}\n  & \\includegraphics[width=\\figsizeaux\\linewidth,valign=m]{figure/exp2/res0_pre_post.png}\n  &\\includegraphics[width=\\figsizeaux\\linewidth,valign=m]{figure/exp2/res0_final.png}\n  & \\includegraphics[width=\\figsizeaux\\linewidth,valign=m]{figure/exp2/res0_lensless.png}\n\\\\[\\newlineaux]\n  \n\t\t$\\alpha=0.1$ \\hspace{0.5em} & \\includegraphics[width=\\figsizeaux\\linewidth,valign=m]{figure/exp2/res0.1_pre.png}\n  & \\includegraphics[width=\\figsizeaux\\linewidth,valign=m]{figure/exp2/res0.1_pre_post.png}\n  & \\includegraphics[width=\\figsizeaux\\linewidth,valign=m]{figure/exp2/res0.1_final.png}\n  & \\includegraphics[width=\\figsizeaux\\linewidth,valign=m]{figure/exp2/res0.1_lensed.png}\n\\\\\n\t\\end{tabular}\n\t\n\t\\endgroup\n\t\\caption{Influence of auxiliary loss from reconstruction output: top row does not use the auxiliary loss during training; bottom row uses the auxiliary loss with a weight of $\\alpha=0.1$.}\n  \\label{fig:residual}\n\\end{figure*}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Auxiliary loss}\n\\label{results:res}\n\n\n\n\n\nWith this experiment, we evaluate the effect of incorporating an auxiliary loss of the unrolled output to the total loss.\nTo this end, we add different amounts of the unrolled output loss, \\ie $\\alpha = [0, 0.01, 0.03, 0.1]$ for \\cref{eq:loss_res} ($\\alpha=0$ is equivalent to no auxiliary loss).\nThe goal is to improve the interpretability of the reconstruction approach, such that insight can be gained from the intermediate outputs.\n\n\\cref{fig:vary_alpha} shows the PSNR and LPIPS of a combined pre- and post-processor approach (\\textit{UNet2+Unrolled20+UNet2}) as $\\alpha$ varies.\nThe metrics deteriorate as $\\alpha$ increases, which is expected \nas we are adding additional constraints to the network in order to avoid vanishing gradients.\n\nNonetheless, this decrease in image quality metrics is minimal: \\SI{0.301}{\\decibel} in PSNR and \\SI{4.13}{\\percent} relative increase in LPIPS from $\\alpha=0$ to $\\alpha=0.1$.\n\\cref{fig:residual} shows an example of intermediate outputs with and without the auxiliary loss during training.\nThe final outputs appear very similar with and without the auxiliary loss, whereas the image after the camera inversion is more discernible with the auxiliary loss.\nFrom these intermediate outputs, we can observe similar results as before: the pre-processor prepares the input image for improved camera inversion while the post-processor puts the finishing touches.\n\n\n\n\n\n\n\n\n\n\n\\subsection{Benchmark with previous work}\n\\label{results:ablation}\n\n\n\\begin{figure*}[t!]\n\\centering\n\t\\begingroup\n\t\\renewcommand{\\arraystretch}{1} \n\t\\setlength{\\tabcolsep}{0.2em} \n\t\n\t\\begin{tabular}{cccccc}\n\t\t  \n    Plug-and-play~\\cite{pnp} & \n    Unrolled20~\\cite{Monakhova:19} &\\makecell{Unrolled20\\\\+DRUNet~\\cite{Monakhova:19}}& \\makecell{TrainInv\\\\+DRUNet~\\cite{flatnet}} & \\makecell{UNet2+Unrolled20\\\\+UNet2 (Proposed)} & Lensed \\\\\n    \n    \n    \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp3/admm_pnp_0.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp3/U20_0.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp3/U20+Drunet_0.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp3/TrainInv+Drunet_0.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp3/Unet+U20+Unet_res0.01_0.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp3/lensed_0.png} \\\\\n    \n    \n    \n    \\insertwithsubimagenew[98 72 90 35]{\\figsizebench\\linewidth}{figure/exp3/admm_pnp_1.png} & \n    \\insertwithsubimagenew[98 72 90 35]{\\figsizebench\\linewidth}{figure/exp3/U20_1.png} & \n    \\insertwithsubimagenew[98 72 90 35]{\\figsizebench\\linewidth}{figure/exp3/U20+Drunet_1.png} & \n    \\insertwithsubimagenew[98 72 90 35]{\\figsizebench\\linewidth}{figure/exp3/TrainInv+Drunet_1.png} & \n    \\insertwithsubimagenew[98 72 90 35]{\\figsizebench\\linewidth}{figure/exp3/Unet+U20+Unet_res0.01_1.png} & \n    \\insertwithsubimagenew[98 72 90 35]{\\figsizebench\\linewidth}{figure/exp3/lensed_1.png} \\\\\n    \n    \n    \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp3/admm_pnp_3.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp3/U20_3.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp3/U20+Drunet_3.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp3/TrainInv+Drunet_3.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp3/Unet+U20+Unet_res0.01_3.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp3/lensed_3.png} \\\\\n    \n    \n    \n    \\insertwithsubimagenew[73 70 115 37]{\\figsizebench\\linewidth}{figure/exp3/admm_pnp_4.png} & \n    \\insertwithsubimagenew[73 70 115 37]{\\figsizebench\\linewidth}{figure/exp3/U20_4.png} & \n    \\insertwithsubimagenew[73 70 115 37]{\\figsizebench\\linewidth}{figure/exp3/U20+Drunet_4.png} & \n    \\insertwithsubimagenew[73 70 115 37]{\\figsizebench\\linewidth}{figure/exp3/TrainInv+Drunet_4.png} & \n    \\insertwithsubimagenew[73 70 115 37]{\\figsizebench\\linewidth}{figure/exp3/Unet+U20+Unet_res0.01_4.png} & \n    \\insertwithsubimagenew[73 70 115 37]{\\figsizebench\\linewidth}{figure/exp3/lensed_4.png} \\\\\n    \n    \n    \n    \\insertwithsubimagenew[34 37 154 70]{\\figsizebench\\linewidth}{figure/exp3/admm_pnp_8.png} & \n    \\insertwithsubimagenew[34 37 154 70]{\\figsizebench\\linewidth}{figure/exp3/U20_8.png} & \n    \\insertwithsubimagenew[34 37 154 70]{\\figsizebench\\linewidth}{figure/exp3/U20+Drunet_8.png} & \n    \\insertwithsubimagenew[34 37 154 70]{\\figsizebench\\linewidth}{figure/exp3/TrainInv+Drunet_8.png} & \n    \\insertwithsubimagenew[34 37 154 70]{\\figsizebench\\linewidth}{figure/exp3/Unet+U20+Unet_res0.01_8.png} & \n    \\insertwithsubimagenew[34 37 154 70]{\\figsizebench\\linewidth}{figure/exp3/lensed_8.png} \\\\\n\t\\end{tabular}\n\t\n\t\\endgroup\n\t\\caption{Results of different reconstruction approaches.}\n  \\label{fig:benchmark}\n\\end{figure*}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{table}[t!]\n  \\centering\n  \\scalebox{0.82}{\n  \\begin{tabular}{|c|c|c|c|c|c|}\n    \\hline\n    Method & PSNR $\\uparrow$           & LPIPS  $\\downarrow$   & \\makecell{\\# learnable \\\\paramaters} & \\makecell{Inference\\\\time [ms]} \\\\\n    \\hline\n    ADMM100~\\cite{Diffuser3D}                      &     15.2   &   0.547    &  - & 56.0\\\\\\hline\n    \n    Plug-and-play~\\cite{pnp} & 14.9 & 0.590 & - &  549 \\\\\\hline\n    Unrolled20~\\cite{Monakhova:19}   &  13.3         &    0.424    &  80  & 13.4 \\\\\\hline\n    \\makecell{Unrolled20\\\\+DRUNet~\\cite{Monakhova:19} } \n     &     23.4    &      0.204    &  32.6M   & 22.4 \\\\\n     \\hline\n    TrainInv+DRUNet~\\cite{flatnet}  &    21.7    &   0.246      &   32.7M  & 8.99 \\\\\n    \\hline \\hline\n    \\makecell{UNet2+Unrolled20\\\\+UNet2 }\n     &     \\textbf{25.3}    &   \\textbf{0.175 }        &  34.0M  &   23.1\\\\\\hline\n    \\makecell{UNet2+TrainInv\\\\+UNet2 }\n     &    22.7   &    0.224   &  34.1M  &  9.75\\\\\\hline\n    \n    \n  \\end{tabular}}\n  \\caption{Results of different models on the test set.}\n  \\label{tab:ablation}\n  \\vspace{-0.5cm}\n\\end{table}\n\nIn this final experiment, we compare the proposed reconstruction pipeline with baselines from previous work.\n\n\\cref{tab:ablation} presents our results, and \\cref{fig:benchmark} compares reconstructions on test set images. The best result is obtained with the proposed pre- and post-processing approach (last rows in \\cref{tab:ablation} and last column in \\cref{fig:benchmark}), with \\textit{UNet2} as a pre-processor and \\textit{UNet2} as the post-processor.\n\nWhile the first two approaches in \\cref{tab:ablation} require no training data, they miss the necessary enhancement to address artifacts of lensless imaging and reconstruction. \n\\textit{Plug-and-play} applies 20 iterations of ADMM and replaces the soft-thresholding proximal step with \\textit{DRUNet} (without having to fine-tune), but \\textit{DRUNet} has only been trained with Gaussian noise and cannot deal with the artifacts either.\nWith training data, \\textit{Unrolled20} significantly improves LPIPS with respect to \\textit{ADMM100} (PSNR is worse as MSE and LPIPS are equally weighted in the loss); but it cannot address color correction.\n\\textit{Unrolled20+DRUNet} is similar to \\textit{Le-ADMM-U} from~\\cite{Monakhova:19}; the added processor (fine-tuned \\textit{DRUNet}) significantly improves performance.\n\\textit{TrainInv+DRUNet}, which also fine-tunes the PSF used for inversion as in~\\cite{flatnet}, also exhibits improved performance compared to \\textit{Unrolled20}. \nHowever, \\textit{TrainInv} is less performant as a camera inverter than \\textit{Unrolled20} as the latter uses multiple iterations, but it also makes \\textit{TrainInv} faster (more than $2\\times$).\nFor both  camera inversion approaches, we obtain the best results by splitting the enhancement parameters between the pre- and post-processors as proposed in this work (last rows of \\cref{tab:ablation}).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Conclusion}\n\\label{sec:conclusion}\n\nIn this paper, we propose a novel lensless image reconstruction pipeline. It consists of three components trained end-to-end: (1) a newly-proposed pre-processor, (2) a camera inversion block, \\eg unrolled ADMM to incorporate knowledge of the physical system, and (3) a post-processor. \n\nOur results demonstrate the robustness of the system to a wide range of SNRs, and the effectiveness of the proposed pipeline: \\SI{1.9}{\\decibel} increase in PSNR and \\SI{14}{\\percent} relative improvement in LPIPS with respect to previously proposed physics-based methods.\nReproducibility and well-structured code are also a priority: the baseline and proposed techniques have been incorporated into \\textit{LenslessPiCam}~\\cite{LenslessPiCam} in an object-oriented fashion.\n\n\nFor future work, it would be interesting to jointly optimize the PSF and the reconstruction parameters, as in~\\cite{khan23}, but with the proposed pipeline.\nMoreover, we already saw the generalizability of the pre-processor to different SNRs than at training. It would be interesting to explore how well the pre- and post-processor generalize for camera inverters that use a different PSF than at training.\n\n\n\n\n\n\n\n\n\n\n\n\n\\pagebreak\n\\bibliographystyle{IEEEbib}\n\n\n\\end{document}\n"}
{"paper_id": "2403-00537", "version": "2403-00537v2", "root_file_path": "d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\data_raw\\2403-00537\\tex\\2403-00537v2\\Paper.tex", "metadata": {"total_length": 37469, "merged_count": 1, "merged_files": ["Paper.tex"], "missing_files": []}, "content": "\n\n\n\n\\documentclass{article}\n\\usepackage{spconf,amsmath,graphicx}\n\\usepackage{mathrsfs,hyperref}\n\\usepackage{stackengine}\n\n\\usepackage{algorithm}\n\\usepackage{bm}\n\\usepackage{siunitx}\n\\usepackage{subcaption}\n\\usepackage{algpseudocode}\\usepackage{xspace}\n\\usepackage[export]{adjustbox}\n\\usepackage{xcolor}\n\\usepackage{makecell}\n\n\n\n\\makeatletter\n\\DeclareRobustCommand\\onedot{\\futurelet\\@let@token\\@onedot}\n\\def\\@onedot{\\ifx\\@let@token.\\else.\\null\\fi\\xspace}\n\n\\def\\eg{\\emph{e.g}\\onedot} \\def\\Eg{\\emph{E.g}\\onedot}\n\\def\\ie{\\emph{i.e}\\onedot} \\def\\Ie{\\emph{I.e}\\onedot}\n\\def\\cf{\\emph{c.f}\\onedot} \\def\\Cf{\\emph{C.f}\\onedot}\n\\def\\etc{\\emph{etc}\\onedot} \\def\\vs{\\emph{vs}\\onedot}\n\\def\\wrt{w.r.t\\onedot} \\def\\dof{d.o.f\\onedot}\n\\def\\etal{\\emph{et al}\\onedot}\n\\makeatother\n\n\n\\algnewcommand{\\LineComment}[1]{\\State \\(\\triangleright\\) #1}\n\n\n\n\n\\usepackage[capitalize]{cleveref}\n\\crefname{section}{Sec.}{Secs.}\n\\crefname{section}{Section}{Sections}\n\\crefname{table}{Table}{Tables}\n\\crefname{table}{Tab.}{Tabs.}\n\\crefname{algorithm}{Alg.}{Algs.}\n\n\n\n\n\n\n\\def\\x{{\\bm x}}\n\\def\\L{{\\cal L}}\n\n\n\n\\newcommand{\\cropimage}[1]\n{\\scalebox{1}[-1]{\\includegraphics[trim=60 70 0 75,clip,width=1.05\\linewidth]{#1}}}\n\\newcommand{\\insertwithsubimage}[2][200 120 190 180]\n{\\stackinset{l}{-0.1cm}{t}{-0.1cm}\n  {\\scalebox{1}[-1]{\\includegraphics[trim=#1,clip,width=1.3cm]{#2}}}\n  {\\cropimage{#2}}}\n\n\\newcommand{\\insertwithsubimagenew}[3][200 120 190 180]\n{\\stackinset{l}{-0.1cm}{t}{-0.1cm}\n  {\\scalebox{1}{\\includegraphics[trim=#1,clip,width=1.3cm]{#3}}}\n  {\\includegraphics[width=#2]{#3}}}\n\n\n\n\n\\title{A Modular and Robust Physics-Based Approach for Lensless Image Reconstruction}\n\n\n\n\\name{Yohann Perron$^*$, Eric Bezzam$^*$, Martin Vetterli\\thanks{\n$^*$These authors contributed equally to this work.\\\\\nThis project was supported by the Open Research Data Program of the ETH Board, and by the Swiss National Science Foundation\ngrant number 200021 181978/1, ``SESAM - Sensing and Sampling: Theory and Algorithms''.\n}}\n\\address{Audiovisual Communications Laboratory\\\\\\'{E}cole Polytechnique F\\'{e}d\\'{e}rale de Lausanne (EPFL)}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{document}\n\n\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present a modular approach for reconstructing lensless measurements. It consists of three components: a newly-proposed pre-processor, a physics-based camera inverter to undo the multiplexing of lensless imaging, and a post-processor. The pre- and post-processors address noise and artifacts unique to lensless imaging before and after camera inversion respectively. By training the three components end-to-end, we obtain a \\SI{1.9}{\\decibel} increase in PSNR and a \\SI{14}{\\percent} relative improvement in a perceptual image metric (LPIPS) with respect to previously proposed physics-based methods. We also demonstrate how the proposed pre-processor provides more robustness to input noise, and how an auxiliary loss can improve interpretability.\n\\end{abstract}\n\n\\begin{keywords}\n  Lensless imaging, modular reconstruction, end-to-end optimization\n\\end{keywords}\n\n\\section{Introduction}\n\\label{sec:intro}\n\n\nLensless imaging systems are computational cameras that replace the lens with a typically thin phase or amplitude mask at a short distance from the sensor~\\cite{Boominathan:22}. This design reduces the constraints imposed by designing traditional lensed systems, allowing for a camera that can be light-weight, compact, and cheap. \nHowever, removing a traditional optical configuration means that a viewable image is not directly formed on the sensor, and that a computational algorithm is needed to reconstruct an image from the highly multiplexed and defocused measurement. \n\nWhen selecting the reconstruction algorithm, there is often a tradeoff between performance and computational complexity. Moreover, the availability of labeled data is a key requirement for approaches that tend towards deep learning. \nMonakhova \\etal~\\cite{Monakhova:19} studied a range of algorithms -- (1) alternating direction method of multipliers (ADMM)~\\cite{ADMM} that incorporates a physical forward model through iterative reconstructions, (2) unrolling a few iterations of ADMM with learned hyperparameters~\\cite{lista}, (3) a U-Net~\\cite{Unet}, and (4) unrolled ADMM follow by a U-Net. \nThe latter three approaches are trained on lensless-lensed image pairs. \nThey found that incorporating physical information of the system can reduce the amount of data needed for robust performance, while also allowing for interpretability.\nSimilarly, Khan \\etal~\\cite{flatnet} jointly train the point spread function (PSF) used within Wiener filtering (for improved de-multiplexing of the camera's mask) and a neural network-based perceptual enhancement component.\n\nIn this paper, we propose a modular reconstruction approach for lensless imaging which consists of (1) a novel pre-processing block to better prepare the content before (2) camera inversion which accounts for the system's response to invert its multiplexing (which can be unrolled ADMM, trainable Wiener filtering, etc), and (3) a post-processing block to handle artifacts from the reconstruction algorithm, color correction, and additional enhancement.\nDuring training, we introduce an auxiliary loss from the camera inversion output to the total loss, to improve the interpretability of the intermediate outputs of our modular reconstruction approach.\n\nThe source code for training and applying both the baseline and proposed reconstruction approaches are available on GitHub as part of \\textit{LenslessPiCam}~\\cite{LenslessPiCam},\\footnote{Demo notebook: \\url{https://go.epfl.ch/lensless-modular}} which is a complete toolkit for lensless imaging hardware and software.\n\n\n\n\n\\section{Lensless Imaging}\n\\label{sec:related_works}\n\n\n\n\n\\subsection{Problem formulation}\n\\label{ssec:lensless_imaging}\n\nAssuming a desired scene is comprised of point sources that are incoherent with each other, a lensless imaging system can be modeled as a linear matrix-vector multiplication with the system matrix $\\bm{H}$:\n\\begin{align}\n    \\label{eq:forward_gen}\n    \\bm{y} = \\bm{H}\\bm{x} + \\bm{n},\n\\end{align}\nwhere $\\bm{y}$ and $\\bm{x}$ are the vectorized lensless measurement and scene intensity respectively, and $\\bm{n}$ is additive noise.\nAs obtaining $\\bm{H}$ would require an expensive calibration process, the point spread functions (PSF) for each point in the scene are approximated as lateral shifts of the on-axis PSF, \\ie linear shift-invariance (LSI) is assumed: \n\\begin{equation}\n\\label{eq:forward}\n    \\bm{y} = \\bm{C}\\bm{P}\\bm{x} + \\bm{n},\n\\end{equation}\nwhere $\\bm{P}$ has a Toeplitz structure with each column being shifted version of the on-axis PSF, and $\\bm{C}$ crops the image to the sensor size~\\cite{Diffuser3D}.\n\nUnder a Gaussian noise assumption, the maximum likelihood estimator of $\\bm{x}$ is given by minimizing the mean-squared error (MMSE) between the measurement $\\bm{y}$ and $\\bm{C}\\bm{P} \\bm{x}$.\n\n\n\n\nDue to the multiplexing characteristic of most lensless camera PSFs, this minimization problem is ill-posed and regularization is needed.\nA typical approach is to use a non-negativity and a sparsity constraint in the total variation space~\\cite{Diffuser3D,PhlatCam}, yielding the following optimization problem:\n\\begin{align}\n    \\label{eq:opt_reg}\n    \\hat{\\bm{x}} = \\arg \\min_{\\bm{x} \\ge 0} \\frac{1}{2} ||\\bm{y} - \\bm{C}\\bm{P}\\bm{x}||_2^2 + \\tau ||\\bm{\\Psi}\\bm{x}||_1,\n\\end{align}\nwhere $\\bm{\\Psi}$ computes finite differences along 2D.\n\\cref{eq:opt_reg} can be solved with an iterative optimization algorithm such as ADMM~\\cite{ADMM}.\n\n\nIn the spirit of deep learning, another approach is to collect a sufficiently large dataset of lensless-lensed pairs, and train a neural network, \\eg U-Net~\\cite{Unet}, to approximate the inverse mapping. \nHowever, this requires significant training resources and a large dataset to achieve good performance.\nAlternatively, there exist a range of method between traditional optimization and deep learning,\nthat can promote consistency with the measurements (\\eg by incorporating a forward model as in \\cref{eq:forward}) and can require less data. These methods are described below and also inspire the proposed method presented in~\\cref{ssec:proposed-method}.\n\n\\subsection{Unrolled reconstruction}\n\\label{ssec:unrolled}\nUnrolled algorithms are a small departure from pure optimization algorithms as ADMM and a step towards data-driven approaches~\\cite{lista}. \nIn unrolled algorithms, a fixed number of iterations of a convex optimization approach are \\emph{unrolled} as layers of a neural network, with each layer $k$ having its own hyperparameters, \\eg 4 per ADMM iteration, and hyperparameters are trained end-to-end using backpropagation. Since the number of parameters is small, namely a few dozen, it is possible to train such techniques with a small dataset.\n\nWhile this approach does not have the same theoretical guarantees as classical convex optimization, they can converge faster and achieve better performance than heuristically selecting hyperparameters. Therefore, the cost of training can be offset by the gain in inference time. In the context of lensless imaging, an unrolled version of ADMM has been explored by Monakhova \\etal \\cite{Monakhova:19} with great success. They achieve similar results with 5 iterations of unrolled ADMM with learned hyperparameters as with 100 iterations with fixed manually-selected hyperparameters, \nand train with only $100$ examples.\nWith $23000$ training examples, they also explore adding a learned denoiser (U-Net of 10M parameters at the output of unrolled ADMM), which produces their best results.\n\n\n\\section{Proposed method}\n\\label{ssec:proposed-method}\n\n\n\n\n\n\n\n\n\\begin{figure}[t!]\n  \n    \\centering\n    \n    \n    \\includegraphics[width=\\linewidth]{figure/imaging_pipeline.png}\n    \n  \n  \\caption{Proposed lensless imaging pipeline.}\n  \\label{fig:denoise}\n\\end{figure}\n\nOur proposed imaging pipeline is shown in \\cref{fig:denoise}, which has (1) a novel pre-processor to reduce the measurement noise and better prepare the measurement for (2) the subsequent camera inversion and (3) a post-processor to reduce the artifacts from the reconstruction algorithm and perform additional enhancement. \n\nThe pre-processor works in a space with well-studied sensor noise sources, but due the multiplexing property of lensless cameras, the measurements lack any of the structure usually expected by most denoisers. \nOn the other hand, the post-processor works in the well-known space of viewable images, but the artifacts from the reconstruction algorithm are complex with very high spatial coherency and cannot be found in typical noisy natural images.\n\n\nBoth processors have a very challenging task and are difficult to design, in particularly simultaneously. \nTo this end, our solution is to train both processors \\textit{and} the camera inversion end-to-end, such that the appropriate processing can be learned from the data rather than heuristically-designed processing. \nWhile previous work has demonstrated the effectiveness of using a reconstruction algorithm with a post-processor \\cite{Monakhova:19,flatnet}, our experiments reveal the additional benefit of incorporating a pre-processor. \n\nThere are several options for the camera inversion block that can cater to the requirements and constraints of the problem at hand (\\eg memory, speed, performance); these approaches include iterative algorithms (ADMM~\\cite{ADMM} and the fast iterative shrinkage-thresholding algorithm (FISTA)~\\cite{beck2009fast}), unrolling iterative algorithms~\\cite{Monakhova:19}, trainable Wiener filtering~\\cite{flatnet}, and Tikhonov regularization~\\cite{flatcam}.\n\n\nThe following sub-sections further describe the motivation behind the proposed components in our reconstruction approach and during training.\n\n\\subsection{Pre- and post-processors}\n\nLensless imaging can be sensitive to noise and to model mismatch, \\eg in the PSF.\nIn solving~\\cref{eq:opt_reg} with ADMM and a noisy PSF estimate $\\bm{\\hat{P}}$ such that $ \\bm{P} = \\bm{\\hat{P}} + \\bm{\\Delta}_{\\bm{\\bm{P}}} $, the authors of~\\cite{9546648} demonstrate that \\textit{each iteration} update of ADMM is a function of (1) the noisy update that uses $\\bm{\\hat{P}}$ and (2) error terms from previous iterations.\nInserting the measurement definition $\\bm{y}$ from~\\cref{eq:forward} into Eq.~15 of~\\cite{9546648}, we can observe a perturbation that arises from model mismatch and noise amplification:\n\\begin{align}\n    \\bm{x}^{(k)} &= \\bm{W}_1 \\bm{\\hat{x}}^{(k)} + \\underbrace{\\bm{W}_2 \\bm{C}^T \\bm{C}\\bm{P}\\bm{x} +\\bm{\\epsilon}^{(k-1)}}_{\\text{model mismatch}} + \\underbrace{\\bm{W}_2 \\bm{C}^T \\bm{n}}_{\\text{noise amplification}} \\label{eq:noiy_admm_update}\n\\end{align}\nwhere $\\bm{\\hat{x}}^{(k)}$ is the noisy ADMM update at the $k^{th}$ iteration, $\\bm{\\epsilon}^{(k-1)}$ are error terms from the previous iteration, $\\{\\rho_x, \\rho_y, \\rho_z\\}$ are ADMM hyperparameters, and: \n\\begin{align}\n\\bm{W}_1 &= \\left( \\bm{W}_3 + \\rho_x \\delta_{\\bm{P}} \\right)^{-1} \\bm{W}_3,\\\\\n\\bm{W}_2 &= (\\bm{W}_3 + \\rho_x \\delta_{\\bm{P}})^{-1} \\Delta_{\\bm{P}}^T \\rho_x (\\bm{C}^T\\bm{C} + \\rho_x \\bm{I})^{-1} \\\\\n\\bm{W}_3 &= \\rho_x \\bm{\\hat{P}}^T \\bm{\\hat{P}} + \\rho_z \\bm{C}^T\\bm{C} + \\rho_y \\bm{I} \\\\\n\\delta_{\\bm{P}} &= \\left( \\bm{\\Delta}_H^T\\bm{P} + \\bm{\\hat{P}}^T \\bm{\\Delta}_P \\right).\n\\end{align}\nFrom  \\cref{eq:noiy_admm_update} we see a clear motivation for the pre- and post-processors:\nfor the \\textit{pre-processor} to (1) minimize the noise $\\bm{n}$ that could be amplified at each iteration and (2) to use non-linear operations to project the measurement to a space where the assumptions of LSI better hold for the camera inversion step;\nand for the \\textit{post-processor} to remove the noise at the output of camera inversion due to model mismatch and noise amplification.\nThe pre-processor can also improve robustness to variations in input noise.\nIn \\cref{results:pre}, we perform an experiment to show the benefits of incorporating a pre-processor as the input signal-to-noise ratio (SNR) varies.\n\n\\subsection{Auxiliary loss}\n\nDuring training, deep neural networks can experience vanishing/exploding gradients at the intermediate layers. \nAn auxiliary loss, \\ie having an intermediate output redirected to the final backprogated loss, can lead to more stable training and allow for the training of deeper neural networks~\\cite{szegedy2015going}.\nFor our application, we redirect the output of the camera inversion block to the output loss, as this image could be discernible and a loss can be computed with the ground-truth lensed image. \nThe proposed auxiliary loss can also ensure more interpretable results, \\eg between the reconstruction algorithm and the post-processor, as shown in the experiment of~\\cref{results:res}.\n\n\\section{Experimental setup}\n\\label{sec:experiments}\n\n\\subsection{Dataset}\n\\label{ssec:dataset}\nWe use the DiffuserCam Lensless Mirflickr (DLM) dataset~\\cite{Monakhova:19}, which consists of 25000 pairs of standard (lensed) images and their associated lensless measurement with a diffuser-based lensless camera.\nThe dataset is collected by projecting an image on a computer monitor, and simultaneously capturing an image with both a lensed and a lensless camera.\nWe follow the train-test split suggested by the authors: the first 1000 image pairs are used as a test set, and the remaining 24000 pairs are used for training.\n\nIn our experiments, we downsample the images by $2\\times$ to a resolution of $240 \\times135 $ pixels. \n\n\n\n\n\n\\subsection{Training and evaluation}\n\\label{ssec:training}\nAll experiments are run on a Dell Precision 5820 Tower X-Series (08B1) machine with an Intel i9-10900X CPU and an NVIDIA RTX A5000 GPU. PyTorch~\\cite{Pytorch} is used for dataset preparation and training.\nAll models are trained for 25 epochs on the 24000 image pairs with a batch size of 8. The Adam optimizer~\\cite{Adam} is used with a learning rate of $10^{-4}$. The loss function is a sum of the MSE  and the LPIPS score (with VGG weights) \\cite{LPIPS} between the model output and the lensed image:\n\\begin{equation}\n    \\mathscr{L}\\left(\\bm{x},\\bm{\\hat{x}}\\right) = \\mathscr{L}_{\\text{MSE}}\\left(\\bm{x},\\bm{\\hat{x}}\\right) + \\mathscr{L}_{\\text{LPIPS}}\\left(\\bm{x},\\bm{\\hat{x}}\\right).\n\\end{equation}\nWith the proposed auxiliary loss with weighting $\\alpha > 0$, the backpropagated loss (when there is a post-processor) is:\n\\begin{equation}\n    \\label{eq:loss_res}\n    \\mathscr{L}_{\\text{res}}\\left(\\bm{x},\\bm{\\hat{x}},\\bm{\\hat{x}}_{\\text{inv}}\\right) = \\mathscr{L}\\left(\\bm{x},\\bm{\\hat{x}}\\right) + \\alpha \\hspace{0.2em} \\mathscr{L}\\left(\\bm{x},\\bm{\\hat{x}}_{\\text{inv}}\\right).\n\\end{equation}\nFor evaluation, we use peak signal-to-noise ratio (PSNR) and LPIPS. The former is measured in decibels (higher is better) while the latter is within $[0,1]$ and lower is better.\n\n\n\\subsection{Reconstruction algorithm and processors}\n\\label{ssec:model}\n\n\\begin{figure}[t!]\n    \\centering\n    \\includegraphics[width=0.95\\linewidth]{figure/unetres.png}\n  \\caption{Architecture of pre- and post-processors.}\n  \\label{fig:unetres}\n\\end{figure}\n\nUnless otherwise noted, all baseline and proposed approaches use ADMM for camera inversion (see \\cref{fig:denoise})  with different variations: 100 iterations with fixed hyperparameters (\\textit{ADMM100}), or unrolled with 20 iterations (\\textit{Unrolled20}).\n\nThe processor components are U-Nets with $d$ residual blocks between each down- and up-sampling layer, as shown in \\cref{fig:unetres}. \nThere are three down-sampling layers using strided convolutions with a stride of two, and three up-sampling layers using transposed convolutions. The number of channels is 64 for the first layer and is doubled at each down-sampling layer. \nAs processors, we either train the above U-Net architecture from scratch with $d=2$, or fine-tune DRUNet~\\cite{DruNet} (above architecture with $d=4$ that has been trained on degraded-clean image pairs).\n\n\nIn our results, we refer to these processors as \\textit{UNet2} and \\textit{DRUNet} respectively.\n\n\\section{Results}\n\\label{ssec:results}\n\nWe perform three experiments to demonstrate the benefits of the proposed techniques:\n\\begin{enumerate}\n    \\item In \\cref{results:pre}, we vary the input SNR to show the utility of the proposed pre-processor component.\n    \\item In \\cref{results:res}, we show how the auxiliary loss from the reconstruction output can improve interpretability.\n    \\item In \\cref{results:ablation}, we perform a comparison with baselines from previous work.\n\\end{enumerate}\n\n\n\n\\subsection{Varying input signal-to-noise ratio}\n\\label{results:pre}\n\n\\begin{figure}[t!]\n    \\centering\n\t\\begin{subfigure}{0.49\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width=0.99\\linewidth]{figure/exp1_PSNR.png} \n\t\t\\caption{PSNR.}\n\t\t\\label{fig:exp1_PSNR}\n\t\\end{subfigure}\n\t\\begin{subfigure}{0.49\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width=0.99\\linewidth]{figure/exp1_LPIPS_Vgg.png}\n        \\caption{LPIPS.}\n\t\t\\label{fig:exp1_LPIPS_Vgg}\n\t\\end{subfigure}\n\t\\caption{Average and standard deviation (shaded) performance on test set for varying signal-to-noise ratio; ``orig'' (x-axis) means no simulated noise is added.}\n\t\\label{fig:vary_snr}\n\\end{figure}\n\n\n\\begin{figure}[t!]\n    \\centering\n\t\\begin{subfigure}{0.49\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width=0.99\\linewidth]{figure/exp2_PSNR.png} \n\t\t\\caption{PSNR.}\n\t\t\\label{fig:exp2_PSNR}\n\t\\end{subfigure}\n\t\\begin{subfigure}{0.49\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width=0.99\\linewidth]{figure/exp2_LPIPS_Vgg.png}\n        \\caption{LPIPS.}\n\t\t\\label{fig:exp2_LPIPS_Vgg}\n\t\\end{subfigure}\n\t\\caption{Average and standard deviation (shaded) performance on test set for varying auxiliary loss weight.}\n\t\\label{fig:vary_alpha}\n \\vspace{-0.4cm}\n\\end{figure}\n\n\n\n\\newcommand{\\figsizebench}{0.16}\n\\begin{figure*}[t!]\n\\centering\n\t\\begingroup\n\t\\renewcommand{\\arraystretch}{1} \n\t\\setlength{\\tabcolsep}{0.2em} \n\t\n\t\\begin{tabular}{ccccc}\n    \\makecell{Lensed\\\\(clean)} & \\makecell{Post-proc.\\\\ (\\SI{0}{\\decibel})} & \\makecell{Pre-proc.\\\\ (\\SI{0}{\\decibel})} & \\makecell{Post-proc.\\\\ (\\SI{20}{\\decibel})} & \\makecell{Pre-proc.\\\\ (\\SI{20}{\\decibel})}\\\\\n    \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp1/0_lensed.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp1/0_Unrolled20+UNet2_0db.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp1/0_UNet2+Unrolled20_0db.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp1/0_Unrolled20+UNet2_20db.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp1/0_UNet2+Unrolled20_20db.png} \\\\\n    \n    \\insertwithsubimagenew[98 72 90 35]{\\figsizebench\\linewidth}{figure/exp1/1_lensed.png} & \n    \\insertwithsubimagenew[98 72 90 35]{\\figsizebench\\linewidth}{figure/exp1/1_Unrolled20+UNet2_0db.png} & \n    \\insertwithsubimagenew[98 72 90 35]{\\figsizebench\\linewidth}{figure/exp1/1_UNet2+Unrolled20_0db.png} & \n    \\insertwithsubimagenew[98 72 90 35]{\\figsizebench\\linewidth}{figure/exp1/1_Unrolled20+UNet2_20db.png} & \n    \\insertwithsubimagenew[98 72 90 35]{\\figsizebench\\linewidth}{figure/exp1/1_UNet2+Unrolled20_20db.png}  \\\\\n    \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp1/3_lensed.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp1/3_Unrolled20+UNet2_0db.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp1/3_UNet2+Unrolled20_0db.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp1/3_Unrolled20+UNet2_20db.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp1/3_UNet2+Unrolled20_20db.png}  \\\\\n    \n    \\insertwithsubimagenew[73 70 115 37]{\\figsizebench\\linewidth}{figure/exp1/4_lensed.png} & \n    \\insertwithsubimagenew[73 70 115 37]{\\figsizebench\\linewidth}{figure/exp1/4_Unrolled20+UNet2_0db.png} & \n    \\insertwithsubimagenew[73 70 115 37]{\\figsizebench\\linewidth}{figure/exp1/4_UNet2+Unrolled20_0db.png} & \n    \\insertwithsubimagenew[73 70 115 37]{\\figsizebench\\linewidth}{figure/exp1/4_Unrolled20+UNet2_20db.png} & \n    \\insertwithsubimagenew[73 70 115 37]{\\figsizebench\\linewidth}{figure/exp1/4_UNet2+Unrolled20_20db.png}  \\\\\n    \n    \\insertwithsubimagenew[34 37 154 70]{\\figsizebench\\linewidth}{figure/exp1/8_lensed.png} & \n    \\insertwithsubimagenew[34 37 154 70]{\\figsizebench\\linewidth}{figure/exp1/8_Unrolled20+UNet2_0db.png} & \n    \\insertwithsubimagenew[34 37 154 70]{\\figsizebench\\linewidth}{figure/exp1/8_UNet2+Unrolled20_0db.png} & \n    \\insertwithsubimagenew[34 37 154 70]{\\figsizebench\\linewidth}{figure/exp1/8_Unrolled20+UNet2_20db.png} & \n    \\insertwithsubimagenew[34 37 154 70]{\\figsizebench\\linewidth}{figure/exp1/8_UNet2+Unrolled20_20db.png}  \\\\\n\t\\end{tabular}\n\t\n\t\\endgroup\n\t\\caption{Example outputs of varying signal-to-noise ratio.}\n  \\label{fig:exp1_compare}\n\\end{figure*}\n\n\n\\newcommand{\\figsizegen}{0.16}\n\\newcommand{\\newlinegen}{1pt}\n\\begin{figure*}[t!]\n\\centering\n\t\\begingroup\n\t\\renewcommand{\\arraystretch}{1} \n\t\\setlength{\\tabcolsep}{0.2em} \n\t\n\t\\begin{tabular}{ccccc}\n\t\t  \\SI{10}{\\decibel} & \\SI{15}{\\decibel} & \\textit{\\SI{20}{\\decibel}} & \\SI{25}{\\decibel} & \\SI{30}{\\decibel}\\\\\n\n    \\insertwithsubimagenew[108 57 80 50]{\\figsizegen\\linewidth}{figure/exp1/3_Unrolled20+UNet2_trainsnr20_testsnr10.png}\n    \n  & \\insertwithsubimagenew[108 57 80 50]{\\figsizegen\\linewidth}{figure/exp1/3_Unrolled20+UNet2_trainsnr20_testsnr15.png}\n  \n  &\\insertwithsubimagenew[108 57 80 50]{\\figsizegen\\linewidth}{figure/exp1/3_Unrolled20+UNet2_trainsnr20_testsnr20.png}\n  \n  & \\insertwithsubimagenew[108 57 80 50]{\\figsizegen\\linewidth}{figure/exp1/3_Unrolled20+UNet2_trainsnr20_testsnr25.png}\n  \n  & \\insertwithsubimagenew[108 57 80 50]{\\figsizegen\\linewidth}{figure/exp1/3_Unrolled20+UNet2_trainsnr20_testsnr30.png}\n  \n\\\\[\\newlinegen]\n        \\insertwithsubimagenew[108 57 80 50]{\\figsizegen\\linewidth}{figure/exp1/3_UNet2+Unrolled20_trainsnr20_testsnr10.png}\n\t\t\n  & \\insertwithsubimagenew[108 57 80 50]{\\figsizegen\\linewidth}{figure/exp1/3_UNet2+Unrolled20_trainsnr20_testsnr15.png}\n  \n  & \\insertwithsubimagenew[108 57 80 50]{\\figsizegen\\linewidth}{figure/exp1/3_UNet2+Unrolled20_trainsnr20_testsnr20.png}\n  \n  & \\insertwithsubimagenew[108 57 80 50]{\\figsizegen\\linewidth}{figure/exp1/3_UNet2+Unrolled20_trainsnr20_testsnr25.png}\n  \n  & \\insertwithsubimagenew[108 57 80 50]{\\figsizegen\\linewidth}{figure/exp1/3_UNet2+Unrolled20_trainsnr20_testsnr30.png}\n  \n\\\\\n\t\\end{tabular}\n\t\n\t\\endgroup\n\t\\caption{Applying \\textit{Unrolled20+UNet2} (top) and \\textit{UNet2+Unrolled20} (bottom), both trained at \\SI{20}{\\decibel}, to various noise levels.}\n  \\label{fig:gensnr}\n\\end{figure*}\n\n\nWith this experiment, we show the effectiveness and necessity of a pre-processor for removing noise prior to the camera inversion.\nTo this end, we add shot noise (\\ie Poisson distribution) to the DLM dataset at various SNRs (\\SI{0}{\\decibel}, \\SI{10}{\\decibel}, \\SI{20}{\\decibel}), and train the following models on the noisy DLM dataset: \\textit{Unrolled20}, \\textit{Unrolled20+UNet2} (just post-processor as in~\\cite{Monakhova:19}), \\textit{UNet2+Unrolled20} (just pre-processor).\n\n\\cref{fig:vary_snr} shows the PSNR and LPIPS of the three approaches as the input SNR varies.\nUsing a pre-processor (\\textit{UNet2+Unrolled20}) is more robust to variations in the input SNR, as its slope in \\cref{fig:vary_snr} is less steep.\nWhile the metrics for using a post-processor (\\textit{Unrolled20+UNet2}) are better, the reconstructions, as shown in~\\cref{fig:exp1_compare}, reveal the limitations of relying on these metrics for evaluating performance.\nWe observe that using a pre-processor (third and last column) is better at recovering finer details than the corresponding outputs when using a post-processor (second and fourth column).\nThe lower image quality metrics for the pre-processor are likely due to the grainier outputs and/or color differences with respect to the \\textit{Lensed} image.\nHowever, these can be easily remedied with the addition of a post-processor.\nIn other words, this experiment reveals the complementary nature of the pre- and post-processors: the pre-processor allows finer details to be recovered by the camera inversion, while the latter enhances the global quality of the image at the output of the camera inversion.\nWe also find that using the pre-processor is more robust to differences between the SNR at training and at inference, \\ie generalizing to unseen noise levels. \nAs seen in \\cref{fig:gensnr}, for both lower and higher SNRs (than used at training) the reconstruction is more robust with the pre-processor.\nIn the following experiments, no noise is added to the original DLM dataset.\n\n\n\\newcommand{\\figsizeaux}{0.17}\n\\newcommand{\\newlineaux}{18pt}\n\\begin{figure*}[t!]\n\\centering\n\t\\begingroup\n\t\\renewcommand{\\arraystretch}{1} \n\t\\setlength{\\tabcolsep}{0.1em} \n\t\n\t\\begin{tabular}{cccc|c}\n\t\t  & After pre-processor & After camera inversion & After post-processor & Lensless/ Lensed \\\\\n\n    $\\alpha=0$ \\hspace{0.5em} & \\includegraphics[width=\\figsizeaux\\linewidth,valign=m]{figure/exp2/res0_pre.png}\n  & \\includegraphics[width=\\figsizeaux\\linewidth,valign=m]{figure/exp2/res0_pre_post.png}\n  &\\includegraphics[width=\\figsizeaux\\linewidth,valign=m]{figure/exp2/res0_final.png}\n  & \\includegraphics[width=\\figsizeaux\\linewidth,valign=m]{figure/exp2/res0_lensless.png}\n\\\\[\\newlineaux]\n  \n\t\t$\\alpha=0.1$ \\hspace{0.5em} & \\includegraphics[width=\\figsizeaux\\linewidth,valign=m]{figure/exp2/res0.1_pre.png}\n  & \\includegraphics[width=\\figsizeaux\\linewidth,valign=m]{figure/exp2/res0.1_pre_post.png}\n  & \\includegraphics[width=\\figsizeaux\\linewidth,valign=m]{figure/exp2/res0.1_final.png}\n  & \\includegraphics[width=\\figsizeaux\\linewidth,valign=m]{figure/exp2/res0.1_lensed.png}\n\\\\\n\t\\end{tabular}\n\t\n\t\\endgroup\n\t\\caption{Influence of auxiliary loss from reconstruction output: top row does not use the auxiliary loss during training; bottom row uses the auxiliary loss with a weight of $\\alpha=0.1$.}\n  \\label{fig:residual}\n\\end{figure*}\n\n\n\n\n\n\\subsection{Auxiliary loss}\n\\label{results:res}\n\n\n\n\n\nWith this experiment, we evaluate the effect of incorporating an auxiliary loss of the unrolled output to the total loss.\nTo this end, we add different amounts of the unrolled output loss, \\ie $\\alpha = [0, 0.01, 0.03, 0.1]$ for \\cref{eq:loss_res} ($\\alpha=0$ is equivalent to no auxiliary loss).\nThe goal is to improve the interpretability of the reconstruction approach, such that insight can be gained from the intermediate outputs.\n\n\\cref{fig:vary_alpha} shows the PSNR and LPIPS of a combined pre- and post-processor approach (\\textit{UNet2+Unrolled20+UNet2}) as $\\alpha$ varies.\nThe metrics slightly deteriorate as $\\alpha$ increases, which is expected \n\n\nas the image prior to the post-processor is poorer in quality.\nNonetheless, this decrease in image quality metrics is minimal: \\SI{0.301}{\\decibel} in PSNR and \\SI{4.13}{\\percent} relative increase in LPIPS from $\\alpha=0$ to $\\alpha=0.1$.\n\\cref{fig:residual} shows an example of intermediate outputs with and without the auxiliary loss during training.\nThe final outputs appear very similar with and without the auxiliary loss, whereas the image after the camera inversion is more discernible with the auxiliary loss.\nFrom these intermediate outputs, we can observe similar results as before: the pre-processor prepares the input image for improved camera inversion while the post-processor puts the finishing touches.\n\n\\subsection{Benchmark with previous work}\n\\label{results:ablation}\n\n\n\\begin{figure*}[t!]\n\\centering\n\t\\begingroup\n\t\\renewcommand{\\arraystretch}{1} \n\t\\setlength{\\tabcolsep}{0.2em} \n\t\n\t\\begin{tabular}{cccccc}\n\t\t  \n    Plug-and-play~\\cite{pnp} & \n    Unrolled20~\\cite{Monakhova:19} &\\makecell{Unrolled20\\\\+DRUNet~\\cite{Monakhova:19}}& \\makecell{TrainInv\\\\+DRUNet~\\cite{flatnet}} & \\makecell{UNet2+Unrolled20\\\\+UNet2 (Proposed)} & Lensed \\\\\n    \n    \n    \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp3/admm_pnp_0.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp3/U20_0.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp3/U20+Drunet_0.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp3/TrainInv+Drunet_0.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp3/Unet+U20+Unet_res0.01_0.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp3/lensed_0.png} \\\\\n    \n    \n    \n    \\insertwithsubimagenew[98 72 90 35]{\\figsizebench\\linewidth}{figure/exp3/admm_pnp_1.png} & \n    \\insertwithsubimagenew[98 72 90 35]{\\figsizebench\\linewidth}{figure/exp3/U20_1.png} & \n    \\insertwithsubimagenew[98 72 90 35]{\\figsizebench\\linewidth}{figure/exp3/U20+Drunet_1.png} & \n    \\insertwithsubimagenew[98 72 90 35]{\\figsizebench\\linewidth}{figure/exp3/TrainInv+Drunet_1.png} & \n    \\insertwithsubimagenew[98 72 90 35]{\\figsizebench\\linewidth}{figure/exp3/Unet+U20+Unet_res0.01_1.png} & \n    \\insertwithsubimagenew[98 72 90 35]{\\figsizebench\\linewidth}{figure/exp3/lensed_1.png} \\\\\n    \n    \n    \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp3/admm_pnp_3.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp3/U20_3.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp3/U20+Drunet_3.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp3/TrainInv+Drunet_3.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp3/Unet+U20+Unet_res0.01_3.png} & \n    \\insertwithsubimagenew[88 17 100 90]{\\figsizebench\\linewidth}{figure/exp3/lensed_3.png} \\\\\n    \n    \n    \n    \\insertwithsubimagenew[73 70 115 37]{\\figsizebench\\linewidth}{figure/exp3/admm_pnp_4.png} & \n    \\insertwithsubimagenew[73 70 115 37]{\\figsizebench\\linewidth}{figure/exp3/U20_4.png} & \n    \\insertwithsubimagenew[73 70 115 37]{\\figsizebench\\linewidth}{figure/exp3/U20+Drunet_4.png} & \n    \\insertwithsubimagenew[73 70 115 37]{\\figsizebench\\linewidth}{figure/exp3/TrainInv+Drunet_4.png} & \n    \\insertwithsubimagenew[73 70 115 37]{\\figsizebench\\linewidth}{figure/exp3/Unet+U20+Unet_res0.01_4.png} & \n    \\insertwithsubimagenew[73 70 115 37]{\\figsizebench\\linewidth}{figure/exp3/lensed_4.png} \\\\\n    \n    \n    \n    \\insertwithsubimagenew[34 37 154 70]{\\figsizebench\\linewidth}{figure/exp3/admm_pnp_8.png} & \n    \\insertwithsubimagenew[34 37 154 70]{\\figsizebench\\linewidth}{figure/exp3/U20_8.png} & \n    \\insertwithsubimagenew[34 37 154 70]{\\figsizebench\\linewidth}{figure/exp3/U20+Drunet_8.png} & \n    \\insertwithsubimagenew[34 37 154 70]{\\figsizebench\\linewidth}{figure/exp3/TrainInv+Drunet_8.png} & \n    \\insertwithsubimagenew[34 37 154 70]{\\figsizebench\\linewidth}{figure/exp3/Unet+U20+Unet_res0.01_8.png} & \n    \\insertwithsubimagenew[34 37 154 70]{\\figsizebench\\linewidth}{figure/exp3/lensed_8.png} \\\\\n\t\\end{tabular}\n\t\n\t\\endgroup\n\t\\caption{Results of different reconstruction approaches.}\n  \\label{fig:benchmark}\n\\end{figure*}\n\n\n\\begin{table}[t!]\n  \\centering\n  \\scalebox{0.82}{\n  \\begin{tabular}{|c|c|c|c|c|c|}\n    \\hline\n    Method & PSNR $\\uparrow$           & LPIPS  $\\downarrow$   & \\makecell{\\# learnable \\\\paramaters} & \\makecell{Inference\\\\time [ms]} \\\\\n    \\hline\n    ADMM100~\\cite{Diffuser3D}                      &     15.2   &   0.547    &  - & 56.0\\\\\\hline\n    \n    Plug-and-play~\\cite{pnp} & 14.9 & 0.590 & - &  549 \\\\\\hline\n    Unrolled20~\\cite{Monakhova:19}   &  13.3         &    0.424    &  80  & 13.4 \\\\\\hline\n    \\makecell{Unrolled20\\\\+DRUNet~\\cite{Monakhova:19} } \n     &     23.4    &      0.204    &  32.6M   & 22.4 \\\\\n     \\hline\n    TrainInv+DRUNet~\\cite{flatnet}  &    21.7    &   0.246      &   32.7M  & 8.99 \\\\\n    \\hline \\hline\n    \\makecell{UNet2+Unrolled20\\\\+UNet2 }\n     &     \\textbf{25.3}    &   \\textbf{0.175 }        &  34.0M  &   23.1\\\\\\hline\n    \\makecell{UNet2+TrainInv\\\\+UNet2 }\n     &    22.7   &    0.224   &  34.1M  &  9.75\\\\\\hline\n    \n    \n  \\end{tabular}}\n  \\caption{Results of different models on the test set.}\n  \\label{tab:ablation}\n  \\vspace{-0.5cm}\n\\end{table}\n\nIn this final experiment, we compare the proposed reconstruction pipeline with baselines from previous work.\n\n\\cref{tab:ablation} presents our results, and \\cref{fig:benchmark} compares reconstructions on test set images. The best result is obtained with the proposed pre- and post-processing approach (last rows in \\cref{tab:ablation} and last column in \\cref{fig:benchmark}), with \\textit{UNet2} as a pre-processor and \\textit{UNet2} as the post-processor.\n\nWhile the first two approaches in \\cref{tab:ablation} require no training data, they miss the necessary enhancement to address artifacts of lensless imaging and reconstruction. \n\\textit{Plug-and-play} applies 20 iterations of ADMM and replaces the soft-thresholding proximal step with \\textit{DRUNet} (without having to fine-tune), but \\textit{DRUNet} has only been trained with Gaussian noise and cannot deal with the artifacts either.\nWith training data, \\textit{Unrolled20} significantly improves LPIPS with respect to \\textit{ADMM100} (PSNR is worse as MSE and LPIPS are equally weighted in the loss); but it cannot address color correction.\n\\textit{Unrolled20+DRUNet} is similar to \\textit{Le-ADMM-U} from~\\cite{Monakhova:19}; the added processor (fine-tuned \\textit{DRUNet}) significantly improves performance.\n\\textit{TrainInv+DRUNet}, where \\textit{TrainInv} fine-tunes the PSF used for inversion as in~\\cite{flatnet}, also exhibits improved performance compared to \\textit{Unrolled20}. \nHowever, \\textit{TrainInv} is less performant as a camera inverter than \\textit{Unrolled20} as the latter uses multiple iterations, but it also makes \\textit{TrainInv} faster (more than $2\\times$).\nFor both  camera inversion approaches, we obtain the best results by splitting the enhancement parameters between the pre- and post-processors as proposed in this work (last rows of \\cref{tab:ablation}).\n\n\n\\section{Conclusion}\n\\label{sec:conclusion}\n\nIn this paper, we propose a novel lensless image reconstruction pipeline. It consists of three components trained end-to-end: (1) a newly-proposed pre-processor, (2) a camera inversion block, \\eg unrolled ADMM to incorporate knowledge of the physical system, and (3) a post-processor. \nOur results demonstrate the robustness of the system to a wide range of SNRs, and the effectiveness of the proposed pipeline: \\SI{1.9}{\\decibel} increase in PSNR and \\SI{14}{\\percent} relative improvement in LPIPS with respect to previously proposed physics-based methods.\nReproducibility and well-structured code are also a priority: the baseline and proposed techniques have been incorporated into \\textit{LenslessPiCam}~\\cite{LenslessPiCam} in an object-oriented fashion.\n\nFor future work, it would be interesting to jointly optimize the PSF and reconstruction parameters for a specific task~\\cite{khan23}.\nMoreover, we saw the generalizability of the pre-processor to different SNRs than at training. \nIt would be interesting to explore how well the proposed modular pipeline generalizes for other lensless systems, and if the components trained for one system can generalize to other systems (without re-training).\n\n\n\n\n\n\n\n\\pagebreak\n\\bibliographystyle{IEEEbib}\n\n\n\\end{document}\n"}
{"paper_id": "2403-00538", "version": "2403-00538v1", "root_file_path": "d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\data_raw\\2403-00538\\tex\\2403-00538v1\\main.tex", "metadata": {"total_length": 31880, "merged_count": 1, "merged_files": ["main.tex"], "missing_files": []}, "content": "\n\n\n\n\\documentclass[\nreprint,\nsuperscriptaddress,\n\n\n\n\n\nshowpacs,preprintnumbers,\nnofootinbib,\n\n\namsmath,assume,\naps,\n\n\n\nprb\n\n\n\n\nshowkeys\n]{revtex4-2}\n\\usepackage{graphicx}\n\\usepackage{dcolumn}\n\\usepackage{bm}\n\n\\usepackage{amssymb}\n\\usepackage{amsmath}\n\\usepackage{amsfonts}\n\\usepackage{xspace}\n\\usepackage{bm,bbm}\n\\usepackage{relsize}\n\\usepackage{siunitx}\n\\usepackage[usenames,dvipsnames]{color}\n\\usepackage{float}\n\\usepackage{hyperref}\n\\usepackage[toc,page]{appendix}\n\\usepackage{float}\n\\usepackage{array}\n\\newcolumntype{L}[1]{>{\\raggedright\\let\\newline\\\\\\arraybackslash\\hspace{0pt}}m{#1}}\n\\newcolumntype{C}[1]{>{\\centering\\let\\newline\\\\\\arraybackslash\\hspace{0pt}}m{#1}}\n\\newcolumntype{R}[1]{>{\\raggedleft\\let\\newline\\\\\\arraybackslash\\hspace{0pt}}m{#1}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\hypersetup{\n    \n    unicode=false,          \n    pdftoolbar=true,        \n    pdfmenubar=true,        \n    pdffitwindow=false,     \n    pdfstartview={FitH},    \n    pdftitle={Optical bistability of zero-energy modes in Su-Schrieffer-Heeger lattice with Kerr nonlinearity},    \n    pdfauthor={Alharbi, Wong, Gong, Leykam, Oh},     \n    pdfsubject={},   \n    pdfcreator={},   \n    pdfproducer={}, \n    pdfkeywords={} {} {}, \n    pdfnewwindow=true,      \n    colorlinks=true,       \n    linkcolor=blue, \n    citecolor=blue,        \n    filecolor=magenta,      \n    urlcolor=blue,           \n\tbreaklinks=true\n} \n\n\n\n\\begin{document}\n\n\n\n\n\n\n\n\\title{Asymmetrical temporal dynamics of topological edge modes in Su-Schrieffer-Heeger lattice with Kerr nonlinearity} \n\n\n\\author{Ghada Alharbi}\n\\affiliation{School of Physics and Astronomy, Cardiff University, Cardiff CF24 3AA, United Kingdom}\n\\affiliation{Department of Physics, University of Tabuk, Tabuk 741,  Kingdom of Saudi Arabia}\n\\author{Stephan Wong}\n\\affiliation{School of Physics and Astronomy, Cardiff University, Cardiff CF24 3AA, United Kingdom}\n\\affiliation{Center for Integrated Nanotechnologies, Sandia National Laboratories, Albuquerque, New Mexico 87185, USA}\n\\author{Yongkang Gong}\n\\affiliation{School of Physics and Astronomy, Cardiff University, Cardiff CF24 3AA, United Kingdom}\n\n\n\\author{Sang Soon Oh}\n\\email[Email:]{ohs2@cardiff.ac.uk}\n\\affiliation{School of Physics and Astronomy, Cardiff University, Cardiff CF24 3AA, United Kingdom}\n\n\\date{\\today}\n\n\\begin{abstract}\nOptical bistability and oscillating phases exist in a Sagnac interferometer and a single ring resonator made of $\\chi^{(3)}$ nonlinear medium where the refractive indices are modulated by the light intensity due to the Kerr nonlinearity. An array of coupled nonlinear ring resonators behave similarly but with more complexity due to the presence of the additional couplings.\nHere, we theoretically demonstrate the bifurcation of topological edge modes which leads to optical bistability in the Su-Schrieffer-Heeger lattice with the Kerr nonlinearity. Additionally, we demonstrate periodic and chaotic switching behaviors in an oscillating phase resulting from the coupling between the topological edge mode and bulk modes with different chiralities, i.e., clockwise and counter-clockwise circulations. \n\\end{abstract}\n\n\\keywords{bi-stability, Kerr effect, topological photonics}\n\n\\maketitle\n\n\\section{Introduction}\nAsymmetrical states emerge when a system lose the balance and thus a symmetry between different components is broken. This can lead to bistability, where the system has two stable states for a single excitation. In some cases, asymmetrical dynamic states can emerge with periodic or chaotic oscillatory  behaviors. In photonic systems, optical bistability can appear when the light transmits through a cavity with a nonlinear medium leading to two different optical states, where one mode is dominant (switched on) and the other is quenched (switched off) \\cite{Boyd2008}. \n\nFor instance, the stable symmetry breaking has been studied for counter-propagating light beams in a Sagnac interferometer \\cite{Asymmetrical1982} and micro-resonator with Kerr nonlinearity \\cite{ DelBino2017a, Silver2018, Shim2016, Xu2021, Coen2022}. More interestingly, nonlinear optical ring resonators can present rich temporal dynamics with oscillatory behaviors, displaying various types of mode switching, such as chaotic, periodic, and self-switching dynamics \\cite{Hill2020a}. These various dynamics are the result of the nonlinear interaction between the counter-propagating modes and they manifest the symmetry breaking, i.e., unequal intensities of the two counter-propagating modes.\n\nRecently, topologically protected modes have been widely studied in photonic systems due to their intriguing properties, such as unidirectional light propagation and robustness to defects and disorders~\\cite{Ozawa2019, Khanikaev2013}. \nIn particular, topological edge modes in a one-dimensional (1D) Su-Schrieffer-Heeger (SSH) configuration and two-dimensional (2D) photonic quantum spin-Hall or quantum valley-Hall structures have been employed in an array of coupled lasers, namely topological lasers \\cite{Noh2018, Han2019, Harari2018b, Bandres2018, Gong2020, Wong2023}.\nMoreover, nonlinear topological photonics has been studied in various platforms such as waveguide arrays \\cite{Leykam2016, Zhou2017, Maczewsky2020, Ivanov2023}, microcavity polariton systems \\cite{ Kartashov2017} and optical resonators \\cite{Hadad2016, Dobrykh2018, Leykam2020,  Dobrykh2018, Roy2021, Ezawa2021, Ezawa2022, Wei2023}. \nThe phase diagrams of a nonlinear SSH model and nonlinear breathing kagome model were drawn for the nonlinear parameters and coupling coefficients between sites \\cite{Ezawa2021}. Also, the edge solitons have shown to be stable at any energy when the ratio between the weak and strong couplings falls below a critical value \\cite{Ma2021}. Up until now, however, no research has demonstrated spontaneous symmetry breaking coming from the nonlinear response for the edge modes in photonic topological insulators.\n\n\nIn this paper, we theoretically show that we can observe asymmetrical temporal dynamics, including optical bistabilities and oscillation phases for topological edge modes in a nonlinear 1D SSH model. The system consists of an array of coupled ring resonators with the Kerr nonlinearity. \n\nUsing the Lugiato-Lefever equation \\cite{Lugiato1987} with additional nearest neighbor couplings, we demonstrate the optical bistability of the topological edge mode in the nonlinear SSH lattice. \n\nFinally, we use Poincar\\'{e} section plots, composed of the maxima of the oscillating intensities, to display the oscillation phases featuring periodic and chaotic switching.\n\n\\section{Linear SSH model with two counter-propagating modes}\nWe start by considering a linear SSH chain which does not have any resonance frequency shift coming from a nonlinearity.\nAs shown in Fig.~\\ref{fig1}(a), the one-dimensional chain has $(N+1)$ unit cells and every unit cell hosts two ring resonators; one on the sublattice A, and the other on the sublattice B. The $(N+1)$-th unit cell has only one ring resonator that belongs to the sublattice A, resulting in $M=2N+1$ ring resonators in total.  \n\nIn photonics, this SSH model can be implemented by alternating the gap size between the ring resonators, resulting in different intra- and inter-cell coupling coefficients $v$ and $w$ (Fig.~\\ref{fig1}(b)). \nThe coupled ring resonators are excited by two optical pumps with the same intensity, both of which are coupled into the first ring resonator but in the opposite directions exciting clockwise (CW) and counter-clockwise (CCW) modes, respectively. \nThen, the optical waves propagate back and forth through all the resonators via the couplings between the ring resonators.\n\\begin{figure}\n\t\\centering\n\t\\includegraphics{fig1}\n\t\\caption{(a) A 1D array of coupled ring resonators with alternating gap sizes.  $s_{in,\\pm}$ is the amplitude of input beams. (b) A schematic of the SSH model with resonators with Kerr nonlinearity. $B$ is the XPM strength, $v$ and $w$ are the nearest neighbor coupling coefficients between ring resonators, and $\\gamma_c$ is the coupling coefficient between the waveguide and the first ring resonator.}\n\t\\label{fig1}\n\\end{figure} \nTo calculate the intensities of circulating optical waves at all ring resonators, we describe the time evolution of the field amplitudes $a_{n}(t)$ and $b_{n}(t)$ in the $n$-th unit cell (Fig.~\\ref{fig1}(b)) for which we use the temporal coupled mode theory \\cite{Haus1983, Suh2004}.  \nThen, the coupled mode equations are written as:\n\\begin{eqnarray}\n \\frac{da_{n,\\pm}}{dt} &=&  i \\Big(\\omega_0  + i \\gamma_n\\Big)  a_{n,\\pm} +  i v b_{n,\\mp} +i w b_{n-1,\\mp}  \\nonumber \\\\ \n&&  + \\delta_{n,1} \\gamma_c s_{in} ,\\nonumber \\\\ \n \\frac{db_{n,\\pm}}{dt} &=&  i \\Big(\\omega_0  + i \\gamma'_n\\Big) b_{n,\\pm}  + i v a_{n,\\mp} + i  w a_{n+1,\\mp} ,\n\\label{CMT}\n\\end{eqnarray}\nwhere\n\\begin{eqnarray}\n  \\gamma_n &=& \\gamma_0 + \\delta_{n,1}\\gamma_c ,\\nonumber \\\\\n  \\gamma'_n &=& \\gamma_0 .\n\\end{eqnarray}\n\n\n\nHere, the subscript $\\pm$ denotes the mode propagation directions, CW and CCW, respectively. \n\n$\\delta_{n,1}$ is the Kronecker delta and $\\omega_0$ is\nthe resonance frequency of the uncoupled ring resonators.\nThe two input beams with the same amplitude $s_{in}$, which is given as $\\sqrt{I_s} e^{i\\omega t}$ for pump intensity $I_s$, are coupled to the CW and CCW modes in the first ring ($a_{1,\\pm}$) with the waveguide-to-ring coupling coefficient $\\gamma_c$. Note that only $a_{n}$'s and $b_{n}$'s are time-dependent functions and we have omitted the symbol $(t)$ for brevity. \n\nTo be more compact, we express the coupled mode equations (Eq.~(\\ref{CMT})) in a matrix form by using the Hamiltonian $\\mathbf{H}$ as\n\\begin{eqnarray}\n\\frac{d \\mathbf{x} }{dt} = \\mathbf{H} \\mathbf{x} + \\mathbf{S} \n\\label{main_eq}\n\\end{eqnarray}\nwhere \n\\begin{equation*}\n    \\mathbf{x} = (a_{1,+} , b_{1,-},  a_{2,+},  b_{2,-}, \\ldots, a_{1,-},  b_{1,+},  a_{2,-} , b_{2,+}, \\ldots )^\\mathsf{T}   . \n\\end{equation*} \nNote that the Hamiltonian $\\mathbf{H}$ can be differently defined after multiplying $i$ in both sides, which makes the equation look like the Schr\\\"{o}dinger equation and makes its eigenvalues correspond to the real parts of the frequencies. However, we have chosen this notation to make Eq.~(\\ref{main_eq}) similar to Lugiato-Lefever equation which we will explain in the following section.  \n\nThen, the Hamiltonian $\\mathbf{H} $ can be split into two terms like:\n\\begin{equation}    \n\\mathbf{H} = \\mathbf{H}_0 + \\mathbf{H}_c\n\\label{linear_hamiltonian}\n\\end{equation}\nwhere\n\\begin{equation}    \n\\mathbf{H}_0 = i (\\omega_0 + i\\gamma_0 ) \\mathbb{I}_M\n\\end{equation}\nwith $\\mathbb{I}_M$ the $(M \\times M)$ identity matrix.\nThe ring-to-ring coupling is expressed as:\n\\begin{equation}    \t\n\\mathbf{H}_c = i \\begin{pmatrix}\n0 &v &0 & 0 &\\cdots\\\\\nv & 0 & w & 0 &\\cdots\\\\\n0 &  w & 0 & v  &\\cdots\\\\\n0 & 0  & v& 0& \\cdots \\\\\n\\vdots& \\vdots&\\vdots& \\vdots& \\ddots\n\\end{pmatrix}.\n\\end{equation} \n\nFinally, the source term  $  \\mathbf{S} $ is expressed as $ [s_{in,+}, 0, 0, \\cdots,  s_{in,-}, 0, 0, \\cdots ]^\\mathsf{T} $. For the remainder of this paper, we assume the symmetric pumping by setting $s_{in,+} = s_{in,-}$.\n\n\nThe coupled mode equations (Eq.~(\\ref{main_eq})) can be solved in both frequency and time domains. \nFor example, in the frequency domain, by assuming $ \\mathbf{x} = \\mathbf{\\tilde{x}} \\exp(i \\omega t)$ and $\\mathbf{S} = 0$, we obtain an eigenvalue equation \n\\begin{equation}   \ni \\omega \\mathbf{\\tilde{x}}  = \\mathbf{H} \\mathbf{\\tilde{x}}\n.\n\\label{frequency_domain}\n\\end{equation}\nSolving the eigenvalue equation gives an frequency spectrum with so-called  zero-energy modes that are topologically protected and localized on one of the edges of the SSH chain with the smaller coupling coefficient among $v$ and $w$. In this work, we will use the term \\emph{edge modes} because their frequencies deviate from the resonance frequency $\\omega_0$ and thus they are not any more zero-energy modes for nonlinear cases. We call the rest of the modes \\emph{bulk modes} as the mode fields are delocalized over the entire SSH lattice.\n\n\\section{Lugiato-Lefever equation for nonlinear SSH model}\nNow we introduce the Kerr nonlinearity in the linear SSH model. \nIn optics, the Kerr nonlinearity induces various nonlinear effects, for instance, self-phase modulation (SPM),  cross-phase modulation (XPM), four-wave mixing, and two-photon absorption \\cite{Agrawal2019}. \nHere, we only consider the SPM and XPM for the counter-propating rotating modes in the ring resonators, both of which lead to a shift of the resonance frequencies of the CW or CCW modes. \nAlthough only the couplings due to the XPM are shown in Fig.~\\ref{fig1}(b), the frequency shift $\\Delta \\omega$ is expressed by $(AI_{n,+} + BI_{n,-})$ where $I_{n,+}$ and $I_{n,-}$ are the intensities of the CW and CCW modes in the $n$-th ring resonator, respectively. $A$ and $B$ are  the SPM and XPM nonlinear coefficients, respectively.\n\nFor simplicity, we use the normalized Lugiato-Lefever equation to describe the field amplitudes in our nonlinear SSH model \\cite{Lugiato1987}. Notably, the equation is equivalent to the one derived from the temporal coupled-mode theory (see the Appendix \\ref{appendix:LLeq} for more details). \n\nWith the time-varying envelope amplitudes $\\tilde{a}(t)$, $\\tilde{b}(t)$, defined as $a(t)=\\tilde{a}(t) e^{i\\omega t}$, $b(t)=\\tilde{b}(t) e^{i\\omega t}$ respectively, the Lugiato-Lefever equation for a 1D SSH array of nonlinear ring resonators can be written as:\n\\begin{eqnarray}\n\\frac{d\\tilde{a}_{n,\\pm}}{d\\Bar{t}} &=&  -\\tilde{a}_{n,\\pm}-i \\eta \\Delta \\tilde{a}_{n,\\pm} \n  + i \\eta (A|\\tilde{a}_{n,\\pm}|^2 + B|\\tilde{a}_{n,\\mp}|^2)  \\tilde{a}_{n,\\pm} \\nonumber \\\\ \n&& +i v \\tilde{b}_{n\\mp} + i w \\tilde{b}_{n-1,\\mp}  +\\delta_{n,1}s_{in}, \n\\nonumber \\\\ \n\\frac{d\\tilde{b}_{n,\\pm}}{d\\Bar{t}} &=&  -\\tilde{b}_{n,\\pm}-i \\eta\\Delta \\tilde{b}_{n,\\pm} + i \\eta (A|\\tilde{b}_{n,\\pm}|^2 + B|\\tilde{b}_{n,\\mp}|^2) \\tilde{b}_{n,\\pm}   \\nonumber \\\\ \n&& +i v \\tilde{a}_{n\\mp} + i w \\tilde{a}_{n+1,\\mp} ,\n\\label{nonlinear_LL_equation}\n\\end{eqnarray}\nwhere $\\Bar{t}=t\\gamma_0$ is the dimensionless time.  The first term on the right-hand side represents damping, while the second term stands for detuning ($\\Delta=(\\omega-\\omega_0)/\\gamma_0$), which is the difference between the frequency of the continuous wave input beams and the resonance frequency of a single ring resonator. The third and fourth terms correspond to the SPM and XPM, respectively, with the normalized nonlinear coefficients $A$ and $B$, and $\\eta = +1$ for a self-focusing medium or $\\eta = -1$ for a self-defocusing medium. The terms with $v$ and $w$ refer to the intra- and inter- couplings between ring resonators as in the linear case.\nFinally, we add a nonlinear term to Eq.~(\\ref{linear_hamiltonian}) to have the Hamiltonian for our nonlinear SSH model: \n\n\\begin{equation}    \n\\mathbf{H} = \\mathbf{H}_0 + \\mathbf{H}_c +  \\mathbf{H}_{NL}  ,\n\\label{nonlinear_hamiltonian}\n\\end{equation}\nwhere \n\\begin{equation}    \n\\mathbf{H}_0 =  -(1 +i \\eta \\Delta )\\mathbb{I}_M,\n\\end{equation} and \n\\begin{equation}    \n\\mathbf{H}_{NL} = i \\eta  \\mathbb{I}_M\\times\n\\begin{pmatrix}\nA|\\tilde{a}_{1,+}|^2 + B|\\tilde{a}_{1,-}|^2  \\\\\nA|\\tilde{b}_{1,-}|^2 + B|\\tilde{b}_{1,+}|^2 \\\\\nA|\\tilde{a}_{2,+}|^2 + B|\\tilde{a}_{2,-}|^2  \\\\\nA|\\tilde{b}_{2,-}|^2 + B|\\tilde{b}_{2,+}|^2\\\\\n\\vdots \\\\\nA|\\tilde{a}_{1,-}|^2 + B|\\tilde{a}_{1,+}|^2  \\\\\nA|\\tilde{b}_{1,+}|^2 + B|\\tilde{b}_{1,-}|^2 \\\\\nA|\\tilde{a}_{2,-}|^2 + B|\\tilde{a}_{2,+}|^2  \\\\\nA|\\tilde{b}_{2,+}|^2 + B|\\tilde{b}_{2,-}|^2\\\\\n\\vdots\n\\end{pmatrix} .\n\\end{equation}\nTo obtain the temporal evolution of the amplitudes for nonlinear SSH lattice, we can solve the time-dependent equation (Eq.(\\ref{main_eq})) with this Hamiltonian. However, we cannot solve the equation in the frequency domain by simply solviing an eigenvalue equation because it is a system of nonlinear equations. \n\n\n\n\n\\begin{figure}\n   \\includegraphics{fig2.pdf}\n\t\\caption{(a) Optical bistability for seven ring resonators ($M=7$) with detuning $\\Delta=1.85$, $v=3$ and $w=7$. (b) Optical bistability with the Kerr nonlinearity in the first ring resonator only. (c), (d) The distributions of intensity for the input intensities corresponding to the dashed vertical lines in (a)}\\label{bistability}\n\\end{figure} \n\n\\section{Optical bistability of topological edge modes}\nOptical bistability in a single ring resonator is a result of the XPM between two counter-propagating modes \\cite{Kaplan1981}. \n\nThis means that the Kerr nonlinearity leads to a shift in the resonance frequency of the two counter-propagating modes due to both SPM and XPM, and the coupling via XPM between them leads to spontaneous symmetry breaking above a certain threshold pump intensity \\cite{DelBino2017a}. \nHere, we want to address the question whether we can observe the optical bistability using an edge mode in a nonlinear SSH lattice model. \n\nTo theoretically observe the optical bistability in the nonlinear SSH lattice, we consider  a SSH array of seven ring resonators ($M=7$) with the nonlinear parameter and the detuning in Ref. \\cite{Kaplan1981} and the alternating coupling coefficients ($v=3$ and $w=7$). In our simulations, we scan the pump intensity $I_s$ for a certain interval with random initial conditions. Indeed, as shown in Fig.~\\ref{bistability}(a), we observe the optical bistability in the range between $\\log(I_s+1)=1.9$ and $\\log(I_s+1)=2.9$ of pump intensity. \n\nNote that the field amplitudes are relatively large for odd sites only (sublattice A), and the intensity decreases exponentially along the right direction for both single stable (Fig.~\\ref{bistability}(c)) and bistable cases (Fig.~\\ref{bistability} (d)), which is the reminiscence of the zero-energy edge modes.\n\n\n\n\n\n\\begin{figure*}\n   \\includegraphics{fig3.pdf}\n\t\\caption{Poincar\\'{e} section of the maxima of the CW and CCW intensity time series for the first ring resonator in 1D SSH lattice composed seven ring resonators with alternating coupling, $v=3$ and $w=7$. (a) Maximum intensity curves for low input intensities $I_s=0.05$ (cyan), $0.1$ (green), $0.2$ (blue), $0.4$ (red), $0.6$ (yellow), $0.8$ (brown). (b) For high input intensities  $I_s = 2$ (red), $6$ (green), $10$ (blue), $14$ (cyan), $16$ (yellow), $20$ (brown), both optical bistability and oscillation regions appear. The vertical dashed lines indicate frequencies determined by the eigenvalue equation for the linear case (Eq.(\\ref{frequency_domain})).} \\label{compare_intensity}\n\\end{figure*} \n\n\nTo explain the origin of the observed optical bistability, we hypothesize that the optical bistability comes from the symmetry breaking in the first ring only. \nFirst, optical bistability in a single ring resonator can occur when the pump intensity is above a certain threshold, called a bifurcation point. This means the first ring will show the optical bistability first as we increase the pump intensity under an excitation close to the zero-energy frequency. Indeed, the detuning $\\Delta = 1.85$ is smaller than the topological band gap ($2|v-w| = 8$) meaning the zero-energy edge mode is  dominantly excited even though it is off-resonance.\nThis is supported by the field intensity distribution in  Fig.~\\ref{bistability}(d). \nAs the intensities in the rest of rings are much smaller than the first ring (Fig.~\\ref{bistability}(d)), only the first ring introduces bistability and \nthe modes in the first ring couple to the other rings successively instead of having additional optical bistability from the rest of the rings. \nSecond, to confirm this propagation, we consider the Kerr effect only in the first ring resonator but keep all other parameters the same. This is equivalent to switching off the Kerr effect in the $2N$ ring resonators except the first ring resonator in our original setting.  \nAs shown in Fig.~\\ref{bistability}(b), the intensity-intensity curve has almost identical shape as the original one except slight reduction in the range of $I_s$ and slight change in the difference between two counter-propagating mode intensities.\n\n\n\n\n\\begin{figure}\n   \\includegraphics{fig4.pdf}\n\t\\caption{Poincar\\'{e} sections of the maxima of oscillating coupled intensity as a function of detuning for input intensity $I_s=20$, for the first ring resonator from the 1D SSH array. The Poincar\\'{e} sections for coupling coefficients ($v=3$, $w=7$) and XPM strength ($B$) of 4 and 7 in (a) and (b), respectively. The red shading indicates a symmetric case, the yellow indicates optical bistability, and the cyan indicates oscillations. The vertical dashed lines refer to resonance frequencies for the linear 1D SSH lattice.} \\label{poincare}\n\\end{figure} \n\n\n\n\n\\begin{figure}   \\includegraphics{fig5.pdf}\n\t\\caption{Snapshots of intensity distribution in the nonlinear 1D SSH lattice for the CW modes in the left column and the CCW modes in the right column for different values of detuning with $I_s=20$, $B=4$.} \\label{distribution_d}\n\\end{figure} \n\n\n\n\n\\begin{figure}   \n\\includegraphics{fig6.pdf}\n\t\\caption{Time series of intensity and their phase space trajectories for $A=1$, $B=4$ and $I_s=20$ at different values of detuning, for the first ring resonators from 1 D SSH array of 7 rings. (a), (b) Optical bistability phase with $\\Delta=5.51$. (c), (d) Oscillations without overlapping trajectories. (e), (f) Periodic switching for $\\Delta=5.93$. (g), (h) Chaotic switching with $\\Delta=7.73$ }\\label{time_ev}\n\\end{figure} \n\n\n\\section{Asymmetrical temporal dynamics}\nNow, let us look at the temporal evolution of the optical intensities of a 1D SSH array that contains seven ring resonators.\nTo visualize oscillation and chaotic phases in our nonlinear system, we will use the Poincar\\'{e} section \nobtained by plotting all the local maxima in a time series of oscillating intensities \\cite{Hill2020a}. \nSince the intensity of each ring in the 1D SSH array follows the same pattern as the intensity of the first ring (see Fig.~\\ref{odd_even} in Appendix \\ref{appendix:poincare_SSH}), we plot the Poincar\\'{e} sections for the first ring resonator only. \n\nAs shown in Fig.~\\ref{compare_intensity} (a)(b), the nonlinear SSH lattice exhibits both bistability and oscillation phases in the range of detuning corresponding to the edge mode and two bulk modes with positive detuning for the linear SSH lattice (denoted as the vertical dashed lines). \nHere, we set $A=1$ and $B=4$ and change the input intensity from 0.05 to 20 denoted with different colors. \nAs we can see in the zoomed view of the plots, these spectra show seven resonance modes; one edge mode with the largest intensity in the middle and six bulk modes on both sides of the edge mode having three on each side.  \nFor low input intensities (Fig.~\\ref{compare_intensity}(a)), the edge mode shifts dramatically and its intensity increases significantly, whereas the bulk modes shift less and their intensities increase slightly. \nThis is due to the localization of intensity at the first ring resonator. \nFor high input intensities (Fig.~\\ref{compare_intensity}(b)), the CW and CCW modes for edge mode undergo an interaction between them via XPM, leading to an optical bistability. Remarkably, the high input intensity leads to the interaction between the edge mode and the bulk mode near $\\Delta = 5.2$, resulting in a series of oscillation phases occurring for both CW and CCW modes.\n\nThe range of detuning of asymmetrical phases also depends on the XPM strength. \nFigure~\\ref{poincare} (a) and (b) compare the Poincar\\'{e} sections for two different values of XPM strength $B$. \nOne can observe that increasing $B$ leads to a larger range of detuning for asymmetrical phases including  optical bistability (yellow) and oscillation phases (blue). \nThe Poincar\\'{e} section as a function of XPM strength can be found in Fig.~~\\ref{poincareB} in Appendix \\ref{appendix:poincare_B}.\n\nTo better understand the asymmetrical dynamic modes, we show the spatial distributions and temporal changes of the excited mode intensity for different detuning in Fig.~\\ref{distribution_d} and Fig.~\\ref{time_ev}. Here, we focus on the case of $I_s = 20$ and $B=4$ as an example of high input intensity. \nFor the optical bistability ($\\Delta = 4.77$) shown in  Fig.~\\ref{distribution_d}(a),(b), both CW and CCW modes have contrasting intensity values, whereas their profiles are similar to the zero-energy edge mode's profile in a linear SSH model with exponentially decaying non-zero odd-site intensities and zero even-site intensities. \nThe deviations can be attributed to the off-resonance excitation and the interaction between CW and CCW modes via the nonlinear process (XPM). \n\nNote that the excited mode is stable as they have constant intensities and appear as two separate points its phase space (Fig.~\\ref{time_ev} (a),(b)). \n \nWhen we increase the detuning further to $\\Delta = 5.51$, both CW and CCW mode profile deviates further away from the zero-energy edge mode but the CCW mode profile deviates less still having low intensities at even sites (Fig.~\\ref{distribution_d}(c),(d)). Here, the largest value at the first site is related to the zero-energy edge mode and also due to the fact we are exciting the ring resonators from the waveguide on the left side. \nThe dynamics for this detuning (Fig.~\\ref{time_ev} (c)(d)) is periodically oscillatory, resulting in two distinct regions in the phase space meaning the CW mode intensity is always larger than the CCW mode intensity (the trajectory for CW is further away from the origin). \nFor slightly larger detuning of $\\Delta = 5.93$, the two trajectories are merged into one meaning that the intensities between the two modes alternates. In the phase space, they are in different two points with the $\\pi$ phase difference in the same trajectory. \nFor a large detuning of $\\Delta = 7.73$,  we see chaotic oscillations showing two separate trajectories covering a similar region in the phase space (Fig. ~\\ref{time_ev}(g),(h)).\n\nIn contrast to the optical bistability coming from the coupling between two counter-propagating modes via nonlinearity, the emergence of the periodic and chaotic oscillations come from the coupling between the edge mode and the bulk modes. \nThe reasoning is below. \nFirst, the resonance frequency shift of the edge mode when increasing the intensity is much larger than the ones for the bulk modes as shown in Fig. ~\\ref{poincare}. \nSecond, although the intensity distributions for CW and CCW are close to the bulk modes, there are clear signatures of the edge modes, i.e., an exponentially decaying odd-site intensities and nearly zero even-site intensities (Fig.~\\ref{distribution_d} (d)(f)(g)). \nThus, our numerical simulations confirm that the edge mode overlap with bulk mode due to the Kerr nonlinearity results in the periodic and chaotic oscillations.\n\n\n\\section{Conclusions}\nIn summary, we have numerically demonstrated the optical bistability and various types of oscillations in a 1D SSH model composed of ring resonators with Kerr nonlinearity. \nWhen the nonlinear terms are introduced in the Lugiato-Lefever equation, the first ring's CW and CCW mode intensities are symmetric until the pump intensity reaches a bifurcation point. Above the bifurcation point, the symmetry is spontaneously broken due to the splitting of the resonance frequencies of the two CW and CCW modes in the first ring resonator. \nFor the high input intensity regime, we have observed oscillating phases including periodic and chaotic oscillations.\nThe periodic oscillation phases can also be classified into two different phases where the trajectories are separate or identical in the phase space of the mode intensities. \nThis emergence of the oscillating phases can be attributed to the coupling between the edge mode and bulk mode due to the large shift of resonance peaks of the edge mode. \n\n\nWe believe that our theoretical models and numerical results will provide valueable insight in understanding the complex dynamics in coupled nonlinear resonator systems with two chiral modes. \n\nAdditionally, the various spatio-temporal dynamics could be applied to optical switching devices as well as the stability analysis of coupled lasers. \n\n\\begin{acknowledgments}\nWe are grateful to Daniel Leykam for fruitful discussions. The work is part-funded by the European Regional Development Fund through the Welsh Government (80762-CU145 (East)).\n\\end{acknowledgments}\n\n\n\\appendix\n\\renewcommand{\\thefigure}{A\\arabic{figure}}\n\\setcounter{figure}{0}\n\n\\section{Derivation of the Lugiato-Lefever equation using the coupled mode theory}\n\\label{appendix:LLeq}\nThe coupled mode theory \\cite{Haus1983} has been used to describe the field amplitude $a$ propagating in an optical ring resonator, which can be written as\n\\begin{equation}\n    \\frac{da}{dt}= i\\omega_0 a - \\gamma a + \\gamma_c s.\n    \\label{cmt}\n\\end{equation}\nHere $\\omega_0$ refers to the resonance frequency, $\\gamma $ and $\\gamma_c$ are the damping and coupling with the source coefficient, respectively.\nwe can express field amplitude $a$ in terms of envelope amplitude $\\tilde{a}$ as:\n\\begin{equation}\n    a = \\tilde{a} e^{i\\omega t},\n\\end{equation}\nby substituting in Eq.(\\ref{cmt}) :\n\n\n\n\\begin{equation}\n    \\frac{d\\tilde{a}}{dt} =  [i(\\omega_0 - \\omega )-\\gamma]  \\tilde{a}+ \\gamma_c \\tilde{s},\n\\end{equation}\nwhere $-\\tilde{\\Delta}=\\omega_0 - \\omega$, then we can rewrite this equation in terms of detuning as :\n\\begin{equation}\n    \\frac{d\\tilde{a}}{dt} =  \n    [-\\gamma - i\\tilde{\\Delta}] \\tilde{a} + \\gamma_c \\tilde{s}.\n\\end{equation}\nThis equation is equivalent to the Lugiato-Lefever equation without nonlinearity terms; the terms in RHS correspond to damping, detuning, and source terms, respectively. \n\n\n\\section{Site-dependence of Poincar\\'{e} sections for nonlinear SSH model}\n\\label{appendix:poincare_SSH}\nFigure~\\ref{odd_even} (a), (b) displays Poincar\\'{e} sections of maxima of oscillating in coupled intensities $I_{max,\\pm}$ for odd (sublattice A) and even (sublattice B) sites in the 1D SSH lattice, respectively. The odd-site intensities of the CW and CCW modes follow the same pattern as the ones for the first ring resonator, while the even-site intensities follow the same patter as the one the second ring resonator. The symmetry is  broken, i.e., the CW and CCW mode intensities are notequal for the optical bistability and oscillation phases.\n\n\n\n\\begin{figure*}\n   \\includegraphics{figA1.pdf}\n\t\\caption{Poincar\\'{e} sections of the maxima of oscillating coupled intensity as a function of detuning for a 1D SSH lattice ($M=7$) with the same parameters in Fig.~\\ref{poincare}. (a) For odd sites in the main text ($I_s=20, v=3, w=7, A=1, B=4$) with the  cyan, green, blue, and red colors  corresponding to the 1st, 3rd, 5th, and 7th ring resonators respectively.  (b) For even sites with the red, green and blue colors corresponding to the 2nd, 4th, and 6th ring resonators, respectively.} \\label{odd_even}\n\\end{figure*}\n\n\\section{Poincar\\'{e} sections of the maxima of oscillating coupled intensity as a function of B}\n\\label{appendix:poincare_B}\n\n\n\n\\begin{figure}\n   \\includegraphics{figA2.pdf}\n\t\\caption{Poincar\\'{e} section of the maxima of oscillating coupled intensity as a function of $B$ for input intensity $I_s=20$, for the first ring resonator from the 1D SSH array. The Poincar\\'{e} sections for coupling coefficients ($v=3$, $w=7$) and $\\Delta=5.5$. Cyan shading indicates oscillations and yellow refers to optical bistability.} \\label{poincareB}\n\\end{figure} \n\nIn Fig.~\\ref{poincareB} we scan $B$ from $1$ to $7$ to observe oscillations in coupled intensity for $A=1$, $I_s=20$, $v=3$ and $w=7$. Here, the Poincar\\'{e} section can be divided into three regions: symmetric stable, asymmetric stable and asymmetric unstable states. We observe a series of bifurcations and oscillation windows for $B\\geq 1.9$.\n\n\n\n\t\n\\end{document}\n\n\t\n"}
{"paper_id": "2403-00538", "version": "2403-00538v2", "root_file_path": "d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\data_raw\\2403-00538\\tex\\2403-00538v2\\main_new.tex", "metadata": {"total_length": 39480, "merged_count": 1, "merged_files": ["main_new.tex"], "missing_files": []}, "content": "\n\n\n\n\\documentclass[\nreprint,\nsuperscriptaddress,\n\n\n\n\n\nshowpacs,preprintnumbers,\nnofootinbib,\n\n\namsmath,assume,\naps,\n\n\n\nprb\n\n\n\n\nshowkeys\n]{revtex4-2}\n\\usepackage{graphicx}\n\\usepackage{dcolumn}\n\\usepackage{bm}\n\n\\usepackage{amssymb}\n\\usepackage{amsmath}\n\\usepackage{amsfonts}\n\\usepackage{xspace}\n\\usepackage{bm,bbm}\n\\usepackage{relsize}\n\\usepackage{siunitx}\n\\usepackage[usenames,dvipsnames]{color}\n\\usepackage{float}\n\\usepackage{hyperref}\n\\usepackage[toc,page]{appendix}\n\\usepackage{float}\n\\usepackage{array}\n\\newcolumntype{L}[1]{>{\\raggedright\\let\\newline\\\\\\arraybackslash\\hspace{0pt}}m{#1}}\n\\newcolumntype{C}[1]{>{\\centering\\let\\newline\\\\\\arraybackslash\\hspace{0pt}}m{#1}}\n\\newcolumntype{R}[1]{>{\\raggedleft\\let\\newline\\\\\\arraybackslash\\hspace{0pt}}m{#1}}\n\\newcommand{\\bigzero}{\\mbox{\\normalfont\\Large\\bfseries 0}}\n\\newcommand{\\rvline}{\\hspace*{-\\arraycolsep}\\vline\\hspace*{-\\arraycolsep}}\n\n\n\n\n\n\n\n\n\n\n\n\n\\hypersetup{\n    \n    unicode=false,          \n    pdftoolbar=true,        \n    pdfmenubar=true,        \n    pdffitwindow=false,     \n    pdfstartview={FitH},    \n    pdftitle={Optical bistability of zero-energy modes in Su-Schrieffer-Heeger lattice with Kerr nonlinearity},    \n    pdfauthor={Alharbi, Wong, Gong, Leykam, Oh},     \n    pdfsubject={},   \n    pdfcreator={},   \n    pdfproducer={}, \n    pdfkeywords={} {} {}, \n    pdfnewwindow=true,      \n    colorlinks=true,       \n    linkcolor=blue, \n    citecolor=blue,        \n    filecolor=magenta,      \n    urlcolor=blue,           \n\tbreaklinks=true\n} \n\n\n\n\\begin{document}\n\n\n\n\n\n\n\n\\title{Asymmetrical temporal dynamics of edge modes in Su-Schrieffer-Heeger lattice with Kerr nonlinearity} \n\n\n\\author{Ghada H. Alharbi}\n\\affiliation{School of Physics and Astronomy, Cardiff University, Cardiff CF24 3AA, United Kingdom}\n\\affiliation{Department of Physics, University of Tabuk, Tabuk 741,  Kingdom of Saudi Arabia}\n\\author{Stephan Wong}\n\\affiliation{School of Physics and Astronomy, Cardiff University, Cardiff CF24 3AA, United Kingdom}\n\\affiliation{Center for Integrated Nanotechnologies, Sandia National Laboratories, Albuquerque, New Mexico 87185, USA}\n\\author{Yongkang Gong}\n\\affiliation{School of Physics and Astronomy, Cardiff University, Cardiff CF24 3AA, United Kingdom}\n\n\n\\author{Sang Soon Oh}\n\\email[Email:]{ohs2@cardiff.ac.uk}\n\\affiliation{School of Physics and Astronomy, Cardiff University, Cardiff CF24 3AA, United Kingdom}\n\n\\date{\\today}\n\n\\begin{abstract}\nOptical bistability and oscillating phases exist in a Sagnac interferometer and a single ring resonator made of $\\chi^{(3)}$ nonlinear medium where the refractive indices are modulated by the light intensity due to the Kerr nonlinearity. An array of coupled nonlinear ring resonators behave similarly but with more complexity due to the presence of the additional couplings.\nHere, we theoretically demonstrate the bifurcation of edge modes which leads to optical bistability in the Su-Schrieffer-Heeger lattice with the Kerr nonlinearity. Additionally, we demonstrate periodic and chaotic switching behaviors in an oscillating phase resulting from the coupling between the edge mode and bulk modes with different chiralities, i.e., clockwise and counter-clockwise circulations. \n\\end{abstract}\n\n\\keywords{bi-stability, Kerr effect, topological photonics}\n\n\\maketitle\n\n\\section{Introduction}\nAsymmetrical states emerge when a system lose the balance and thus a symmetry between different components is broken. This can lead to bistability, where the system has two stable states for a single excitation. In some cases, asymmetrical dynamic states can emerge with periodic or chaotic oscillatory  behaviors. In photonic systems, optical bistability can appear when the light transmits through a cavity with a nonlinear medium leading to two different optical states, where one mode is dominant (switched on) and the other is quenched (switched off) \\cite{Boyd2008}. \n\nFor instance, the stable symmetry breaking has been studied for counter-propagating light beams in a Sagnac interferometer \\cite{Asymmetrical1982}and micro-resonator with Kerr nonlinearity \\cite{ DelBino2017a, Silver2018, Shim2016, Xu2021, coen_2024} and also for a singular direction of input light in a 1D chain of ring resonators \\cite{Ghosh2024}. More interestingly, nonlinear optical ring resonators can present rich temporal dynamics with oscillatory behaviors, displaying various types of mode switching, such as chaotic, periodic, and self-switching dynamics \\cite{Hill2020a}. These various dynamics are the result of the nonlinear interaction between the counter-propagating modes and they manifest the symmetry breaking, i.e., unequal intensities of the two counter-propagating modes.\n\nTopological phases of matters are classified  depending on the symmetry and dimension, featuring topological defects and gapless modes in topological insulators and superconductors~\\cite{Teo2010, Slager2019}. Further, three-dimensional topological band insulator can have dislocation-line modes~\\cite{Slager2014}.\nMore recently, topologically protected modes have been widely studied in photonic systems due to their intriguing properties, such as unidirectional light propagation and robustness to defects and disorders~\\cite{Ozawa2019, Khanikaev2013}. \nIn particular, topological edge modes in a one-dimensional (1D) Su-Schrieffer-Heeger (SSH) configuration and two-dimensional (2D) photonic quantum spin-Hall or quantum valley-Hall structures have been employed in an array of coupled lasers, namely topological lasers \\cite{Noh2018, Han2019, Harari2018b, Bandres2018, Gong2020, Wong2023}.\nMoreover, nonlinear topological photonics has been studied in various platforms such as waveguide arrays \\cite{Leykam2016, Zhou2017, Maczewsky2020, Ivanov2023}, microcavity polariton systems \\cite{ Kartashov2017, Weifeng2019} and optical resonators \\cite{Hadad2016, Dobrykh2018, Leykam2020,  Dobrykh2018, Roy2021, Ezawa2021, Ezawa2022, Wei2023}. \nThe phase diagrams of a nonlinear SSH model and nonlinear breathing kagome model were drawn for the nonlinear parameters and coupling coefficients between sites \\cite{Ezawa2021}. Also, the edge solitons have shown to be stable at any energy when the ratio between the weak and strong couplings falls below a critical value \\cite{Ma2021}. Up until now, however, no research has demonstrated spontaneous symmetry breaking coming from the nonlinear response for the edge modes in photonic topological insulators.\n\n\nIn this paper, we theoretically show that we can observe asymmetrical temporal dynamics, including optical bistabilities and oscillation phases for edge modes in a nonlinear 1D SSH model. The system consists of an array of coupled ring resonators with the Kerr nonlinearity. \n\nUsing the Lugiato-Lefever equation \\cite{Lugiato1987} with additional nearest neighbor couplings, we demonstrate the optical bistability of the topological edge mode in the nonlinear SSH lattice. \n\nFinally, we use Poincar\\'{e} section plots, composed of the maxima of the oscillating intensities, to display the oscillation phases featuring periodic and chaotic switching.\n\n\\section{Linear SSH model with two counter-propagating modes}\nWe start by considering a linear SSH chain which does not have any resonance frequency shift coming from a nonlinearity.\nAs shown in Fig.~\\ref{fig1}(a), the one-dimensional chain has $(N+1)$ unit cells and every unit cell hosts two ring resonators; one on the sublattice A, and the other on the sublattice B. The $(N+1)$-th unit cell has only one ring resonator that belongs to the sublattice A, resulting in $M=2N+1$ ring resonators in total.  \n\nIn photonics, this SSH model can be implemented by alternating the gap size between the ring resonators, resulting in different intra- and inter-cell coupling coefficients $v$ and $w$ (Fig.~\\ref{fig1}(b)). \nThe coupled ring resonators are excited by two optical pumps with the same intensity, both of which are coupled into the first ring resonator but in the opposite directions exciting clockwise (CW) and counter-clockwise (CCW) modes, respectively. \nThen, the optical waves propagate back and forth through all the resonators via the couplings between the ring resonators.\n\\begin{figure}\n\t\\centering\n\t\\includegraphics{fig1}\n\t\\caption{(a) A 1D array of coupled ring resonators with alternating gap sizes.  $s_{in,\\pm}$ is the amplitude of input beams. (b) A schematic of the SSH model with resonators with Kerr nonlinearity. $B$ is the XPM strength, $v$ and $w$ are the nearest neighbor coupling coefficients between ring resonators, and $\\gamma_c$ is the coupling coefficient between the waveguide and the first ring resonator.}\n\t\\label{fig1}\n\\end{figure} \nTo calculate the intensities of circulating optical waves at all ring resonators, we describe the time evolution of the field amplitudes $a_{n}(t)$ and $b_{n}(t)$ in the $n$-th unit cell (Fig.~\\ref{fig1}(b)) for which we use the temporal coupled mode theory \\cite{Haus1983, Suh2004}.  \nThen, the coupled mode equations are written as:\n\\begin{eqnarray}\n \\frac{da_{n,\\pm}}{dt} &=&  i \\Big(\\omega_0  + i \\gamma_n\\Big)  a_{n,\\pm} +  i v b_{n,\\mp} +i w b_{n-1,\\mp}  \\nonumber \\\\ \n&&  + \\delta_{n,1} \\gamma_c s_{in} ,\\nonumber \\\\ \n \\frac{db_{n,\\pm}}{dt} &=&  i \\Big(\\omega_0  + i \\gamma'_n\\Big) b_{n,\\pm}  + i v a_{n,\\mp} + i  w a_{n+1,\\mp} ,\n\\label{CMT}\n\\end{eqnarray}\nwhere\n\\begin{eqnarray}\n  \\gamma_n &=& \\gamma_0 + \\delta_{n,1}\\gamma_c ,\\nonumber \\\\\n  \\gamma'_n &=& \\gamma_0 .\n\\end{eqnarray}\n\n\n\nHere, the subscript $\\pm$ denotes the mode propagation directions, CW and CCW, respectively. \n\n$\\delta_{n,1}$ is the Kronecker delta and $\\omega_0$ is\nthe resonance frequency of the uncoupled ring resonators.\nThe two input beams with the same amplitude $s_{in}$, which is given as $\\sqrt{I_s} e^{i\\omega t}$ for pump intensity $I_s$, are coupled to the CW and CCW modes in the first ring ($a_{1,\\pm}$) with the waveguide-to-ring coupling coefficient $\\gamma_c$. Note that only $a_{n}$'s and $b_{n}$'s are time-dependent functions and we have omitted the symbol $(t)$ for brevity. \n\nTo be more compact, we express the coupled mode equations (Eq.~(\\ref{CMT})) in a matrix form by using the Hamiltonian $\\mathbf{H}$ as\n\\begin{eqnarray}\n\\frac{d \\mathbf{x} }{dt} = \\mathbf{H} \\mathbf{x} + \\mathbf{S} \n\\label{main_eq}\n\\end{eqnarray}\nwhere \n\\begin{equation*}\n    \\mathbf{x} = (a_{1,+} , b_{1,-},  a_{2,+},  b_{2,-}, \\ldots, a_{1,-},  b_{1,+},  a_{2,-} , b_{2,+}, \\ldots )^\\mathsf{T}   . \n\\end{equation*} \nNote that the Hamiltonian $\\mathbf{H}$ can be differently defined after multiplying $i$ in both sides, which makes the equation look like the Schr\\\"{o}dinger equation and makes its eigenvalues correspond to the real parts of the frequencies. However, we have chosen this notation to make Eq.~(\\ref{main_eq}) similar to Lugiato-Lefever equation which we will explain in the following section.  \n\nThen, the Hamiltonian $\\mathbf{H} $ can be split into two terms like:\n\\begin{equation}    \n\\mathbf{H} = \\mathbf{H}_0 + \\mathbf{H}_c\n\\label{linear_hamiltonian}\n\\end{equation}\nwhere\n\\begin{equation}    \n\\mathbf{H}_0 = i (\\omega_0 + i\\gamma_0 ) \\mathbb{I}_{2M}\n\\end{equation}\nwith $\\mathbb{I}_{2M}$ the $(2M \\times 2M)$ identity matrix.\nThe ring-to-ring coupling is expressed as:\n\\begin{equation}    \t\n\\mathbf{H}_c = i \\begin{pmatrix}\n0 &v &0 & 0 &\\cdots\\\\\nv & 0 & w & 0 &\\cdots\\\\\n0 &  w & 0 & v  &\\cdots\\\\\n0 & 0  & v& 0& \\cdots \\\\\n\\vdots& \\vdots&\\vdots& \\vdots& \\ddots\n\\end{pmatrix}.\n\\end{equation} \n\nFinally, the source term  $  \\mathbf{S} $ is expressed as $ [s_{in,+}, 0, 0, \\cdots,  s_{in,-}, 0, 0, \\cdots ]^\\mathsf{T} $. For the remainder of this paper, we assume the symmetric pumping by setting $s_{in,+} = s_{in,-}$.\n\n\nThe coupled mode equations (Eq.~(\\ref{main_eq})) can be solved in both frequency and time domains. \nFor example, in the frequency domain, by assuming $ \\mathbf{x} = \\mathbf{\\tilde{x}} \\exp(i \\omega t)$ and $\\mathbf{S} = 0$, we obtain an eigenvalue equation \n\\begin{equation}   \ni \\omega \\mathbf{\\tilde{x}}  = \\mathbf{H} \\mathbf{\\tilde{x}}\n.\n\\label{frequency_domain}\n\\end{equation}\nSolving the eigenvalue equation gives an frequency spectrum with so-called  zero-energy modes that are topologically protected and localized on one of the edges of the SSH chain with the smaller coupling coefficient among $v$ and $w$. In this work, we will use the term \\emph{edge modes} because their frequencies deviate from the resonance frequency $\\omega_0$ and thus they are not any more zero-energy modes for nonlinear cases. We call the rest of the modes \\emph{bulk modes} as the mode fields are delocalized over the entire SSH lattice.\n\n\\section{Lugiato-Lefever equation for nonlinear SSH model}\nNow we introduce the Kerr nonlinearity in the linear SSH model. \nIn optics, the Kerr nonlinearity induces various nonlinear effects, for instance, self-phase modulation (SPM),  cross-phase modulation (XPM), four-wave mixing, and two-photon absorption \\cite{Agrawal2019}. \nHere, we only consider the SPM and XPM for the counter-propating rotating modes in the ring resonators, both of which lead to a shift of the resonance frequencies of the CW or CCW modes. \nAlthough only the couplings due to the XPM are shown in Fig.~\\ref{fig1}(b), the frequency shift $\\Delta \\omega$ is expressed by $(AI_{n,+} + BI_{n,-})$ where $I_{n,+}$ and $I_{n,-}$ are the intensities of the CW and CCW modes in the $n$-th ring resonator, respectively. $A$ and $B$ are  the SPM and XPM nonlinear coefficients, respectively.\n\nFor simplicity, we use the normalized Lugiato-Lefever equation to describe the field amplitudes in our nonlinear SSH model \\cite{Lugiato1987}. Notably, the equation is equivalent to the one derived from the temporal coupled-mode theory (see Appendix \\ref{appendix:LLeq} for more details). \n\nWith the time-varying envelope amplitudes $\\tilde{a}(t)$, $\\tilde{b}(t)$, defined as $a(t)=\\tilde{a}(t) e^{i\\omega t}$, $b(t)=\\tilde{b}(t) e^{i\\omega t}$ respectively, the Lugiato-Lefever equation for a 1D SSH array of nonlinear ring resonators can be written as:\n\\begin{eqnarray}\n\\frac{d\\tilde{a}_{n,\\pm}}{d\\Bar{t}} &=&  -\\tilde{a}_{n,\\pm}-i \\eta \\Delta \\tilde{a}_{n,\\pm} \n  + i \\eta (A|\\tilde{a}_{n,\\pm}|^2 + B|\\tilde{a}_{n,\\mp}|^2)  \\tilde{a}_{n,\\pm} \\nonumber \\\\ \n&& +i v \\tilde{b}_{n\\mp} + i w \\tilde{b}_{n-1,\\mp}  +\\delta_{n,1}s_{in}, \n\\nonumber \\\\ \n\\frac{d\\tilde{b}_{n,\\pm}}{d\\Bar{t}} &=&  -\\tilde{b}_{n,\\pm}-i \\eta\\Delta \\tilde{b}_{n,\\pm} + i \\eta (A|\\tilde{b}_{n,\\pm}|^2 + B|\\tilde{b}_{n,\\mp}|^2) \\tilde{b}_{n,\\pm}   \\nonumber \\\\ \n&& +i v \\tilde{a}_{n\\mp} + i w \\tilde{a}_{n+1,\\mp} ,\n\\label{nonlinear_LL_equation}\n\\end{eqnarray}\nwhere $\\Bar{t}=t\\gamma_0$ is the dimensionless time.  The first term on the right-hand side represents damping, while the second term stands for detuning ($\\Delta=(\\omega-\\omega_0)/\\gamma_0$), which is the difference between the frequency of the continuous wave input beams and the resonance frequency of a single ring resonator. The third and fourth terms correspond to the SPM and XPM, respectively, with the normalized nonlinear coefficients $A$ and $B$, and $\\eta = +1$ for a self-focusing medium or $\\eta = -1$ for a self-defocusing medium. The terms with $v$ and $w$ refer to the intra- and inter- couplings between ring resonators as in the linear case.\nFinally, we add a nonlinear term to Eq.~(\\ref{linear_hamiltonian}) to have the Hamiltonian for our nonlinear SSH model: \n\n\\begin{equation}    \n\\mathbf{H} = \\mathbf{H}_0 + \\mathbf{H}_c +  \\mathbf{H}_{NL}  ,\n\\label{nonlinear_hamiltonian}\n\\end{equation}\nwhere \n\\begin{equation}    \n\\mathbf{H}_0 =  -(1 +i \\eta \\Delta )\\mathbb{I}_{2 M},\n\\end{equation} and \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{equation}    \n\\mathbf{H}_{NL} = i \\eta  \\mathbb{I}_{2M}\\times\n\\mathbf{diag} \\begin{pmatrix}\n\\mu_{1,a}^{cw},  &\n\\mu_{1,b}^{cw}, &\n\\hdots, &\n\\mu_{1,a}^{ccw},  &\n\\mu_{1,b}^{ccw}, &\n\\hdots\n\\end{pmatrix} ,\n\\end{equation}\nwhere \n\\begin{equation}\n \\mu_{n,a}^{cw} = A|\\tilde{a}_{n,+}|^2 + B|\\tilde{a}_{n,-}|^2 , \\nonumber   \n \\end{equation}\n  and \n  \\begin{equation}\n  \\mu_{n,a}^{ccw} = A|\\tilde{a}_{n,-}|^2 + B|\\tilde{a}_{n,+}|^2 \\nonumber .\n  \\end{equation}\n Similarly we can define $\\mu_{n,b}^{cw}$ and $\\mu_{n,b}^{ccw}$.\nBecause of the nonlinear terms, the time-dependent equation (Eq.(\\ref{main_eq})) cannot be changed into an eigenvalue equation.\nInstead, we can obtain the temporal evolution of the amplitudes $\\tilde{a}_{n,\\pm}$, $\\tilde{b}_{n,\\pm}$ by solving it numerically. \nIn this work, we use the Runge-Kutta fourth order method to integrate Eq.(\\ref{main_eq}) with respect to the time.\n\n\n\n\n\\begin{figure}\n   \\includegraphics{fig2_w_gamma_c.pdf}\n\t\\caption{(a) Optical bistability for seven ring resonators ($M=7$) with detuning $\\Delta=2.1$, $A=1$, $B=2.5$, $\\gamma_c=1$, $v=3$ and $w=7$. (b) Optical bistability with the Kerr nonlinearity in the first ring resonator only. (c), (d) The distributions of intensity for the input intensities corresponding to the dashed vertical lines in (a).}\\label{bistability}\n\\end{figure} \n\n\\section{Optical bistability of topological edge modes}\n\n\n\n\n\\begin{figure*}\n   \\includegraphics{fig_poincare_7N_I_s_gamma_c_compress.pdf}\n\t\\caption{Poincar\\'{e} section of the maxima of the CW and CCW intensity time series for the first ring resonator in 1D SSH lattice composed seven ring resonators with alternating coupling, $v=3$ and $w=7$, here $A=1$, $B=4$ and $\\gamma_c=1$. (a) Maximum intensity curves for low input intensities $I_s=0.05$ (yellow), $2$ (cyan), $3$ (green), $4$ (blue), $5$ (red). (b) For high input intensities  $I_s=20$ (yellow), $25$ (cyan), $30$ (green), $35$ (blue), $40$ (red),  both optical bistability and oscillation regions appear. The vertical dashed lines indicate frequencies determined by the eigenvalue equation for the linear case (Eq.(\\ref{frequency_domain})).} \\label{compare_intensity}\n\\end{figure*} \n\nOptical bistability in a single ring resonator is a result of the XPM between two counter-propagating modes \\cite{Kaplan1981}. \n\nThis means that the Kerr nonlinearity leads to a shift in the resonance frequency of the two counter-propagating modes due to both SPM and XPM, and the coupling via XPM between them leads to spontaneous symmetry breaking above a certain threshold pump intensity \\cite{DelBino2017a}. \nHere, we want to answer the question whether we can observe the optical bistability using an edge mode in a nonlinear SSH lattice model. \n\nTo theoretically observe the optical bistability in the nonlinear SSH lattice, we consider a SSH array of seven ring resonators ($M=7$) with detuning of $2.1$, $A=1$, $B=2.5$ and $\\gamma_c=1$ as and the alternating coupling coefficients ($v=3$ and $w=7$). In our simulations, we vary the pump intensity $I_s$ for a certain interval with random initial conditions. Indeed, as shown in Fig.~\\ref{bistability}(a), we observe the optical bistability where the symmetry between CW and CCW modes is broken for the pump intensity range between $\\log(I_s+1)=3.4$ and $\\log(I_s+1)=5.9$. This means that one of the CW and CCW modes becomes stronger while the other mode is suppressed. As shown in Fig.~\\ref{bistability} (c, d), the amplitude decreases gradually towards the bulk of the lattice system. \n\nNote that the amplitude is relatively large for odd sites only (sublattice A), and the intensity decreases exponentially along the right direction for both single stable (Fig.~\\ref{bistability}(c)) and bistable cases (Fig.~\\ref{bistability} (d)), which is the reminiscence of the zero-energy edge modes.\n\n\n\nTo explain the origin of the observed optical bistability, we hypothesize that the optical bistability comes from the symmetry breaking in the first ring only. \nFirst, optical bistability in a single ring resonator can occur when the pump intensity is above a certain threshold called a bifurcation point. This means the first ring will show the optical bistability first as we increase the pump intensity under an excitation close to the zero-energy frequency. Indeed, the detuning $\\Delta = 1.85$ is smaller than the topological band gap ($2|v-w| = 8$) meaning the zero-energy edge mode is  dominantly excited even though the system is at off-resonance.\nThis is supported by the field intensity distribution in  Fig.~\\ref{bistability}(d). \nAs the intensities in the rest of rings are much smaller than the first ring (Fig.~\\ref{bistability}(d)), only the first ring introduces bistability and \nthe modes in the first ring couple to the other rings successively instead of having additional optical bistability from the rest of the rings. \nSecond, to confirm this propagation of asymmetric intensities, we consider the Kerr effect only in the first ring resonator but keep all other parameters the same. This is equivalent to switching off the Kerr effect in the $2N$ ring resonators except the first ring resonator in our original setting.  \nAs shown in Fig.~\\ref{bistability}(b), the intensity-intensity curve has almost identical shape as the original one except slight reduction in the range of $I_s$ and slight change in the difference between two counter-propagating mode intensities.\n\n\n\n\n\\begin{figure}\n   \\includegraphics{fig_poincare_C_PR_C_7N_4_7B_1gamma_c.pdf}\n\t\\caption{(a) Poincar\\'{e} sections of the maxima of oscillating coupled intensity as a function of detuning for pump intensity $I_s=40$, for the first ring resonator from the 1D SSH array. The Poincar\\'{e} sections for coupling coefficients ($v=3$, $w=7$) and XPM strength ($B$) of 4. The red shading indicates a symmetric case, the yellow indicates optical bistability, and the cyan indicates oscillations. The vertical dashed lines refer to resonance frequencies for the linear 1D SSH lattice. (b) Dissymmetry for CW and CCW intensity modes. (c) polarization in A and B sites. (d) Participation ratio for intensity.} \\label{poincare_pr_c}\n\\end{figure} \n\n\n\n\n\\begin{figure}   \\includegraphics{fig5_w_gamma_c.pdf}\n\t\\caption{Snapshots of intensity distribution in the nonlinear 1D SSH lattice with $I_s=40$, $B=4$,  two figures in each row have the same detuning values for the CW modes and the CCW modes, respectively.  (a), (b) in the optical bistability regime, (c), (d) in the oscillation regime, and (e), (f)  in the oscillation switching regime.} \\label{distribution_d}\n\\end{figure} \n\n\n\n\n\\begin{figure}   \n\\includegraphics{fig6_w_gamma_c.pdf}\n\t\\caption{Time series of intensity and their phase space trajectories for $A=1$, $B=4$ and $I_s=40$ at different values of detuning, for the first ring resonators from 1 D SSH array of 7 rings. (a), (b) Optical bistability phase with $\\Delta=4.77$. (c), (d) Oscillations without overlapping trajectories. (e), (f) Periodic switching for $\\Delta=7.51$. (g), (h) Chaotic switching with $\\Delta=8.78$. }\\label{time_ev}\n\\end{figure} \n\n\n\n\n\n\n\n\n\n\n\\section{Asymmetrical temporal dynamics}\nNow, let us look at the temporal evolution of the optical intensities of a 1D SSH array that contains seven ring resonators.\nTo visualize oscillation and chaotic phases in our nonlinear system, we will use the Poincar\\'{e} section \nobtained by plotting all the local maxima in a time series of oscillating intensities \\cite{Hill2020a}. \nSince the intensity of each ring in the 1D SSH array follows the same pattern as the intensity of the first ring (see Fig.~\\ref{odd_even} in Appendix \\ref{appendix:poincare_SSH}), we plot the Poincar\\'{e} sections for the first ring resonator only. \n\nAs shown in Fig.~\\ref{compare_intensity} (a)(b), the nonlinear SSH lattice exhibits both bistability and oscillation phases in the range of detuning corresponding to the edge mode and two bulk modes with positive detuning for the linear SSH lattice (denoted as the vertical dashed lines). \nHere, we set $A=1$ and $B=4$ and change the pump intensity from 0.05 to 20 denoted with different colors. \nAs we can see in the zoomed view of the plots, these spectra show seven resonance modes; one edge mode with the largest intensity in the middle and six bulk modes on both sides of the edge mode having three on each side.  \nFor low pump intensities (Fig.~\\ref{compare_intensity}(a)), the edge mode shifts to larger detuning values dramatically and its intensity increases significantly, whereas the bulk modes shift less and their intensities increase slightly. \nThis is due to the localization of intensity at the first ring resonator. \nFor high pump intensities (Fig.~\\ref{compare_intensity}(b)), the CW and CCW modes for edge mode undergo an interaction between them via XPM, leading to an optical bistability. \nRemarkably, the high pump intensity leads to the interaction between the edge mode and the bulk mode near $\\Delta = 6.8$ for $I_s=40$ as shown in Fig.~\\ref{poincare_pr_c}(a), resulting in a series of oscillation phases occurring for both CW and CCW modes.\nNote that CW and CCW intensities for bistable and oscillation phases can be exchanged any time because the chirality depends on the history of the dynamic system which is determined by the initial conditions in our case. \n\nTo quantify the degree of the CW-CCW symmetry breaking (see Appendix \\ref{appendix:Sublattice symmetry}), we introduce the dissymmetry parameter defined in a similar manner for a single microcavity in Ref. \\cite{Cao2020} as:\n\\begin{equation}\n        D = \\frac{\\sum_n I_{CW,n}-\\sum_n I_{CCW,n}}{\\sum_n I_{CW,n} + \\sum_n I_{CCW,n}}.  \n        \\label{C}\n\\end{equation}\nThen, a positive (negative) value of $D$ means the CW (CCW) mode is stronger than the other, corresponding to the optical bistability, and $D=0$ means the mode is CW-CCW symmetric. \nFor the bistability regime, the nonlinear system shows relatively large dissymmetry close to 1 as shown in Fig.~\\ref{poincare_pr_c}(b), meaning the CW-CCW symmetry is strongly broken. \nHowever, for the oscillations regime ($\\Delta>6.9$), the dissymmetry has a random value between -1 and 1 and becomes very sensitive to the detuning. \nA similar trend can be observed in the dissymmetry calculated for the first ring only, reconfirming that the origin of the optical bistability is mainly due to the bifurcation of CW and CCW modes in the first ring.\n\nTo investigate the sublattice symmetry breaking (see Appendix \\ref{appendix:Sublattice symmetry}), we calculate polarization ($P$) in terms of $\\sum_n I_{A,n}$ (for sublattice A) and $\\sum_n I_{B,n}$ (for sublattice B) for each direction using the following equation:\n\\begin{equation}\n        P = \\frac{\\sum_n I_{A,n}-\\sum_n I_{B,n}}{\\sum_n I_{A,n} + \\sum_n I_{B,n}}.  \n        \\label{polarization}\n\\end{equation}\nPositive polarization values in Fig.~\\ref{poincare_pr_c}(c) indicate that the modes for the bistability regime are localized at sublattice A; a value close to 1 confirms that the energy of the modes is concentrated at A sites. \nThis is a reminiscence of topological zero-energy mode which would show a completely polarized distribution, i.e., $P=1$.\n\nTo characterize the localization of the intensity of dynamic modes in the whole SSH lattice, we calculate the participation ratio ($PR$), which is given by \\cite{Longhi2018}:\n\\begin{equation}\n        PR = \\frac{(\\sum_n I_n)^2}{\\sum_n |I_n|^2}.\n        \\label{PR}\n\\end{equation}\nA large value of $PR$ refers to delocalization.\nAt small values of detuning($\\Delta \\leq 1.5$),  as shown in Fig.~\\ref{poincare_pr_c}(d), the nonlinear SSH lattice exhibits almost identical delocalization behavior for both CW and CCW modes. \nIn contrast, for larger detuning ($\\Delta \\geq 1.5$), one of CW and CCW modes is more localized that the other. \n\n\nTo better understand the asymmetrical dynamic modes, we show the spatial distributions and temporal changes of the excited mode intensity for different detuning in Fig.~\\ref{distribution_d} and Fig.~\\ref{time_ev}. Here, we focus on the case of $I_s = 40$ and $B=4$ as an example of high pump intensity. \nFor the optical bistability ($\\Delta = 4.77$) shown in  Fig.~\\ref{distribution_d}(a),(b), both CW and CCW modes have contrasting intensity values, whereas their profiles are similar to the zero-energy edge mode's profile in a linear SSH model with exponentially decaying non-zero odd-site intensities and zero even-site intensities. \nThe deviations can be attributed to the off-resonance excitation and the interaction between CW and CCW modes via the nonlinear process (XPM). \n\nNote that the excited mode is stable as they have constant intensities and appear as two separate points in the phase space (Fig.~\\ref{time_ev} (a),(b)). \n \nWhen we increase the detuning further to $\\Delta = 7.3$, both CW and CCW mode profile deviates further away from the zero-energy edge mode but the CCW mode profile deviates less still having low intensities at even sites (Fig.~\\ref{distribution_d}(c),(d)). Here, the largest value at the first site is related to the zero-energy edge mode and also due to the fact we are exciting the ring resonators from the waveguide on the left side. \nThe dynamics for this detuning (Fig.~\\ref{time_ev} (c)(d)) is periodically oscillatory, resulting in two distinct regions in the phase space meaning the CW mode intensity is always larger than the CCW mode intensity (the trajectory for CW is further away from the origin). \nFor slightly larger detuning of $\\Delta = 7.51$ (Fig.~\\ref{time_ev} (e)(f)), the two trajectories are merged into one meaning that the intensities between the two modes alternate. In the phase space, they are located at different two points with the $\\pi$ phase difference in the same trajectory. \nFor a large detuning of $\\Delta = 8.78$,  we see chaotic oscillations showing two separate trajectories covering a similar region in the phase space (Fig. ~\\ref{time_ev}(g),(h)).\n\nIn contrast to the optical bistability coming from the coupling between two counter-propagating modes via nonlinearities (XPM and SPM), the emergence of the periodic and chaotic oscillations has its origin in the coupling between the edge mode and the bulk modes. \nThe reasoning is as follows: \nFirstly, the resonance frequency shift of the edge mode when increasing the intensity is much larger than the ones for the bulk modes (Fig. ~\\ref{poincare_pr_c}(a)), and\nthere are clear signatures of the edge modes, i.e., an exponentially decaying odd-site intensities and nearly zero even-site intensities (Fig.~\\ref{distribution_d} (c-h) although the intensity distributions are getting close to the bulk modes. \nThus, our numerical simulations confirm that the edge mode overlap with bulk mode due to the Kerr nonlinearity results in the periodic and chaotic oscillations.\n\n\n\\section{Conclusions}\nIn summary, we have numerically demonstrated the optical bistability and various types of oscillations in a 1D SSH model composed of ring resonators with Kerr nonlinearity. \nWhen the nonlinear terms are introduced in the Lugiato-Lefever equation, the first ring's CW and CCW mode intensities are symmetric until the pump intensity reaches a bifurcation point. Above the bifurcation point, the symmetry is spontaneously broken due to the splitting of the resonance frequencies of the two CW and CCW modes in the first ring resonator. \nFor the high intensity regime, we have observed oscillating phases including periodic and chaotic oscillations.\nWe have classified the periodic oscillation phases into two different phases where the trajectories are separate or identical in the phase space of the mode intensities. \nThis emergence of the oscillating phases can be attributed to the coupling between the edge mode and bulk mode due to the large shift of resonance peaks of the edge mode. \n\n\nWe believe that our theoretical model and numerical results will provide valuable insight in understanding the complex dynamics in coupled nonlinear resonator systems with two chiral modes. \nIn practice, the asymmetrical dynamic modes can be demonstrated experimentally using an optical ring resonator array, such as, a silicon microring resonator array \\cite{Bogaerts2012}. \nIn particular, the bifurcation states can be a building block for optical memory and switching devices allowing for storing and maintaining information by controlling the pump intensities depending on the direction of light circulation \\cite{DelBino:21, Woodley2020}. Additionally, the various spatio-temporal dynamics could be applied to the stability analysis of coupled lasers. \n \n\n\\begin{acknowledgments}\nWe are grateful to Daniel Leykam for fruitful discussions. The work is part-funded by the European Regional Development Fund through the Welsh Government (80762-CU145 (East)).\n\\end{acknowledgments}\n\n\n\\appendix\n\\renewcommand{\\thefigure}{A\\arabic{figure}}\n\\setcounter{figure}{0}\n\n\\section{Derivation of the Lugiato-Lefever equation using the coupled mode theory}\n\\label{appendix:LLeq}\nThe coupled mode theory \\cite{Haus1983} has been used to describe the field amplitude $a$ propagating in an optical ring resonator, which can be written as\n\\begin{equation}\n    \\frac{da}{dt}= i\\omega_0 a - \\gamma a + \\gamma_c s.\n    \\label{cmt}\n\\end{equation}\nHere $\\omega_0$ refers to the resonance frequency, $\\gamma $ and $\\gamma_c$ are the damping and coupling with the source coefficient, respectively.\nwe can express field amplitude $a$ in terms of envelope amplitude $\\tilde{a}$ as:\n\\begin{equation}\n    a = \\tilde{a} e^{i\\omega t},\n\\end{equation}\nby substituting in Eq.(\\ref{cmt}) :\n\n\n\n\\begin{equation}\n    \\frac{d\\tilde{a}}{dt} =  [i(\\omega_0 - \\omega )-\\gamma]  \\tilde{a}+ \\gamma_c \\tilde{s},\n\\end{equation}\nwhere $-\\tilde{\\Delta}=\\omega_0 - \\omega$, then we can rewrite this equation in terms of detuning as :\n\\begin{equation}\n    \\frac{d\\tilde{a}}{dt} =  \n    (-\\gamma - i\\tilde{\\Delta}) \\tilde{a} + \\gamma_c \\tilde{s}.\n\\end{equation}\nThis equation is equivalent to the Lugiato-Lefever equation without nonlinearity terms; the terms in RHS correspond to damping, detuning, and source terms, respectively. \n\n\n\\section{Sublattice symmetry and CW-CCW symmetry}\n\\label{appendix:Sublattice symmetry}\n\nIn the linear regime, i.e., when $I_s \\simeq 0$, our SSH model (Fig. \\ref{fig1}(b)) has two distinctive symmetries called sublattice symmetry and CW-CCW symmetry. While both of them are called chiral symmetry in the literature, we use the terms sublattice symmetry and CW-CCW symmetry to avoid confusion.  \nThe Hamiltonian of the SSH model can be written in the following block matrix form:\n\\begin{equation} \n\\mathbf{H} = \n\\begin{pmatrix}\n\n  \\mathbf{H}_{cw}\n&\n \\bigzero \\\\\n\n  \\bigzero \n&\n  \\mathbf{H}_{ccw}\n\n\\end{pmatrix},\n\\end{equation}\nwhere $\\mathbf{H}_{cw}$ ($\\mathbf{H}_{ccw}$) is the SSH Hamiltonian for CW (CCW) modes with the size of $M \\times M$. They can be written as:\n\n\\begin{equation} \n\\label{Hcw}\n\\mathbf{H}_{cw} = \n\\begin{pmatrix}\n \\mu_{1,a}^{cw} & v & 0 & 0 &  \\hdots \\\\\n  v &  \\mu_{1,b}^{cw}  & w & 0 &  \\hdots \\\\\n  0 & w & \\ddots  & \\ddots  & \\vdots \\\\\n  \\vdots & \\vdots & \\ddots & \\ddots  & \\ddots\n\\end{pmatrix} ,\n\\end{equation}\n\nand\n\n\\begin{equation} \n\\label{Hccw}\n\\mathbf{H}_{ccw} = \n\\begin{pmatrix}\n \\mu_{1,a}^{ccw} & v & 0 & 0 &  \\hdots \\\\\n  v &  \\mu_{1,b}^{ccw}  & w & 0 &  \\hdots \\\\\n  0 & w & \\ddots  & \\ddots  & \\vdots  \\\\\n  \\vdots & \\vdots & \\ddots & \\ddots  & \\ddots\n\\end{pmatrix},\n\\end{equation}\nrespectively. Here, the diagonal terms, corresponding to the onsite energy in a tight-binding description, represent the shift of resonance frequencies:\n\\begin{equation}\n \\mu_{n,a}^{cw} = -\\eta \\Delta + A|\\tilde{a}_{n,+}|^2 + B|\\tilde{a}_{n,-}|^2 ,   \n \\label{shift1}\n\\end{equation}\n  and \n\\begin{equation}\n  \\mu_{n,a}^{ccw} = -\\eta \\Delta + A|\\tilde{a}_{n,-}|^2 + B|\\tilde{a}_{n,+}|^2.\n   \\label{shift2}\n\\end{equation}\n$\\mu_{n,b}^{cw}$ and $\\mu_{n,b}^{ccw}$ are defined similary.\n\nFirst, let us check the sublattice symmetry of the nonlinear SSH Hamiltonian. \nThe sublattice symmetry operator $\\mathbf{\\Gamma }$ for the subspace with the size of $M \\times M$ is given by :\n\\begin{equation}\n\\mathbf{\\Gamma }=\n    \\begin{pmatrix}\n      -1       & 0      & 0      & \\hdots \\\\ \n      0       & 1     & 0      & \\hdots \\\\\n      0       & 0      & -1      & \\ddots \\\\ \n      \\vdots  & \\vdots & \\ddots & \\ddots\n    \\end{pmatrix}. \n\\end{equation} \nIf we apply $\\mathbf{\\Gamma }$ to $\\mathbf{H}_{cw}$ ($\\mathbf{H}_{ccw}$), we obtain\n\n\\begin{equation}\n    \\mathbf{\\Gamma} \\mathbf{H}_{cw} \\mathbf{\\Gamma}^\\dagger = \n    \\begin{pmatrix}\n    \\mu_{1,a}^{cw} & -v & 0 & 0 &  \\hdots \\\\\n  -v &  \\mu_{1,b}^{cw}  & -w & 0 &  \\hdots \\\\\n  0 & -w & \\ddots  & \\ddots  & \\vdots \\\\\n  \\vdots & \\vdots & \\ddots & \\ddots  & \\ddots    \n    \\end{pmatrix},\n\\end{equation}\nand \n\\begin{equation}\n    \\mathbf{\\Gamma} \\mathbf{H}_{ccw} \\mathbf{\\Gamma}^\\dagger = \n    \\begin{pmatrix}\n    \\mu_{1,a}^{ccw} & -v & 0 & 0 &  \\hdots \\\\\n    -v &  \\mu_{1,b}^{ccw}  & -w & 0 &  \\hdots \\\\\n    0 & -w & \\ddots  & \\ddots  & \\vdots  \\\\\n  \\vdots & \\vdots & \\ddots & \\ddots  & \\ddots    \n    \\end{pmatrix}.\n\\end{equation}\n\nThis means that the sublattice symmetry is broken in the strong nonlinear regime because the diagonal terms of $\\mathbf{H}_{cw}$ and $\\mathbf{H}_{ccw}$ do not vanish making  $\\mathbf{\\Gamma} \\mathbf{H }\\mathbf{\\Gamma}^\\dagger \\neq -\\mathbf{H}$.\nIn the weak nonlinearity regime, however, the diagonal terms approximately equal zero, satisfying $\\mathbf{\\Gamma} \\mathbf{H }\\mathbf{\\Gamma}^\\dagger = -\\mathbf{H}$, the sublattice symmetry is recovered. Here, we assume the detuning is zero. \n\nNext, we check the CW-CCW symmetry. Let us define a block mirror symmetry operator matrix $\\mathbf{P}$ as:\n\\begin{equation} \n\\mathbf{P} = \n\\begin{pmatrix}\n  \\bigzero\n&\n \\mathbb{I}_M \\\\\n  \\mathbb{I}_M\n&\n \\bigzero \n\\end{pmatrix},\n\\end{equation}\nwhere $\\mathbb{I}_M$ is an identity matrix with size of $M \\times M$. Then, we apply $\\mathbf{P}$ to the full Hamiltonian as below.\n\\begin{equation}\n    \\mathbf{P} \\mathbf{H }\\mathbf{P}^{-1} = \\begin{pmatrix}\n   \\mathbf{H}_{ccw}\n&\n \\bigzero \\\\\n  \\bigzero\n&\n  \\mathbf{H}_{cw} \n\\end{pmatrix}.\n\\end{equation}\nFrom \\ref{Hcw}, \\ref{Hccw}, one can notice that $\\mathbf{P} \\mathbf{H }\\mathbf{P}^{-1} = \\mathbf{H }$ is satisfied only when $\\tilde{a}_{n,-} = \\tilde{a}_{n,+}$ which corresponds to the symmetrical dynamic modes in the weak nonlinear regime. For strong nonlinear regime, we obtain $\\mathbf{P} \\mathbf{H }\\mathbf{P}^{-1} \\neq \\mathbf{H }$ giving rise to asymmetrical dynamics.\n\n\n\\section{Site-dependence of Poincar\\'{e} sections for nonlinear SSH model}\n\\label{appendix:poincare_SSH}\nFigure~\\ref{odd_even} (a), (b) displays Poincar\\'{e} sections of maxima of oscillating in coupled intensities $I_{max,\\pm}$ for odd (sublattice A) and even (sublattice B) sites in the 1D SSH lattice, respectively. The odd-site intensities of the CW and CCW modes follow the same pattern as the ones for the first ring resonator, while the even-site intensities follow the same patter as the one the second ring resonator. The symmetry is broken, i.e., the CW and CCW mode intensities are not equal for the optical bistability and oscillation phases.\n\n\n\n\\begin{figure*}\n   \\includegraphics{fig_poincare_ssh_7N_I_s_even_odd_w_gamma_c_compress.pdf}\n\t\\caption{Poincar\\'{e} sections of the maxima of oscillating coupled intensity as a function of detuning for a 1D SSH lattice ($M=7$) with the same parameters in Fig.~\\ref{poincare_pr_c}(a). (a) For odd sites in the main text ($I_s=40, v=3, w=7, A=1, B=4, \\gamma_c=1$) with the cyan, green, blue, and red colors corresponding to the 1st, 3rd, 5th, and 7th ring resonators respectively.  (b) For even sites with the red, green and blue colors corresponding to the 2nd, 4th, and 6th ring resonators, respectively.} \\label{odd_even}\n\\end{figure*}\n\n\n\n\n\n\n\n\n    \n \n\n\n\n\n\n\n\n\n\n\n\n\n\t\n\\end{document}\n\n\t\n"}
{"paper_id": "2403-00539", "version": "2403-00539v1", "root_file_path": "d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\data_raw\\2403-00539\\tex\\2403-00539v1\\main.tex", "metadata": {"total_length": 61065, "merged_count": 1, "merged_files": ["main.tex"], "missing_files": []}, "content": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\documentclass[acmsmall]{acmart}\n\n\n\n\n\n\n\n\n\\setcopyright{rightsretained}\n\\acmDOI{10.1145/3643742}\n\\acmYear{2024}\n\\copyrightyear{2024}\n\\acmSubmissionID{fse24main-p240-p}\n\\acmJournal{PACMSE}\n\\acmVolume{1}\n\\acmNumber{FSE}\n\\acmArticle{16}\n\\acmMonth{7}\n\\received{2023-09-29}\n\\received[accepted]{2024-01-23}\n\n\\newcommand{\\islem}[1]{{\\color{red}#1 --\\textbf{Islem}}}\n\\newcommand{\\michael}[1]{{\\color{brown}#1 --\\textbf{M}}}\n\\newcommand{\\rfc}[1]{{\\color{deepgreen}#1}}\n\\newcommand{\\update}[1]{{\\color{blue}#1}}\n\\newcommand{\\todo}[1]{{\\color{violet}TODO: #1}}\n\n\\newcommand{\\code}[1]{{\\small \\texttt{#1}}}\n\n\\usepackage{graphicx}\n\\usepackage{tikz}\n\\def\\checkmark{\\tikz\\fill[scale=0.3](0,.35) -- (.35,0) -- (1,.7) -- (.25,.15) -- cycle;} \n\\usepackage{multirow}\n\\usepackage{listings}\n\\usepackage{tcolorbox}\n\\usepackage{color}\n\\usepackage{xcolor}\n\\usepackage{graphicx}\n\n\\usepackage{algorithmic}\n\\usepackage[ruled,linesnumbered,vlined]{algorithm2e}\n\\usepackage{setspace}\n\\usepackage{listings}\n\\usepackage{multicol}\n\\usepackage{makecell}\n\\usepackage{amsthm}\n\\usepackage{enumitem}\n\\usepackage{tikz}\n\\usepackage{xspace}\n\\usepackage{wrapfig}\n\\usepackage{calc, scalerel}\n\\usepackage{mathtools}\n\\usepackage{caption}\n\\usepackage{subcaption}\n\\usepackage{colortbl}\n\\usepackage{bbm}\n\\usepackage{pifont}\n\\usepackage{hyperref}\n\n\\definecolor{celadon}{rgb}{0.67, 0.88, 0.69}\n\\definecolor{codegreen}{rgb}{0,0.6,0}\n\\definecolor{codegray}{rgb}{0.5,0.5,0.5}\n\\definecolor{codepurple}{rgb}{0.58,0,0.82}\n\n\\lstdefinestyle{mystyle}{\n    commentstyle=\\color{codegreen},\n    keywordstyle=\\color{magenta},\n    numberstyle=\\tiny\\color{codegray},\n    stringstyle=\\color{codepurple},\n    basicstyle=\\ttfamily\\footnotesize,\n    breakatwhitespace=false,\n    breaklines=true,\n    captionpos=b,\n    keepspaces=true,\n    numbers=none,\n    numbersep=5pt,\n    showspaces=false,\n    showstringspaces=false,\n    showtabs=false,\n    tabsize=2\n}\n\\lstset{style=mystyle}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\newcounter{findingCounter}\n\\setcounter{findingCounter}{1}\n\\newenvironment{finding}{\n}\n\n\\newcommand{\\name}{DyPyBench}\n\\newcommand{\\dynamic}{$G_{dynamic}$}\n\\newcommand{\\static}{$G_{static}$}\n\\newcommand{\\size}{45}\n\n\n\n\n\n\n\n\\begin{document}\n\n\n\n\n\n\\title{DyPyBench: A Benchmark of Executable Python Software}\n\n\\author{Islem Bouzenia}\n\\orcid{0000-0002-3920-3839}\n\\affiliation{\n\t\\institution{University of Stuttgart}\n\t\\city{Suttgart}\n\t\\country{Germany}\n}\n\\email{fi\\_bouzenia@esi.dz}\n\n\\author{Bajaj Piyush Krishan}\n\\orcid{0009-0003-1227-2265}\n\\affiliation{\n\t\\institution{University of Stuttgart}\n\t\\city{Stuttgart}\n\t\\country{Germany}\n}\n\\email{st173644@stud.uni-stuttgart.de}\n\n\\author{Michael Pradel}\n\\orcid{0000-0003-1623-498X}\n\\affiliation{\n\t\\institution{University of Stuttgart}\n\t\\city{Suttgart}\n\t\\country{Germany}\n}\n\\email{michael@binaervarianz.de}\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{abstract}\nPython has emerged as one of the most popular programming languages, extensively utilized in domains such as machine learning, data analysis, and web applications.\nPython's dynamic nature and extensive usage make it an attractive candidate for dynamic program analysis.\nHowever, unlike for other popular languages, there currently is no comprehensive benchmark suite of executable Python projects, which hinders the development of dynamic analyses.\nThis work addresses this gap by presenting \\name{}, the first benchmark of Python projects that is large-scale, diverse, ready-to-run (i.e., with fully configured and prepared test suites), and ready-to-analyze (by integrating with the DynaPyt dynamic analysis framework).\nThe benchmark encompasses 50 popular open-source projects from various application domains, with a total of 681k lines of Python code, and 30k test cases.\n\\name{} enables various applications in testing and dynamic analysis, of which we explore three in this work:\n(i) Gathering dynamic call graphs and empirically comparing them to statically computed call graphs, which exposes and quantifies limitations of existing call graph construction techniques for Python.\n(ii) Using \\name{} to build a training data set for LExecutor, a neural model that learns to predict values that otherwise would be missing at runtime.\n(iii) Using dynamically gathered execution traces to mine API usage specifications, which establishes a baseline for future work on specification mining for Python.\nWe envision \\name{} to provide a basis for other dynamic analyses and for studying the runtime behavior of Python code.\n\\end{abstract}\n\n\n\n\n\n\\begin{CCSXML}\n\t<ccs2012>\n\t<concept>\n\t<concept_id>10011007.10011006.10011072</concept_id>\n\t<concept_desc>Software and its engineering~Software libraries and repositories</concept_desc>\n\t<concept_significance>500</concept_significance>\n\t</concept>\n\t</ccs2012>\n\\end{CCSXML}\n\n\\ccsdesc[500]{Software and its engineering~Software libraries and repositories}\n\n\n\\keywords{Python benchmark, dynamic analysis, executable, collection of software repositories}\n\\maketitle\n\n\n\n\\section{Introduction}\n\nPython has grown rapidly in recent years to become one of the most frequently used programming languages in a variety of fields, including machine learning, data analysis, and web applications.\nFor example, a 2022 report on programming languages used in projects hosted on GitHub lists Python as the second-most popular of all languages.\\footnote{\\url{https://octoverse.github.com/2022/top-programming-languages}}\nThe success of Python can be attributed to its simplicity, flexibility, and extensive collection of libraries and frameworks.\nPython's dynamic nature, which allows for dynamic type checking and modification of object structures during runtime, further enhances its appeal to developers.\n \nThe dynamic nature of Python also presents challenges when it comes to program analysis, error detection, security assessment, and performance optimization.\nVarious dynamic program analysis techniques have been developed to address these challenges, e.g., to detect bugs~\\cite{Xu2016a}, to enforce differential privacy~\\cite{DBLP:conf/csfw/AbuahSDN21}, to slice programs~\\cite{DBLP:conf/compsac/ChenCZXCX14}, or to understand the performance of a program via profiling~\\cite{berger2023triangulating}.\nThese techniques play a crucial role in identifying programming errors, security vulnerabilities, and performance bottlenecks.\n\nTo help develop, evaluate, and compare dynamic analysis techniques, other popular programming languages have benchmark suites of executable code.\nFor example, DaCapo for Java~\\cite{Blackburn2006}, SPEC CPU for C/C++~\\cite{Henning2006}, and Da Capo con Scala~\\cite{sewe2011capo} have contributed significantly to work by both researchers and practitioners on their respective programming languages.\n\nDespite the popularity and importance of Python, there currently is no equivalent benchmark suite for Python, which hinders the progress and evaluation of dynamic analyses.\nInstead, researchers, who want to work on a novel dynamic program analysis for Python, would like to study the runtime behavior of Python code, or want to gather data from executions of Python software, must spend significant efforts on creating a suitable set of executable programs.\nAs a result, many evaluations that involve dynamic analysis of Python are performed on a relatively small set of benchmark programs, e.g., nine projects~\\cite{fse2022-DynaPyt}, five projects~\\cite{fse2023-LExecutor}, six projects~\\cite{chen2014dynamic}, and eleven projects~\\cite{Xu2016a}.\nMoreover, since every team working on Python creates their own benchmark, it is difficult to compare different techniques and results with each other.\n \nIn response to this gap, this paper presents \\name{}, a novel benchmark of executable Python projects.\nWe envision our benchmark to facilitate future work on dynamically analyzing Python.\nTo this end, the benchmark is the first to offer four important properties -- large-scale, diverse, ready-to-run, and ready-to-analyze -- as described in the following:\n\\begin{itemize}\n\t\\item \\emph{Large-scale}.\n\tTo offer users an extensive collection of executable code, the benchmark includes a substantial number of popular and non-trivial projects.\n\tThe definition of ``large-scale'' may vary depending on the specific use case.\n\tIn this work, we have curated a dataset comprising 50 projects, collectively amounting to 681k lines of code.\n\tThis size is an order of magnitude larger than the benchmarks considered in previous evaluations, as listed above.\n\t\n\t\\item \\emph{Diverse}.\n\tGiven Python's numerous applications, it is critical that a benchmark covers a wide number of application domains, which each have their own code style and libraries.\n\t\\name{} attempts to represent the breadth and diversity of Python usage by including projects systematically sampled from different application domains, enabling extensive assessments and studies of code behavior across different contexts.\n\t\n\t\\item \\emph{Ready-to-run}.\n\tExecuting the code of a real-world, complex project is non-trivial, as it requires a suitable project configuration, installing third-party dependencies, and inputs that exercise the code.\n\tThe effort spent on setting up projects can be significant, especially for users who are not familiar with the project, and usually, this effort needs to be spent again and again for each new project.\n\t\\name{} comes with all projects already set up and uses the test suites of these projects to provide inputs.\n\tTo facilitate running the code irrespective of variations in project dependencies, configurations, and execution environments, \\name{} provides a unified interface that allows users to run the benchmark with a single command.\n\tMoreover, we provide a Docker image that encapsulates the benchmark and all its dependencies, ensuring reproducibility and longevity.\n\t\n\t\\item \\emph{Ready-to-analyze}.\n\tTo facilitate the dynamic analysis of the projects in \\name{}, the benchmark integrates with DynaPyt~\\cite{fse2022-DynaPyt}, a general-purpose dynamic analysis framework for Python.\n\tThe integration allows for instrumenting and analyzing projects with a single command, making it straightforward to apply a new dynamic analysis to a wide range of projects.\n\\end{itemize}\n\nTo evaluate the usefulness and practicality of \\name{}, we perform a series of experiments covering a wide range of research areas.\nIn particular, we apply our benchmark in three usage scenarios:\n\\begin{enumerate}\n\t\\item We use \\name{} to empirically compare statically and dynamically created call graphs.\n\tTo this end, we use the dynamic analysis infrastructure built into \\name{} to gather dynamic call graphs and compare them to the results of a state of the art static call graph generator~\\cite{DBLP:conf/icse/SalisSLSM21}.\n\tOur results help understand the strengths and weaknesses of both approaches, and provide some guidelines for future work on call graph construction.\n\t\\item We use \\name{} to create training data for LExecutor~\\cite{fse2023-LExecutor}, a recent neural network-based technique for learning-guided execution.\n\tIn this scenario, we apply the existing source code instrumentation tool provided by LExecutor to the code in \\name{} and then execute our benchmark.\n\tBy using the resulting 436,355 data examples  as training data for the LExecutor model, we find that more data helps improve the accuracy of the model.\n\t\\item Finally, we use \\name{} to mine specifications from execution traces.\n\tFor this application, we again build upon the dynamic analysis support built into our benchmark, this time to extract traces of function calls. \\name{} provided thousands of sequences of calls that allowed to extract some meaningful patterns, which sets a baseline for future work on dynamic specification mining for Python.\t\n\\end{enumerate}\nFor all three of these applications, one would usually have to spend a significant effort on setting up suitable projects and finding inputs, e.g., in the form of test suites, for exercising their code.\nInstead, \\name{} provides this setup, which hugely facilitates these and future dynamic analyses for Python.\n \nIn summary our work contributes the following:\n\\begin{itemize}\n\\item We address the lack of a comprehensive benchmark suite of executable Python projects by creating \\name{}.\n\n\\item We integrate our benchmark with the general-purpose dynamic analysis framework DynaPyt~\\cite{fse2022-DynaPyt}, which allows for performing arbitrary dynamic analyses on the entire benchmark with minimal effort.\n\n\\item We illustrate the usefulness of \\name{} in three application scenarios.\n\\end{itemize}\n\n\n\\section{Methodology}\n\nThis section describes our methodology for creating a benchmark of executable Python software.\nWe begin by presenting the criteria for selecting projects to include in the benchmark, and then describe our methodology for making these projects ready-to-execute and ready-to-analyze.\n\n\\subsection{Selecting Projects}\n\\label{sec:selecting}\n\nTo ensure the diversity and representativeness of the benchmark, we select projects from the Awesome Python repository,\\footnote{\\url{https://github.com/vinta/awesome-python}} which is a curated list of popular, open-source Python projects.\nThe Awesome Python repository has 173k stars on GitHub and more than 400 contributors, which are indicators of its popularity and adoption by the community.\nThe curated list contains 679 projects, including libraries, frameworks, and applications, which are classified into 90 categories.\n\nWe select a subset of the projects from the curated list, following three criteria designed to ensure the diversity, quality, and representativeness of our benchmark.\nFirst, to increase the quality and relevance of the benchmark, we consider only projects that have at least 500 stars on their respective GitHub repositories.\nGitHub stars serve as an indication of a project's popularity, reflecting its usefulness and community support.\nSecond, to make \\name{} diverse, we sample projects from different categories in the Awesome Python list.\nEach category covers a different application domain, such as web crawling, machine learning, and robotics.\nWe randomly sample from all projects in the Awesome Python list, with the constraint to pick at most one project from each category.\nFinally, as our goal is to create an executable benchmark, we focus on projects with test suites. Specifically, we focus on test suites that can be executed with pytest, i.e., one of the most popular testing frameworks for Python.\nThe selected tests include tests written based on Python's built-in \\code{unittest} framework, as well as tests written with pytest itself.\n\nAlternative to sampling projects from the Awesome Python list, we could have selected the most downloaded projects from the Python Package Index (PyPI).\nHowever, we observe that the most downloaded projects do not fully represent the diversity of the Python ecosystem, but are biased toward kinds of projects that many others depend on, such as tools to build and set up projects.\nInstead, the Awesome Python list provides an independently curated list of projects grouped into a diverse set of application domains.\nA downside of using the Awesome Python list is that it focuses on open-source projects only, and hence, may not be fully representative of the entire Python ecosystem.\nWe decided to focus on open-source projects, as they are more likely to be used in research and are more accessible to the community.\n\n\n\\subsection{Enabling Execution}\n\nAs executability is an important property of our benchmark, we invest significant efforts into automating the process of setting up the projects and their test suites.\nOur benchmark can be set up in two ways.\nFirst, we provide a Docker image that encapsulates the benchmark and all its dependencies.\nThis image can be used to run the benchmark on any machine that supports Docker, ensuring reproducibility and longevity.\nSecond, we provide a command-line interface that allows users to automatically install and set up the benchmark and its projects.\nThis process involves cloning the projects, installing their dependencies, and configuring their test suites.\nTo the extent possible, \\name{} invokes standard Python tools for installing, building, and testing projects.\nHowever, some projects require various non-standard steps, such as installing unspecified third-party dependencies or slightly adapting the developer-provided test suites.\nTo address these cases, we manually inspect the projects and make the necessary adjustments to ensure that they are ready-to-execute.\nFrom the perspective of a user of our benchmark, this process is transparent, as the benchmark provides a single command for setting up and executing all or individual projects.\n\nThe following describes the steps of our automated setup process in detail.\nAt first, we clone the project repositories from GitHub.\nTo ensure reproducibility, we clone the repositories at a specific commit, which is the latest commit on or before January 18, 2023.\nThen, we create a Python virtual environment for each project, which ensures that the project's dependencies are installed in an isolated manner.\nNext, we install the project's dependencies within the virtual environment.\nIf available, this step uses the project's \\code{requirements.txt} file, which we augment with additional dependencies if necessary.\nFinally, we install the project itself, ensuring that it is properly set up within its dedicated virtual environment.\n\nTo ensure that the test suites of the projects are ready-to-execute, we configure them to run with pytest.\nThis step involves specifying the locations of the test cases within the project.\nFor some projects, additional dependencies may be required to execute the test suite.\nThese dependencies are often not mentioned in the requirements file but can be found in the project's README instructions.\nTo handle this scenario, we manually collect the necessary dependencies for each project.\nFurthermore, we overwrite specific test files of some projects, e.g., for tests that run into an infinite loop or that consume an unacceptably high amount of memory.\nThis affects a total of 51 test files across 18 projects.\nFinally, we validate that the test suites are ready-to-execute by running all the above steps on a fresh machine and by checking that the tests execute successfully.\n\n\n\\begin{table*}[t]\n\t\\caption{Projects in the benchmark.}\n\t\\label{tab:projects}\n\t\\setlength{\\tabcolsep}{4pt}\n\t\\small\n\t\\begin{tabular}{@{}ll|ll|ll@{}}\n\t\t\\toprule\n\t\tProject & Domain & Project & Domain & Project & Domain \\\\\n\t\t\\midrule\n\t\takshare & Downloader & grab & Web crawling & python-decouple & Configuration  \\\\\n\t\tarrow & Date \\& time & graphene & GraphQL & python-diskcache & Caching \\\\\n\t\tblack & Code formatter  & gunicorn & WSGI servers & python-future & Compatibility \\\\\n\t\tblinker & Misc.  & html2text & Web content & python-patterns & Algorithms \\\\\n\t\tsupervisor & DevOps & lektor & Static sites & PythonRobotics & Robotics \\\\\n\t\tcelery & Task queues & marshmallow & Serialization & pyvips & Parallel img. proc. \\\\\n\t\tcerberus & Data validation & mezzanine & CMS & requests & HTTP clients \\\\\n\t\tclick & CLI app dev. & moviepy & Video & schedule & Job schedul. \\\\\n\t\tcode2flow & Code analysis & pdoc & Documentation & seaborn & Data visual. \\\\\n\t\tdelegator.py & Processes & pickledb & Database & streamparse & Distr. comput. \\\\\n\t\tdh-virtualenv & Distribution & pillow & Image proc. & structlog & Logging \\\\\n\t\telasticsearch-dsl & Search  & pudb & Debugging & thefuck & Cmd-line tools \\\\\n\t\terrbot & ChatOps tools  & pydub & Audio & uvicorn & ASGI servers \\\\\n\t\tflask-api & RESTful API & pyfilesystem2 & Files & webassets & Web asset mgmt. \\\\\n\t\tfuncy & Functl.\\ progr. & pyjwt & Authentic. & wtforms & Forms  \\\\\n\t\tfurl & URL manip. & pypdf & Format proc.  & zerorpc-python & RPC servers \\\\\n\t\tgeopy & Geolocation & pyquery & HTML manip. \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\\end{table*}\n\n\n\n\\subsection{Enabling Dynamic Analysis}\n\\label{sec:dynamic analysis}\n\nOne of the main goals of this work is to facilitate dynamic analysis of Python projects.\nTo this end, the benchmark integrates with DynaPyt~\\cite{fse2022-DynaPyt}, a general-purpose dynamic analysis framework for Python.\nDynaPyt supports a range of analyses by providing an API of callbacks that are triggered whenever a specific kind of runtime event occurs, such as a function call, the creation of a new object, or a binary operation.\nWe show two DynaPyt-based applications of \\name{} in Sections~\\ref{sec:call graphs} and~\\ref{sec:spec mining}.\n\nPerforming a dynamic analysis with DynaPyt requires instrumenting the source code of the target project.\n\\name{} facilitates this step by providing a single command for instrumenting all or selected projects in the benchmark.\nThe command applies DynaPyt's instrumentation tool to the source code of the selected projects.\n\\name{} offers two variants of this command:\nOne variant instruments all Python code in a project, which is useful to analyze the executions of both the tests and the project's main code.\nThe other variant instruments only the application code, i.e., excluding the test suite.\nWe use the latter variant, e.g., for performing a dynamic call graph analysis (Section~\\ref{sec:call graphs}).\nFinally, \\name{} provides a command for running the instrumented projects, which automatically invokes the callbacks into a user-provided dynamic analysis whenever a runtime event occurs that is of interest to the analysis.\n\nBeyond dynamic analyses built with DynaPyt, the benchmark can, of course, also be used with other dynamic analysis tools.\nFor example, one of our applications (Section~\\ref{sec:lexecutor}) is based on a custom, instrumentation-based dynamic analysis.\nFurthermore, the benchmark could also be easily run with dynamic analysis tools that do not require source-level instrumentation, but that instead build upon Python's built-in tracing library \\code{sys.settrace}.\n\n\n\n\n\n\\section{\\name{}}\n\nBy applying the methodology described in the previous section, we create \\name{}, a large-scale, diverse, ready-to-run, and ready-to-analyze benchmark of Python projects.\nThis section presents the composition of the benchmark (Section~\\ref{ss:composition}), its runtime properties (Section~\\ref{ss:runtime}), and details of its implementation (Section~\\ref{ss:implementation}).\n\n\\subsection{Composition of the Benchmark}\n\\label{ss:composition}\n\n\\name{} consists of 50 projects, listed in Table~\\ref{tab:projects}.\nThe code in these projects sums up to 681k lines of Python code, which can be considered large-scale in comparison to current community standards.\nFor comparison, recent papers on dynamic analyses for Python are usually evaluated with an order of magnitude fewer projects, e.g., nine projects~\\cite{fse2022-DynaPyt}, five projects~\\cite{fse2023-LExecutor}, six projects~\\cite{chen2014dynamic}, and eleven projects~\\cite{Xu2016a}.\n\n\nEach project in \\name{} covers a different application domain, where the domains are determined by the Awesome Python repository (Section~\\ref{sec:selecting}).\nTable~\\ref{tab:projects} lists the 50 domains along with the projects included in the benchmark.\n\nWithin the selected projects, 94.5\\% of files are Python files, whereas some files are written in other  languages (2.0\\% JavaScript, 1.2\\% HTML, 0.8\\% C).\n\n\\subsection{Runtime Properties of the Benchmark}\n\\label{ss:runtime}\n\n\\begin{table}[t]\n\t\\caption{Properties of \\name{}.}\n\t\\label{tab:properties}\n\t\\setlength{\\tabcolsep}{20pt}\n\t\\begin{tabular}{@{}lr@{}}\n\t\t\\toprule\n\t\tMetric & Value \\\\\n\t\t\\midrule\n\t\tProjects & 50 \\\\\n\t\tLines of code &681k \\\\\n\t\tTest cases: \\\\\n\t\t\\hspace{1em} Total & 29,511 \\\\\n\t\t\\hspace{1em} Passing &27,569 \\\\\n\t\t\\hspace{1em} Failing & 270 \\\\\n\t\t\\hspace{1em} Skipped & 1,672 \\\\\n\t\tLines of executed code: \\\\\n\t\t\\hspace{1em} Total lines & 558k \\\\\n\t\t\\hspace{1em} Coverage & 82\\% \\\\\n\t\tExecution time: \\\\\n\t\t\\hspace{1em} Avg./project & 71 seconds \\\\\n\t\t\\hspace{1em} Min. & 1 seconds \\\\\n\t\t\\hspace{1em} Max. & 1,362 seconds \\\\\n\t\t\\hspace{1em} Total & 3,568 seconds \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\\end{table}\n\nBecause a key property of \\name{} is to be executable, the following takes a deeper look into the runtime properties of the benchmark.\nTable~\\ref{tab:properties} summarizes the key figures.\nThe projects in \\name{} come with a comprehensive set of test cases, totaling 29,511.\nThe number of test cases per project ranges from 1 to 3,947, with an average of 590 test cases per project.\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=1.01\\linewidth]{figures/proj_test_cases}\n\t\\caption[]{Number of successful, failed, and skipped test cases for each project of the benchmark.}\n\n\t\\label{fig:testcasescover}\n\\end{figure}\n\nOut of all test cases in the benchmark, 27,569 test cases pass, while 270 test cases fail and the rest gets skipped.\n\\name{} has 31 projects with no failing test cases, while 13 projects have between 1 and 10 failing tests.\nFigure~\\ref{fig:testcasescover} shows the number of successful, failing and skipped test cases per project.\n\nThe pass rate, i.e., the percentage of passing test cases among all test cases, across all projects in \\name{} is 93\\%.\nTo better understand the distribution of test case outcomes across projects, Figure~\\ref{fig:passratecdf} shows the cumulative distribution of test case pass rate.\nThe plot shows that 41 projects have a pass rate exceeding 90\\%, and 46 projects have 80\\% or more test cases passing.\nThis high pass rate not only allows users of our benchmark to execute a lot of code, but also to observe mostly valid executions.\n\nThere are two main reasons why some projects have failing test cases.\nFirst, some tests fail due to incompatible hardware.\nFor example, some test cases are designed to run on a specific hardware, e.g., a GPU or a specific CPU architecture.\nOur Docker-based setup provides software abstraction, but not hardware abstraction, which can lead to test failures depending on the used machine.\nSecond, some tests fail due to incompatible or unavailable software.\nExamples include test cases designed to run on a particular operating system and test cases that use online services that require authentication.\nBecause our Docker-based setup provides a single consistent software environment, including a specific operating system, tests that require another software environment are doomed to fail.\n\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=0.5\\linewidth]{figures/pass_rate}\n\t\\caption{Cumulative distribution of test suite pass rates for all projects.}\n\t\\label{fig:passratecdf}\n\\end{figure}\n\n\nFigure~\\ref{fig:coverage} shows the distribution of covered and uncovered statements as a result of test execution for each project. The figure illustrates that the number of missed statements is noticeably low for the vast majority of the projects. Furthermore, 27 projects have coverage levels that exceed 85\\%. This high coverage not only demonstrates comprehensive testing within our benchmark, but it also plays an important role in providing diverse and meaningful data for a variety of applications, as discussed in more detail in Section~\\ref{sec:applications}.\n\\begin{figure}\n\t\\centering\n\n\t\\includegraphics[width=1.01\\linewidth]{figures/proj_cov}\n\t\\caption{Distribution of covered statements vs uncovered statements during the execution of test suites for each project.}\n\t\\label{fig:coverage}\n\\end{figure}\n\n\nFinally, we report the wall-clock time required to execute the test suites of the projects in \\name{}.\nThe following numbers are obtained on a standard computer with an Intel Xeon CPU running at 2.10GHz and 32GB of memory. We launch all the test suites on a single CPU core.\nThe time to execute the test suite of single project ranges from 1 second to 1,362 seconds. Figure~\\ref{fig:projectstesttime} illustrates the runtime for each project (rounded to full seconds), as well as the corresponding number of test cases.\nThe total runtime for all projects combined amounts to 3,568 seconds, i.e., about one hour.\nThe average time required to execute a complete test suite is 71 seconds.\nNotably, 39 projects can complete their test suite within 50 seconds or less, while there are four projects with runtimes exceeding 200 seconds.\nThe diversity in project sizes and runtimes of their corresponding test suites allows users of \\name{} to sample projects with different characteristics, depending on the specific needs.\n\n\\begin{figure*}[t]\n\t\\centering\n\t\\includegraphics[width=1.01\\linewidth]{figures/proj_test_time}\n\t\\caption{Number of test cases vs. execution time for each projects.}\n\t\\label{fig:projectstesttime}\n\\end{figure*}\n\n\n\n\\subsection{Implementation}\n\\label{ss:implementation}\n\nInspired by other widely used benchmarks in the community~\\cite{Just2014,DBLP:journals/corr/abs-1903-06725}, \\name{} comes with a convenient command-line interface that allows users to interact with the benchmark and its projects.\nFor example, the interface supports running all or selected projects, instrumenting their source code, or applying a dynamic analysis to them.\n\nFor reproducibility and to ensure the longevity of the benchmark, we provide an Ubuntu 20.04-based Docker image that encapsulates the benchmark and all its dependencies.\nWe use Python~3.10 as the default Python version across all our Python environments, considering its widespread usage and stability.\nTo support dynamic analysis, the benchmark integrates with DynaPyt version 0.3.1.\n\nCurrently, \\name{} contains 50 projects, but it is designed to be easily extensible.\nThe benchmark can be extended by adding the GitHub URL of new projects to a specific text file.\nOur installation script will try to install and configure the new projects based on frequently observed setup patterns.\nIn case our script fails to automatically install a project, the user can edit the installation script to add a special case.\n\nAll our experiments are conducted on a machine with an Intel Xeon Gold 6230 CPU, 64GB RAM limit and 300GB disk space.\nThe disk space is needed only when performing the applications described below on all projects at once.\n\n\\section{Applications}\n\\label{sec:applications}\n\nWe demonstrate the usability and usefulness of DyPyBench by studying three applications of the benchmark. \nFirst, we compare dynamic and static call graphs extracted for the programs in DyPyBench (Section~\\ref{sec:call graphs}). \nSecond, we use the benchmark to generate training data for LExecutor~\\cite{fse2023-LExecutor}, a machine learning-based technique for executing incomplete code snippets (Section~\\ref{sec:lexecutor}).\nFinally, we dive into the domain of mining specifications from execution traces (Section~\\ref{sec:spec mining}).\n\n\n\\subsection{Studying Static and Dynamic Call Graphs}\n\\label{sec:call graphs}\n\nCall graphs are a fundamental building block for various program analyses.\nBecause of their importance, various algorithms for computing call graphs have been proposed, e.g., for C++~\\cite{bacon1996fast}, Java~\\cite{tip2000scalable}, Scala~\\cite{ali2014constructing}, JavaScript~\\cite{Feldthaus2013a}, and Python~\\cite{DBLP:conf/icse/SalisSLSM21}.\nTo better understand the properties of such algorithms, several studies compare the call graphs produced by different analyses with each other, e.g., for C~\\cite{murphy1998empirical}, Java~\\cite{Reif2019,OnTheRecallICSE2020}, JavaScript~\\cite{chakraborty2022automatic}, and WebAssembly~\\cite{issta2023-tough-call}.\nHowever, to the best of our knowledge, there is no study that compares dynamic and static call graphs for Python.\n\nThe following application of \\name{} fills this gap by comparing call graphs generated by PyCG~\\cite{DBLP:conf/icse/SalisSLSM21}, a state-of-the-art static call graph analysis for Python, with dynamic call graphs generated by DynaPyt~\\cite{fse2022-DynaPyt}.\nBecause \\name{} is ready-to-analyze and already integrated with DynaPyt, obtaining dynamic call graphs of the 50 projects in our benchmark requires relatively little effort.\n\n\\subsubsection{Experimental Setup}\n\nA call graph $G = (F, E)$ consists of set $F$ of functions and a set $E$ of edges.\nAn edge $e \\in F \\times F$ represents the fact that a caller function invokes a callee function.\nIf a function is called multiple times by the same caller, the call graph contains only a single between the caller and the callee.\nTo identify functions, both DynaPyt and PyCG resolve the fully qualified name, including the module name.\nFor a given Python project, we construct a static call graph $G_{static}$ by applying PyCG to the project's source code.\nAs mentioned in the documentation of PyCG, we specify the project path and the set of all Python files in the project.\nTo generate a dynamic call graph $G_{dynamic}$, we use the existing dynamic call graph analysis provided by DynaPyt~\\cite{fse2022-DynaPyt}.\nTo this end, we instrument and then execute the projects in our benchmark, as described in Section~\\ref{sec:dynamic analysis}.\nWe set a time limit of six hours and a memory limit of 60GB for the analysis of a single project. Furthermore, we set a timeout of 30s for each individual test case when run with the DynaPyt analysis.\nMoreover, we ignore the results of an analysis on a project in case the analysis itself crashes.\n\nOnce we obtain the call graphs $G_{static}$ and $G_{dynamic}$ for the projects in \\name{}, we compare them with each other.\nFor a quantitative comparison, we consider three sets:\n(i) the set of call graph edges, (ii) the set of all callers, i.e., nodes with at least one outgoing edge, and (iii) the set of callees, i.e., nodes with at least one incoming edge.\n\n\nFor both analyses, we ignore calls made by the underlying unit testing tool and calls made directly inside a test case, because we are interested in the calls made within the analyzed project.\nFor example, if a test file \\code{test\\_app.py} contains a test case function called \\code{test\\_1}, then the call to \\code{test\\_1} appears neither in $G_{static}$ nor $G_{dynamic}$.\nLikewise, any calls made directly inside \\code{test\\_1} are also ignored.\nIn contrast, calls made by functions invoked by \\code{test\\_1} are added to the call graphs.\n\nIn addition to the quantitative comparison, we manually inspect a sample of 97 differences between the static and dynamic call graphs.\nThe goal of this inspection is to understand the differences and to classify them by their root cause.\n\n\n\\subsubsection{Results}\n\n\\paragraph{Static call graphs}\nPyCG successfully creates a call graph for 39 out of 50 projects.\nThe missing projects are due to six projects that exceed the six-hour timeout, three projects that exceed the memory limit of 60GB, and two projects where PyCG crashes with an exception. \n\nAcross the 39 successful projects, the static call graphs have a total of 60,565 edges, with an average of 1,552 edges per project.\nThese edges are between 33,795 unique callers and 10,537 unique callees, where uniqueness is computed at the project level, i.e., if the same function appears in two different projects we count it twice.\n\n\n\\paragraph{Dynamic call graphs}\nDynaPyt successfully creates dynamic call graphs for all 50 projects.\nOn average, executing \\name{} while computing the call graph takes 215 minutes per project.\nThis time is longer than simply executing \\name{} without computing call graphs because DynaPyt introduces additional overhead by instrumenting the code.\n\nThe dynamic call graphs have a total of 9,575 edges, giving an average of 191 edges per project. These edges are between 3,078 unique callers and 5,384 unique callees. Considering only the 39 projects where PyCG runs successfully, DynaPyt generates 6,306 edges between 2,049 callers and 3,741 callees.\n\n\\begin{figure*}[t]\n\t\\captionsetup[subfigure]{aboveskip=-3pt,belowskip=-3pt}\n\t\\begin{subfigure}[b]{0.32\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\textwidth]{figures/edges.png}\n\t\t\\caption{Edges}\n\t\t\\label{fig:edges}\n\t\\end{subfigure}\n\t\\begin{subfigure}[b]{0.32\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\textwidth]{figures/callers.png}\n\t\t\\caption{Callers}\n\t\t\\label{fig:callers}\n\t\\end{subfigure}\n\t\\begin{subfigure}[b]{0.32\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\textwidth]{figures/callees.png}\n\t\t\\caption{Callees}\n\t\t\\label{fig:callees}\n\t\\end{subfigure}\n\t\n\t\\caption{Venn diagrams of edges, callers, and callees in dynamic call graphs (left) and static call graphs (right).}\n\t\n\t\\label{fig:venncallgraph}\n\\end{figure*}\n\n\\paragraph{Comparison of edges}\nFigure~\\ref{fig:edges} compares the edges found in the static and dynamic call graphs across the 39 projects that could be successfully analyzed by both the static and dynamic analysis.\nThe static call graphs have many more edges than their dynamic counterparts, which is expected because the static analysis considers all code of a project, whereas the dynamic analysis focuses on code exercised by the tests in \\name{}.\n\nSurprisingly, many edges found by the dynamic analysis are not detected by the static analysis.\nIdeally, a static call graph should be sound, i.e., include all call edges that can occur at runtime.\nInstead, we find that only 49\\% of the edges present within DynaPyt's call graphs are within the set of edges found in PyCG's call graphs. \n\n\\paragraph{Comparison of callers}\nTo better understand the differences between the static and dynamic call graphs, we compare their sets of callers and callees.\nAs illustrated in Figure~\\ref{fig:callers}, we notice a big difference when comparing the sets of callers derived from PyCG and DynaPyt.\nWhile almost all the callers collected by DynaPyt (95\\%) are present in the set of callers identified by PyCG, the static call graph contains many callers missed by the dynamic analysis.\nWe attribute this difference to the limited code coverage in the exercised tests, which does not exercise all possible behavior of the projects.\n\n\\paragraph{Comparison of callees}\nWhen checking the set of callees, an even more important disparity emerges between DynaPyt and PyCG, as shown in Figure~\\ref{fig:callees}.\nOnly 48.5\\% of DynaPyt's callees are included in the set of PyCG's callees.\nThat is, the static call graph is missing many called functions, even though \\name{} offers evidence in the form of actual executions that these functions get called.\n\n\\paragraph{Root causes of differences}\nTo understand the reasons that cause the static and dynamic call graphs to differ, we first automatically analyze why edges are missing, and then manually inspect a sample of missing callers and callees.\nOur exploration begins with an examination of edges in the dynamic call graph (\\dynamic{}) that do not find correspondence in the static call graph (\\static{}). Remarkably, 51\\% of the edges in \\dynamic{} remain unrepresented in \\static{}. A detailed analysis of these disparities reveals that for 92\\% of such edges, the reason is that the callee is missing in \\static{} (while the callers exist). Furthermore, in 8\\% of cases, both the callers and callees coexist within \\static{}, albeit without an edge connecting them.\nWe also analyze \\static{} edges that do not appear in \\dynamic{}. This analysis shows that for 79\\% of such edges both the callers and the callees are missing from the dynamic call graph, while for 20\\% of these edges only the callees are missing from \\dynamic{}.\n\nNext, we manually inspect disparities in callers, more specifically the 5\\% of callers appearing in \\dynamic{} but not in \\static{}.\nOur manual inspection shows that the mismatch is caused by differences in the way DynaPyt and PyCG refer to the fully qualified name of some callers. For example, within the Flask project, there is a wrapper around the \\code{Requests} library, which makes PyCG resolve the function to \\code{FlaskAPI.Requests.something}. Instead, DynaPyt refers directly to the original caller, without the  wrapper, which results in the caller's name being \\code{Requests.something}. \n\nFinally, we analyze the callees captured by \\dynamic{} but still missing from \\static{}. We identify two primary causes for callees missed by the static analysis:\n\\begin{itemize}\n\t\\item Our manual inspection shows that PyCG's is missing many calls to methods offered by Python's built-in types. For instance, a call \\code{\"abc\".strip()} is adequately detected by DynaPyt, recording them it as a call to \\code{str.strip}, but PyCG fails to capture the call.\n\tBased on a list of Python's built-in function names, we find that this particular phenomenon accounts for 59\\% of all callees missing in the static call graph. In addition, calls in the form of \\code{super.method()} are also not included in the call graph of PyCG. \n\t\\item Another reason is, again, the resolution of modules names, which causes differences in the fully qualified names of functions.\n\tThe Flask example given above illustrates this problem. Differences in function name resolution account for roughly 12\\% of the observed mismatches.\t\n\\end{itemize}\n\n\n\\subsubsection{Implications}\nTo the best of our knowledge, the call graphs generated with \\name{} are the first large-scale dataset of dynamic call graphs for Python.\nSuch a dataset may serve as a ground truth of calls that a sound static analysis should at least detect.\nFor example, our finding that PyCG is missing method calls made on objects of built-in types, such as strings, calls for considering such calls in future static call graph analyses.\n\nMoreover, the dataset provides a basis for a more detailed empirical study of call graph generation algorithms for Python.\nOne challenge to be addressed in such a study is to define a uniform way of resolving functions into a fully qualified name, to avoid spurious differences between different call graphs for the same project.\n\nFurthermore, the dynamic call graphs can help evaluate and develop techniques for optimizing and pruning statically generated call graphs~\\cite{Utture2022}.\n\nFinally, our observation that dynamic call graphs can be generated quicker than static call graphs for some projects motivates future work on more efficient static call graph algorithms.\n\n\n\n\n\\subsection{Gathering Runtime Data for Training a Neural Model}\n\\label{sec:lexecutor}\n\nNeural models of software are becoming increasingly popular for various applications~\\cite{NeuralSoftwareAnalysis}, e.g., code completion, bug detection, and type prediction.\nWhile the majority of approaches focuses on training models on source code and other static artifacts of software, there is an emerging subfield that trains models on data gathered during the execution of programs~\\cite{pei2020trex,fse2021,pei2021stateformer,fse2023-LExecutor}.\nA major challenge in this line of work is to gather large-scale datasets of runtime data, which is required to obtain effective neural models.\nThe following demonstrates that \\name{} can help address this challenge.\n\n\\subsubsection{Experimental Setup}\n\nAs a case study, we use LExecutor~\\cite{fse2023-LExecutor}, a recent technique that queries a neural model to predict otherwise missing values, and hence, allows for executing arbitrary code snippets.\nThe neural model underlying LExecutor is trained on a dataset of value-use events.\nEach such event consists of a value observed during the execution of a program and the code context in which the value gets used.\nThe original dataset used to train LExecutor consists of 214,365 value-use events, which were gathered from five projects.\n\nTo evaluate the usefulness of \\name{} for training LExecutor, we use the benchmark to generate an additional dataset of value-use events.\nTo this end, we instrument the projects in \\name{} with the custom Python instrumentation that is part of LExecutor and then execute the instrumented benchmark, which yields a dataset of value-use events.\nWe keep a random 5\\% of the dataset for validation, and use the remaining 95\\% to train a new LExecutor model.\nDuring training, we follow the default setup in the original LExecutor work, i.e. its fine-grained value abstraction and the CodeT5 model, except that we train for five instead of ten epochs due to computational constraints.\n\n\\subsubsection{Results}\n\n\\begin{figure}\n\t\\includegraphics[width=0.7\\linewidth]{figures/lexecutor_acc}\n\t\\caption{Accuracy of LExecutor~\\cite{fse2023-LExecutor} model trained on \\name{}-based data.}\n\t\\label{fig:lexecutor_acc}\t\n\\end{figure}\n\nExecuting \\name{} results in a total of 436,355 value-use events.\n\nThat is, thanks to \\name{}, the available training data is more than double the size compared to the original LExecutor work.\n\nTo assess the impact of using training datasets of different sizes, we train different LExecutor models with increasingly large subsets of the combined training dataset.\nSpecifically, we train models with subsets of 10\\%, 20\\%, 30\\%, 40\\%, 50\\%, 60\\%, 70\\%, 80\\%, 90\\%, and 100\\% of the data, respectively.\nWe then measure accuracy on the held-out validation data.\nFollowing the original LExecutor work, we report top-1 accuracy, i.e., the percentage of predictions for which the correct value is the most likely prediction.\n\nFigure~\\ref{fig:lexecutor_acc} shows the accuracy of the resulting models.\nThe results illustrate that, perhaps unexpectedly, having more training data leads to a more accurate model, i.e., \\name{} provides an easy way to improve the accuracy of the original model.\n\n\\subsubsection{Implications}\n\nThe continuously increasing interest in neural models of software calls for large-scale datasets of runtime data.\n\\name{} can help address this need by providing a ready-to-analyze benchmark of executable Python projects.\nWe envision future work to build upon our benchmark to generate various other kinds of runtime datasets, e.g., for predicting function names from traces of function executions, for predicting static type annotations from types observed at runtime, or for predicting the next value of a variable from its previous values.\n\n\n\\subsection{Mining Specifications from Execution Traces}\n\\label{sec:spec mining}\n\nSpecification mining~\\cite{Ammons2002} is a technique that extracts specifications from existing software.\nVarious techniques for mining specifications have been proposed~\\cite{robillard2012automated}, many of which focus on temporal constraints between function calls.\nSpecification mining plays a role in various applications, including identifying inconsistencies and anomalies, establishing best practices, and gaining a deeper understanding about the common behavior of a given programs.\nThe following demonstrates that \\name{} can be used to mine specifications from execution traces of Python code.\n\n\\subsubsection{Experimental Setup}\nIn our experiment, we leverage call traces generated by DynaPyt to uncover patterns in function calls within the projects in DyPyBench. The process unfolds as follows:\n\n\\begin{enumerate}\n\t\\item We customize DynaPyt's call graph analysis to capture traces of function calls, where each trace entry consists of a caller and a callee function.\n\t\n\t\\item We instrument and execute the projects in \\name{} to capture sequences of function calls for each project. Similar to the call graphs analysis, we only instrument and collect traces of the main source code of a project, without instrumenting the test cases themselves.\n\t\n\t\\item To refine the data, we post-process the call sequences to inline callees into each caller functions, and to remove sequences that are incomplete due to exceptions.\n\t\n\t\\item We apply an efficient implementation\\footnote{\\url{https://github.com/chuanconggao/PrefixSpan-py}} of the PrefixSpan algorithm~\\cite{han2001prefixspan} to mine frequent patterns from the sequences of functions called within a specific function.\n\tWe give PrefixSpan a list of sequences, each of which corresponds to the functions called within another function, in the order of their execution.\n\tWe apply the mining algorithm to the aggregated set of sequences of all projects in \\name{}.\n\tTo reduce the computational cost of the mining algorithm, we consider only sequences of calls that have a length lower or equal to 100 calls, which corresponds to 94\\% of all call traces.\n\t\n\t\\item Finally, we extract the top-100 patterns with a length greater than two. \n\t\n\\end{enumerate}\nDue to time constraints we limit the collection of call traces to 72 hours, which results in traces from 27 projects. While collecting traces using DynaPyt takes a relatively long time, generating the patterns with PrefixSpan is quite fast when invoked with no constraints on the collected patterns. For example, for all the sequences we collect, PrefixSpan is able to generate the top-100 patterns in less than ten seconds. However, this time is highly influenced by the length of sequences, which is the reason why we exclude sequences longer than 100 calls.\n\n\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=0.5\\linewidth]{figures/seq_length_dist}\n\t\\caption{Distribution of extracted calls sequences by length.}\n\t\\label{fig:seqlengthdist}\n\\end{figure}\n\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=1.01\\linewidth]{figures/number_sequences}\n\t\\caption{Number of extracted sequences per project.}\n\t\\label{fig:numbersequences}\n\\end{figure}\n\n\\subsubsection{Results}\nIn total, we extract 16,538 sequences, with an average of 612 sequences per project. The average length is eight, which means that the average function transitively performs eight function invocations.\nThe CDF in Figure~\\ref{fig:seqlengthdist} shows the distribution of the length of the  collected sequences. The plot indicates that most of the sequences (91\\%) contain less than ten calls. This implies that the most frequent patterns extracted by PrefixSpan are going to be relatively short patterns.\nIn addition, the plot in Figure~\\ref{fig:numbersequences} illustrates the number of call sequences collected from each project, which ranges between tens of sequences and several thousands. The distribution, however, is far from uniform, which based on our initial results, leads to the patterns extracted from one project dominating the top-100 patterns that we mine from the entire set of projects. To prevent this bias, we set a limit on the number of sequences that we consider from each project during the pattern mining. We define the limit as the mean number of sequences (613) over all projects, plus one standard deviation (1,280), which totals to 1,893 sequences.\nAmong the top-100 patterns, we randomly pick five patterns and present them in Table~\\ref{tab:examples} alongside their frequency and an example of their occurrence in a project in \\name{}. The patterns range from sequences of built-in functions and standard library functions (e.g., in rows 1 and 2) to library-specific patterns (e.g., in rows 3 and 4). While the extracted patterns look simple and straightforward, the code reflecting those patterns can be complicated due to multiple branches (e.g., in row 3 and 4) or because they occur in a complex expressions, such as in row 2. Collecting execution traces and mining frequent patterns condenses such behavior into a temporal specification. \n\n\\begin{table}\n\t\\caption{Examples of patterns among top-100 mined patterns.}\n\t\\label{tab:examples}\n\t\\setlength{\\tabcolsep}{15pt}\n\t\\begin{tabular}{@{}lrl@{}}\n\t\t\\toprule\nPattern & \\multicolumn{1}{l}{Freq.} & \\multicolumn{1}{l}{Code examples from DyPyBench} \\\\ \n\\midrule\n\\begin{lstlisting}[aboveskip=-.8em,belowskip=-.7em]\nbuiltins.isinstance\nbuiltins.isinstance\n\\end{lstlisting} & 1,701 & \n\\begin{lstlisting}[aboveskip=-.8em,belowskip=-.7em,language=Python]\nif isinstance(ty, tuple):\n  return Tuple(ty)\nif isinstance(ty, ParamType):\n  return ty\n\\end{lstlisting}\\\\\n\\hline\n\\begin{lstlisting}[aboveskip=-.8em,belowskip=-.7em]\nbytes.join\nbuiltins.str\nstr.encode\n\\end{lstlisting} & 1,008 & \n\\begin{lstlisting}[aboveskip=-.8em,belowskip=-.7em,language=Python]\nreturn b\"\".join([b\"HTTP/1.1\",\n  str(status_code).encode(), \n  b\"\\s\", phrase, b\"\\r\\n\"])\n\\end{lstlisting}\\\\\n\\hline\n\\begin{lstlisting}[aboveskip=-.8em,belowskip=-.7em]\nPattern.match\nMatch.span\nstr.isidentifier\n\\end{lstlisting} & 730 &  \\begin{lstlisting}[aboveskip=-.8em,belowskip=-.7em,language=Python]\npseudomatch=pseudoprog.match(line,pos)\nif pseudomatch:  # scan for tokens\n  start, end=pseudomatch.span(1)\n  #code in between\nif ...\nelif initial.isidentifier():\n  # ...\n\\end{lstlisting}\\\\\n\\hline\n\\begin{lstlisting}[aboveskip=-.8em,belowskip=-.7em]\nthefuck...encode_utf8\nstr.replace\nshlex.split\nstr.replace\nthefuck...decode_utf8\n\\end{lstlisting} & 565 &  \\begin{lstlisting}[aboveskip=-.8em,belowskip=-.7em,language=Python]\nencoded = self.encode_utf8(command)\ntry:\n  splitted=[s.replace(\"??\", \"\\\\ \") \n  \tfor s in shlex.split(\n  \tencoded.replace('\\\\ ','??'))]\nexcept ValueError:\n  splitted = encoded.split(' ')\nreturn self.decode_utf8(splitted)\n\\end{lstlisting}\\\\\n\\hline\n\\begin{lstlisting}[aboveskip=-.8em,belowskip=-.7em]\nlogging.getLogger\nlogger.setLevel\n\\end{lstlisting} & 205 &  \\begin{lstlisting}[aboveskip=-.8em,belowskip=-.7em,language=Python]\nlogger = logging.getLogger(\n\t\"urllib3.connectionpool\")\nlogger.setLevel(logging.WARNING)\n\\end{lstlisting}\\\\\n\\bottomrule\n\t\\end{tabular}\n\\end{table}\n\n\\subsubsection{Implications}\nDespite many research results on specification mining in general~\\cite{robillard2012automated}, our work is the first application of specification mining to Python.\n\\name{} provides a natural foundation for developing and exploring specification mining in Python projects, as it provides a large number of ready-to-run and ready-to-analyze projects.\nBeyond specification mining, future work could explore other applications in software analysis that leverage our dataset of call sequences, e.g., to detect anomalies and inconsistencies.\n\n\\section{Related Work}\n\n\\paragraph{Dynamic analysis and testing of Python}\n\nPython has been the target of several dynamic analyses, e.g.,\nto slice programs~\\cite{DBLP:conf/compsac/ChenCZXCX14},\nto detect type-related bugs~\\cite{Xu2016a}, and \nto enforce differential privacy~\\cite{DBLP:conf/csfw/AbuahSDN21}.\nScalene~\\cite{berger2023triangulating} offers an efficient CPU and memory profiler.\n\\name{} can serve as a dataset to test and evaluate these and future dynamic analyses.\n\nPynguin~\\cite{Lukasczyk2019} is a unit-level, search-based test generator for Python.\nLExecutor~\\cite{fse2023-LExecutor} proposes learning-guided execution, which executes code by injecting otherwise missing runtime values.\nBoth Pynguin and LExecutor share with \\name{} the goal of executing code.\n\\name{} complements these existing efforts by providing a reproducible setup that allows for executing and analyzing a set of real-world projects with little effort.\n\nDynaPyt~\\cite{fse2022-DynaPyt} is a general-purpose dynamic analysis framework.\nBecause it allows for implementing arbitrary dynamic analyses, we integrate DynaPyt into this work and show two applications of it (Sections~\\ref{sec:call graphs} and~\\ref{sec:spec mining}).\n\n\\paragraph{Other work on Python}\nBeyond dynamic analysis, the increasing importance of Python has lead to a stream of work on studying Python software and on supporting Python developers.\nA study of flaky tests~\\cite{DBLP:conf/icst/GruberLK021} relates to this work because they try to automatically execute the test suites of thousands of Python projects.\n\\name{} differs by focusing on a reusable benchmark, where each project and its setup is carefully checked by a human before being included into the benchmark, whereas the prior work is based on a large-scale, fully automated experiment.\nAnother difference is the size and characteristics of the projects.\nAs highlighted in~\\cite{DBLP:conf/icst/GruberLK021}, approximately 75\\% of their projects have fewer than 10 tests, 60\\% exhibit a coverage below 10\\%, and over 90\\% of the projects consist of less than 10k lines of code.\nIn contrast, our benchmark contains only 4\\% of projects with less than 10 test cases, a minimum coverage of 48\\% (average: 82\\%), and an average code base size of 14k lines per project.\n\nThe lack of static type annotations by default has lead to several techniques for inferring and predicting type annotations~\\cite{Xu2016,fse2020,Allamanis2020,Yan2023}, to work on comparing gradual type systems for Python~\\cite{Rak-amnouykit2020}, and to work on studying type annotations in the Python ecosystem~\\cite{fse2022_type_study}.\n\\name{} could be used to create a ground truth of types observed at runtime, which can serve as training data for learning-based type predictors or be used during their evaluation.\n\nAnother study investigates the performance benefits of using Python idioms~\\cite{Zhang2023}.\nFuture work on studying the performance of Python code can benefit from \\name{} as a ready-to-execute collection of real-world code.\n\n\n\\paragraph{Dynamic analysis of other languages}\n\n\nBeyond Python, there are various dynamic analyses, e.g., to detect concurrency bugs~\\cite{Savage1997,OCallahan2003,Flanagan2004}, type-related problems~\\cite{An2011,icse2015}, and other common bug patterns~\\cite{issta2015}.\nOther analyses find optimization opportunities~\\cite{Xu2009,oopsla2015}, or infer API usage protocols~\\cite{Yang2006,ase2009} and input grammars~\\cite{Hoeschele2016}.\nSecurity-oriented analyses include taint analysis~\\cite{Clause2007} and dynamic detectors of similar functions~\\cite{Egele2014}.\n\n\nTo reduce the effort of building a dynamic analysis, dynamic analysis frameworks have been proposed for most popular languages, e.g., \nDynamoRIO~\\cite{bruening2003infrastructure}, Pin~\\cite{luk2005pin}, and Valgrind~\\cite{Nethercote2007}, which all target x86 binaries,\nJalangi~\\cite{Sen2013} for JavaScript,\nWasabi~\\cite{asplos2019} for WebAssembly,\nDiSL~\\cite{DBLP:conf/aosd/MarekVZABQ12} for Java, and RoadRunner~\\cite{Flanagan2010}, which specifically targets concurrency-related dynamic analyses.\n\nAs \\name{} targets Python, it cannot directly support the above analyses, but the breadth and depth of prior work underlines the importance of dynamic analysis, and hence, benchmarks like ours.\n\n\\paragraph{Benchmarks of executable code}\n\nOur work has been partially inspired by other benchmarks of executable code.\nSimilar to \\name{}, several benchmarks offer a set of real-world programs with inputs to execute their code, e.g., DaCapo~\\cite{Blackburn2006} for Java, SPEC CPU~\\cite{Henning2006} for C/C++, and Da Capo con Scala~\\cite{sewe2011capo}.\nOther executable benchmarks are aimed at specific tools and techniques, e.g., LAVA~\\cite{Dolan-Gavitt2016} and Magma~\\cite{hazimeh2020magma}, which are aimed at evaluating fuzzers,\nSecBenchJS~\\cite{icse2023-SecBenchJS}, which is aimed at evaluating techniques for detecting or mitigating vulnerabilities in JavaScript, and\nParsec, which offers multi-threaded C/C++ programs ~\\cite{bienia11benchmarking}.\nImportantly, none of the above benchmarks is for Python, which is the gap our work tries to fill.\n\n\\paragraph{Other benchmarks}\n\nMany other benchmark sets have proven valuable, of which we can list only a few here.\nTo evaluate and compare bug-related techniques, there are Defects4J~\\cite{Just2014}, Bugs.jar~\\cite{saha2018bugs}, and BugSwarm~\\cite{DBLP:journals/corr/abs-1903-06725} for Java, BugsJS for JavaScript~\\cite{gyimesi2019bugsjs}, and BugsInPy~\\cite{widyasari2020bugsinpy} for Python.\nMore specialized benchmarks include a set of JavaScript performance bugs~\\cite{icse2016-perf} and a collection of concurrency bugs~\\cite{yuan2021gobench}.\nTo evaluate deep learning models of code, benchmarks such as CodeXGLUE~\\cite{ lu2021codexglue} and HumanEval~\\cite{chen2021evaluating}, are widely used.\nWe envision \\name{} to help support future work on dynamic analysis for Python in a way similar to the above benchmarks.\n\n\n\\section{Conclusion}\n\nThis paper addresses a significant gap in the Python programming ecosystem by introducing \\name{}, a dynamic benchmark suite of executable Python projects.\nThe benchmark offers a unique combination of features by being large-scale, diverse, ready-to-run, and ready-to-analyze.\nWith 50 projects, 681k lines of code (of which 558k are executed), and 29,511 test cases, \\name{} offers a rich set of executions to analyze and study.\nWe illustrate the practicality and utility of \\name{} in three usage scenarios: comparing static and dynamic call graphs; creating training data for learning-guided execution; and mining specifications from execution traces.\nWe envision our work to provide a foundation for many other dynamic analyses and for studying the runtime behavior of Python software.\n\n\\section*{Data availability}\nOur DyPyBench image is available on Zenodo: \\url{https://zenodo.org/doi/10.5281/zenodo.10683759}.\nAlternatively, the image can also be pulled from DockerHub: \\url{https://hub.docker.com/r/islemdockerdev/dypybench}.\nProject page for updates and issues: \\url{https://github.com/sola-st/DyPyBench}\n\n\\section*{Acknowledgments}\nThis work was supported by the European Research Council (ERC, grant agreement 851895), and by the German Research Foundation within the ConcSys, DeMoCo, and QPTest projects.\n\n\\bibliographystyle{ACM-Reference-Format}\n\n\\end{document}\n\\endinput\n"}
{"paper_id": "2403-00540", "version": "2403-00540v1", "root_file_path": "d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\data_raw\\2403-00540\\tex\\2403-00540v1\\EpsilonGreedy.tex", "metadata": {"total_length": 39675, "merged_count": 1, "merged_files": ["EpsilonGreedy.tex"], "missing_files": []}, "content": "\\documentclass{article}\n\n\n\n\\usepackage{arxiv}\n\n\\usepackage[utf8]{inputenc} \n\\usepackage[T1]{fontenc}    \n\\usepackage{hyperref}       \n\\usepackage{url}            \n\\usepackage{booktabs}       \n\\usepackage{amsfonts}       \n\\usepackage{nicefrac}       \n\\usepackage{microtype}      \n\\usepackage{lipsum}\t\t\n\\usepackage{graphicx}\n\\usepackage[numbers,sort&compress]{natbib}\n\\usepackage{doi}\n\\usepackage{algorithm,algorithmicx,algpseudocode}\n\\usepackage{amsmath}\n\\usepackage{amssymb}\n\\usepackage{mathtools}\n\\usepackage{amsthm}\n\\usepackage[font=small,skip=4pt]{caption}\n\\usepackage[capitalize,noabbrev]{cleveref}\n\\hypersetup{\n    colorlinks,\n    linkcolor={red!50!black},\n    citecolor={blue!50!black},\n    urlcolor={blue!80!black}\n}\n\\title{Epsilon-Greedy Thompson Sampling to Bayesian Optimization}\n\n\\date{} \t\t\t\t\t\n\n\\author{{\\hspace{1mm}Bach Do}\\\\\n\tDepartment of Civil and Environmental Engineering\\\\\n\tUniversity of Houston\\\\\n\tHouston, TX 77004 \\\\\n\t\\texttt{bdo3@uh.edu} \\\\\n\t\\And\n {\\hspace{1mm}Ruda Zhang} \\\\\n\tDepartment of Civil and Environmental Engineering\\\\\n\tUniversity of Houston\\\\\n\tHouston, TX 77004 \\\\\n\t\\texttt{rudaz@uh.edu} \\\\\n}\n\n\n\\renewcommand{\\headeright}{ }\n\\renewcommand{\\shorttitle}{$\\varepsilon$-greedy Thompson Sampling}\n\n\n\\begin{document}\n\\hypersetup{allcolors = black}\n\\maketitle\n\n\\begin{abstract}\n    Thompson sampling (TS) serves as a solution for addressing the exploitation-exploration dilemma in Bayesian optimization (BO).\n    While it prioritizes exploration by randomly generating and maximizing sample paths of Gaussian process (GP) posteriors, TS weakly manages its exploitation by gathering information about the true objective function after each exploration is performed.\n    In this study, we incorporate the epsilon-greedy ($\\varepsilon$-greedy) policy, a well-established selection strategy in reinforcement learning, into TS to improve its exploitation.\n    We first delineate two extremes of TS applied for BO, namely the generic TS and a sample-average TS.\n    The former and latter promote exploration and exploitation, respectively.\n    We then use $\\varepsilon$-greedy policy to randomly switch between the two extremes. \n    A small value of $\\varepsilon \\in (0,1)$ prioritizes exploitation, and vice versa.\n    We empirically show that $\\varepsilon$-greedy TS with an appropriate $\\varepsilon$ is better than one of its two extremes and competes with the other.\n\\end{abstract}\n\n\n\\keywords{Thompson sampling \\and Epsilon-greedy policy \\and Bayesian optimization }\n\n\n\\section{Introduction}\n\\label{intro}\nWe consider the following minimization problem:\n\\begin{equation}\\label{eqn1}\n\t\\begin{aligned}\n\t\t\\underset{\\bf x}{\\min} \\ \\ & f({\\bf x})\\\\\n\t\t\\textrm{s.t.} \\ \\ \n\t\t& \\bf x \\in \\mathcal{X}, \n\t\\end{aligned}\n\\end{equation} \nwhere ${\\bf x} \\in \\mathbb{R}^d$ is the vector of $d$ input variables selected in a bounded, compact domain $\\mathcal{X}$, and $f:\\mathbb{R}^d \\mapsto \\mathbb{R}$ is a real-valued objective function.\nIn science and engineering, $f({\\bf x})$ is often costly to compute which unfortunately hinders the use of any standard derivative-based optimizers.\n\nBayesian optimization (BO) is a global optimization technique that relies on a probabilistic model of the objective function, coupled with an optimization policy to guide the optimization process \\cite{Snoek2012,Frazier2018,Garnett2023,Do2023mfbo}.\nBO demonstrates its remarkable performance when optimizing small- to moderate-dimensional optimization problems with costly or black-box objective functions.\nGiven a dataset consisting of several observations of the input variables and the objective function, a Gaussian process (GP) posterior built from this dataset often serves as the probabilistic model representing our beliefs about the objective function.\nMeanwhile, the optimization policy specifying what we value in the dataset is defined through an acquisition function. This acquisition function is cost-effective to evaluate given the GP posterior, making it convenient for processing optimization.\nDifferent considerations to derive the optimization policy include (1) the value of the objective function \\cite{Hennig2022}, (2) the information about the minimum location \\cite{Villemonteix2009,Hennig2012,HernandezLobato2014}, and (3) the information about the minimum value \\cite{WangZ2017}.   \nSeveral notable acquisition functions, including expected improvement (EI) \\cite{Jones1998}, weighted EI \\cite{Sobester2005}, GP upper confidence bound (GP-UCB) \\cite{Srinivas2010}, and knowledge gradient \\cite{Frazier2008}, are developed to balance exploitation and exploration.\n\nThompson sampling (TS) is a stochastic policy to address the exploitation-exploration dilemma in multi-armed bandit problems \\cite{Chapelle2011,Russo2018}.\nIn each iteration, TS selects an arm from a set of finite arms.\nEach arm corresponds to a stochastic reward from an unknown distribution.\nThe goal is to craft a sequence of arms that maximizes the cumulative reward under assumption that the rewards are independent of time and conditionally independent given the selected arms.\n\nWhen applied to BO, (the generic) TS generates a sequence of input variable points through a mechanism that involves random sampling from an unknown posterior distribution of the global minimum location \\cite{Kandasamy2018}.\nThus, we can consider TS a BO method of a random acquisition function.\nSeveral information-theoretic optimization policies also compute their acquisition functions based on a set of samples generated by TS \\cite{HernandezLobato2014,WangZ2017}. \nWhen the GP posterior exhibits high uncertainty, TS tends to produce diverse input variable points during the optimization process, thereby prioritizing exploration.\nAs the number of observations increases, it transitions to exploiting the knowledge gained about the true objective function.\nSuch an exploitation strategy, however, is inferior due to the randomness of TS. \nThis motivates the quest for an intervention to improve the exploitation of TS. \n\nIn this work, we incorporate $\\varepsilon$-greedy policy into TS to improve its exploitation.\n$\\varepsilon$-greedy policy is a selection strategy in reinforcement learning to address the exploitation-exploration dilemma \\cite{Sutton2018}.\nGiven $\\varepsilon \\in (0,1)$, the policy chooses an action by either maximizing the average reward function with probability $1-\\varepsilon$ or selecting it randomly with probability $\\varepsilon$.\nThe selection strategy is pure exploitation (i.e., greedy) when $\\varepsilon = 0$, or pure exploration when $\\varepsilon = 1$.  \nOur approach performs the generic TS (with probability $\\varepsilon$) for exploration and a new fashion of TS called sample-average TS (with probability $1-\\varepsilon$) for exploitation.\n\nSeveral works have introduced $\\varepsilon$-greedy policy to BO and multi-armed bandit problems. \n\\citet{DeAth2021} proposed two schemes for applying the policy to BO. \nThe first scheme performs exploration (with probability $\\varepsilon$) by randomly selecting a point on the Pareto frontier that is obtained by minimizing the posterior mean and maximizing the posterior standard deviation simultaneously.\nThe second scheme performs exploration by randomly selecting a point in the input variable space.\nBoth schemes implement exploitation (with probability $1-\\varepsilon$) by minimizing the posterior mean function.\n\\citet{Jin2023} introduced the so-called $\\varepsilon$-exploring TS to multi-armed bandit problems.\nGiven the posterior distributions of aims, exploration (with probability $\\varepsilon$) sets the estimated reward function for each aim as a random sample drawn from the associated posterior distribution, while exploitation (with probability $1-\\varepsilon$) sets the estimated reward function as the sample mean function. \nHowever, the performance of $\\varepsilon$-greedy TS for BO is still unexplored.\n\nOur contributions are as follows: (1) $\\varepsilon$-greedy TS to BO, which is a simple, computation-efficient method to improve the exploitation of TS; and (2) empirical evaluations demonstrating that $\\varepsilon$-greedy TS with appropriate $\\varepsilon$ is better than one of its two extremes (i.e., the generic TS and the sample-average TS) and competes with the other.\n\n\\cref{bgr} provides a general background needed to develop $\\varepsilon$-greedy TS. \\cref{method} describes the method.\nThe experimental evaluation and discussion are given in  \\cref{experiments}.\n\n\\section{Background}\n\\label{bgr}\n\n\\subsection{Gaussian processes}\n\\label{bgr-GP}\n\nConsider a training dataset $\\mathcal{D}=\\left\\{ {\\bf X},{\\bf Y}\\right\\}=\\{{\\bf x}^k,y^k\\}_{k=1}^N$, where $\\textbf{x}^k\\in\\mathbb{R}^d$ are observations of $d$-dimensional vectors of input variables, and $y^k\\in\\mathbb{R}$ denote the corresponding observations of the objective function.\nWe wish to build from $\\mathcal{D}$ a mapping $y({\\bf x})=f({\\bf x}) + \\varepsilon_\\text{n}: \\mathbb{R}^d \\mapsto \\mathbb{R}$, where $f({\\bf x})$ is a regression function and $\\varepsilon_\\text{n} \\sim \\mathcal{N}(0,\\sigma^2_\\text{n})$  is an additive zero-mean Gaussian noise with variance $\\sigma^2_\\text{n}$.\nThis observation noise is assumed to be independent and identically distributed.\n\nA GP assumes that any finite subset of an infinite set of regression function values has a joint Gaussian distribution \\cite{Rasmussen2006}.\nThis is encoded in the following GP prior:\n\\begin{equation}\\label{eqn2}\n\tf(\\cdot) \\sim \\mathcal{GP} \\left(0,\\kappa(\\cdot,\\cdot|\\boldsymbol \\theta_{\\text{x}})\\right),\n\\end{equation}\nwhere $\\kappa({\\bf x},{\\bf x}'|\\boldsymbol \\theta_{\\text{x}}) = \\text{cov}[f({\\bf x}),f({\\bf x}')]: \\mathbb{R}^d \\times \\mathbb{R}^d \\mapsto \\mathbb{R} $ is a positive semi-definite covariance function parameterized by a vector $\\boldsymbol \\theta_{\\text{x}}$ of hyperparameters.\n\nBy conditioning such prior knowledge on $\\mathcal{D}$ and utilizing the observation noise assumption, we can show that the vector of observations $\\{y({\\bf x}^1),\\dots,y({\\bf x}^N)\\}$ is distributed according to an $N$-variate Gaussian with zero mean and covariance matrix ${\\bf A} = {\\bf K} + \\sigma^2_\\text{n} {\\bf I}_N$, where the $(i,j)$th element of ${\\bf K}$ is $\\kappa({\\bf x}^i,{\\bf x}^j|\\boldsymbol \\theta_{\\text{x}})$ and ${\\bf I}_N$ denotes an $N$-by-$N$ identity matrix.\nBy further applying the conditional multivariate Gaussian \\cite{Bishop2006}, we obtain the posterior predictive distribution at an unseen input variable vector ${\\bf x}^\\star$, i.e., $p\\left(f({\\bf x}^\\star)|{\\bf x}^\\star,\\mathcal{D}\\right) = \\mathcal{N}\\left(\\mu_\\text{f}({\\bf x}^\\star),\\sigma_\\text{f}^2({\\bf x}^\\star)\\right)$, which contains the information about the mapping we wish to build. The predictive mean and variance are \n\\begin{equation}\\label{eqn3}\n\t\\mu_\\text{f}({\\bf x}^\\star)= {\\bf K^\\star}^\\intercal {\\bf A}^{-1} {\\bf Y},\n\\end{equation}\n\\begin{equation}\\label{eqn4}\n\t\\sigma_\\text{f}^2({\\bf x}^\\star)=\\kappa({\\bf x}^\\star,{\\bf x}^\\star|\\boldsymbol \\theta_{\\text{x}})\n\t-{\\bf K^\\star}^\\intercal {\\bf A}^{-1} {\\bf K^\\star},\n\\end{equation}\nwhere\n\\begin{equation}\\label{eqn5}\n\t{\\bf K^\\star} = \\left[\\kappa({\\bf x}^\\star,{\\bf x}^1|\\boldsymbol \\theta_{\\text{x}}),\\dots,\\kappa({\\bf x}^\\star,{\\bf x}^N|\\boldsymbol \\theta_{\\text{x}})\\right]^\\intercal.\n\\end{equation}\n\n\\begin{algorithm}[t]\n\t\\caption{Generic Thompson sampling for Bayesian optimization.}\n\t\\label{alg:TS}\n\t\\begin{algorithmic}[1]\n\t\t\n\t\t\\State \\textbf{input:} input variable domain $\\mathcal{X}$, number of initial observations $N$, threshold for number of BO iterations $K$\n\t\t\n\t\t\\State Generate $N$ samples of ${\\bf x}$ using Latin hypercube sampling\n\t\t\n\t\t\\For {$i=1:N$} \n\t\t\\State $y^i \\gets f({\\bf x}^i) + \\varepsilon_\\text{n}^i$ \n\t\t\\EndFor\n\t\t\n\t\t\\State $\\mathcal{D}^0 \\gets \\{{\\bf x}^i,y^i\\}_{i=1}^N$\n\t\t\n\t\t\\State $\\{{\\bf x}_{\\min},y_{\\min}\\} \\gets \\min\\{y^i,\\, i=1,\\dots, N\\}$\n\t\t\n\t\t\\For {$k=1:K$} \n\t\t\\State Build a GP posterior $\\hat{f}^k({\\bf x})|\\mathcal{D}^{k-1}$\n\t\t\\State Generate a sample path $g({\\bf x}|\\mathcal{D}^{k-1})$ from $\\hat{f}^k({\\bf x})|\\mathcal{D}^{k-1}$ \\label{alg1_10}\n\t\t\\State ${\\bf x}^{k} \\gets \\underset{{\\bf x}}{\\mathrm{arg\\,min}} \\ \\ g({\\bf x}|\\mathcal{D}^{k-1})$ s.t. ${\\bf x} \\in \\mathcal{X}$; ${\\bf x} \\notin \\mathcal{D}^{k-1}$\n\t\t\\State $y^{k} \\gets f({\\bf x}^{k}) + \\varepsilon_\\text{n}^k$\n\t\t\\State $\\mathcal{D}^k\\gets\\mathcal{D}^{k-1} \\cup \\{{\\bf x}^{k},y^{k}\\}$\n\t\t\\State $\\{{\\bf x}_{\\min},y_{\\min}\\} \\gets \\min\\{y_{\\min},y^{k}\\}$\n\t\t\\EndFor\n\t\t\n\t\t\\State \\textbf{return} $\\{{\\bf x}_{\\min},y_{\\min}\\}$\n\t\\end{algorithmic}\n\\end{algorithm}\n\n\\subsection{Bayesian optimization via Thompson sampling}\n\\label{bgr-TS}\n\nAs described in \\cref{intro}, the generic TS generates a sequence of input variable points ${\\bf x}^k$ $(k=1,\\dots,K)$ using a mechanism that involves random sampling from an unknown posterior distribution $p({\\bf x}^\\star|\\mathcal{D}_{k-1})$ of the global minimum ${\\bf x}^\\star$.\nHere $K$ represents a finite budget on the number of BO iterations.\nLeveraging the fact that ${\\bf x}^\\star$ is fully determined by the objective function, the generic TS follows two simple steps in each iteration.\nFirst, it draws a sample path from the GP posterior $\\hat{f}^k({\\bf x})|\\mathcal{D}^{k-1}$ for which the detailed implementation is given in \\cref{bgr-sampling}.\nIt then minimizes the obtained sample path to find a sample ${\\bf x}^\\star$ that is assigned as the new input point ${\\bf x}^k$.\n\\cref{alg:TS} summarizes the implementation of the generic TS.\n\n\\cref{Fig1} shows several sample paths generated from the GP posterior of function $f(x)=x\\sin{x}$ and an approximate distribution of their minimum locations.\nMore specifically, a GP model with zero mean and squared\nexponential covariance function is first built from ten observations.\nA total of 50 sample paths are then drawn, and five of them are shown in \\cref{Fig1}(a). \nMinimizing the generated sample paths and using a kernel density estimation for the obtained solutions result in an approximate distribution of $p({\\bf x}^\\star|\\mathcal{D})$ as shown in \\cref{Fig1}(b).\nNote that the generic TS in \\cref{alg:TS} generates only one sample path in each iteration and does not attempt to approximate the posterior distribution of the minimum location.\n\n\\begin{figure*}[t!]\n\t\\centering\n\t\\centering{\\includegraphics[scale=0.8]{Fig1.pdf}}\n\t\\caption{Sample paths from the GP posterior for $f(x) = x\\sin(x)$ and distribution of their minimums. (a) GP predictions and five sample paths drawn from the GP posterior; and (b) Approximate distribution of $p( x^\\star|\\mathcal{D})$ obtained from minimizing 50 sample paths.}\n\t\\label{Fig1}\n\\end{figure*}\n\n\\subsection{Sampling from Gaussian process posteriors}\n\\label{bgr-sampling}\n\nGiven the GP posterior $\\hat{f}^k({\\bf x})|\\mathcal{D}^{k-1}$ characterized by a stationary covariance function, we follow the spectral sampling approach by \\citet{HernandezLobato2014} to generate a sample path $g({\\bf x}|\\mathcal{D}^{k-1})$ in Line~10 of \\cref{alg:TS}.\nThis approach approximates the GP prior in \\cref{eqn2} using a Bayesian\nlinear model of randomized feature maps.\nSuch an approximation is rooted in Bochner’s theorem that guarantees the existence of a Fourier dual $p({\\bf s})$ (${\\bf s} \\in \\mathbb{R}^d$) of the stationary covariance function, which is called spectral density when a finite non-negative Borel measure is interpreted as a distribution \\cite{Wendland2004}.\nThe spectral density functions corresponding to several isotropic covariance functions of Matérn class can be found in \\citet{RiutortMayol2022}.\nOnce $p({\\bf s})$ is determined, we can represent $\\kappa({\\bf x},{\\bf x}'|\\boldsymbol \\theta_{\\text{x}})$ by the following randomized feature map \\cite{Rahimi2007}:\n\\begin{equation}\\label{eqn6}\n\t\\kappa({\\bf x},{\\bf x}'|\\boldsymbol \\theta_{\\text{x}}) = \\boldsymbol{\\phi}({\\bf x})^\\intercal \\boldsymbol{\\phi}({\\bf x}').\n\\end{equation}\nThe the feature map $\\boldsymbol{\\phi}({\\bf x})$ can be approximated by \\cite{HernandezLobato2014}\n\\begin{equation}\\label{eqn7}\n\t\\boldsymbol{\\phi}({\\bf x}) = \\sqrt{2 \\kappa(0|\\boldsymbol \\theta_{\\text{x}})/N_\\text{p}} \\cos \\left({\\bf W} {\\bf x}^\\intercal + {\\bf b}\\right),\n\\end{equation}\nwhere ${\\bf W}$ and ${\\bf b}$ stack $N_\\text{p}$ spectral points generated from $p({\\bf s})$ and $N_\\text{p}$ points drawn from the uniform distribution $\\mathcal{U}[0,2\\pi]$, respectively; and $\\kappa(0|\\boldsymbol \\theta_{\\text{x}})$ is well defined because $\\kappa({\\bf x},{\\bf x}'|\\boldsymbol \\theta_{\\text{x}})$ is a stationary covariance function, which reads $\\kappa({\\bf x},{\\bf x}'|\\boldsymbol \\theta_{\\text{x}}) = \\kappa(\\left\\|{\\bf x}- {\\bf x}' \\right\\||\\boldsymbol \\theta_{\\text{x}})$.\n\nOnce the approximate feature map is formulated, the GP prior as a kernel machine can be approximated by the following Bayesian linear model: \n\\begin{equation}\\label{eqn8}\n\tf({\\bf x}) \\approx \\boldsymbol{\\beta}^\\intercal \\boldsymbol{\\phi}({\\bf x}),\n\\end{equation}\nwhere the weight vector $\\boldsymbol{\\beta}$ is a multivariate Gaussian.\nBy conditioning \\cref{eqn8} on the data, we obtain the mean and covariance matrix of conditional $\\boldsymbol{\\beta}$, as\n\\begin{subequations}\\label{eqn9}\n\t\\begin{align}\n\t\t\\boldsymbol{\\mu}_{\\beta} & = \\left(\\boldsymbol{\\Phi}^\\intercal \\boldsymbol{\\Phi} +\\sigma_\\text{n}^2 {\\bf I}_N\\right)^{-1} \\boldsymbol{\\Phi}^\\intercal {\\bf y},\\\\\n\t\t\\boldsymbol{\\Sigma}_{\\beta} & = \\left(\\boldsymbol{\\Phi}^\\intercal \\boldsymbol{\\Phi} +\\sigma_\\text{n}^2 {\\bf I}_N\\right)^{-1} \\sigma_\\text{n}^2,\n\t\\end{align}\n\\end{subequations}\nwhere $\\boldsymbol{\\Phi} = [\\boldsymbol{\\phi}({\\bf x}),\\dots,\\boldsymbol{\\phi}({\\bf x}^N)] \\in \\mathbb{R}^{N_\\text{p} \\times N}$.\n\nIn summary, the generation of $g({\\bf x}|\\mathcal{D}^{k-1})$ using random features is detailed in \\cref{alg:spectral}.\nNote that Line~10 of \\cref{alg:spectral} involves the Cholesky decomposition of matrix $\\left(\\boldsymbol{\\Phi}^\\intercal \\boldsymbol{\\Phi} +\\sigma_\\text{n}^2 {\\bf I}_N\\right)$ to facilitate the calculation of its inverse. \nTo increase the accuracy of sample paths, we may use alternative posterior sampling techniques such as the decoupled sampling method \\cite{Wilson2020}.\n\n\\begin{algorithm}[t]\n\t\\caption{Generation of sample paths using random features.}\n\t\\label{alg:spectral}\n\t\\begin{algorithmic}[1]\n\t\t\\State \\textbf{Input:} dataset $\\mathcal{D}_{k-1}$, type of stationary covariance function $\\kappa(\\cdot,\\cdot|\\boldsymbol{\\theta}_{\\bf x})$, number of spectral points $N_\\text{p}$\n\t\t\n\t\t\\State Build a GP model from $\\mathcal{D}_{k-1}$\n\t\t\\State Derive $p(\\bf s)$ from the posterior covariance function $\\kappa(\\cdot,\\cdot|\\boldsymbol{\\theta}_{\\bf x})$\n\t\t\n\t\t\\For {$i=1:N_\\text{p}$} \n\t\t\\State $[{\\bf W}]_i \\sim p(\\bf s)$\n\t\t\\State $[{\\bf b}]_i \\sim \\mathcal{U}[0,2\\pi]$\n\t\t\\EndFor\n\t\t\n\t\t\\State Formulate $\\boldsymbol{\\phi}({\\bf x})$; see \\cref{eqn7}\n\t\t\n\t\t\\State $\\boldsymbol{\\Phi} \\gets [\\boldsymbol{\\phi}({\\bf x}),\\dots,\\boldsymbol{\\phi}({\\bf x}^N)]$\n\t\t\n\t\t\\State Compute $\\boldsymbol{\\mu}_{\\beta}$ and $\\boldsymbol{\\Sigma}_{\\beta}$; see \\cref{eqn9} \\label{alg2_10}\n\t\t\n\t\t\\State $\\boldsymbol{\\beta} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{\\beta}, \\boldsymbol{\\Sigma}_{\\beta})$\n\t\t\n\t\t\\State \\textbf{return} $g({\\bf x})|\\mathcal{D}^{k-1} \\gets \\boldsymbol{\\beta}^\\intercal \\boldsymbol{\\phi}({\\bf x})$\n\t\\end{algorithmic}\n\\end{algorithm}\n\n\\section{Epsilon-greedy Thompson sampling to Bayesian optimization}\n\\label{method}\n\n\\subsection{Thompson sampling via sample-average method}\n\\label{method-avaraging}\n\nIn addition to the generic TS, we perform another form of TS for BO via the sample-average method \\cite{Balandat2020}.\nThis technique, referred to as sample-average TS (or simply averaging TS), is to enhance the exploitation of TS.\nMore specifically, we call \\cref{alg:spectral} to generate in Line~10 of \\cref{alg:TS} a total of $N_\\text{s}$ sample paths $h^s({\\bf x}|\\mathcal{D}^{k-1})$ ($s=1,\\dots,N_\\text{s}$). These sample paths are then used to define the following average sample path:\n\\begin{equation}\\label{eqn10}\n\tg({\\bf x}|\\mathcal{D}^{k-1}) = \\frac{1}{N_\\text{s}}\\sum_{s=1}^{N_\\text{s}} h^s({\\bf x}|\\mathcal{D}^{k-1}).\n\\end{equation}\nFinally, we minimize $g({\\bf x}|\\mathcal{D}^{k-1})$ to find the new input variable point.\nThis approach differs from that of \\citet{Balandat2020} which generates the sample paths using randomized quasi Monte-Carlo techniques.\n\nWe see that if $N_\\text{s} = \\infty$, $g({\\bf x}|\\mathcal{D}^{k-1})$ in \\cref{eqn10} is indeed the GP posterior mean in \\cref{eqn3} whose minimum tends to promote exploitation.\nIf $N_s = 1$, the sample-average TS recovers the generic TS that favors exploration. \nBetween these extremes, therefore, is an unknown value of $N_s$ at which TS balances exploitation and exploration.\nHowever, we do not attempt to find such a value in this work.\nWe instead set $N_\\text{s}$ at a sufficiently large value, say $N_\\text{s}=50$.\n\\begin{algorithm}[tb]\n\t\\caption{$\\varepsilon$-greedy Thompson sampling for Bayesian optimization.}\n\t\\label{alg:epsilon_greedyTS}\n\t\\begin{algorithmic}[1]\n\t\t\n\t\t\\State \\textbf{input:} input variable domain $\\mathcal{X}$, number of initial observations $N$, threshold for number of BO iterations $K$, number of spectral points $N_\\text{p}$, number of sample paths $N_\\text{s}$, value of $\\varepsilon \\in (0,1)$\n\t\t\n\t\t\\State Generate $N$ samples of ${\\bf x}$\n\t\t\n\t\t\\For {$i=1:N$} \n\t\t\\State $y^i \\gets f({\\bf x}^i) + \\varepsilon_\\text{n}^i$ \n\t\t\\EndFor\n\t\t\n\t\t\\State $\\mathcal{D}^0 \\gets \\{{\\bf x}^i,y^i\\}_{i=1}^N$\n\t\t\n\t\t\\State $\\{{\\bf x}_{\\min},y_{\\min}\\} \\gets \\min\\{y^i,\\, i=1,\\dots, N\\}$\n\t\t\n\t\t\\For {$k=1:K$} \n\t\t\\State Build a GP posterior $\\hat{f}^k({\\bf x})|\\mathcal{D}^{k-1}$\n\t\t\\State Generate $r \\sim \\mathcal{U}[0,1]$\n\t\t\\If {$r \\leq \\varepsilon$}\n\t\t\\State Generate $g({\\bf x}|\\mathcal{D}^{k-1})$ from $\\hat{f}^k({\\bf x})|\\mathcal{D}^{k-1}$\n\t\t\\Else\n\t\t\\For {$s=1:N_\\text{s}$} \n\t\t\\State Generate $h^s({\\bf x}|\\mathcal{D}^{k-1})$ from $\\hat{f}^k({\\bf x})|\\mathcal{D}^{k-1}$\n\t\t\\EndFor\n\t\t\\State $g({\\bf x}|\\mathcal{D}^{k-1}) \\gets \\frac{1}{N_\\text{s}}\\sum_{s=1}^{N_\\text{s}} h^s({\\bf x}|\\mathcal{D}^{k-1})$\n\t\t\\EndIf\n\t\t\\State ${\\bf x}^{k} \\gets \\underset{{\\bf x}}{\\mathrm{arg\\,min}} \\ \\ g({\\bf x}|\\mathcal{D}^{k-1})$ s.t. ${\\bf x} \\in \\mathcal{X}$; ${\\bf x} \\notin \\mathcal{D}^{k-1}$\n\t\t\\State $y^{k} \\gets f({\\bf x}^{k}) + \\varepsilon_\\text{n}^k$\n\t\t\\State $\\mathcal{D}^k\\gets\\mathcal{D}^{k-1} \\cup \\{{\\bf x}^{k},y^{k}\\}$\n\t\t\\State $\\{{\\bf x}_{\\min},y_{\\min}\\} \\gets \\min\\{y_{\\min},y^{k}\\}$\n\t\t\\EndFor\n\t\t\n\t\t\\State \\textbf{return} $\\{{\\bf x}_{\\min},y_{\\min}\\}$\n\t\\end{algorithmic}\n\\end{algorithm}\n \n\\begin{figure*}[ht]\n\t\\centering\n\t\\centering{\\includegraphics[scale=0.8]{Fig2.pdf}}\n\t\\caption{Performance of EI, LCB, averaging TS, generic TS, and $\\varepsilon$-greedy TS methods for 2d Ackley and 2d Rosenbrock functions. Medians and interquartile ranges from 15 runs of each method for (a) 2d Ackley function and (b) 2d Rosenbrock function; Medians, and the 25th and 75th percentiles from 15 best-found observations at iteration 50 for (c) 2d Ackley function and (d) 2d Rosenbrock function.}\n\t\\label{Fig2}\n\\end{figure*}\n \n\\subsection{$\\varepsilon$-greedy Thompson sampling}\n\\label{method-epsgreedyTS}\n\nWe see that the generic TS and the averaging TS for a sufficiently large number of sample paths represent two extremes of TS: the former and latter address exploration and exploitation, respectively. \nThis distinction motivates our approach that uses $\\varepsilon$-greedy policy to randomly switch between these extremes.\nAccordingly, we implement the generic TS with probability $\\varepsilon$ to explore the input variable space.\nWe implement the averaging TS with probability $1-\\varepsilon$ to guide the search toward exploitation.\nWe do not use the posterior mean in \\cref{eqn3} for exploitation because we observe that computing its derivatives is more expensive than computing the derivatives of the average sample path in \\cref{eqn10}.\nMoreover, it is straightforward to recover the generic TS by simply setting $\\varepsilon=1$ or $N_\\text{s}=1$.\n\\cref{alg:epsilon_greedyTS} details the $\\varepsilon$-greedy TS when applied to BO.\n\nOur approach addresses the exploitation-exploration dilemma by the following factors: (1) $\\varepsilon$ \\textendash a small $\\varepsilon$ promotes exploitation, and vice versa; (2) $N_\\text{s}$ \\textendash a sufficiently large $N_\\text{s}$ encourages exploitation; and (3) the accuracy of GP model \\textendash an accurate GP model can induce significant exploitation.\n\n\\begin{figure*}[ht]\n\t\\centering\n\t\\centering{\\includegraphics[scale=0.8]{Fig3.pdf}}\n\t\\caption{Performance of EI, LCB, averaging TS, generic TS, and $\\varepsilon$-greedy TS methods for 6d Hartmann and 10d Michalewicz functions. Medians and interquartile ranges from 15 runs of each method for (a) 6d Hartmann function and (b) 10d Michalewicz function; Medians, and the 25th and 75th percentiles from 15 best-found observations at iteration 100 for (c) 6d Hartmann function and (d) 10d Michalewicz function.}\n\t\\label{Fig3}\n\\end{figure*}\n\n\\begin{figure*}[ht]\n\t\\centering\n\t\\centering{\\includegraphics[scale=0.8]{Fig4.pdf}}\n\t\\caption{Performance $\\varepsilon$-greedy TS for different $\\varepsilon$ values and $N_\\text{s} = 50$. Medians and interquartile ranges from 15 runs of each method for (a) 2d Ackley function and (b) 2d Rosenbrock function; Medians, and the 25th and 75th percentiles from 15 best-found observations at iteration 50 for (c) 2d Ackley function and (d) 2d Rosenbrock function.}\n\t\\label{Fig4}\n\\end{figure*}\n\n\\begin{figure*}[!htb]\n\t\\centering\n\t\\centering{\\includegraphics[scale=0.8]{Fig5.pdf}}\n\t\\caption{Performance $\\varepsilon$-greedy TS for different $N_\\text{s}$ values and $\\varepsilon = 0.5$. Medians, and the 25th and 75th percentiles from 15 best-found observations at the last iteration for (a) 2d Ackley function, (b) Rosenbrock function, (c) 6d Hartmann function, and (d) 10d Michalewicz function.}\n\t\\label{Fig5}\n\\end{figure*}\n\n\\begin{figure*}[!htb]\n\t\\centering\n\t\\centering{\\includegraphics[scale=0.8]{Fig6.pdf}}\n\t\\caption{Approximate distributions of unit runtime for selecting the next input variable point with different $\\varepsilon$ values and $N_\\text{s} = 50$. (a) 2d Ackley function; and (b) 2d Rosenbrock function.}\n\t\\label{Fig6}\n\\end{figure*}\n\n\\begin{figure*}[!htb]\n\t\\centering\n\t\\centering{\\includegraphics[scale=0.8]{Fig7.pdf}}\n\t\\caption{Approximate distributions of unit runtime for selecting the next input variable point with different $N_\\text{s}$ values and $\\varepsilon = 0.5$. (a) 2d Ackley function; and (b) 2d Rosenbrock function.}\n\t\\label{Fig7}\n\\end{figure*}\n\n\\section{Experiments}\n\\label{experiments}\n\nWe test the empirical performance of $\\varepsilon$-greedy TS on challenging optimization problems of four benchmark functions: the 2d Akley function, the 2d Rosenbrock function, the 6d Hartmann function, and the 10d Michalewicz function.\nThe analytical expressions for these functions and their global minimums are given in \\cref{appe:functions}. \nIn our experiments, we use the squared exponential covariance function for building GP models.\nTo find optimal hyperparameters for each problem, we follow \\citet{Bradford2018} and use the DIRECT algorithm \\cite{Finkel2006} whose output is then set as the initial point of a derivative-based optimizer.\nWe carry out all experiments using a PC with an Intel Core i7-1165G7 2.80 GHz CPU and 8.0 GB memory.\nWe compare the optimization results from $\\varepsilon$-greedy TS with those from other BO methods, including EI, the lower confidence bound (LCB), the averaging TS, and the generic TS.\n\nWe randomly generate 15 initial datasets for each problem using Latin hypercube sampling \\cite{Forrester2008}.\nWe then start each BO method from each of these datasets.\nIn each optimization iteration, we record the best-found observation value of the observation error $\\log_{10}(y_{\\min}-f^\\star)$ and the corresponding input variable vector. Here $y_{\\min}$ and $f^\\star$ represent the best observation of the objective function found in each iteration and its true minimum value, respectively.\nFor the 2d Akley and 2d Rosenbrock functions, we set the number of initial observations and the number of BO iterations at $N=10d$ and $K=50$.\nFor the 6d Hartmann and 10d Michalewicz functions, we set $N=5d$ and $K=100$.\nIn all experiments, we sample $1000$ spectral points to formulate each sample path. \n\n\\subsection{Optimization results}\n\\label{experiments:otpresult}\n\n\\cref{Fig2} shows the medians and interquartile ranges from 15 runs of each BO method for the 2d Akley and 2d Rosenbrock functions.\n\\cref{Fig3} shows similar quantities for the 6d Hartmann and 10d Michalewicz functions.\nIn general, the optimization results from $\\varepsilon$-greedy TS for an appropriate $\\varepsilon$ are better than those from one of its two extremes (i.e., averaging and the generic TS) and competitive with the results from the other.\nHowever, differences in performances of these methods are minor for the 6d Hartmann and 10d Michalewicz functions.\nThis may be attributed to exploration arising from the inaccuracy of GP models in high-dimensional spaces.\nIn other words, deliberate exploitation of $\\varepsilon$-greedy TS is hindered by an increase in the number of input variables.\nWith a proper $\\varepsilon$ value, $\\varepsilon$-greedy TS can provide the best objective values among those from the considered TS methods.\nThe advantage of $\\varepsilon$-greedy TS over EI or LCB is problem-dependent.\n\n\\subsection{Effect of $\\varepsilon$ values}\n\\label{experiments:epseffect}\n\nWe investigate the effects of $\\varepsilon$ values on the performance of $\\varepsilon$-greedy TS.\nFor the 2d Akley and 2d Rosenbrock functions, we set $\\varepsilon \\in \\{0.1,0.3,0.5,0.7,0.9\\}$ and fix $N_\\text{s}$ at $50$.\nWe use $\\varepsilon \\in \\{0.1,0.5,0.9\\}$ and $N_\\text{s}=50$ for the 6d Hartmann and 10d Michalewicz functions.\n\nAs shown in Figures~\\ref{Fig3}(c) and \\ref{Fig4}(c), the performance of $\\varepsilon$-greedy TS for the 2d Akley and 6d Hartmann functions becomes worse when using a large value of $\\varepsilon$.\nThe method works well for a moderate value of $\\varepsilon$ when minimizing the 2d Rosenbrock and 10d Michalewicz functions; see Figures~\\ref{Fig3}(d) and \\ref{Fig4}(d).\nThe observed trends of optimization results when varying $\\varepsilon$ suggest that there exists an optimal $\\varepsilon$ that corresponds to the best performance of $\\varepsilon$-greedy TS for each problem. \nSuch an optimal value, for example, may be selected from $[0.1,0.5]$ for the 2d Akley function, or from  $[0.3,0.7]$ for the 2d Rosenbrock function; see Figures~\\ref{Fig4}(c) and (d).\n\nAs discussed in \\cref{intro}, increasing $\\varepsilon$ results in more exploration of an $\\varepsilon$-greedy strategy.\nFor $\\varepsilon$-greedy TS, increasing $\\varepsilon$ encourages the algorithm to explore more unseen regions of input variable space, which is confirmed in Figures~\\ref{Fig8} and \\ref{Fig9} of \\cref{appe:addresults}.\n\n\\subsection{On selection of optimal $\\varepsilon$}\n\\label{experiments:selectepseffect}\n\nWhile our optimization results indicate that it is safe to set $\\varepsilon=0.5$ to balance exploitation and exploration, an optimal $\\varepsilon$ strongly depends on the problem of interest.\nA problem that requires more exploration to find an optimal solution (e.g., the 2d Rosenbrock function) may benefit from a large $\\varepsilon$ value.\nMeanwhile, a problem requiring more exploitation (e.g., the 2d Akley function) might be better addressed with a smaller $\\varepsilon$.\nUnfortunately, it is unclear to determine whether a black-box objective function favors exploitation or exploration to locate its minimum.\nThis, ironically, motivates the use of $\\varepsilon$-greedy TS to prevent our search from wrong directions when prioritizing exploitation or exploration only.\nIt is also worth noting that any attempts to find an optimal value of $\\varepsilon$ should correspond to a specific accuracy level of initial GP models because lowering the fidelity of GP models always encourages exploration, irrespective of the selection of $\\varepsilon$.\n\n\\subsection{Effect of sample path numbers}\n\\label{experiments:epsNsffect}\n\nWe investigate how $N_\\text{s}$ values affect the optimization results by setting $N_\\text{s} \\in \\{20,50,100\\}$ for fixed $\\varepsilon = 0.5$.\n\\cref{Fig5} shows the medians, and the 25th and 75th percentiles of the best observations found at the last iteration for different $N_\\text{s}$ values.\nThe solutions observed exhibit less sensitivity to changes in $N_\\text{s}$ when it reaches a sufficiently large value.\nThis can be explained by an observation that increasing $N_\\text{s}$ while it is still sufficiently large has a modest impact on exploitation of $\\varepsilon$-greedy TS; see Figures~\\ref{Fig10} and \\ref{Fig11} of \\cref{appe:addresults}.\n\n\\subsection{Computational cost}\n\\label{experiments:cost}\n\nWe also investigate the effects of varying $\\varepsilon$ and $N_\\text{s}$ values on the runtime per dataset sample (i.e., unit runtime) for selecting the next input variable point.\n\\cref{Fig6} shows approximate distributions of unit runtime for selecting the next input variable point using the averaging TS with $N_\\text{s}=50$, $\\varepsilon$-greedy TS with different $\\varepsilon$ values and $N_\\text{s}=50$, and the generic TS for the 2d Ackley and 2d Rosenbrock functions.\n\\cref{Fig7} shows similar quantities when $\\varepsilon$-greedy TS uses different $N_\\text{s}$ values and fixed $\\varepsilon=0.5$.\nHere the generic TS corresponds to $N_\\text{s}=1$.\nWe see in \\cref{Fig6} that increasing $\\varepsilon$ results in a slight decrease in the unit runtime.\nThis observation can be attributed to the fact that a larger $\\varepsilon$ necessitates more exploration of calling the generic TS, which is generally cheaper than the averaging TS.\nIn addition, the unit runtimes associated with $\\varepsilon=0.5$ and $\\varepsilon=0.9$ are comparable with that of the generic TS. \n\\cref{Fig7} show marginal differences in empirical mode values of unit runtime for $N_\\text{s}=20$, $N_\\text{s}=50$, and the generic TS.\nAn excessively large value of $N_\\text{s}$, e.g., $N_\\text{s} = 100$, increases the unit runtime of $\\varepsilon$-greedy TS.\nHowever, it does not guarantee an improvement in the performance of $\\varepsilon$-greedy TS; see \\cref{Fig5}.\n\n\\section{Conclusion}\n\nWe introduced $\\varepsilon$-greedy TS to BO for optimizing costly objective functions.\nOur empirical findings reveal that the method for an appropriate value of $\\varepsilon$ outperforms one of its two extremes (i.e., the averaging TS and the generic TS) and competes with the other one.\nThe computational cost of our method for a proper $\\varepsilon$ and a sufficiently large number of sample paths (e.g., $N_\\text{s} = 20$ or $50$) is comparable to that of the generic TS.\n\nWhile several $\\varepsilon$-greedy algorithms and the generic TS are guaranteed to converge eventually \\cite {DeAth2021,Garnett2023}, we look forward to a theoretical analysis to elucidate the convergence properties of $\\varepsilon$-greedy TS.\nAdditionally, we are keen on exploring its extensions to high-dimensional settings, e.g., with support from a subspace-based approach \\cite{Nayebi2019} or a trust-region method \\cite{Eriksson2019}, and investigating its adaptability to varying $\\varepsilon$ during the optimization process.\n\n\\bibliographystyle{unsrtnat}\n \n\n\\newpage\n\\appendix\n\n\\section{Benchmark test functions.}\\label{appe:functions}\n\nThe analytical expressions for the test functions used in \\cref{experiments} are given below. The global solutions of these functions are detailed in \\cite{Surjanovic2013}.\n\n\\textbf{Ackley function:}\n\n\\begin{equation}\\label{eqn11}\n\tf({\\bf x}) = -a \\exp\\left(-b\\sqrt{\\frac{1}{d} \\sum_{i=1}^{d}x^2_i}\\right) - \\exp\\left(\\frac{1}{d} \\sum_{i=1}^{d}\\cos(cx_i)\\right) +a + \\exp(1),\n\\end{equation}\nwhere $d$ denotes the number of dimensions, $a = 20$, $b = 0.2$, and $c = 2\\pi$.\nThe function is evaluated on $\\mathcal{X}=[-5,5]^d$ and has a global minimum at ${\\bf x}^\\star = {\\bf 0}$ with $f^\\star = f({\\bf x}^\\star) = 0$.\n\n\\textbf{Rosenbrock function:}\n\n\\begin{equation}\\label{eqn12}\n\tf({\\bf x}) = \\sum_{i=1}^{d-1} \\left[100(x_{i+1}-x_i^2)^2 + (x_i - 1)^2\\right].\n\\end{equation}\nThe function is evaluated on $\\mathcal{X}=[-5,10]^d$ and has a global minimum at ${\\bf x}^\\star = [1,\\dots,1]^\\intercal$ with $f^\\star = f({\\bf x}^\\star) = 0$.\n\n\\textbf{6d Hartmann function:}\n\\begin{equation}\\label{eqn13}\n\tf({\\bf x}) = -\\sum_{i=1}^{4} a_i \\exp \\left( -\\sum_{j=1}^{6} A_{ij} (x_j-P_{ij})^2 \\right),\n\\end{equation}\nwhere \n\\begin{subequations}\\label{eqn14}\n\t\\begin{align}\n\t\t{\\bf a} & = [1,1.2,3,3.2]^\\intercal,\\\\\n\t\t{\\bf A} & = \\begin{bmatrix}\n\t\t\t10 & 3 & 17 & 3.5 & 1.7 & 8 \\\\\n\t\t\t0.05 & 10 & 17 & 0.1 & 8 & 14 \\\\\n\t\t\t3 & 3.5 & 1.7 & 10 & 17 & 8 \\\\\n\t\t\t17 & 8 & 0.05 & 10 & 0.1 & 14\n\t\t\\end{bmatrix},\\\\\n\t\t{\\bf P} & = 10^{-4}\\begin{bmatrix}\n\t\t\t1312 &1696 &5569 &124 &8283 &5886\\\\\n\t\t\t2329 &4135 &8307 &3736 &1004 &9991\\\\\n\t\t\t2348 &1451 &3522 &2883 &3047 &6650\\\\\n\t\t\t4047 &8828 &8732 &5743 &1091 &381\n\t\t\\end{bmatrix}.\n\t\\end{align}\n\\end{subequations}\nThe input domain of the function is $\\mathcal{X}=[0,1]^d$.\nThe function has a global minimum at ${\\bf x}^\\star = [0.20169,0.150011,0.476874,0.275332,0.311625,0.6573]^\\intercal$ with $f^\\star = f({\\bf x}^\\star) = -3.32237$.\n\n\n\\textbf{Michalewicz function:}\n\n\\begin{equation}\\label{eqn15}\n\tf({\\bf x}) = -\\sum_{i=1}^{d} \\sin(x_i) \\sin^{2m} \\left(\\frac{i x_i^2}{\\pi}\\right),\n\\end{equation}\nwhere $m = 10$.\nThe function is evaluated on $\\mathcal{X}=[0,\\pi]^d$ and has a global minimum value $f^\\star = -9.66015$ for $d=10$.\n\n\\section{Additional results.}\\label{appe:addresults}\n\nThis section provides some additional results to show how $\\varepsilon$ and $N_\\text{s}$ values affect the exploration and exploitation of $\\varepsilon$-greedy TS for the 2D Ackley and 2D Rosenbrock functions.\nFigures~\\ref{Fig8} and \\ref{Fig9} show that increasing the value of $\\varepsilon$ significantly enhances the exploration of input variable space.\nThe reduction in $N_\\text{s}$ exhibits a modest effect on exploration when it remains sufficiently large; see Figures~\\ref{Fig10} and \\ref{Fig11}.\n\n\\begin{figure*}[h]\n\t\\centering\n\t\\centering{\\includegraphics[scale=0.75]{Fig8.pdf}}\n\t\\caption{Initial and added points for 2D Ackley function with $N_\\text{s} = 50$ and different values of $\\varepsilon$. (a) Averaging ($\\varepsilon = 0$); (b) $\\varepsilon = 0.1$; (c) $\\varepsilon = 0.3$; (d) $\\varepsilon = 0.7$; (e) $\\varepsilon = 0.9$; and (f) $\\varepsilon = 1$ (TS).}\n\t\\label{Fig8}\n\\end{figure*}\n\n\\begin{figure*}[h]\n\t\\centering\n\t\\centering{\\includegraphics[scale=0.75]{Fig9.pdf}}\n\t\\caption{Initial and added points for 2D Rosenbrock function with $N_\\text{s} = 50$ and different values of $\\varepsilon$. (a) Averaging ($\\varepsilon = 0$); (b) $\\varepsilon = 0.1$; (c) $\\varepsilon = 0.3$; (d) $\\varepsilon = 0.7$; (e) $\\varepsilon = 0.9$; and  (f) $\\varepsilon = 1$ (TS).}\n\t\\label{Fig9}\n\\end{figure*}\n\n\\begin{figure*}[ht]\n\t\\centering\n\t\\centering{\\includegraphics[scale=0.75]{Fig10.pdf}}\n\t\\caption{Initial and added input variable points for 2D Ackley function with $\\varepsilon = 0.5$ and different values of $N_\\text{s}$. (a) $N_\\text{s} = 100$; (b) $N_\\text{s} = 50$; (c) $N_\\text{s} = 20$; and (d) $N_\\text{s} = 1$ (TS).}\n\t\\label{Fig10}\n\\end{figure*}\n\n\\begin{figure*}[ht]\n\t\\centering\n\t\\centering{\\includegraphics[scale=0.75]{Fig11.pdf}}\n\t\\caption{Initial and added input variable points for 2D Rosenbrock function with $\\varepsilon = 0.5$ and different values of $N_\\text{s}$. (a) $N_\\text{s} = 100$; (b) $N_\\text{s} = 50$; (c) $N_\\text{s} = 20$; and (d) $N_\\text{s} = 1$ (TS).}\n\t\\label{Fig11}\n\\end{figure*}\n\n\\end{document}\n"}
{"paper_id": "2403-00540", "version": "2403-00540v2", "root_file_path": "d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\data_raw\\2403-00540\\tex\\2403-00540v2\\GreedyBO.tex", "metadata": {"total_length": 54726, "merged_count": 1, "merged_files": ["GreedyBO.tex"], "missing_files": []}, "content": "\\documentclass{article}\n\\usepackage{arxiv}\n\n\\usepackage[utf8]{inputenc} \n\\usepackage[T1]{fontenc}    \n\\usepackage{booktabs}       \n\\usepackage{amsfonts}       \n\\usepackage{nicefrac}       \n\\usepackage{microtype}      \n\n\\usepackage[numbers,sort&compress]{natbib}\n\\usepackage{doi}\n\\usepackage{algorithm,algpseudocode}\n\\usepackage{amsmath}\n\\usepackage{amssymb}\n\\usepackage{mathtools}\n\\usepackage{amsthm}\n\\usepackage{multirow, array, threeparttable}\n\\usepackage[font=small,skip=4pt]{caption}\n\\usepackage{subcaption}\n\n\\usepackage[dvipsnames]{xcolor}\n\\usepackage{graphicx}   \n\\usepackage{hyperref}   \n\\hypersetup{\n    colorlinks,\n    linkcolor={red!50!black},\n    citecolor={green!50!black},\n    urlcolor={red!80!black}\n}\n\\usepackage[capitalize,nameinlink]{cleveref}\n\\DeclareMathOperator{\\diag}{diag}\n\\newcommand\\myNote[1]{\\textcolor{red!50!black}{(#1})}\n\\newcommand\\myRev[1]{\\textcolor{black!50!black}{#1}}\n\n\\newcommand{\\edit}[1]{\\textcolor{red}{#1}} \n\\newcommand{\\newloc}[1]{\\textcolor{blue!80!black}{#1}} \n\\newcommand{\\tbd}[1]{\\textcolor{orange}{#1}} \n\n\\title{Epsilon-Greedy Thompson Sampling to Bayesian Optimization}\n\n\n\\author{{\\hspace{1mm}Bach Do}\\\\\n\tCivil and Environmental Engineering\\\\\n\tUniversity of Houston\\\\\n\tHouston, TX 77004 \\\\\n\t\\texttt{bdo3@uh.edu} \\\\\n\t\\And\n{\\hspace{1mm}Taiwo Adebiyi} \\\\\n\tCivil and Environmental Engineering\\\\\n\tUniversity of Houston\\\\\n\tHouston, TX 77004 \\\\\n\t\\texttt{taadebi2@cougarnet.uh.edu} \\\\\n        \\And\n {\\hspace{1mm}Ruda Zhang} \\\\\n\tCivil and Environmental Engineering\\\\\n\tUniversity of Houston\\\\\n\tHouston, TX 77004 \\\\\n\t\\texttt{rudaz@uh.edu} \\\\\n}\n\n\\date{}\n\n\\renewcommand{\\headeright}{ }\n\\renewcommand{\\shorttitle}{$\\varepsilon$-greedy Thompson Sampling}\n\n\\begin{document}\n\\maketitle\n\n\\begin{abstract}\nBayesian optimization (BO) has become a powerful tool for solving simulation-based engineering optimization problems thanks to its ability to integrate physical and mathematical understandings, consider uncertainty, and address the exploitation--exploration dilemma.\nThompson sampling (TS) is a preferred solution for BO to handle the exploitation--exploration trade-off.\nWhile it prioritizes exploration by generating and minimizing random sample paths from probabilistic models---a fundamental ingredient of BO---TS weakly manages exploitation by gathering information about the true objective function after it obtains new observations.\nIn this work, we improve the exploitation of TS by incorporating the $\\varepsilon$-greedy policy, a well-established selection strategy in reinforcement learning.\nWe first delineate two extremes of TS, namely the generic TS and the sample-average TS.\nThe former promotes exploration, while the latter favors exploitation.\nWe then adopt the $\\varepsilon$-greedy policy to randomly switch between these two extremes. \nSmall and large values of $\\varepsilon$ govern exploitation and exploration, respectively.\nBy minimizing two benchmark functions and solving an inverse problem of a steel cantilever beam,\nwe empirically show that $\\varepsilon$-greedy TS equipped with an appropriate $\\varepsilon$ is more robust than its two extremes,\nmatching or outperforming the better of the generic TS and the sample-average TS.\n\\end{abstract}\n\n\\keywords{Thompson sampling \\and Bayesian optimization \\and $\\varepsilon$-greedy policy \\and Exploitation--exploration dilemma \\and Cyclic constitutive law }\n\n\\section{Introduction}\n\\label{sec:introduction}\n\nConsider the following minimization problem:\n\\begin{equation}\\label{eqn1}\n\t\\begin{aligned}\n\t\t\\underset{\\bf x}{\\min} \\ \\ & f({\\bf x})\\\\\n\t\t\\textrm{subject to} \\ \\ \n\t\t& \\bf x \\in \\mathcal{X}, \n\t\\end{aligned}\n\\end{equation} \nwhere ${\\bf x} \\in \\mathbb{R}^d$ is the vector of $d$ input variables\nselected in a bounded, compact domain $\\mathcal{X} \\subset \\mathbb{R}^d$,\nand $f({\\bf x}): \\mathcal{X} \\mapsto \\mathbb{R}$ is a real-valued objective function.\nIn science and engineering applications, the objective function $f({\\bf x})$ is often a black-box function evaluated via costly simulations, which unfortunately hinders the use of any mature, derivative-based numerical optimization programs.\n\nBayesian optimization (BO)~\\cite{Snoek2012,Shahriari2016,Frazier2018,Garnett2023,Do2023mfbo} is a global sequential optimization technique well-suited for solving small- and moderate-dimensional optimization problems with costly or black-box objective functions.\nIt finds application in diverse domains of science and engineering, including hyperparameter tuning of machine learning algorithms~\\cite{Snoek2012}, identification of material parameters~\\cite{Karandikar2022,Kuhn2022}, material design~\\cite{Tran2019,Zhang2020}, airfoil design~\\cite{Zheng2020}, adaptive experimental design~\\cite{Greenhill2020}, and accelerator physics~\\cite{Roussel2021}.\nFor a recent comprehensive review of its applications in engineering design, see \\cite{Do2023mfbo}.\nAt its core, BO guides the optimization process using a probabilistic surrogate model $\\widehat{f}$ of the objective function,\nusually a Gaussian process (GP), coupled with an optimization policy~\\cite{Garnett2023}.\nGiven a dataset that has several observations of the input variables and the corresponding objective function values, a GP posterior built from this dataset often serves as the probabilistic model $\\widehat{f}$ encapsulating our beliefs about the black-box objective function.\nThe optimization policy specifying what we value in the dataset is defined through an acquisition function $\\alpha(\\mathbf{x})$.\nThis acquisition function can be deterministic or stochastic, and is cost-effective to evaluate given $\\widehat{f}$, making it convenient for processing optimization.\nDifferent considerations that should be taken into account when deriving $\\alpha(\\mathbf{x})$ from $\\widehat{f}$ include, for example, the value of the objective function~\\cite{Hennig2022}, the information about the minimum location~\\cite{Villemonteix2009,Hennig2012,HernandezLobato2014}, and the information about the minimum value~\\cite{WangZ2017}.\n\nVarious algorithms have been developed to address the classic exploitation--exploration dilemma in BO, where exploitation involves selecting new solutions anticipated to improve the objective function value and exploration entails sampling new solutions to reduce uncertainty in predictions of the objective function.\nAn effective BO algorithm is deemed to balance these opposing concerns~\\cite{Garnett2023}.    \nNotable deterministic acquisition functions such as expected improvement (EI)~\\cite{Jones1998}, weighted EI~\\cite{Sobester2005}, GP upper confidence bound (GP-UCB)~\\cite{Srinivas2010}, knowledge gradient (KG)~\\cite{Frazier2008}, and likelihood-weighting~\\cite{Blanchard2021} can manage the exploitation--exploration trade-off.\nHowever, they are considered myopic policies, and therefore tend to favor exploitation~\\cite{Hennig2022}.\n\nThompson sampling (TS) is a preferred algorithm to address the exploitation--exploration dilemma in solving multi-armed bandit problems~\\cite{Thompson1933,Chapelle2011,Russo2018}.\nMore specifically, TS selects arms (or actions) from  \na set of possible arms over a limited number of iterations.\nEach arm corresponds to a stochastic reward drawn from an unknown distribution.\nThe goal is to craft a sequence of arms that maximizes the cumulative reward assuming that the rewards are independent of time and conditionally independent given the selected arms.\nWhen applied to GP-based BO, TS generates a sequence of solution points using a mechanism that involves random sampling from an unknown posterior distribution of the global minimum location $\\bf{x}^\\star$~\\cite{Kandasamy2018}.\nThus, we can consider TS a BO method that uses a stochastic acquisition function.\nIt is also worth noting that several information-theoretic optimization policies of BO, such as predictive entropy~\\cite{HernandezLobato2014} and max-value entropy~\\cite{WangZ2017}, compute their acquisition functions based on a set of samples generated by TS.\n\nWhile TS naturally manages the exploitation--exploration trade-off,\nits randomness can downplay the role of exploitation in optimization.\nWhen the GP posterior exhibits high uncertainty, TS prioritizes exploration as it can diversify the selection of new solutions if there is not much information about $\\bf{x}^\\star$ we have gained.\nAs the number of observations increases, the algorithm transitions to exploiting knowledge about the true objective function.\nThis exploitation strategy, however, is inferior due to the randomness of TS, motivating the quest for an intervention to improve its exploitation. \n\nIn this work, we incorporate the $\\varepsilon$-greedy policy into TS to improve its exploitation.\nThis policy is a selection strategy of reinforcement learning to address the tension between exploitation and exploration~\\cite{Sutton2018}.\nGiven $\\varepsilon \\in (0,1)$, the policy chooses an action by either maximizing an average reward function with probability $1-\\varepsilon$ (i.e., exploitation or greedy) or selecting it randomly with probability $\\varepsilon$ (i.e., exploration).\nThe selection strategy is pure exploitation when $\\varepsilon = 0$, or pure exploration when $\\varepsilon = 1$.  \nSimilarly, our proposed approach implements the generic TS (with probability $\\varepsilon$) for exploration and a new fashion of TS called sample-average TS (with probability $1-\\varepsilon$) for exploitation.\n\nSeveral works have explored the $\\varepsilon$-greedy policy in BO and multi-armed bandit problems. \nDe Ath et al.~\\cite{DeAth2021} proposed two schemes for applying the policy to BO. \nThe first scheme performs exploration (with probability $\\varepsilon$) by randomly selecting a point on the Pareto frontier obtained by simultaneously minimizing the posterior mean and maximizing the posterior standard deviation.\nThe second scheme performs exploration by randomly selecting a point in the input variable space.\nBoth schemes implement exploitation (with probability $1-\\varepsilon$) by minimizing the posterior mean function.\nJin et al.~\\cite{Jin2023} introduced the so-called $\\varepsilon$-exploring TS to multi-armed bandit problems.\nGiven the posterior distributions of aims, exploration (with probability $\\varepsilon$) sets the estimated reward function for each aim as a random sample drawn from the associated posterior distribution, while exploitation (with probability $1-\\varepsilon$) sets the estimated reward function as the sample mean function. \nYet the performance of $\\varepsilon$-greedy TS for BO remains unexplored.\n\nOur contributions are as follows:\n(1) introducing $\\varepsilon$-greedy TS to BO, which is a simple, effective method to improve the exploitation of TS;\nand (2) empirical evaluations demonstrating that $\\varepsilon$-greedy TS with an appropriate $\\varepsilon$\nis more robust than its two extremes (i.e., the generic TS and the sample-average TS),\nmatching or outperforming the better of the two across various problems.\n\n\nThe rest of this paper progresses as follows.\n\\cref{sec:background} provides a background essential for the development of $\\varepsilon$-greedy TS.\n\\cref{sec:method} describes the method in detail.\n\\cref{sec:experiments} presents the empirical performance of $\\varepsilon$-greedy TS on minimizing two benchmark functions and finding constitutive parameters for a steel cantilever beam.\nFinally, \\cref{sec:conclusions} concludes this paper.\n\n\\section{Background}\n\\label{sec:background}\nThis section provides an overview of GP modeling (\\cref{sec:GP}), the implementation of the generic TS for BO (\\cref{sec:TS}), and a simple method to generate sample paths from GP posteriors for use of TS (\\cref{sec:sampling}).\n\n\\subsection{Overview of Gaussian Processes}\n\\label{sec:GP}\n\nLet $\\mathcal{D}=\\left( {\\bf X},{\\bf y}\\right)=\\left({\\bf x}^i,y^i\\right)_{i=1}^N$ denote a training dataset, where $\\textbf{x}^i$ are observations of a $d$-dimensional vector of input variables and $y^i$ the corresponding observations of the objective function.\nWe wish to build from $\\mathcal{D}$ an observation model $y({\\bf x})=f({\\bf x}) + \\varepsilon_\\text{n}: \\mathbb{R}^d \\mapsto \\mathbb{R}$, where $\\varepsilon_\\text{n} \\sim \\mathcal{N}(0,\\sigma^2_\\text{n})$ is additive zero-mean Gaussian noise with variance $\\sigma^2_\\text{n}$.\nThis observation noise is assumed to be independent and identically distributed.\n\nA GP model assumes that any finite subset of an infinite set of objective function values has a joint Gaussian distribution~\\cite{Rasmussen2006}.\nOftentimes, this assumption is encoded using the following GP prior:\n\\begin{equation}\\label{eqn2}\n\tf(\\cdot) \\sim \\mathcal{GP} \\left(0,\\kappa(\\cdot,\\cdot|\\boldsymbol{\\theta})\\right),\n\\end{equation}\nwhere $\\kappa({\\bf x},{\\bf x}'|\\boldsymbol{\\theta}) = \\text{cov}[f({\\bf x}),f({\\bf x}')]: \\mathbb{R}^d \\times \\mathbb{R}^d \\mapsto \\mathbb{R} $ is a positive semi-definite covariance function parameterized by a vector $\\boldsymbol{\\theta}$ of hyperparameters.\nCommon-used covariance functions include squared exponential (SE), Matérn 3/2, Matérn 5/2, and automatic relevance determination squared exponential (ARD SE)~\\cite{Garnett2023,Rasmussen2006}.\nBy conditioning on $\\mathcal{D}$ the model prior given in \\cref{eqn2} and utilizing the observation noise assumption, it is straightforward to show that the vector of observations $\\left[y({\\bf x}^1),\\dots,y({\\bf x}^N)\\right]^\\intercal$ is distributed according to an $N$-variate Gaussian with zero mean and covariance matrix ${\\bf C} = {\\bf K} + \\sigma^2_\\text{n} {\\bf I}_N$, where the $(i,j)$th element of ${\\bf K}$ is $\\kappa({\\bf x}^i,{\\bf x}^j|\\boldsymbol{\\theta})$ and ${\\bf I}_N$ the $N$-by-$N$ identity matrix.\nBy further applying the conditional multivariate Gaussian~\\cite{Bishop2006}, we obtain the posterior predictive distribution of the objective function value at an unseen input variable vector ${\\bf x}_\\star$, i.e., $p\\left(f({\\bf x}_\\star)|{\\bf x}_\\star,\\mathcal{D}\\right) = \\mathcal{N}\\left(\\mu_\\text{f}({\\bf x}_\\star),\\sigma_\\text{f}^2({\\bf x}_\\star)\\right)$, which encodes information about the observation model we wish to build. The mean and variance of the predictive distribution read \n\\begin{equation}\\label{eqn3}\n\t\\mu_\\text{f}({\\bf x}_\\star)= {\\bf K_\\star}^\\intercal {\\bf C}^{-1} {\\bf Y},\n\\end{equation}\n\\begin{equation}\\label{eqn4}\n\t\\sigma_\\text{f}^2({\\bf x}_\\star)=\\kappa({\\bf x}_\\star,{\\bf x}_\\star|\\boldsymbol{\\theta})\n\t-{\\bf K_\\star}^\\intercal {\\bf C}^{-1} {\\bf K_\\star},\n\\end{equation}\nwhere\n\\begin{equation}\\label{eqn5}\n\t{\\bf K_\\star} = \\left[\\kappa({\\bf x}_\\star,{\\bf x}^1|\\boldsymbol{\\theta}),\\dots,\\kappa({\\bf x}_\\star,{\\bf x}^N|\\boldsymbol{\\theta})\\right]^\\intercal.\n\\end{equation}\n\nTraining GP models involves finding an optimal set of hyperparameters $\\boldsymbol{\\theta}$ that ensures robust predictive performance of trained GPs across unseen input variables.\nExact training methods emulating all possible hyperparameter hypotheses are often impractical, leading to popular use of approximate training methods.\nThese approximate training methods fall into two main categories: deterministic approximations and Monte Carlo methods~\\cite{Mackay2003}.\nDeterministic approximations include the maximum likelihood method identifying a set of hyperparameters that maximizes the parameter likelihood $p(\\mathcal{D}|\\boldsymbol{\\theta})$, and Laplace's method approximating the posterior $p(\\boldsymbol{\\theta}|\\mathcal{D})$ as a multivariate Gaussian with a posterior mean equal to the maximum likelihood estimation and a posterior covariance estimated from Laplace approximation.\nMonte-Carlo methods, such as importance sampling, the Metropolis, Gibbs sampling, and slice sampling, generate approximate samples of $\\boldsymbol{\\theta}$ from the posterior $p(\\boldsymbol{\\theta}|\\mathcal{D})$ given the likelihood $p(\\mathcal{D}|\\boldsymbol{\\theta})$ and a prior $p(\\boldsymbol{\\theta})$.\nSeveral of the aforementioned approximate training methods have been incorporated in reliable GP toolboxes such as DACE~\\cite{Lophaven2002}, GPML~\\cite{Rasmussen2010}, GPstuff~\\cite{Vanhatalo2013}, pyGPs~\\cite{Neumann2015}, and GPflow~\\cite{Matthews2017}.\n\n\\begin{figure}[t]\n\t\\centering\n\t\\begin{subfigure}[c]{0.4\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-samplepaths.pdf}\n\t\t\\caption{}\n\t\t\\label{fig:paths_a}\n\t\\end{subfigure}\t\n\t\\begin{subfigure}[c]{0.4\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-solutiondistribution.pdf}\n\t\t\\caption{}\n\t\t\\label{fig:paths_b}\n\t\\end{subfigure}\n\t\\caption{Sample paths from the GP posterior for $f(x) = x\\sin(x)$ and distribution of their minimum locations. (a) GP predictions and five sample paths drawn from the GP posterior; (b) Approximate conditional distribution $p( x^\\star|\\mathcal{D})$ obtained from minimum locations of 50 sample paths.} \n\t\\label{fig:paths}\n\\end{figure}\n\n\\begin{algorithm}[t]\n\t\\caption{Generic Thompson sampling for Bayesian optimization}\n\t\\label{alg:TS}\n\t\\begin{algorithmic}[1]\n\t\t\n\t\t\\State \\textbf{input:} input variable domain $\\mathcal{X}$, number of initial observations $N$, threshold for number of BO iterations $K$\n\t\t\n\t\t\\State Generate $N$ initial samples of ${\\bf x}$\n\t\t\n\t\t\\For {$i=1:N$} \n\t\t\\State $y^i \\gets f({\\bf x}^i) + \\varepsilon_\\text{n}^i$ \n\t\t\\EndFor\n\t\t\n\t\t\\State $\\mathcal{D}^0 \\gets \\{{\\bf x}^i,y^i\\}_{i=1}^N$\n\t\t\n\t\t\\State $\\{{\\bf x}_{\\min},y_{\\min}\\} \\gets \\min\\{y^i,\\, i=1,\\dots, N\\}$\n\t\t\n\t\t\\For {$k=1:K$} \n\t\t\\State Build a GP posterior $\\widehat{f}^k({\\bf x})|\\mathcal{D}^{k-1}$\n\t\t\\State Generate a sample path $g({\\bf x}|\\mathcal{D}^{k-1})$ from $\\widehat{f}^k({\\bf x})|\\mathcal{D}^{k-1}$ \\label{alg:TS_L10}\n\t\t\\State ${\\bf x}^{k} \\gets \\underset{{\\bf x}}{\\mathrm{arg\\,min}} \\ \\ g({\\bf x}|\\mathcal{D}^{k-1})$ s.t. ${\\bf x} \\in \\mathcal{X}$; ${\\bf x} \\notin \\mathcal{D}^{k-1}$\n\t\t\\State $y^{k} \\gets f({\\bf x}^{k}) + \\varepsilon_\\text{n}^k$\n\t\t\\State $\\mathcal{D}^k\\gets\\mathcal{D}^{k-1} \\cup \\{{\\bf x}^{k},y^{k}\\}$\n\t\t\\State $\\{{\\bf x}_{\\min},y_{\\min}\\} \\gets \\min\\{y_{\\min},y^{k}\\}$\n\t\t\\EndFor\n\t\t\n\t\t\\State \\textbf{return} $\\{{\\bf x}_{\\min},y_{\\min}\\}$\n\t\\end{algorithmic}\n\\end{algorithm}\n\n\\subsection{Bayesian Optimization via Thompson Sampling}\n\\label{sec:TS}\n\nAs briefly described in \\cref{sec:introduction}, the generic TS generates a sequence of solution points ${\\bf x}^k$ $(k=1,\\dots,K)$ by randomly sampling from an unknown posterior distribution $p({\\bf x}^\\star|\\mathcal{D}^{k-1})$ of the global minimum ${\\bf x}^\\star$, where $K$ represents a finite budget on the number of BO iterations.\nLeveraging the fact that ${\\bf x}^\\star$ is fully determined by the objective function, the generic TS follows two simple steps in each iteration.\nIt first generates a sample path from the GP posterior $\\widehat{f}^k({\\bf x})|\\mathcal{D}^{k-1}$ for which the detailed implementation is described in \\cref{sec:sampling}.\nIt then minimizes the generated sample path to find a minimum location ${\\bf x}^\\star$.\nBy doing so, ${\\bf x}^\\star$, assigned as the new solution ${\\bf x}^k$, is a sample from $p({\\bf x}^\\star|\\mathcal{D}^{k-1})$.\n\\cref{alg:TS} summarizes the implementation of the generic TS.\n\n\\Cref{fig:paths} shows several sample paths generated from the GP posterior of univariate function $f(x)=x\\sin{x}$ for $x \\in [0,20]$ and an approximate distribution of their minimum locations.\nSpecifically, we build from ten observations a GP model with zero mean and SE covariance function.\nFrom this GP model, we generate 50 sample paths and five of them are shown in \\cref{fig:paths_a}. \nMinimizing the generated sample paths and utilizing a kernel density estimation for the obtained solutions result in an approximate distribution $p({\\bf x}^\\star|\\mathcal{D})$, as shown in \\cref{fig:paths_b}.\nNote that the generic TS in \\cref{alg:TS} generates and minimizes only one sample path in each iteration and does not attempt to approximate the posterior distribution of the minimum location. The following section describes how we can randomly generate sample path $g({\\bf x}|\\mathcal{D}^{k-1})$ in \\cref{alg:TS_L10} of \\cref{alg:TS}.\n\n\\begin{figure}[t]\n\t\\centering\n\t\\begin{subfigure}[c]{0.4\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-approximateSE.pdf}\n\t\t\\caption{}\n\t\t\\label{fig:approximatekernel_a}\n\t\\end{subfigure}\n\t\\centering\n\t\\begin{subfigure}[c]{0.4\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-approximateMatern.pdf}\n\t\t\\caption{}\n\t\t\\label{fig:approximatekernel_b}\n\t\\end{subfigure}\n\t\\caption{Approximate covariance functions using random features from different numbers of spectral point samples. (a) SE; (b) Matérn 5/2.}\n\t\\label{fig:approximatekernel}\n\\end{figure}\n\n\\begin{table*}[h]\n\t\\caption{Spectral density functions associated with common-used covariance functions~\\cite{RiutortMayol2022}.}\n\t\\label{table1}\n\t\\centering\n\t\\begin{threeparttable}\n\t\\renewcommand{\\arraystretch}{1}\n\t\\begin{tabular}{lll}\n\t\t\\hline\\noalign{\\smallskip}\n\t\t {Covariance function} & $\\kappa({\\bf x},{\\bf x}'|\\boldsymbol{\\theta})$ & Spectral density function $p({\\bf s})$\\\\\n\t\t\\hline\\noalign{\\smallskip}\n\t\t\n\t\tSE & $\\sigma_\\text{f}^2 \\exp{\\left(-\\frac{1}{2}\\frac{r^2}{l^2}\\right)}$ \\tnote{(1)}   &$\\sigma_\\text{f}^2(\\sqrt{2\\pi})^d l^d \\exp{\\left(-\\frac{1}{2} l^2 {\\bf s}^\\intercal {\\bf s}\\right)}$  \\\\\n\t\t\\noalign{\\smallskip}\n\t\t\n\t\tARD SE & $\\sigma_\\text{f}^2 \\exp{\\left(-\\frac{1}{2}\\sum_{i=1}^{d}\\frac{(x_i-x_i')^2}{l_i^2}\\right)}$ \\tnote{(2)}    &$\\sigma_\\text{f}^2 (\\sqrt{2\\pi})^d \\left(\\prod_{i=1}^{d}l_i\\right) \\exp{\\left(-\\frac{1}{2} \\sum_{i=1}^{d}l_i^2 s_i^2\\right)}$  \\\\\n\t\t\\noalign{\\smallskip}\n\t\t\n\t\tMatérn 3/2 & $\\sigma_\\text{f}^2 \\left( 1+ \\frac{\\sqrt{3}r}{l}\\right) \\exp{\\left(-\\frac{\\sqrt{3}r}{l}\\right)}$ \\tnote{(3)} & $\\sigma_\\text{f}^2 \\frac{2^d \\pi^{d/2} \\Gamma\\left(\\frac{d+3}{2}\\right)3^{3/2}}{\\frac{1}{2} \\sqrt{\\pi} l^3} \\left(\\frac{3}{l^2} + {\\bf s}^\\intercal {\\bf s} \\right)^{-\\frac{d+3}{2}}$ \\tnote{(4)} \\\\\n\t\t\\noalign{\\smallskip}\n\t\t\n\t\tMatérn 5/2 & $\\sigma_\\text{f}^2 \\left( 1+ \\frac{\\sqrt{5}r}{l} + \\frac{5 r^2}{3 l^2}\\right) \\exp{\\left(-\\frac{\\sqrt{5}r}{l}\\right)}$ \\tnote{(3)} & $\\sigma_\\text{f}^2 \\frac{2^d \\pi^{d/2} \\Gamma\\left(\\frac{d+5}{2}\\right)5^{5/2}}{\\frac{3}{4} \\sqrt{\\pi} l^5} \\left(\\frac{5}{l^2} + {\\bf s}^\\intercal {\\bf s} \\right)^{-\\frac{d+5}{2}}$ \\tnote{(4)}\\\\\n\t\t\\hline\\noalign{\\smallskip}\n\t\t\n\t\\end{tabular}\n\t\\begin{tablenotes}\n\t\t\\item[(1)] $r = \\sqrt{({\\bf x}-{\\bf x}')^\\intercal ({\\bf x}-{\\bf x}')}$ and $\\boldsymbol{\\theta} = [\\sigma_\\text{f},l]^\\intercal$, where $l>0$ and $\\sigma_\\text{f}>0$  are length scale and marginal standard deviation, respectively\n\t\t\n\t\t\\item[(2)] $\\boldsymbol{\\theta} = [\\sigma_\\text{f},l_1,\\dots,l_d]^\\intercal$, where $l_i>0$ $(i=1,\\dots,d)$ is length scale for the $i$th input variable\n\t\t\n\t\t\\item[(3)]\t$r = \\sqrt{({\\bf x}-{\\bf x}')^\\intercal ({\\bf x}-{\\bf x}')}$ and $\\boldsymbol{\\theta} = [\\sigma_\\text{f},l]^\\intercal$\n\t\t\n\t\t\\item[(4)] $\\Gamma(\\cdot) = $ Gamma function\n\t\\end{tablenotes}\n\\end{threeparttable}\n\\end{table*}\n\n\\subsection{Sampling from Gaussian Process Posteriors}\n\\label{sec:sampling}\n\nGiven the GP posterior $\\widehat{f}^k({\\bf x})|\\mathcal{D}^{k-1}$ characterized by a stationary covariance function, we follow the spectral sampling approach by Hernández-Lobato at el.~\\cite{HernandezLobato2014} to generate a sample path $g({\\bf x}|\\mathcal{D}^{k-1})$.\nThis approach approximates the GP prior in \\cref{eqn2} using a Bayesian linear model of randomized basis functions to avoid the computational cost due to exhaustive sampling from marginal distributions of the objective function values at finite sets of input locations, which scales cubically in the number of input locations.\nSuch an approximation is rooted in Bochner’s theorem that guarantees the existence of a Fourier dual $p({\\bf s})$ (${\\bf s} \\in \\mathbb{R}^d$) of the stationary covariance function, which is called spectral density when a finite non-negative Borel measure is interpreted as a distribution~\\cite{Wendland2004}.\nThe spectral density functions associated with several covariance functions of Matérn class and ARD SE are listed in \\cref{table1}.\nOnce $p({\\bf s})$ is determined, we can represent $\\kappa({\\bf x},{\\bf x}'|\\boldsymbol{\\theta})$ using the following randomized feature map~\\cite{Rahimi2007}:\n\\begin{equation}\\label{eqn6}\n\t\\kappa({\\bf x},{\\bf x}'|\\boldsymbol{\\theta}) = \\boldsymbol{\\phi}({\\bf x})^\\intercal \\boldsymbol{\\phi}({\\bf x}'),\n\\end{equation}\nwhere the feature map $\\boldsymbol{\\phi}({\\bf x})\\in \\mathbb{R}^{N_\\text{p}}$ can be approximated by~\\cite{HernandezLobato2014}\n\\begin{equation}\\label{eqn7}\n\t\\boldsymbol{\\phi}({\\bf x}) = \\sqrt{2 \\kappa(0|\\boldsymbol{\\theta})/N_\\text{p}} \\cos \\left({\\bf W} {\\bf x} + {\\bf b}\\right).\n\\end{equation}\nHere, ${\\bf W} \\in \\mathbb{R}^{N_\\text{p} \\times d}$ and ${\\bf b}\\in \\mathbb{R}^{N_\\text{p}}$ stack $N_\\text{p}$ spectral points generated from $p({\\bf s})$ and $N_\\text{p}$ points drawn from the uniform distribution $\\mathcal{U}[0,2\\pi]$, respectively. $\\kappa(0|\\boldsymbol{\\theta})$ is well defined because $\\kappa({\\bf x},{\\bf x}'|\\boldsymbol{\\theta})$ is a stationary covariance function satisfying $\\kappa({\\bf x},{\\bf x}'|\\boldsymbol{\\theta}) = \\kappa(\\left\\|{\\bf x}- {\\bf x}' \\right\\||\\boldsymbol{\\theta})$.\n\n\\Cref{fig:approximatekernel} shows approximate SE and Matérn 5/2 covariance functions computed from \\cref{eqn6} for different numbers of spectral point samples.\nWe see that, the approximate covariance functions converge to the true ones when increasing the number of spectral points.\n\nOnce the approximate feature map is formulated, the GP prior as a kernel machine can be approximated by the following Bayesian linear model: \n\\begin{equation}\\label{eqn8}\n\tf({\\bf x}) \\approx \\boldsymbol{\\beta}^\\intercal \\boldsymbol{\\phi}({\\bf x}),\n\\end{equation}\nwhere the prior of weight vector $\\boldsymbol{\\beta}$ is $\\mathcal{N}({\\bf 0}, {\\bf I}_{N_\\text{p}})$.\nBy further conditioning \\cref{eqn8} on the data, we obtain the following mean and covariance matrix of posterior $\\boldsymbol{\\beta}$~\\cite{HernandezLobato2014}\n\\begin{subequations}\\label{eqn9}\n\t\\begin{align}\n\t\t\\boldsymbol{\\mu}_{\\beta} & = \\left(\\boldsymbol{\\Phi}^\\intercal \\boldsymbol{\\Phi} +\\sigma_\\text{n}^2 {\\bf I}_{N_\\text{p}}\\right)^{-1} \\boldsymbol{\\Phi}^\\intercal {\\bf y},\\\\\n\t\t\\boldsymbol{\\Sigma}_{\\beta} & = \\left(\\boldsymbol{\\Phi}^\\intercal \\boldsymbol{\\Phi} +\\sigma_\\text{n}^2 {\\bf I}_{N_\\text{p}}\\right)^{-1} \\sigma_\\text{n}^2,\n\t\\end{align}\n\\end{subequations}\nwhere $\\boldsymbol{\\Phi} = [\\boldsymbol{\\phi}({\\bf x}),\\dots,\\boldsymbol{\\phi}({\\bf x}^N)]^\\intercal \\in \\mathbb{R}^{N \\times N_\\text{p}}$. To this end, we can approximate the GP posterior using a Bayesian linear model that weights the randomized basis functions associated with the posterior covariance function by samples from $\\boldsymbol{\\beta}$ posterior.\n\nThe generation of $g({\\bf x}|\\mathcal{D}^{k-1})$ using random features is detailed in \\cref{alg:spectral}.\nNote that \\cref{alg:spectral_L10} of \\cref{alg:spectral} involves the Cholesky decomposition of matrix $\\boldsymbol{\\Phi}^\\intercal \\boldsymbol{\\Phi} +\\sigma_\\text{n}^2 {\\bf I}_{N_\\text{p}}$ to facilitate the calculation of its inverse. \nAlternative posterior sampling techniques such as the decoupled sampling method based on updating GP prior samples~\\cite{Wilson2020} can be adopted to increase the sample path accuracy.\n\n\\begin{algorithm}[t]\n\t\\caption{Generation of sample paths using random features}\n\t\\label{alg:spectral}\n\t\\begin{algorithmic}[1]\n\t\t\\State \\textbf{Input:} dataset $\\mathcal{D}^{k-1}$, type of stationary covariance function $\\kappa(\\cdot,\\cdot|\\boldsymbol{\\theta})$, number of spectral points $N_\\text{p}$\n\t\t\n\t\t\\State Build a GP model from $\\mathcal{D}^{k-1}$\n\t\t\\State Formulate $p(\\bf s)$ from the posterior covariance function $\\kappa(\\cdot,\\cdot|\\boldsymbol{\\theta})$, see \\cref{table1}\n\t\t\n\t\t\\For {$j=1:N_\\text{p}$} \n\t\t\\State $[{\\bf W}]_j \\sim p(\\bf s)$\n\t\t\\State $[{\\bf b}]_j \\sim \\mathcal{U}[0,2\\pi]$\n\t\t\\EndFor\n\t\t\n\t\t\\State Formulate $\\boldsymbol{\\phi}({\\bf x})$, see \\cref{eqn7}\n\t\t\n\t\t\\State $\\boldsymbol{\\Phi} \\gets [\\boldsymbol{\\phi}({\\bf x}),\\dots,\\boldsymbol{\\phi}({\\bf x}^N)]^\\intercal$\n\t\t\n\t\t\\State Compute $\\boldsymbol{\\mu}_{\\beta}$ and $\\boldsymbol{\\Sigma}_{\\beta}$, see \\cref{eqn9} \\label{alg:spectral_L10}\n\t\t\n\t\t\\State $\\boldsymbol{\\beta} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{\\beta}, \\boldsymbol{\\Sigma}_{\\beta})$\n\t\t\n\t\t\\State \\textbf{return} $g({\\bf x})|\\mathcal{D}^{k-1} \\gets \\boldsymbol{\\beta}^\\intercal \\boldsymbol{\\phi}({\\bf x})$\n\t\\end{algorithmic}\n\\end{algorithm}\n\n\\begin{algorithm}[t]\n\t\\caption{$\\varepsilon$-greedy Thompson sampling for Bayesian optimization}\n\t\\label{alg:epsilon_greedyTS}\n\t\\begin{algorithmic}[1]\n\t\t\n\t\t\\State \\textbf{input:} input variable domain $\\mathcal{X}$, number of initial observations $N$, threshold for number of BO iterations $K$, number of spectral points $N_\\text{p}$, number of sample paths $N_\\text{s}$, value of $\\varepsilon \\in (0,1)$\n\t\t\n\t\t\\State Generate $N$ samples of ${\\bf x}$\n\t\t\n\t\t\\For {$i=1:N$} \n\t\t\\State $y^i \\gets f({\\bf x}^i) + \\varepsilon_\\text{n}^i$ \n\t\t\\EndFor\n\t\t\n\t\t\\State $\\mathcal{D}^0 \\gets \\{{\\bf x}^i,y^i\\}_{i=1}^N$\n\t\t\n\t\t\\State $\\{{\\bf x}_{\\min},y_{\\min}\\} \\gets \\min\\{y^i,\\, i=1,\\dots, N\\}$\n\t\t\n\t\t\\For {$k=1:K$} \n\t\t\\State Build a GP posterior $\\widehat{f}^k({\\bf x})|\\mathcal{D}^{k-1}$\n\t\t\\State Generate $r \\sim \\mathcal{U}[0,1]$\n\t\t\\If {$r \\leq \\varepsilon$}\n\t\t\\State Sample $g({\\bf x}|\\mathcal{D}^{k-1})$\n\t\t\\Else\n\t\t\\For {$s=1:N_\\text{s}$} \n\t\t\\State Sample $h^s({\\bf x}|\\mathcal{D}^{k-1})$\n\t\t\\EndFor\n\t\t\\State $g({\\bf x}|\\mathcal{D}^{k-1}) \\gets \\frac{1}{N_\\text{s}}\\sum_{s=1}^{N_\\text{s}} h^s({\\bf x}|\\mathcal{D}^{k-1})$\n\t\t\\EndIf\n\t\t\\State ${\\bf x}^{k} \\gets \\underset{{\\bf x}}{\\mathrm{arg\\,min}} \\ \\ g({\\bf x}|\\mathcal{D}^{k-1})$ s.t. ${\\bf x} \\in \\mathcal{X}$; ${\\bf x} \\notin \\mathcal{D}^{k-1}$\n\t\t\\State $y^{k} \\gets f({\\bf x}^{k}) + \\varepsilon_\\text{n}^k$\n\t\t\\State $\\mathcal{D}^k\\gets\\mathcal{D}^{k-1} \\cup \\{{\\bf x}^{k},y^{k}\\}$\n\t\t\\State $\\{{\\bf x}_{\\min},y_{\\min}\\} \\gets \\min\\{y_{\\min},y^{k}\\}$\n\t\t\\EndFor\n\t\t\n\t\t\\State \\textbf{return} $\\{{\\bf x}_{\\min},y_{\\min}\\}$\n\t\\end{algorithmic}\n\\end{algorithm}\n\n\\section{Epsilon-greedy Thompson Sampling}\n\\label{sec:method}\n\nIn this section, we first introduce another version of TS called sample-average TS (\\cref{sec:method-avaraging}) and then describe $\\varepsilon$-greedy TS using the sample-average and generic TS in the context of $\\varepsilon$-greedy policy (\\cref{sec:method-epsgreedyTS}).\n\n\\subsection{Sample-average Thompson Sampling}\n\\label{sec:method-avaraging}\n\nTo enhance the exploitation of TS, we implement another version of TS called sample-average TS (or averaging TS)~\\cite{Balandat2020}.\nSpecifically, we call \\cref{alg:spectral} to generate in \\cref{alg:TS_L10} of \\cref{alg:TS} a total of $N_\\text{s}$ sample paths $h^s({\\bf x}|\\mathcal{D}^{k-1})$ ($s=1,\\dots,N_\\text{s}$). These sample paths are used to define the following average sample path:\n\\begin{equation}\\label{eqn10}\n\tg({\\bf x}|\\mathcal{D}^{k-1}) = \\frac{1}{N_\\text{s}}\\sum_{s=1}^{N_\\text{s}} h^s({\\bf x}|\\mathcal{D}^{k-1}).\n\\end{equation}\nWe then minimize the average sample path $g({\\bf x}|\\mathcal{D}^{k-1})$ to find a new solution in each iteration of BO.\nThis approach differs from that of Balandat et al.~\\cite{Balandat2020} which generates sample paths using randomized quasi Monte-Carlo techniques.\n\nThe sample-average TS and the generic TS can be considered two extremes of TS. If $N_\\text{s} = \\infty$, $g({\\bf x}|\\mathcal{D}^{k-1})$ in \\cref{eqn10} is indeed the GP posterior mean in \\cref{eqn3} whose minimum promotes exploitation.\nIf $N_s = 1$, the sample-average TS recovers the generic TS that favors exploration. \nBetween these two extremes, therefore, is an optimal state associated with an unknown value of $N_s$ at which TS balances exploitation and exploration.\nHowever, we do not attempt to find such an optimal state in this work.\nWe instead set $N_\\text{s}$ at a sufficiently large value, say $N_\\text{s}=50$, to enforce the exploitation of TS.\n\n\\subsection{$\\varepsilon$-greedy Thompson Sampling}\n\\label{sec:method-epsgreedyTS}\n\nThe two distinct extremes of TS motivate the use of $\\varepsilon$-greedy policy to randomly switch between them.\nSpecifically, we implement the generic TS with probability $\\varepsilon$ to explore the input variable space.\nWe invoke the sample-average TS with probability $1-\\varepsilon$ to guide the search toward exploitation.\nWe do not use the posterior mean in \\cref{eqn3} for exploitation because we observe that computing its derivatives is more expensive than computing the derivatives of the average sample path in \\cref{eqn10}.\nIt is also straightforward to recover the generic TS from $\\varepsilon$-greedy TS by simply setting $\\varepsilon=1$ or $N_\\text{s}=1$.\n\\cref{alg:epsilon_greedyTS} details the proposed $\\varepsilon$-greedy TS.\n\nThe proposed algorithm addresses the exploitation--exploration dilemma by the following factors: (1) $\\varepsilon$ \\textendash a small value of $\\varepsilon$ promotes exploitation and (2) $N_\\text{s}$ \\textendash a sufficiently large value of $N_\\text{s}$ encourages exploitation.\n\n\\begin{figure}[t]\n\t\\centering\n\t\\begin{subfigure}[c]{0.40\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-ackleyHistory.pdf}\n\t\t\\caption{}\n\t\t\\label{fig:TestFunctions_a}\n\t\\end{subfigure}\n\t\\begin{subfigure}[c]{0.40\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-rosenHistory.pdf}\n\t\t\\caption{}\n\t\t\\label{fig:TestFunctions_b}\n\t\\end{subfigure}\n\t\n\t\n\t\\begin{subfigure}[c]{0.40\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-ackleyBox.pdf}\n\t\t\\caption{}\n\t\t\\label{fig:TestFunctions_c}\n\t\\end{subfigure}\n\t\\begin{subfigure}[c]{0.40\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-rosenBox.pdf}\n\t\t\\caption{}\n\t\t\\label{fig:TestFunctions_d}\n\t\\end{subfigure}\n\t\n\t\\caption{Performance of EI, LCB, averaging TS, generic TS, and $\\varepsilon$-greedy TS methods for the 2d Ackley and 6d Rosenbrock functions. Optimization histories for (a) the 2d Ackley function and (b) the 6d Rosenbrock function. Medians and interquartile ranges of final solutions from 100 runs of each BO method for (c) the 2d Ackley function and (d) the 6d Rosenbrock function.} \n\t\\label{fig:TestFunctions}\n\\end{figure}\n\n\\begin{figure}[ht]\n\t\\centering\n\t\\begin{subfigure}[c]{0.4\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-ackleyAddedpointaverTS.pdf}\n\t\t\\caption{Averaging TS, $\\varepsilon = 0$}\n\t\t\\label{fig:AddedPoints_a}\n\t\\end{subfigure}\n\t\\begin{subfigure}[c]{0.4\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-ackleyAddedpointepsTS01.pdf}\n\t\t\\caption{$\\varepsilon$-greedy TS, $\\varepsilon = 0.1$}\n\t\t\\label{fig:AddedPoints_b}\n\t\\end{subfigure}\n\t\n\t\\begin{subfigure}[c]{0.4\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-ackleyAddedpointepsTS09.pdf}\n\t\t\\caption{$\\varepsilon$-greedy TS, $\\varepsilon = 0.9$}\n\t\t\\label{fig:AddedPoints_c}\n\t\\end{subfigure}\n\t\\begin{subfigure}[c]{0.4\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-ackleyAddedpointTS.pdf}\n\t\t\\caption{Generic TS, $\\varepsilon = 1$}\n\t\t\\label{fig:AddedPoints_d}\n\t\\end{subfigure}\n\t\\caption{Initial and added points for 2d Ackley function with $N_\\text{s} = 50$ and different values of $\\varepsilon$.} \n\t\\label{fig:AddedPoints}\n\\end{figure}\n\n\\begin{figure}[ht]\n\t\\centering\n\t\\begin{subfigure}[c]{0.4\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-ackleyTime.pdf}\n\t\t\\caption{2d Ackley}\n\t\t\\label{fig:RunTime_a}\n\t\\end{subfigure}\n\t\\begin{subfigure}[c]{0.41\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-rosenTime.pdf}\n\t\t\\caption{6d Rosenbrock}\n\t\t\\label{fig:RunTime_b}\n\t\\end{subfigure}\n\t\\caption{Approximate distributions of runtime for selecting a new solution point with different $\\varepsilon$ values and $N_\\text{s} = 50$.} \n\t\\label{fig:RunTime}\n\\end{figure}\n\n\\begin{figure*}[ht]\n\t\\centering\n\t\\includegraphics[scale=0.55]{figure/fig-beamModel.pdf}\n\t\\caption{The cantilever beam, its finite-element (FE) mesh, and loading history RH1 for cyclic test~\\cite{Do2022}.}\n\t\\label{fig:beamModel}\n\\end{figure*}\n\n\\section{Numerical Examples}\n\\label{sec:experiments}\n\nWe test the empirical performance of $\\varepsilon$-greedy TS on two challenging minimization problems of the 2d Ackley and 6d Rosenbrock functions (\\cref{sec:example-functions}), and on the identification problem of a steel cantilever beam subjected to cyclic loading (\\cref{sec:example-beam}).\n\n\\subsection{Benchmark Functions}\n\\label{sec:example-functions}\n\nConsider minimization problems of the 2d Ackley and 6d Rosenbrock functions~\\cite{Surjanovic2013}.\nThe analytical expressions for these functions and their global minimums are given as follows: \n\n\\textbf{2d Ackley function}\n\\begin{equation}\\label{eqn11}\n\t\\begin{aligned}\n\t\tf({\\bf x}) = & -a \\exp\\left(-b\\sqrt{\\frac{1}{2} \\sum_{i=1}^{2}x^2_i}\\right) \\\\\n\t\t&- \\exp\\left(\\frac{1}{2} \\sum_{i=1}^{2}\\cos(cx_i)\\right) +a + \\exp(1),\n\t\\end{aligned}\n\\end{equation}\nwhere $a = 20$, $b = 0.2$, and $c = 2\\pi$.\nThe function is evaluated on $\\mathcal{X}=[-10,10]^2$ and has a global minimum at ${\\bf x}^\\star = [0,0]^\\intercal$ with $f^\\star = f({\\bf x}^\\star) = 0$.\n\n\\textbf{6d Rosenbrock function}\n\\begin{equation}\\label{eqn12}\n\tf({\\bf x}) = \\sum_{i=1}^{5} \\left[100(x_{i+1}-x_i^2)^2 + (x_i - 1)^2\\right],\n\\end{equation}\nwhere $\\mathcal{X}=[-5,10]^6$. The function has a global minimum at ${\\bf x}^\\star = [1,\\dots,1]^\\intercal$ with $f^\\star = f({\\bf x}^\\star) = 0$.\n\nWe use the ARD SE covariance function for GP modeling.\nWe call the DACE toolbox~\\cite{Lophaven2002} to find optimal GP hyperparameters for each minimization problem.\nWe draw $10^3$ spectral points to approximate the sample paths of GP posteriors. \nTo minimize the generated sample paths, we use the DIRECT algorithm~\\cite{Finkel2006} whose output solution is set as the initial point of an interior-point optimizer.\nParameters for the DIRECT algorithm including the function tolerance, maximum number of function evaluations, maximum number of iterations, and maximum number of rectangle divisions are $10^{-9}$, $10^3d$, $10^4d$, and $10^4d$, respectively.\nParameters for the interior-point optimizer including the function tolerance, optimality tolerance, maximum number of iterations, maximum number of function evaluations, and termination tolerance on ${\\bf x}$ are $10^{-12}$, $10^{-12}$, $200$, $5\\times10^{3}$, and $10^{-12}$, respectively.  \nWe carry out all experiments for the benchmark functions using the Carya Cluster of the University of Houston.\nWe compare the optimization results obtained from $\\varepsilon$-greedy TS with those from other BO methods, including EI, the lower confidence bound (LCB), the averaging TS, and the generic TS.\n\nTo ensure fair comparisons, we randomly generate 100 initial datasets for each problem using Latin hypercube sampling~\\cite{Forrester2008}, and start each BO method from each of these datasets.\nIn each BO iteration, we record the best-found value of observation error $\\log_{10}(y_{\\min}-f^\\star)$ and the corresponding solution vector. Here, $y_{\\min}$ and $f^\\star$ represent the best observation of the objective function found in each iteration and its true minimum value, respectively.\nWe set the number of initial observations and the number of BO iterations at $N=5d$ and $K=50$ for the 2d Ackley function, and $N=10d$ and $K=200$ for the 6d Rosenbrock function.\n\n\\Cref{fig:TestFunctions_a,fig:TestFunctions_b} show the medians and interquartile ranges obtained from 100 runs of each BO method for the 2d Ackley and 6d Rosenbrock functions, respectively.\n\\Cref{fig:TestFunctions_c,fig:TestFunctions_d} show the medians and interquartile ranges of final solutions.\nIn both cases, the optimization results from $\\varepsilon$-greedy TS for an appropriate $\\varepsilon$ are better than those from one of its two extremes and competitive with the results from the other.\nIn addition, $\\varepsilon$-greedy TS for $\\varepsilon = 0.5$ can provide the best objective values among those from the considered BO methods.\n\nWhile the optimization results in \\cref{fig:TestFunctions} suggest that it is safe to set $\\varepsilon=0.5$ to balance exploitation and exploration, an optimal $\\varepsilon$ may depend on the nature of the problem of interest.\nA problem that requires more exploitation for its optimal solution (e.g., the 2d Ackley function) may benefit from a smaller $\\varepsilon$.\nA problem with more exploration (e.g., the 6d Rosenbrock function) might be better addressed with a larger $\\varepsilon$.\nYet determining whether a black-box objective function leans toward exploitation or exploration for its extrema is often elusive.\nThis, ironically, underscores the rationale behind employing $\\varepsilon$-greedy TS to prevent our search from overly favoring either exploitation or exploration.\nIt is also worth noting that any endeavors to find an optimal value of $\\varepsilon$ for a particular problem should correspond to a specific accuracy level of the initial GP model because lowering the model fidelity always encourages exploration, which is irrespective of the selection of $\\varepsilon$.\n\nAs discussed in \\cref{sec:introduction}, increasing $\\varepsilon$ value results in more exploration of the $\\varepsilon$-greedy strategy, and this holds true for $\\varepsilon$-greedy TS.\nAn increase in $\\varepsilon$ encourages the algorithm to explore more unseen regions of the input variable space, which is confirmed by observing how the algorithm, with varying $\\varepsilon$ values, selects new solution points for the Ackley function, as shown in \\cref{fig:AddedPoints}.\n\nWe also investigate the effect of varying $\\varepsilon$ on the runtime for selecting a new solution point.\n\\Cref{fig:RunTime} shows approximate distributions of runtime for selecting a new solution point by the sample-average TS with $N_\\text{s}=50$, $\\varepsilon$-greedy TS with different $\\varepsilon$ values and $N_\\text{s}=50$, and the generic TS for the 2d Ackley and 6d Rosenbrock functions.\nThere are two distinct clusters of the runtime values for $\\varepsilon$-greedy TS.\nOne cluster aligns with the runtime of the generic TS and the other corresponds to the runtime of the sample-average TS.\nIn addition,\nthe average runtimes for each BO trial of the sample-average TS,\n$\\varepsilon$-greedy TS with $\\varepsilon=$ 0.1, 0.5, 0.9, and the generic TS\nare 124, 102, 64, 18, and 7 s for the 2d Ackley function, respectively.\nThose for the 6d Rosenbrock functions are 1377, 1188, 809, 229, and 111 s, respectively.\nWe see that, increasing $\\varepsilon$ reduces the runtime of each BO trial.\nThis can be attributed to the fact that a larger $\\varepsilon$ requires more exploration of calling the generic TS, which is cheaper than the sample-average TS.\n\n\\subsection{Cantilever Beam}\n\\label{sec:example-beam}\n\nIn this last example, we use $\\varepsilon$-greedy TS to identify the constitutive parameters for simulating the behavior of a steel cantilever beam under a static cyclic loading history RH1, as shown in \\cref{fig:beamModel}. The web and flange of the beam have different material parameters. In the experiment conducted by Yamada and Jiao~\\cite{Yamada2016}, forced vertical displacement was applied at the free end of the beam. The defection angle $\\theta$ was defined as the ratio of the vertical tip displacement $\\Delta$ mm to the beam length $ L = 800$ mm, i.e., $\\theta = \\Delta/L$.\nValues of moment reaction $M$ about $x$-axis at the beam support were measured.\n\nLet ${\\bf x}$ denote the parameters underlying a constitutive model for the cyclic behavior of the beam, for which the detailed description is given in \\cref{sec:example-beam-materialmodel}. Let $M_{t}^\\text{s}$ denote the simulated value of the moment reaction at the $t$th time step of a discretized loading history of RH1 consisting of $T$ steps, and $M_{t}^\\text{m}$ the corresponding measured value.\nWe formulate the following misfit function $f(\\textbf{x})$ for finding an optimal set of ${\\bf x}$~\\citep{Ohsaki2016}:\n\\begin{equation}\\label{eqn13}\n\tf({\\bf x}) = \\sqrt{\\frac{1}{T}\\displaystyle\\sum_{t=1}^{T}\\left[M_{t}^\\text{s}({\\bf x})-M_{t}^\\text{m}\\right]^2}.\n\\end{equation}\n\nTo evaluate $f({\\bf x})$, we model the beam using Abaqus 2022.\nWe reduce the sensitivity of simulation estimates to the finite-element mesh density by generating a fine mesh of 4960 nodes and 3510 linear hexahedral elements of type C3D8, as shown in \\cref{fig:beamModel}.\nThe maximum increment size for each loading history is set as 0.01 s.\n\n\\begin{table}[tb]\n\t\\caption{Material parameter intervals for the cantilever beam.}\n\t\\label{table2}\n\t\\centering\n\t\\begin{tabular}{llcc}\n\t\t\\hline\\noalign{\\smallskip}\n\t\tComponent & Parameter & Lower bound & Upper bound\\\\\n\t\t\\hline\\noalign{\\smallskip}\n\t\t\\multirow{5}{*}{\\parbox[c]{.08\\linewidth}{Web}} & $E$ [GPa] & $175.05$   &$-$  \\\\\n\t\t& $\\nu$ & $0.3$   &$-$  \\\\\n\t\t& $\\sigma_{\\text{y},0}$ [MPa] & $300$ & $340$ \\\\\n\t\t& $Q_\\infty$ [MPa] & $10$ & $100$ \\\\\n\t\t& $b$ & $5$ &$30$ \\\\\n\t\t& $C_1$ [MPa] & $2 \\times 10^3$ &$10^4$ \\\\\n\t\t& $\\gamma_1$ & $10$ &$200$ \\\\\n\t\t\\hline\\noalign{\\smallskip}\n\t\t\\multirow{5}{*}{\\parbox[c]{.08\\linewidth}{Flange}} & $E$ [GPa] & $175.05$   &$-$  \\\\\n\t\t& $\\nu$ & $0.3$   &$-$  \\\\\n\t\t& $\\sigma_{\\text{y},0}$ [MPa] & $270$ & $300$ \\\\\n\t\t& $Q_\\infty$ [MPa] & $10$ & $100$ \\\\\n\t\t& $b$ & $5$ &$30$ \\\\\n\t\t& $C_1$ [MPa] & $2 \\times 10^3$ &$10^4$ \\\\\n\t\t& $\\gamma_1$ & $10$ &$200$ \\\\\n\t\t\\hline\\noalign{\\smallskip}\n\t\\end{tabular}\n\\end{table}\n\n\\subsubsection{Nonlinear Combined Isotropic/Kinematic Hardening}\n\\label{sec:example-beam-materialmodel}\n\nFollowing \\cite{Do2022}, we capture the cyclic elastoplastic behavior of the beam using the nonlinear combined isotropic/kinematic hardening model~\\cite{Lemaitre1994}. This model combines the properties of both isotropic and nonlinear kinematic hardening to describe the relationship between strain and stress states at each time instant.\nThis relationship is established based on the material status that, either elastic or plastic, is detected by the following von Mises yield condition:\n\\begin{equation}\\label{eqn14}\n\tF=\\|\\boldsymbol{\\xi}\\|-\\sqrt{\\frac{2}{3}}\\sigma_{\\text{y}}\\leq 0,\n\\end{equation}\nwhere $\\boldsymbol{\\xi}=\\text{dev}[\\boldsymbol{\\sigma}]-\\text{dev}[\\boldsymbol{\\alpha}]$ is the shifted-stress tensor, $\\text{dev}[\\cdot]$ the deviatoric part of $[\\cdot]$, $\\|\\cdot\\|$ the $L^2$-norm of the tensor, $\\boldsymbol{\\sigma}$ the point stress tensor, $\\boldsymbol{\\alpha}$ the back-stress tensor, and $\\sigma_{\\text{y}}$ the yield stress.\n\nThe isotropic hardening increases the size of the yield surface $F=0$ during the evolution of plastic deformation, while fixing the shape and location of this surface.\nIn this case, $\\boldsymbol{\\xi}$ in \\cref{eqn14} does not involve the back-stress tensor $\\boldsymbol{\\alpha}$, resulting in an isotropic yield surface of the stress.\nThis explains why the isotropic hardening cannot capture the Bauschinger effect, which describes an observation that the elastic region associated with monotonically increasing loading is much larger than that associated with reversed loading.\nSince structural steels exhibit a saturation point of the stress at large deformation, the isotropic hardening for them models the increment of yield surface size using the following monotonically increasing nonlinear function~\\cite{Voce1948}:\n\\begin{equation}\\label{eqn15}\n\t\\sigma_{\\text{y}}=\\sigma_{\\text{y},0}+Q_\\infty[1-\\exp(-b\\epsilon_{\\text{eq}}^{\\text{p}})],\n\\end{equation}\n\\noindent\nwhere $\\sigma_{\\text{y},0}$ is the initial yield stress, $Q_\\infty$ the difference of the stress saturation and $\\sigma_{\\text{y},0}$, $b$ the isotropic saturation rate, and $\\epsilon_{\\text{eq}}^{\\text{p}}$ the current equivalent plastic strain determined based on its previous state and the rate $\\dot{\\epsilon}_{\\text{eq}}^\\text{p}$.\n\nUnlike the isotropic hardening, the kinematic hardening does not change the size and shape of the yield surface during the evolution of plastic deformation.\nInstead, it relocates the center of the yield surface by performing a rigid translation in the evolution direction of the plastic strain.\nThis allows the model to capture the Bauschinger effect.\nIn pure kinematic hardening, the yield function in \\cref{eqn14} has $\\sigma_\\text{y}$ fixed at $\\sigma_{\\text{y},0}$ and the back-stress tensor $\\boldsymbol{\\alpha}$ defined as the superposition of $n_{\\text{k}}$ back-stress components, such that~\\cite{Chaboche1983}\n\\begin{equation}\\label{eqn16}\n\t\\boldsymbol{\\alpha}=\\displaystyle\\sum_{k=1}^{n_{\\text{k}}} \\boldsymbol{\\alpha}_k.\n\\end{equation}\nHere, the evolution of the $k$th component $\\boldsymbol{\\alpha}_k$ is described by a nonlinear kinematic hardening rule as~\\cite{Armstrong1966}\n\\begin{equation}\\label{eqn17}\n\t\\dot{\\boldsymbol{\\alpha}}_k=\\sqrt{\\frac{2}{3}}C_k\\dot{\\epsilon}_{\\text{eq}}^{\\text{p}}\\textbf{n}-\\gamma_k\\dot{\\epsilon}_{\\text{eq}}^{\\text{p}}\\boldsymbol{\\alpha}_k,\n\\end{equation}\nwhere $\\textbf{n}= \\boldsymbol{\\xi}/\\|\\boldsymbol{\\xi}\\|$, and $C_k$ and $\\gamma_k$ the translation and relaxation rates of the back-stress component $k$, respectively.\n\nThe nonlinear combined isotropic/kinematic hardening model makes use of \\cref{eqn15,eqn16,eqn17} for checking the material status at each time instant.\nIf we use one back-stress component for this model, its underlying parameters read $\\textbf{x}=[E,\\nu,Q_\\infty,b,\\sigma_{\\text{y},0},C_1,\\gamma_1]^\\intercal$, where Young's modulus $E$ and Poisson's ratio $\\nu$ are incorporated in the isotropic elastic tensor.\n\n\\begin{figure}[t]\n\t\\centering\n\t\\begin{subfigure}[c]{0.40\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-beamHistory.pdf}\n\t\t\\caption{}\n\t\t\\label{fig:BeamHistory_a}\n\t\\end{subfigure}\n\t\\begin{subfigure}[c]{0.40\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-beamBox.pdf}\n\t\t\\caption{}\n\t\t\\label{fig:BeamHistory_b}\n\t\\end{subfigure}\n\t\\caption{Performance of EI, LCB, averaging TS, generic TS, and $\\varepsilon$-greedy TS methods for the cantilever beam. (a) Optimization histories; (b) Medians and interquartile ranges of final solutions from 10 runs of each BO method.} \n\t\\label{fig:BeamHistory}\n\\end{figure}\n\n\\begin{figure}[t]\n\t\\centering\n\t\\begin{subfigure}[c]{0.32\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-beamRH1prediction.pdf}\n\t\t\\caption{}\n\t\t\\label{fig:Beamprediction_a}\n\t\\end{subfigure}\n\t\\begin{subfigure}[c]{0.32\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-beamRH2prediction.pdf}\n\t\t\\caption{}\n\t\t\\label{fig:Beamprediction_b}\n\t\\end{subfigure}\n\t\\begin{subfigure}[c]{0.32\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-beamRH3prediction.pdf}\n\t\t\\caption{}\n\t\t\\label{fig:Beamprediction_c}\n\t\\end{subfigure}\n\t\\caption{Measured and simulated $M$--$\\theta$ curves of the cantilever beam under different cyclic loading histories. The simulated $M$--$\\theta$ curves correspond to the best parameter set identified from $\\epsilon$-TS and the experimental results from RH1. (a) RH1; (b) RH2; (c) RH3. Here, experimental data obtained from RH2 and RH3 are unseen by the identification process.} \n\t\\label{fig:Beamprediction}\n\\end{figure}\n\n\\subsubsection{Constitutive Parameters by Bayesian Optimization}\n\\label{sec:example-beam-results}\n\nDuring the minimization of misfit function $f(\\mathbf{x})$, we keep Young's modulus and Poisson’s ratio for the web and flange of the beam as constants, set at $E=175.05$ GPa and $\\nu=0.3$~\\cite{Yamada2016}.\nAs a result, we identify ten parameters for the beam in which five parameters are associated with the web and the remaining five with the flange.\nThe permissible range for each parameter is taken from~\\cite{Do2022} and listed in~\\cref{table2}.\nTo initiate BO, we generate ten distinct initial datasets, half containing 50 data points each and the remainder 100 data points each.\nWith a total of $100$ BO iterations planned, we execute the optimization process using the Carya Cluster housed at the University of Houston.\nThis cluster hosts 9984 Intel CPU cores and 327680 Nvidia GPU cores integrated within 188 compute and 20 GPU nodes.\nThis infrastructure allows us to complete a costly beam simulation in three minutes.\n\n\\Cref{fig:BeamHistory} shows the optimization histories and variations in the final solution obtained from different BO methods for the cantilever beam, where $f_{\\min}$ represents the best observation of $f({\\bf x})$ found in each BO iteration.\nThe identification results obtained from $\\varepsilon-$greedy TS with $\\varepsilon = 0.1$ and $0.5$\nare comparable to or better than that from the sample-average TS,\nand are better than that from the generic TS.\nMore notably, $\\varepsilon-$greedy TS for $\\varepsilon = 0.5$ provides the best set of identified parameters.\n\nWe use the best set of parameters identified from $\\epsilon$-greedy TS to predict the $M$--$\\theta$ curves of the beam under RH1 and other two cyclic loading histories RH2 and RH3.\nThe experimental data associated with RH2 and RH3 were not fed to the parameter identification process.\nThe agreement observed between the measured and simulated $M$--$\\theta$ curves for each loading history, as shown in \\cref{fig:Beamprediction}, confirms the reliable prediction performance achieved with the identified parameters.\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nWe introduced $\\varepsilon$-greedy Thompson sampling (TS) to optimizing costly simulation-based objective functions.\nThe method addresses the classic exploitation--exploration dilemma by randomly switching between its two extremes, namely the generic TS and the sample-average TS.\nOur empirical findings reveal that $\\varepsilon$-greedy TS with an appropriate $\\varepsilon$ value robustly achieves the better performance of its two extremes across various problems,\nand can provide the best result in practical engineering applications.\n\nWhile several $\\varepsilon$-greedy algorithms and the generic TS are guaranteed to converge eventually~\\cite{DeAth2021,Garnett2023}, we look forward to a theoretical analysis to elucidate the convergence properties of $\\varepsilon$-greedy TS.\nAdditionally, we are keen on exploring its extensions to high-dimensional settings with support, for example, from a subspace-based approach~\\cite{Nayebi2019,ZhangRD2022gps} or a trust-region method~\\cite{Eriksson2019}, and investigating its adaptability to varying $\\varepsilon$ during the optimization process.\n\n\\section*{Acknowledgments}\n\nThe authors thank the University of Houston for providing startup fund to support this research.\nThanks to Prof. Satoshi Yamada at the University of Tokyo and Prof. Makoto Ohsaki at Kyoto University for providing experimental results of the cantilever beam used in \\cref{sec:example-beam}.\n\n\\bibliographystyle{unsrtnat}\n \n\n\\end{document}\n"}
{"paper_id": "2403-00540", "version": "2403-00540v3", "root_file_path": "d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\data_raw\\2403-00540\\tex\\2403-00540v3\\GreedyBO.tex", "metadata": {"total_length": 57622, "merged_count": 1, "merged_files": ["GreedyBO.tex"], "missing_files": []}, "content": "\\documentclass[12pt]{article}\n\\usepackage{arxiv}\n\n\\usepackage[utf8]{inputenc} \n\\usepackage[T1]{fontenc}    \n\\usepackage{booktabs}       \n\\usepackage{amsfonts}       \n\\usepackage{nicefrac}       \n\\usepackage{microtype}      \n\n\\usepackage[numbers,sort&compress]{natbib}\n\\usepackage{doi}\n\\usepackage{algorithm,algpseudocode}\n\\usepackage{amsmath}\n\\usepackage{amssymb}\n\\usepackage{mathtools}\n\\usepackage{amsthm}\n\\usepackage{multirow, array, threeparttable}\n\\usepackage[font=normalsize,skip=4pt]{caption}\n\\usepackage{subcaption}\n\n\\usepackage[dvipsnames]{xcolor}\n\\usepackage{graphicx}   \n\\usepackage{hyperref}   \n\\hypersetup{\n    colorlinks,\n    linkcolor={blue!60!black},\n    citecolor={red!60!black},\n    urlcolor={blue!80!black}\n}\n\\usepackage[capitalize,nameinlink]{cleveref}\n\\DeclareMathOperator{\\diag}{diag}\n\\newcommand\\myNote[1]{\\textcolor{red!50!black}{(#1})}\n\\newcommand\\myRev[1]{\\textcolor{black!50!black}{#1}}\n\n\\newcommand{\\edit}[1]{\\textcolor{red}{#1}} \n\\newcommand{\\newloc}[1]{\\textcolor{blue!80!black}{#1}} \n\\newcommand{\\tbd}[1]{\\textcolor{orange}{#1}} \n\n\\title{Epsilon-Greedy Thompson Sampling to Bayesian Optimization}\n\n\n\\author{{\\hspace{1mm}Bach Do}\\\\\n\tUniversity of Houston\\\\\n\t\\texttt{bdo3@uh.edu} \\\\\n\t\\And\n{\\hspace{1mm}Taiwo Adebiyi} \\\\\n\tUniversity of Houston\\\\\n\t\\texttt{taadebiyi2@uh.edu} \\\\\n        \\And\n {\\hspace{1mm}Ruda Zhang}\\thanks{Corresponding author.} \\\\\n\tUniversity of Houston\\\\\n\t\\texttt{rudaz@uh.edu} \\\\\n}\n\n\\date{}\n\n\\renewcommand{\\headeright}{ }\n\\renewcommand{\\shorttitle}{$\\varepsilon$-greedy Thompson Sampling}\n\n\\begin{document}\n\\maketitle\n\n\\begin{abstract}\nBayesian optimization (BO) has become a powerful tool for solving simulation-based engineering optimization problems thanks to its ability to integrate physical and mathematical understandings, consider uncertainty, and address the exploitation--exploration dilemma.\nThompson sampling (TS) is a preferred solution for BO to handle the exploitation--exploration trade-off.\nWhile it prioritizes exploration by generating and minimizing random sample paths from probabilistic models---a fundamental ingredient of BO---TS weakly manages exploitation by gathering information about the true objective function after it obtains new observations.\nIn this work, we improve the exploitation of TS by incorporating the $\\varepsilon$-greedy policy, a well-established selection strategy in reinforcement learning.\nWe first delineate two extremes of TS, namely the generic TS and the sample-average TS.\nThe former promotes exploration, while the latter favors exploitation.\nWe then adopt the $\\varepsilon$-greedy policy to randomly switch between these two extremes. \nSmall and large values of $\\varepsilon$ govern exploitation and exploration, respectively.\nBy minimizing two benchmark functions and solving an inverse problem of a steel cantilever beam,\nwe empirically show that $\\varepsilon$-greedy TS equipped with an appropriate $\\varepsilon$\nis more robust than its two extremes,\nmatching or outperforming the better of the generic TS and the sample-average TS.\\footnote[1]{This is the accepted version of the following article:\nDo, B.; Adebiyi, T. \\& Zhang, R.\nEpsilon-Greedy Thompson Sampling to Bayesian Optimization.\n\\textit{Journal of Computing and Information Science in Engineering}, (2024),\n\\url{https://doi.org/10.1115/1.4066858}.\n}\n\\end{abstract}\n\n\\keywords{Thompson sampling \\and Bayesian optimization \\and $\\varepsilon$-greedy policy \\and Exploitation--exploration dilemma \\and Cyclic constitutive law }\n\n\\section{Introduction}\n\\label{sec:introduction}\n\nConsider the following minimization problem:\n\\begin{equation}\\label{eqn1}\n\t\\begin{aligned}\n\t\t\\underset{\\bf x}{\\min} \\ \\ & f({\\bf x})\\\\\n\t\t\\textrm{subject to} \\ \\ \n\t\t& \\bf x \\in \\mathcal{X}, \n\t\\end{aligned}\n\\end{equation} \nwhere ${\\bf x} \\in \\mathbb{R}^d$ is the vector of $d$ input variables\nselected in a bounded, compact domain $\\mathcal{X} \\subset \\mathbb{R}^d$,\nand $f({\\bf x}): \\mathcal{X} \\mapsto \\mathbb{R}$ is a real-valued objective function.\nIn science and engineering applications, the objective function $f({\\bf x})$ is often a black-box function evaluated via costly simulations, which unfortunately hinders the use of any mature, gradient-based numerical optimization algorithms.\n\nBayesian optimization (BO)~\\cite{Snoek2012,Shahriari2016,Frazier2018,Garnett2023,Do2023mfbo,ZhangRD2024mfml} is a global sequential optimization technique well-suited for solving small- and moderate-dimensional optimization problems with costly or black-box objective functions.\nIt finds application in diverse domains of science and engineering, including hyperparameter tuning of machine learning algorithms~\\cite{Snoek2012}, identification of material parameters~\\cite{Karandikar2022,Kuhn2022}, material design~\\cite{Tran2019,Zhang2020}, airfoil design~\\cite{Zheng2020}, adaptive experimental design~\\cite{Greenhill2020}, and accelerator physics~\\cite{Roussel2021}.\nFor a recent comprehensive review of its applications in engineering design, see \\cite{Do2023mfbo}.\nAt its core, BO guides the optimization process using a probabilistic surrogate model $\\widehat{f}$ of the objective function,\nusually a Gaussian process (GP), coupled with an optimization policy~\\cite{Garnett2023}.\nGiven a dataset that has several observations of the input variables and the corresponding objective function values, a GP posterior built from this dataset often serves as the probabilistic model $\\widehat{f}$ encapsulating our beliefs about the black-box objective function.\nThe optimization policy specifying what we value in the dataset is defined through an acquisition function $\\alpha(\\mathbf{x})$.\nThis acquisition function can be deterministic or stochastic, and is cost-effective to evaluate given $\\widehat{f}$, making it convenient for processing optimization.\nDifferent considerations that should be taken into account when formulating $\\alpha(\\mathbf{x})$ from $\\widehat{f}$ include, for example, the value of the objective function~\\cite{Hennig2022}, the information about the minimum location~\\cite{Villemonteix2009,Hennig2012,HernandezLobato2014}, and the information about the minimum value~\\cite{WangZ2017}.\n\nVarious algorithms have been developed to address the classic exploitation--exploration dilemma in BO, where exploitation involves selecting new solutions anticipated to improve the objective function value immediately and exploration focuses on sampling new solutions to reduce uncertainty in predictions of the objective function for a long-term improvement in the solution.\nAn effective BO algorithm is deemed to balance these opposing concerns~\\cite{Garnett2023}.    \nNotable deterministic acquisition functions, such as expected improvement (EI)~\\cite{Jones1998}, weighted EI~\\cite{Sobester2005}, GP upper confidence bound (GP-UCB)~\\cite{Srinivas2010}, knowledge gradient (KG)~\\cite{Frazier2008}, and likelihood-weighting~\\cite{Blanchard2021}, can manage the exploitation--exploration trade-off.\nHowever, they are considered myopic policies, and therefore tend to favor exploitation~\\cite{Hennig2022}.\n\nThompson sampling (TS) is a preferred algorithm to address the exploitation--exploration dilemma in solving multi-armed bandit problems~\\cite{Thompson1933,Chapelle2011,Russo2014,Russo2018}.\nMore specifically, TS selects arms (or actions) from  \na finite set of possible arms over a limited number of iterations.\nEach arm corresponds to a stochastic reward drawn from an unknown distribution.\nThe goal is to craft a sequence of arms that maximizes the cumulative reward assuming that the rewards are independent of time and conditionally independent given the selected arms.\nWhen applied to GP-based BO, TS (or GP-TS \\cite{Garnett2023}) generates a sequence of new solution points using the mechanism that involves random sampling from unknown posterior distributions of the global minimum location $\\bf{x}^\\star$~\\cite{Kandasamy2018}, which is due to the imperfect knowledge of $f$ described by the probabilistic model $\\widehat{f}$.\nRather than finding the posterior distributions directly, TS generates functions from $\\widehat{f}$ and minimizes them for samples of $\\bf{x}^\\star$.\nThus, we can consider TS a BO method that minimizes stochastic acquisition functions generated from the model posteriors for selecting new solution points.\nIt is also worth noting that several information-theoretic optimization policies of BO, such as predictive entropy~\\cite{HernandezLobato2014} and max-value entropy~\\cite{WangZ2017}, compute their acquisition functions based on a set of samples generated by TS.\n\nWhile TS naturally manages the exploitation--exploration trade-off,\nits randomness can reduce the role of exploitation in optimization.\nWhen the GP posterior exhibits high uncertainty at the beginning of the optimization process, TS prioritizes exploration as it can diversify the selection of new solutions when limited information has been gained about the optimal solution $\\bf{x}^\\star$.\nAs the number of observations increases and the GP posterior becomes concentrated, the algorithm transitions to exploiting knowledge about the true objective function.\nThis exploitation strategy, however, is inferior due to the randomness of TS, motivating the quest for an intervention to improve its exploitation. \n\nIn this work, we incorporate the $\\varepsilon$-greedy policy into TS to improve its exploitation.\nThis policy is a selection strategy of reinforcement learning to address the tension between exploitation and exploration~\\cite{Sutton2018}.\nGiven $\\varepsilon \\in (0,1)$, the policy chooses an action by either maximizing an average reward function with probability $1-\\varepsilon$ (i.e., exploitation or greedy) or selecting it randomly with probability $\\varepsilon$ (i.e., exploration).\nThe selection strategy is pure exploitation when $\\varepsilon = 0$, or pure exploration when $\\varepsilon = 1$.  \nSimilarly, our proposed approach implements the generic TS (with probability $\\varepsilon$) for exploration and a new fashion of TS called sample-average TS (with probability $1-\\varepsilon$) for exploitation.\n\nSeveral works have explored the $\\varepsilon$-greedy policy in BO and multi-armed bandit problems. \nDe Ath et al.~\\cite{DeAth2021} proposed two schemes for applying the policy to BO. \nThe first scheme performs exploration (with probability $\\varepsilon$) by randomly selecting a point on the Pareto frontier obtained by simultaneously minimizing the posterior mean and maximizing the posterior standard deviation.\nThe second scheme performs exploration by randomly selecting a point in the input variable space.\nBoth schemes implement exploitation (with probability $1-\\varepsilon$) by minimizing the posterior mean function.\nJin et al.~\\cite{Jin2023} introduced the so-called $\\varepsilon$-exploring TS ($\\varepsilon$-TS) to multi-armed bandit problems.\nGiven the posterior distributions of arms, exploration (with probability $\\varepsilon$) sets the estimated reward function for each arm as a random sample drawn from the associated posterior distribution, while exploitation (with probability $1-\\varepsilon$) sets the estimated reward function as the sample mean function. \nYet the performance of $\\varepsilon$-greedy TS for BO remains unexplored.\n\nOur contributions are as follows:\n(1) $\\varepsilon$-greedy TS to BO, which is a simple, effective method to improve the exploitation of TS;\nand (2) empirical evaluations demonstrating that $\\varepsilon$-greedy TS with an appropriate $\\varepsilon$\nis more robust than its two extremes (i.e., the generic TS and the sample-average TS),\nmatching or outperforming the better of the two across various problems.\n\n\nThe rest of this paper progresses as follows.\n\\cref{sec:background} provides a background essential for the development of $\\varepsilon$-greedy TS.\n\\cref{sec:method} describes the method in detail.\n\\cref{sec:experiments} presents the empirical performance of $\\varepsilon$-greedy TS on minimizing two benchmark functions and finding constitutive parameters for a steel cantilever beam.\nFinally, \\cref{sec:conclusions} concludes this paper.\n\n\\section{Background}\n\\label{sec:background}\nThis section provides an overview of GP modeling (\\cref{sec:GP}), the implementation of the generic TS for BO (\\cref{sec:TS}), and a simple method to generate sample paths from GP posteriors for use of TS (\\cref{sec:sampling}).\n\n\\subsection{Overview of Gaussian Processes}\n\\label{sec:GP}\n\nLet $\\mathcal{D}=\\left\\{ \\left( {\\bf X},{\\bf y}\\right) \\right\\}=\\left\\{ \\left({\\bf x}^i,y^i\\right) \\right\\}_{i=1}^N$ denote a training dataset, where $\\textbf{x}^i$ are observations of a $d$-dimensional vector of input variables and $y^i$ the corresponding observations of the objective function.\nWe wish to build from $\\mathcal{D}$ an observation model $y({\\bf x})=f({\\bf x}) + \\varepsilon_\\text{n}: \\mathbb{R}^d \\mapsto \\mathbb{R}$, where $\\varepsilon_\\text{n} \\sim \\mathcal{N}(0,\\sigma^2_\\text{n})$ is additive zero-mean Gaussian noise with variance $\\sigma^2_\\text{n}$.\nThis observation noise is assumed to be independent and identically distributed.\n\nA GP model assumes that any finite subset of an infinite set of objective function values has a joint Gaussian distribution~\\cite{Rasmussen2006}.\nOftentimes this assumption is encoded using the following GP prior:\n\\begin{equation}\\label{eqn2}\n\tf(\\cdot) \\sim \\mathcal{GP} \\left(0,\\kappa(\\cdot,\\cdot|\\boldsymbol{\\theta})\\right),\n\\end{equation}\nwhere $\\kappa({\\bf x},{\\bf x}'|\\boldsymbol{\\theta}) = \\text{cov}[f({\\bf x}),f({\\bf x}')]: \\mathbb{R}^d \\times \\mathbb{R}^d \\mapsto \\mathbb{R} $ is a positive semi-definite covariance function parameterized by a vector $\\boldsymbol{\\theta}$ of hyperparameters.\nCommon-used covariance functions include squared exponential (SE), Matérn 3/2, Matérn 5/2, and automatic relevance determination squared exponential (ARD SE)~\\cite{Garnett2023,Rasmussen2006}.\nBy conditioning on $\\mathcal{D}$ the model prior given in \\cref{eqn2} and utilizing the observation noise assumption, it is straightforward to show that the vector of observations $\\left[y({\\bf x}^1),\\dots,y({\\bf x}^N)\\right]^\\intercal$ is distributed according to an $N$-variate Gaussian with zero mean and covariance matrix ${\\bf C} = {\\bf K} + \\sigma^2_\\text{n} {\\bf I}_N$, where the $(i,j)$th element of ${\\bf K}$ is $\\kappa({\\bf x}^i,{\\bf x}^j|\\boldsymbol{\\theta})$ and ${\\bf I}_N$ the $N$-by-$N$ identity matrix.\nBy further applying the conditional multivariate Gaussian~\\cite{Bishop2006}, we obtain the posterior predictive distribution of the objective function value at an unseen input variable vector ${\\bf x}_\\star$, i.e., $p\\left(f({\\bf x}_\\star)|{\\bf x}_\\star,\\mathcal{D}\\right) = \\mathcal{N}\\left(\\mu_\\text{f}({\\bf x}_\\star),\\sigma_\\text{f}^2({\\bf x}_\\star)\\right)$, which encodes information about the model we wish to build. The mean and variance of the predictive distribution read \n\\begin{equation}\\label{eqn3}\n\t\\mu_\\text{f}({\\bf x}_\\star)= {\\bf K_\\star}^\\intercal {\\bf C}^{-1} {\\bf Y},\n\\end{equation}\n\\begin{equation}\\label{eqn4}\n\t\\sigma_\\text{f}^2({\\bf x}_\\star)=\\kappa({\\bf x}_\\star,{\\bf x}_\\star|\\boldsymbol{\\theta})\n\t-{\\bf K_\\star}^\\intercal {\\bf C}^{-1} {\\bf K_\\star},\n\\end{equation}\nwhere\n\\begin{equation}\\label{eqn5}\n\t{\\bf K_\\star} = \\left[\\kappa({\\bf x}_\\star,{\\bf x}^1|\\boldsymbol{\\theta}),\\dots,\\kappa({\\bf x}_\\star,{\\bf x}^N|\\boldsymbol{\\theta})\\right]^\\intercal.\n\\end{equation}\n\nTraining GP models involves finding an optimal set of hyperparameters $\\boldsymbol{\\theta}$ that ensures robust predictive performance of trained GPs across unseen input variables.\nExact training methods emulating all possible hyperparameter hypotheses are often impractical, leading to the prevalent use of approximate training methods.\nThese approximate training methods fall into two main categories: deterministic approximations and Monte Carlo methods~\\cite{Mackay2003}.\nDeterministic approximations include the maximum likelihood method identifying a set of hyperparameters that maximizes the parameter likelihood $p(\\mathcal{D}|\\boldsymbol{\\theta})$, and Laplace's method approximating the posterior $p(\\boldsymbol{\\theta}|\\mathcal{D})$ as a multivariate Gaussian with a posterior mean equal to the maximum likelihood estimation and a posterior covariance estimated from Laplace approximation.\nMonte-Carlo methods, such as importance sampling, the Metropolis, Gibbs sampling, and slice sampling, generate approximate samples of $\\boldsymbol{\\theta}$ from the posterior $p(\\boldsymbol{\\theta}|\\mathcal{D})$ given the likelihood $p(\\mathcal{D}|\\boldsymbol{\\theta})$ and a prior $p(\\boldsymbol{\\theta})$.\nSeveral of the aforementioned approximate training methods have been incorporated in reliable GP toolboxes such as DACE~\\cite{Lophaven2002}, GPML~\\cite{Rasmussen2010}, GPstuff~\\cite{Vanhatalo2013}, pyGPs~\\cite{Neumann2015}, and GPflow~\\cite{Matthews2017}.\n\n\\begin{figure}[t]\n\t\\centering\n\t\\begin{subfigure}[c]{0.49\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-samplepaths.pdf}\n\t\t\\caption{}\n\t\t\\label{fig:paths_a}\n\t\\end{subfigure}\n\t\\begin{subfigure}[c]{0.49\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-solutiondistribution.pdf}\n\t\t\\caption{}\n\t\t\\label{fig:paths_b}\n\t\\end{subfigure}\n\t\\caption{Sample paths from the GP posterior for $f(x) = x\\sin(x)$ and distribution of their minimum locations. (a) GP predictions and five sample paths drawn from the GP posterior; (b) Approximate conditional distribution $p( x^\\star|\\mathcal{D})$ obtained from minimum locations of 50 sample paths.} \n\t\\label{fig:paths}\n\\end{figure}\n\n\\begin{algorithm}[t]\n\t\\caption{Generic Thompson sampling for Bayesian optimization}\n\t\\label{alg:TS}\n\t\\begin{algorithmic}[1]\n\t\t\n\t\t\\State \\textbf{input:} input variable domain $\\mathcal{X}$, number of initial observations $N$, threshold for number of BO iterations $K$, standard deviation of observation noise $\\sigma_\\text{n}$\n\t\t\n\t\t\\State Generate $N$ initial samples of ${\\bf x}$\n\t\t\n\t\t\\For {$i=1:N$} \n\t\t\\State $y^i \\gets f({\\bf x}^i) + \\varepsilon_\\text{n}^i$ \n\t\t\\EndFor\n\t\t\n\t\t\\State $\\mathcal{D}^0 \\gets \\{{\\bf x}^i,y^i\\}_{i=1}^N$\n\t\t\n\t\t\\State $\\{{\\bf x}_{\\min},y_{\\min}\\} \\gets \\min\\{y^i,\\, i=1,\\dots, N\\}$\n\t\t\n\t\t\\For {$k=1:K$} \n\t\t\\State Build a GP posterior $\\widehat{f}^k({\\bf x})|\\mathcal{D}^{k-1}$\n\t\t\\State Generate a sample path $g({\\bf x}|\\mathcal{D}^{k-1})$ from $\\widehat{f}^k({\\bf x})|\\mathcal{D}^{k-1}$ \\label{alg:TS_L10}\n\t\t\\State ${\\bf x}^{k} \\gets \\underset{{\\bf x}}{\\mathrm{arg\\,min}} \\ \\ g({\\bf x}|\\mathcal{D}^{k-1})$ s.t. ${\\bf x} \\in \\mathcal{X}$; ${\\bf x} \\notin \\mathcal{D}^{k-1}$\n\t\t\\State $y^{k} \\gets f({\\bf x}^{k}) + \\varepsilon_\\text{n}^k$\n\t\t\\State $\\mathcal{D}^k\\gets\\mathcal{D}^{k-1} \\cup \\{{\\bf x}^{k},y^{k}\\}$\n\t\t\\State $\\{{\\bf x}_{\\min},y_{\\min}\\} \\gets \\min\\{y_{\\min},y^{k}\\}$\n\t\t\\EndFor\n\t\t\n\t\t\\State \\textbf{return} $\\{{\\bf x}_{\\min},y_{\\min}\\}$\n\t\\end{algorithmic}\n\\end{algorithm}\n\\subsection{Bayesian Optimization via Thompson Sampling}\n\\label{sec:TS}\n\nAs briefly described in \\cref{sec:introduction}, the generic TS generates a sequence of solution points ${\\bf x}^k$ $(k=1,\\dots,K)$ by randomly sampling from an unknown posterior distribution $p({\\bf x}^\\star|\\mathcal{D}^{k-1})$ of the global minimum ${\\bf x}^\\star$, where $K$ represents a finite budget on the number of BO iterations.\nLeveraging the fact that ${\\bf x}^\\star$ is fully determined by the objective function, the generic TS follows two simple steps in each iteration.\nIt first generates a sample path from the GP posterior $\\widehat{f}^k({\\bf x})|\\mathcal{D}^{k-1}$ for which the detailed implementation is described in \\cref{sec:sampling}.\nIt then minimizes the generated sample path to find a minimum location ${\\bf x}^\\star$.\nBy doing so, ${\\bf x}^\\star$, assigned as the new solution ${\\bf x}^k$, is a sample from $p({\\bf x}^\\star|\\mathcal{D}^{k-1})$.\n\\cref{alg:TS} summarizes the implementation of the generic TS.\n\n\\Cref{fig:paths} shows sample paths generated from the GP posterior of univariate function $f(x)=x\\sin{x}$ for $x \\in [0,20]$ and an approximate distribution derived from their minimum locations.\nSpecifically, we build from ten observations a GP model with zero mean and SE covariance function.\nFrom this GP model, we generate 50 sample paths, and five of them are shown in \\cref{fig:paths_a}. \nMinimizing the generated sample paths and utilizing a kernel density estimation on the obtained solutions result in an approximate distribution $p({\\bf x}^\\star|\\mathcal{D})$, as shown in \\cref{fig:paths_b}.\nNote that the generic TS in \\cref{alg:TS} generates and minimizes only one sample path in each iteration and does not attempt to approximate the posterior distribution of the minimum location. The following section describes how we can randomly generate sample path $g({\\bf x}|\\mathcal{D}^{k-1})$ in \\cref{alg:TS_L10} of \\cref{alg:TS}.\n\n\n\\subsection{Sampling from Gaussian Process Posteriors}\n\\label{sec:sampling}\n\n\\begin{table*}[t]\n\t\\caption{Spectral density functions for common-used stationary covariance functions~\\cite{RiutortMayol2022}.}\n\t\\label{table1}\n\t\\centering\n\t\\begin{threeparttable}\n\t\\renewcommand{\\arraystretch}{1}\n\t\\begin{tabular}{lll}\n\t\t\\hline\\noalign{\\smallskip}\n\t\t {Covariance} & $\\kappa({\\bf x},{\\bf x}'|\\boldsymbol{\\theta})$ & Spectral density function $S({\\bf s})$\\\\\n\t\t\\hline\\noalign{\\smallskip}\n\t\t\n\t\tSE & $\\sigma_\\text{f}^2 \\exp{\\left(-\\frac{1}{2}\\frac{r^2}{l^2}\\right)}$ \\tnote{(1)}   &$\\sigma_\\text{f}^2(\\sqrt{2\\pi})^d l^d \\exp{\\left(-\\frac{1}{2} l^2 {\\bf s}^\\intercal {\\bf s}\\right)}$  \\\\\n\t\t\\noalign{\\smallskip}\n\t\t\n\t\tARD SE & $\\sigma_\\text{f}^2 \\exp{\\left(-\\frac{1}{2}\\sum_{i=1}^{d}\\frac{(x_i-x_i')^2}{l_i^2}\\right)}$ \\tnote{(2)}    &$\\sigma_\\text{f}^2 (\\sqrt{2\\pi})^d \\left(\\prod_{i=1}^{d}l_i\\right) \\exp{\\left(-\\frac{1}{2} \\sum_{i=1}^{d}l_i^2 s_i^2\\right)}$  \\\\\n\t\t\\noalign{\\smallskip}\n\t\t\n\t\tMatérn 3/2 & $\\sigma_\\text{f}^2 \\left( 1+ \\frac{\\sqrt{3}r}{l}\\right) \\exp{\\left(-\\frac{\\sqrt{3}r}{l}\\right)}$ \\tnote{(3)} & $\\sigma_\\text{f}^2 \\frac{2^d \\pi^{d/2} \\Gamma\\left(\\frac{d+3}{2}\\right)3^{3/2}}{\\frac{1}{2} \\sqrt{\\pi} l^3} \\left(\\frac{3}{l^2} + {\\bf s}^\\intercal {\\bf s} \\right)^{-\\frac{d+3}{2}}$ \\tnote{(4)} \\\\\n\t\t\\noalign{\\smallskip}\n\t\t\n\t\tMatérn 5/2 & $\\sigma_\\text{f}^2 \\left( 1+ \\frac{\\sqrt{5}r}{l} + \\frac{5 r^2}{3 l^2}\\right) \\exp{\\left(-\\frac{\\sqrt{5}r}{l}\\right)}$ \\tnote{(3)} & $\\sigma_\\text{f}^2 \\frac{2^d \\pi^{d/2} \\Gamma\\left(\\frac{d+5}{2}\\right)5^{5/2}}{\\frac{3}{4} \\sqrt{\\pi} l^5} \\left(\\frac{5}{l^2} + {\\bf s}^\\intercal {\\bf s} \\right)^{-\\frac{d+5}{2}}$ \\tnote{(4)}\\\\\n\t\t\\hline\\noalign{\\smallskip}\n\t\t\n\t\\end{tabular}\n\t\\begin{tablenotes}\n\t\t\\item[(1)] $r = \\sqrt{({\\bf x}-{\\bf x}')^\\intercal ({\\bf x}-{\\bf x}')}$ and $\\boldsymbol{\\theta} = [\\sigma_\\text{f},l]^\\intercal$, where $l>0$ and $\\sigma_\\text{f}>0$  are length scale and marginal standard deviation, respectively\n\t\t\n\t\t\\item[(2)] $\\boldsymbol{\\theta} = [\\sigma_\\text{f},l_1,\\dots,l_d]^\\intercal$, where $l_i>0$ $(i=1,\\dots,d)$ is length scale for the $i$th input variable\n\t\t\n\t\t\\item[(3)]\t$r = \\sqrt{({\\bf x}-{\\bf x}')^\\intercal ({\\bf x}-{\\bf x}')}$ and $\\boldsymbol{\\theta} = [\\sigma_\\text{f},l]^\\intercal$\n\t\t\n\t\t\\item[(4)] $\\Gamma(\\cdot) = $ Gamma function\n\t\\end{tablenotes}\n\\end{threeparttable}\n\\end{table*}\n\n\nGiven the GP posterior $\\widehat{f}^k({\\bf x})|\\mathcal{D}^{k-1}$ characterized by a stationary covariance function, we follow the spectral sampling approach to generate a sample path $g({\\bf x}|\\mathcal{D}^{k-1})$, see e.g., \\cite{Rahimi2007} and \\cite{HernandezLobato2014}.\nThis approach approximates the GP prior in \\cref{eqn2} using a Bayesian linear model of randomized basis functions to avoid the computational cost due to exhaustive sampling from marginal distributions of the objective function values at finite sets of input locations, which scales cubically in the number of input locations.\nSuch an approximation is rooted in Bochner’s theorem that guarantees the existence of a Fourier dual $S({\\bf s})$ (${\\bf s} \\in \\mathbb{R}^d$) of the stationary covariance function, which is called spectral density when a finite non-negative Borel measure is interpreted as a distribution~\\cite{Wendland2004}.\nThe spectral density functions associated with several covariance functions of Matérn class and ARD SE are listed in \\cref{table1}.\nOnce $S({\\bf s})$ is determined, we can represent $\\kappa({\\bf x},{\\bf x}'|\\boldsymbol{\\theta})$ using the following randomized feature map~\\cite{Rahimi2007}:\n\\begin{equation}\\label{eqn6}\n\t\\kappa({\\bf x},{\\bf x}'|\\boldsymbol{\\theta}) = \\boldsymbol{\\phi}({\\bf x})^\\intercal \\boldsymbol{\\phi}({\\bf x}'),\n\\end{equation}\nwhere the feature map $\\boldsymbol{\\phi}({\\bf x})\\in \\mathbb{R}^{N_\\text{p}}$ can be approximated by~\\cite{Rahimi2007,HernandezLobato2014}\n\\begin{equation}\\label{eqn7}\n\t\\boldsymbol{\\phi}({\\bf x}) = \\sqrt{2 \\kappa(0|\\boldsymbol{\\theta})/N_\\text{p}} \\cos \\left({\\bf W} {\\bf x} + {\\bf b}\\right).\n\\end{equation}\nHere ${\\bf W} \\in \\mathbb{R}^{N_\\text{p} \\times d}$ and ${\\bf b}\\in \\mathbb{R}^{N_\\text{p}}$ stack $N_\\text{p}$ spectral points generated from the normalized spectral density $p({\\bf s})=S({\\bf s})/\\kappa(0|\\boldsymbol{\\theta})$ and $N_\\text{p}$ points drawn from the uniform distribution $\\mathcal{U}[0,2\\pi]$, respectively. $\\kappa(0|\\boldsymbol{\\theta})$ is well defined because $\\kappa({\\bf x},{\\bf x}'|\\boldsymbol{\\theta})$ is a stationary covariance function satisfying $\\kappa({\\bf x},{\\bf x}'|\\boldsymbol{\\theta}) = \\kappa(\\left\\|{\\bf x}- {\\bf x}' \\right\\||\\boldsymbol{\\theta})$.\n\n\\begin{figure}[t!]\n\t\\centering\n\t\\begin{subfigure}[c]{0.49\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-approximateSE.pdf}\n\t\t\\caption{}\n\t\t\\label{fig:approximatekernel_a}\n\t\\end{subfigure}\n\t\\centering\n\t\\begin{subfigure}[c]{0.49\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-approximateMatern.pdf}\n\t\t\\caption{}\n\t\t\\label{fig:approximatekernel_b}\n\t\\end{subfigure}\n\t\\caption{Approximate covariance functions using random features from different numbers of spectral point samples. (a) SE; (b) Matérn 5/2.}\n\t\\label{fig:approximatekernel}\n\\end{figure}\n\n\n\\begin{algorithm}[t]\n\t\\caption{Generation of sample paths using random features}\n\t\\label{alg:spectral}\n\t\\begin{algorithmic}[1]\n\t\t\\State \\textbf{Input:} dataset $\\mathcal{D}^{k-1}$, type of stationary covariance function $\\kappa(\\cdot,\\cdot|\\boldsymbol{\\theta})$, number of spectral points $N_\\text{p}$, standard deviation of observation noise $\\sigma_\\text{n}$\n\t\t\n\t\t\\State Build a GP model from $\\mathcal{D}^{k-1}$\n\t\t\\State Formulate $p(\\bf s)$ from the posterior covariance function $\\kappa(\\cdot,\\cdot|\\boldsymbol{\\theta})$, see \\cref{table1}\n\t\t\n\t\t\\For {$j=1:N_\\text{p}$} \n\t\t\\State $[{\\bf W}]_j \\sim p(\\bf s)$\n\t\t\\State $[{\\bf b}]_j \\sim \\mathcal{U}[0,2\\pi]$\n\t\t\\EndFor\n\t\t\n\t\t\\State Formulate $\\boldsymbol{\\phi}({\\bf x})$, see \\cref{eqn7}\n\t\t\n\t\t\\State $\\boldsymbol{\\Phi} \\gets [\\boldsymbol{\\phi}({\\bf x}),\\dots,\\boldsymbol{\\phi}({\\bf x}^N)]^\\intercal$\n\t\t\n\t\t\\State Compute $\\boldsymbol{\\mu}_{\\beta}$ and $\\boldsymbol{\\Sigma}_{\\beta}$, see \\cref{eqn9} \\label{alg:spectral_L10}\n\t\t\n\t\t\\State $\\boldsymbol{\\beta} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{\\beta}, \\boldsymbol{\\Sigma}_{\\beta})$\n\t\t\n\t\t\\State \\textbf{return} $g({\\bf x})|\\mathcal{D}^{k-1} \\gets \\boldsymbol{\\beta}^\\intercal \\boldsymbol{\\phi}({\\bf x})$\n\t\\end{algorithmic}\n\\end{algorithm}\n\n\\Cref{fig:approximatekernel} shows approximate SE and Matérn 5/2 covariance functions computed from \\cref{eqn6} for different numbers of spectral point samples.\nWe see that the approximate covariance functions converge to the true ones when the number of spectral points increases.\n\nOnce the approximate feature map is formulated, the GP prior as a kernel machine can be approximated by the following Bayesian linear model: \n\\begin{equation}\\label{eqn8}\n\tf({\\bf x}) \\approx \\boldsymbol{\\beta}^\\intercal \\boldsymbol{\\phi}({\\bf x}),\n\\end{equation}\nwhere the prior of $\\boldsymbol{\\beta}$ is $\\mathcal{N}({\\bf 0}, {\\bf I}_{N_\\text{p}})$.\nBy further conditioning \\cref{eqn8} on the data, we obtain the following mean and covariance of the posterior of $\\boldsymbol{\\beta}$~\\cite{HernandezLobato2014}\n\\begin{subequations}\\label{eqn9}\n\t\\begin{align}\n\t\t\\boldsymbol{\\mu}_{\\beta} & = \\left(\\boldsymbol{\\Phi}^\\intercal \\boldsymbol{\\Phi} +\\sigma_\\text{n}^2 {\\bf I}_{N_\\text{p}}\\right)^{-1} \\boldsymbol{\\Phi}^\\intercal {\\bf y},\\\\\n\t\t\\boldsymbol{\\Sigma}_{\\beta} & = \\left(\\boldsymbol{\\Phi}^\\intercal \\boldsymbol{\\Phi} +\\sigma_\\text{n}^2 {\\bf I}_{N_\\text{p}}\\right)^{-1} \\sigma_\\text{n}^2,\n\t\\end{align}\n\\end{subequations}\nwhere $\\boldsymbol{\\Phi} = [\\boldsymbol{\\phi}({\\bf x}),\\dots,\\boldsymbol{\\phi}({\\bf x}^N)]^\\intercal \\in \\mathbb{R}^{N \\times N_\\text{p}}$. To this end, we can approximate the GP posterior using a Bayesian linear model that weights the randomized basis functions using samples from the posterior of $\\boldsymbol{\\beta}$.\n\nThe generation of $g({\\bf x}|\\mathcal{D}^{k-1})$ using random features is detailed in \\cref{alg:spectral}.\nNote that \\cref{alg:spectral_L10} of \\cref{alg:spectral} involves the Cholesky decomposition of matrix $\\boldsymbol{\\Phi}^\\intercal \\boldsymbol{\\Phi} +\\sigma_\\text{n}^2 {\\bf I}_{N_\\text{p}}$ to facilitate the calculation of its inverse. \nAlternative posterior sampling techniques such as the decoupled method~\\cite{Wilson2020}, which generates GP posteriors by updating the corresponding GP prior samples, can be adopted to increase the sample path accuracy.\n\n\n\\section{Epsilon-greedy Thompson Sampling}\n\\label{sec:method}\n\nIn this section, we first introduce another version of TS called sample-average TS (\\cref{sec:method-avaraging}). We then describe $\\varepsilon$-greedy TS using the sample-average and generic TS in the context of $\\varepsilon$-greedy policy (\\cref{sec:method-epsgreedyTS}).\n\n\\subsection{Sample-average Thompson Sampling}\n\\label{sec:method-avaraging}\n\nTo enhance the exploitation of TS, we implement another version of TS called sample-average TS (or averaging TS)~\\cite{Balandat2020}.\nSpecifically, we call \\cref{alg:spectral} to generate in \\cref{alg:TS_L10} of \\cref{alg:TS} a total of $N_\\text{s}$ sample paths $h^s({\\bf x}|\\mathcal{D}^{k-1})$ ($s=1,\\dots,N_\\text{s}$). These sample paths are used to define the following average sample path:\n\\begin{equation}\\label{eqn10}\n\tg({\\bf x}|\\mathcal{D}^{k-1}) = \\frac{1}{N_\\text{s}}\\sum_{s=1}^{N_\\text{s}} h^s({\\bf x}|\\mathcal{D}^{k-1}).\n\\end{equation}\nAnother approach to generate an average sample path is to use a decoupled representation via pathwise conditioning \\cite{Adebiyi2024bdu,Adebiyi2024roots},\nwhich can be computed at the same cost of generating one sample path,\nregardless of the nominal number $N_s$ of sample paths being averaged.\n\nWe then minimize the average sample path $g({\\bf x}|\\mathcal{D}^{k-1})$ to find a new solution in each iteration of BO.\nThis approach differs from that of Balandat et al.~\\cite{Balandat2020} which generates sample paths using randomized quasi Monte-Carlo techniques.\n\nThe sample-average TS and the generic TS can be considered two extremes of TS. If $N_\\text{s} = \\infty$, $g({\\bf x}|\\mathcal{D}^{k-1})$ in \\cref{eqn10} is indeed the GP posterior mean in \\cref{eqn3} whose minimum promotes exploitation.\nIf $N_s = 1$, the sample-average TS recovers the generic TS that favors exploration. \nThus, there exists an optimal state between these two extremes that corresponds to an unknown value of $N_s$ where TS balances exploitation and exploration.\nHowever, we do not attempt to find such an optimal state in this work.\nWe instead set $N_\\text{s}$ at a sufficiently large value, say $N_\\text{s}=50$, to enforce the exploitation of TS.\n\n\\subsection{$\\varepsilon$-greedy Thompson Sampling}\n\\label{sec:method-epsgreedyTS}\n\nThe two distinct extremes of TS motivate the use of $\\varepsilon$-greedy policy to randomly switch between them.\nSpecifically, we implement the generic TS with probability $\\varepsilon$ to explore the input variable space.\nWe invoke the sample-average TS with probability $1-\\varepsilon$ to guide the search toward exploitation.\nWe do not use the posterior mean in \\cref{eqn3} for exploitation because we observe that computing its derivatives is more expensive than computing the derivatives of the average sample path in \\cref{eqn10}, which adds computational cost if optimization is performed via a gradient-based algorithm.\nIt is also straightforward to recover the generic TS from $\\varepsilon$-greedy TS by simply setting $\\varepsilon=1$ or $N_\\text{s}=1$.\n\\cref{alg:epsilon_greedyTS} details the proposed $\\varepsilon$-greedy TS.\n\nThe proposed algorithm addresses the exploitation--exploration dilemma by the following factors: (1) $\\varepsilon$ \\textendash a small value of $\\varepsilon$ promotes exploitation and (2) $N_\\text{s}$ \\textendash a sufficiently large value of $N_\\text{s}$ encourages exploitation.\n\n\\begin{algorithm}[t]\n\t\\caption{$\\varepsilon$-greedy Thompson sampling for Bayesian optimization}\n\t\\label{alg:epsilon_greedyTS}\n\t\\begin{algorithmic}[1]\n\t\t\n\t\t\\State \\textbf{input:} input variable domain $\\mathcal{X}$, number of initial observations $N$, threshold for number of BO iterations $K$, number of spectral points $N_\\text{p}$, number of sample paths $N_\\text{s}$, value of $\\varepsilon \\in (0,1)$, standard deviation of observation noise $\\sigma_\\text{n}$\n\t\t\n\t\t\\State Generate $N$ samples of ${\\bf x}$\n\t\t\n\t\t\\For {$i=1:N$} \n\t\t\\State $y^i \\gets f({\\bf x}^i) + \\varepsilon_\\text{n}^i$ \n\t\t\\EndFor\n\t\t\n\t\t\\State $\\mathcal{D}^0 \\gets \\{{\\bf x}^i,y^i\\}_{i=1}^N$\n\t\t\n\t\t\\State $\\{{\\bf x}_{\\min},y_{\\min}\\} \\gets \\min\\{y^i,\\, i=1,\\dots, N\\}$\n\t\t\n\t\t\\For {$k=1:K$} \n\t\t\\State Build a GP posterior $\\widehat{f}^k({\\bf x})|\\mathcal{D}^{k-1}$\n\t\t\\State Generate $r \\sim \\mathcal{U}[0,1]$\n\t\t\\If {$r \\leq \\varepsilon$}\n\t\t\\State Sample $g({\\bf x}|\\mathcal{D}^{k-1})$\n\t\t\\Else\n\t\t\\For {$s=1:N_\\text{s}$} \n\t\t\\State Sample $h^s({\\bf x}|\\mathcal{D}^{k-1})$\n\t\t\\EndFor\n\t\t\\State $g({\\bf x}|\\mathcal{D}^{k-1}) \\gets \\frac{1}{N_\\text{s}}\\sum_{s=1}^{N_\\text{s}} h^s({\\bf x}|\\mathcal{D}^{k-1})$\n\t\t\\EndIf\n\t\t\\State ${\\bf x}^{k} \\gets \\underset{{\\bf x}}{\\mathrm{arg\\,min}} \\ \\ g({\\bf x}|\\mathcal{D}^{k-1})$ s.t. ${\\bf x} \\in \\mathcal{X}$; ${\\bf x} \\notin \\mathcal{D}^{k-1}$\n\t\t\\State $y^{k} \\gets f({\\bf x}^{k}) + \\varepsilon_\\text{n}^k$\n\t\t\\State $\\mathcal{D}^k\\gets\\mathcal{D}^{k-1} \\cup \\{{\\bf x}^{k},y^{k}\\}$\n\t\t\\State $\\{{\\bf x}_{\\min},y_{\\min}\\} \\gets \\min\\{y_{\\min},y^{k}\\}$\n\t\t\\EndFor\n\t\t\n\t\t\\State \\textbf{return} $\\{{\\bf x}_{\\min},y_{\\min}\\}$\n\t\\end{algorithmic}\n\\end{algorithm}\n\n\n\n\\section{Numerical Examples}\n\\label{sec:experiments}\n\nWe test the empirical performance of $\\varepsilon$-greedy TS on two challenging minimization problems of the 2d Ackley and 6d Rosenbrock functions (\\cref{sec:example-functions}), and on the identification problem of a steel cantilever beam subjected to cyclic loading (\\cref{sec:example-beam}).\n\n\\subsection{Benchmark Functions}\n\\label{sec:example-functions}\n\nConsider minimization problems of the 2d Ackley and 6d Rosenbrock functions~\\cite{Surjanovic2013}.\nThe analytical expressions for these functions and their global minimums are given as follows: \n\n\\textbf{2d Ackley function}\n\\begin{equation}\\label{eqn11}\n\t\\begin{aligned}\n\t\tf({\\bf x}) = & -a \\exp\\left(-b\\sqrt{\\frac{1}{2} \\sum_{i=1}^{2}x^2_i}\\right) \\\\\n\t\t&- \\exp\\left(\\frac{1}{2} \\sum_{i=1}^{2}\\cos(cx_i)\\right) +a + \\exp(1),\n\t\\end{aligned}\n\\end{equation}\nwhere $a = 20$, $b = 0.2$, and $c = 2\\pi$.\nThe function is evaluated on $\\mathcal{X}=[-10,10]^2$ and has a global minimum at ${\\bf x}^\\star = [0,0]^\\intercal$ with $f^\\star = f({\\bf x}^\\star) = 0$.\n\n\\textbf{6d Rosenbrock function}\n\\begin{equation}\\label{eqn12}\n\tf({\\bf x}) = \\sum_{i=1}^{5} \\left[100(x_{i+1}-x_i^2)^2 + (x_i - 1)^2\\right],\n\\end{equation}\nwhere $\\mathcal{X}=[-5,10]^6$. The function has a global minimum at ${\\bf x}^\\star = [1,\\dots,1]^\\intercal$ with $f^\\star = f({\\bf x}^\\star) = 0$.\n\nWhile $\\varepsilon$-greedy TS can be applied to any stationary covariance function listed in \\cref{table1}, we restrict our experiments to the ARD SE covariance function, assuming no specific prior knowledge about the characteristics of the objective functions.\nWe call the DACE toolbox~\\cite{Lophaven2002} to find optimal GP hyperparameters for each minimization problem.\nWe set $\\sigma_\\text{n} = 10^{-3}$ for $z-$score standardized output observations and generate $10^3$ spectral points to approximate the sample paths of GP posteriors. \nTo minimize the generated sample paths, we use the DIRECT algorithm~\\cite{Finkel2006} whose output solution is set as the initial point of an interior-point optimizer.\nParameters for the DIRECT algorithm including the function tolerance, maximum number of function evaluations, maximum number of iterations, and maximum number of rectangle divisions are $10^{-9}$, $10^3d$, $10^4d$, and $10^4d$, respectively.\nParameters for the interior-point optimizer including the function tolerance, optimality tolerance, maximum number of iterations, maximum number of function evaluations, and termination tolerance on ${\\bf x}$ are $10^{-12}$, $10^{-12}$, $200$, $5\\times10^{3}$, and $10^{-12}$, respectively.  \nWe carry out all experiments for the benchmark functions using the Carya Cluster of the University of Houston.\nWe compare the optimization results obtained from $\\varepsilon$-greedy TS with those from other BO methods, including EI, the lower confidence bound (LCB), the averaging TS, and the generic TS.\n\nTo determine an appropriate value of $N_\\text{s}$ in \\cref{eqn10} which trade-offs between the exploitation and computational cost, our preliminary experiments investigate how changes in $N_\\text{s}$ affect the optimization results for a fixed value of $\\varepsilon$.\nWe observe that when the number of sample paths reaches a sufficiently large value of $N_\\text{s}=50$, the solutions exhibit less sensitivity to changes in $N_\\text{s}$.\n\nTo ensure fair comparisons, we randomly generate 100 initial datasets for each problem using Latin hypercube sampling~\\cite{Forrester2008}, and start each BO method from each of these datasets.\nIn each BO iteration, we record the best-found value of observation error $\\log(y_{\\min}-f^\\star)$ and the corresponding solution vector. Here $y_{\\min}$ and $f^\\star$ represent the best observation of the objective function found in each iteration and its true minimum value, respectively.\nWe set the number of initial observations and the number of BO iterations at $N=5d$ and $K=50$ for the 2d Ackley function, and $N=10d$ and $K=200$ for the 6d Rosenbrock function.\n\n\\begin{figure}[t]\n\t\\centering\n\t\\begin{subfigure}[c]{0.49\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-ackleyHistory.pdf}\n\t\t\\caption{}\n\t\t\\label{fig:TestFunctions_a}\n\t\\end{subfigure}\n\t\\begin{subfigure}[c]{0.49\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-rosenHistory.pdf}\n\t\t\\caption{}\n\t\t\\label{fig:TestFunctions_b}\n\t\\end{subfigure}\n\t\n\t\n\t\\begin{subfigure}[c]{0.49\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-ackleyBox.pdf}\n\t\t\\caption{}\n\t\t\\label{fig:TestFunctions_c}\n\t\\end{subfigure}\n\t\\begin{subfigure}[c]{0.49\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-rosenBox.pdf}\n\t\t\\caption{}\n\t\t\\label{fig:TestFunctions_d}\n\t\\end{subfigure}\n\t\n\t\\caption{Performance of EI, LCB, averaging TS, generic TS, and $\\varepsilon$-greedy TS methods for the 2d Ackley and 6d Rosenbrock functions. Optimization histories for (a) the 2d Ackley function and (b) the 6d Rosenbrock function. Medians and interquartile ranges of final solutions from 100 runs of each BO method for (c) the 2d Ackley function and (d) the 6d Rosenbrock function.} \n\t\\label{fig:TestFunctions}\n\\end{figure}\n\n\\Cref{fig:TestFunctions_a,fig:TestFunctions_b} show the medians and interquartile ranges obtained from 100 runs of each BO method for the 2d Ackley and 6d Rosenbrock functions, respectively.\n\\Cref{fig:TestFunctions_c,fig:TestFunctions_d} show the medians and interquartile ranges of final solutions.\nIn both cases, the optimization results from $\\varepsilon$-greedy TS for an appropriate $\\varepsilon$ are better than those from one of its two extremes and competitive with the results from the other.\nIn addition, $\\varepsilon$-greedy TS for $\\varepsilon = 0.5$ can provide the best objective values among those from the considered BO methods.\n\n\n\\begin{figure}[t]\n\t\\centering\n\t\\begin{subfigure}[c]{0.49\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-ackleyAddedpointaverTS.pdf}\n\t\t\\caption{Averaging TS, $\\varepsilon = 0$}\n\t\t\\label{fig:AddedPoints_a}\n\t\\end{subfigure}\n\t\\begin{subfigure}[c]{0.49\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-ackleyAddedpointepsTS01.pdf}\n\t\t\\caption{$\\varepsilon$-greedy TS, $\\varepsilon = 0.1$}\n\t\t\\label{fig:AddedPoints_b}\n\t\\end{subfigure}\n\t\n\t\\begin{subfigure}[c]{0.49\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-ackleyAddedpointepsTS09.pdf}\n\t\t\\caption{$\\varepsilon$-greedy TS, $\\varepsilon = 0.9$}\n\t\t\\label{fig:AddedPoints_c}\n\t\\end{subfigure}\n\t\\begin{subfigure}[c]{0.49\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-ackleyAddedpointTS.pdf}\n\t\t\\caption{Generic TS, $\\varepsilon = 1$}\n\t\t\\label{fig:AddedPoints_d}\n\t\\end{subfigure}\n\t\\caption{Initial and added points for 2d Ackley function with $N_\\text{s} = 50$ and different values of $\\varepsilon$.} \n\t\\label{fig:AddedPoints}\n\\end{figure}\n\nWhile the optimization results in \\cref{fig:TestFunctions} suggest that it is safe to set $\\varepsilon=0.5$ to balance exploitation and exploration, an optimal $\\varepsilon$ may depend on the nature of the problem of interest.\nA problem that requires more exploitation for its optimal solution (e.g., the 2d Ackley function) may benefit from a smaller $\\varepsilon$.\nA problem with more exploration (e.g., the 6d Rosenbrock function) might be better addressed with a larger $\\varepsilon$.\nYet determining whether a black-box objective function leans toward exploitation or exploration for its extrema is often elusive.\nThis, ironically, underscores the rationale behind employing $\\varepsilon$-greedy TS to prevent our search from overly favoring either exploitation or exploration.\nIn $\\varepsilon$-TS for multi-armed bandits, $\\varepsilon$ can be set as $\\varepsilon = 1/N_{a}$ for achieving simultaneous minimax and asymptotic optimality \\cite{Jin2023}, where $N_{a}$ is the number of arms to select. \nThis, however, does not apply to BO because the number of new solution points, which are similar to arms in multi-armed bandits, is uncountable.\nIt is also worth noting that any endeavors to find an optimal value of $\\varepsilon$ for a particular problem should correspond to a specific accuracy level of the initial GP model because lowering the model fidelity always encourages exploration, which is irrespective of the selection of $\\varepsilon$.\n\nAs discussed in \\cref{sec:introduction}, increasing $\\varepsilon$ value results in more exploration of the $\\varepsilon$-greedy strategy, and this holds true for $\\varepsilon$-greedy TS.\nAn increase in $\\varepsilon$ encourages the algorithm to explore more unseen regions of the input variable space, which is confirmed by observing how the algorithm, with varying $\\varepsilon$ values, selects new solution points for the Ackley function, as shown in \\cref{fig:AddedPoints}.\n\nWe also investigate the effect of varying $\\varepsilon$ on the runtime for selecting a new solution point.\n\\Cref{fig:RunTime} shows approximate distributions of runtime for selecting a new solution point by the sample-average TS with $N_\\text{s}=50$, $\\varepsilon$-greedy TS with different $\\varepsilon$ values and $N_\\text{s}=50$, and the generic TS for the 2d Ackley and 6d Rosenbrock functions.\nThere are two distinct clusters of the runtime values for $\\varepsilon$-greedy TS.\nOne cluster aligns with the runtime of the generic TS and the other corresponds to the runtime of the sample-average TS.\nIn addition,\nthe average runtimes for each BO trial of the sample-average TS,\n$\\varepsilon$-greedy TS with $\\varepsilon=$ 0.1, 0.5, 0.9, and the generic TS\nare 124, 102, 64, 18, and 7 s for the 2d Ackley function, respectively.\nThose for the 6d Rosenbrock functions are 1377, 1188, 809, 229, and 111 s, respectively.\nWe see that increasing $\\varepsilon$ reduces the runtime of each BO trial.\nThis can be attributed to the fact that a larger $\\varepsilon$ requires more exploration of calling the generic TS, which is cheaper than the sample-average TS.\n\n\\begin{figure}[t]\n\t\\centering\n\t\\begin{subfigure}[c]{0.49\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-ackleyTime.pdf}\n\t\t\\caption{2d Ackley}\n\t\t\\label{fig:RunTime_a}\n\t\\end{subfigure}\n\t\\begin{subfigure}[c]{0.49\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-rosenTime.pdf}\n\t\t\\caption{6d Rosenbrock}\n\t\t\\label{fig:RunTime_b}\n\t\\end{subfigure}\n\t\\caption{Approximate distributions of runtime for selecting a new solution point with different $\\varepsilon$ values and $N_\\text{s} = 50$.} \n\t\\label{fig:RunTime}\n\\end{figure}\n\n\n\\subsection{Cantilever Beam}\n\\label{sec:example-beam}\n\nIn this example, we use $\\varepsilon$-greedy TS to identify the constitutive parameters for simulating the behavior of a steel cantilever beam under a static cyclic loading history RH1, as shown in \\cref{fig:beamModel}. The web and flange of the beam have different material parameters. In the experiment conducted by Yamada and Jiao~\\cite{Yamada2016}, forced vertical displacement was applied at the free end of the beam. The defection angle $\\theta$ was defined as the ratio of the vertical tip displacement $\\Delta$ mm to the beam length $ L = 800$ mm, i.e., $\\theta = \\Delta/L$.\nValues of moment reaction $M$ about $x$-axis at the beam support were measured.\n\nLet ${\\bf x}$ represent the vector of parameters underlying a constitutive model for the cyclic behavior of the beam, for which the detailed description is given in \\cref{sec:example-beam-materialmodel}. Let $M_{t}^\\text{s}$ represent the simulated value of the moment reaction at the $t$th time step of a discretized loading history of RH1 consisting of $T$ steps, and $M_{t}^\\text{m}$ the corresponding measured value.\nWe formulate the following misfit function $f(\\textbf{x})$ for finding an optimal set of ${\\bf x}$~\\citep{Ohsaki2016}:\n\\begin{equation}\\label{eqn13}\n\tf({\\bf x}) = \\sqrt{\\frac{1}{T}\\displaystyle\\sum_{t=1}^{T}\\left[M_{t}^\\text{s}({\\bf x})-M_{t}^\\text{m}\\right]^2}.\n\\end{equation}\n\nTo evaluate $f({\\bf x})$, we model the beam using Abaqus 2022.\nWe reduce the sensitivity of simulation estimates to the finite-element mesh density by generating a fine mesh of 4960 nodes and 3510 linear hexahedral elements of type C3D8, see \\cref{fig:beamModel}.\nThe maximum increment size for each loading history is set as 0.01 s.\n\n\\begin{figure*}[t]\n\t\\centering\n\t\\includegraphics[scale=0.55]{figure/fig-beamModel.pdf}\n\t\\caption{The cantilever beam, its finite-element (FE) mesh, and loading history RH1 for cyclic test~\\cite{Do2022}.}\n\t\\label{fig:beamModel}\n\\end{figure*}\n\n\n\\subsubsection{Nonlinear Combined Isotropic/Kinematic Hardening}\n\\label{sec:example-beam-materialmodel}\n\nFollowing \\cite{Do2022}, we capture the cyclic elastoplastic behavior of the beam using the nonlinear combined isotropic/kinematic hardening model~\\cite{Lemaitre1994}. This model combines the properties of both isotropic and nonlinear kinematic hardening to describe the relationship between strain and stress states at each time instant.\nThis relationship is established based on the material status that, either elastic or plastic, is detected by the following von Mises yield condition:\n\\begin{equation}\\label{eqn14}\n\tF=\\|\\boldsymbol{\\xi}\\|-\\sqrt{\\frac{2}{3}}\\sigma_{\\text{y}}\\leq 0,\n\\end{equation}\nwhere $\\boldsymbol{\\xi}=\\text{dev}[\\boldsymbol{\\sigma}]-\\text{dev}[\\boldsymbol{\\alpha}]$ is the shifted-stress tensor, $\\text{dev}[\\cdot]$ the deviatoric part of $[\\cdot]$, $\\|\\cdot\\|$ the $L^2$-norm of the tensor, $\\boldsymbol{\\sigma}$ the point stress tensor, $\\boldsymbol{\\alpha}$ the back-stress tensor, and $\\sigma_{\\text{y}}$ the yield stress.\n\nThe isotropic hardening increases the size of the yield surface $F=0$ during the evolution of plastic deformation, while fixing the shape and location of this surface.\nIn this case, $\\boldsymbol{\\xi}$ in \\cref{eqn14} does not involve the back-stress tensor $\\boldsymbol{\\alpha}$, resulting in an isotropic yield surface of the stress.\nSince structural steels exhibit a saturation point of the stress at large deformation, the isotropic hardening for them models the increment of yield surface size using the following monotonically increasing nonlinear function~\\cite{Voce1948}:\n\\begin{equation}\\label{eqn15}\n\t\\sigma_{\\text{y}}=\\sigma_{\\text{y},0}+Q_\\infty[1-\\exp(-b\\epsilon_{\\text{eq}}^{\\text{p}})],\n\\end{equation}\n\\noindent\nwhere $\\sigma_{\\text{y},0}$ is the initial yield stress, $Q_\\infty$ the difference of the stress saturation and $\\sigma_{\\text{y},0}$, $b$ the isotropic saturation rate, and $\\epsilon_{\\text{eq}}^{\\text{p}}$ the current equivalent plastic strain determined based on its previous state and the associated plastic strain rate $\\dot{\\epsilon}_{\\text{eq}}^\\text{p}$.\n\nUnlike the isotropic hardening, the kinematic hardening does not change the size and shape of the yield surface during the evolution of plastic deformation.\nInstead, it relocates the center of the yield surface by a rigid translation in the evolution direction of the plastic strain.\nThis allows the model to capture the Bauschinger effect.\nIn pure kinematic hardening, the yield function in \\cref{eqn14} has $\\sigma_\\text{y}$ fixed at $\\sigma_{\\text{y},0}$ and the back-stress tensor $\\boldsymbol{\\alpha}$ defined as the superposition of $n_{\\text{k}}$ back-stress components, such that~\\cite{Chaboche1983}\n\\begin{equation}\\label{eqn16}\n\t\\boldsymbol{\\alpha}=\\displaystyle\\sum_{k=1}^{n_{\\text{k}}} \\boldsymbol{\\alpha}_k.\n\\end{equation}\nHere the evolution of the $k$th component $\\boldsymbol{\\alpha}_k$ is described by a nonlinear kinematic hardening rule, as~\\cite{Armstrong1966}\n\\begin{equation}\\label{eqn17}\n\t\\dot{\\boldsymbol{\\alpha}}_k=\\sqrt{\\frac{2}{3}}C_k\\dot{\\epsilon}_{\\text{eq}}^{\\text{p}}\\textbf{n}-\\gamma_k\\dot{\\epsilon}_{\\text{eq}}^{\\text{p}}\\boldsymbol{\\alpha}_k,\n\\end{equation}\nwhere $\\textbf{n}= \\boldsymbol{\\xi}/\\|\\boldsymbol{\\xi}\\|$, and $C_k$ and $\\gamma_k$ the translation and relaxation rates of the back-stress component $k$, respectively.\n\nThe nonlinear combined isotropic/kinematic hardening model makes use of \\cref{eqn15,eqn16,eqn17} for checking the material status at each time instant. If we use one back-stress component for this model, its underlying parameters read $\\textbf{x}=[E,\\nu,Q_\\infty,b,\\sigma_{\\text{y},0},C_1,\\gamma_1]^\\intercal$, where Young's modulus $E$ and Poisson's ratio $\\nu$ are incorporated in the isotropic elastic tensor.\n\n\\begin{table}[tb]\n\t\\caption{Material parameter intervals for the cantilever beam.}\n\t\\label{table2}\n\t\\centering\n\t\\begin{tabular}{llcc}\n\t\t\\hline\\noalign{\\smallskip}\n\t\tComponent & Parameter & Lower bound & Upper bound\\\\\n\t\t\\hline\\noalign{\\smallskip}\n\t\t\\multirow{5}{*}{\\parbox[c]{.08\\linewidth}{Web}} & $E$ [GPa] & $175.05$   &$-$  \\\\\n\t\t& $\\nu$ & $0.3$   &$-$  \\\\\n\t\t& $\\sigma_{\\text{y},0}$ [MPa] & $300$ & $340$ \\\\\n\t\t& $Q_\\infty$ [MPa] & $10$ & $100$ \\\\\n\t\t& $b$ & $5$ &$30$ \\\\\n\t\t& $C_1$ [MPa] & $2 \\times 10^3$ &$10^4$ \\\\\n\t\t& $\\gamma_1$ & $10$ &$200$ \\\\\n\t\t\\hline\\noalign{\\smallskip}\n\t\t\\multirow{5}{*}{\\parbox[c]{.08\\linewidth}{Flange}} & $E$ [GPa] & $175.05$   &$-$  \\\\\n\t\t& $\\nu$ & $0.3$   &$-$  \\\\\n\t\t& $\\sigma_{\\text{y},0}$ [MPa] & $270$ & $300$ \\\\\n\t\t& $Q_\\infty$ [MPa] & $10$ & $100$ \\\\\n\t\t& $b$ & $5$ &$30$ \\\\\n\t\t& $C_1$ [MPa] & $2 \\times 10^3$ &$10^4$ \\\\\n\t\t& $\\gamma_1$ & $10$ &$200$ \\\\\n\t\t\\hline\\noalign{\\smallskip}\n\t\\end{tabular}\n\\end{table}\n\n\n\\subsubsection{Constitutive Parameters by Bayesian Optimization}\n\\label{sec:example-beam-results}\n\nDuring the minimization of misfit function $f(\\mathbf{x})$, we keep Young's modulus and Poisson’s ratio for the web and flange of the beam as constants, set at $E=175.05$ GPa and $\\nu=0.3$~\\cite{Yamada2016}.\nAs a result, we identify ten parameters for the beam of which five parameters are associated with the web and the remaining five with the flange.\nThe permissible range for each parameter is taken from~\\cite{Do2022} and listed in~\\cref{table2}.\nTo initiate BO, we generate ten distinct initial datasets, half containing 50 data points each and the remainder 100 data points each.\nWith a total of $100$ BO iterations planned, we execute the optimization process using the Carya Cluster housed at the University of Houston.\nThis cluster hosts 9984 Intel CPU cores and 327680 Nvidia GPU cores integrated within 188 compute and 20 GPU nodes.\nThis infrastructure allows us to complete a costly beam simulation in three minutes.\n\n\\Cref{fig:BeamHistory} shows the optimization histories and variations in the final solution obtained from different BO methods for the cantilever beam, where $f_{\\min}$ represents the best observation of $f({\\bf x})$ found in each BO iteration.\nThe identification results obtained from $\\varepsilon-$greedy TS with $\\varepsilon = 0.1$ and $0.5$\nare comparable to or better than that from the sample-average TS,\nand are better than that from the generic TS.\nMore notably, $\\varepsilon-$greedy TS for $\\varepsilon = 0.5$ provides the best set of identified parameters.\n\nWe use the best set of parameters identified from $\\varepsilon$-greedy TS to predict the $M$--$\\theta$ curves of the beam under RH1 and other two cyclic loading histories RH2 and RH3.\nThe experimental data associated with RH2 and RH3 were not fed to the parameter identification process.\nThe agreement observed between the measured and simulated $M$--$\\theta$ curves for each loading history, as shown in \\cref{fig:Beamprediction}, confirms the reliable prediction performance achieved with the identified parameters.\n\n\\begin{figure}[t]\n\t\\centering\n\t\\begin{subfigure}[c]{0.49\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-beamHistory.pdf}\n\t\t\\caption{}\n\t\t\\label{fig:BeamHistory_a}\n\t\\end{subfigure}\n\t\\begin{subfigure}[c]{0.49\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-beamBox.pdf}\n\t\t\\caption{}\n\t\t\\label{fig:BeamHistory_b}\n\t\\end{subfigure}\n\t\\caption{Performance of EI, LCB, averaging TS, generic TS, and $\\varepsilon$-greedy TS methods for the cantilever beam. (a) Optimization histories; (b) Medians and interquartile ranges of final solutions from ten runs of each BO method.} \n\t\\label{fig:BeamHistory}\n\\end{figure}\n\n\\begin{figure}[t]\n\t\\centering\n\t\\begin{subfigure}[c]{0.32\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-beamRH1prediction.pdf}\n\t\t\\caption{}\n\t\t\\label{fig:Beamprediction_a}\n\t\\end{subfigure}\n\t\\begin{subfigure}[c]{0.32\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-beamRH2prediction.pdf}\n\t\t\\caption{}\n\t\t\\label{fig:Beamprediction_b}\n\t\\end{subfigure}\n\t\\begin{subfigure}[c]{0.32\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\hsize]{figure/fig-beamRH3prediction.pdf}\n\t\t\\caption{}\n\t\t\\label{fig:Beamprediction_c}\n\t\\end{subfigure}\n\t\\caption{Measured and simulated $M$--$\\theta$ curves of the cantilever beam under different cyclic loading histories. The simulated $M$--$\\theta$ curves correspond to the best parameter set identified from $\\varepsilon$-TS and the experimental results from RH1. (a) RH1; (b) RH2; (c) RH3. Here experimental data obtained from RH2 and RH3 are unseen by the identification process.} \n\t\\label{fig:Beamprediction}\n\\end{figure}\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nWe introduced $\\varepsilon$-greedy Thompson sampling (TS) to optimizing costly simulation-based objective functions.\nThe method addresses the classic exploitation--exploration dilemma by randomly switching between its two extremes, namely the generic TS and the sample-average TS.\nOur empirical findings reveal that $\\varepsilon$-greedy TS with an appropriate $\\varepsilon$ value\nrobustly achieves the better performance of its two extremes across the benchmark problems,\nand can provide the best result in a practical engineering application.\n\nWhile several $\\varepsilon$-greedy algorithms and the generic TS are guaranteed to converge eventually~\\cite{DeAth2021,Garnett2023}, we look forward to a theoretical analysis to elucidate the convergence properties of $\\varepsilon$-greedy TS.\nThe regret bound analysis of $\\varepsilon$-TS \\cite{Jin2023} for multi-armed bandits can serve as a starting point for this endeavor.\nWe are also interested in exploring extensions of $\\varepsilon$-greedy TS to high-dimensional settings where good approximations of the GP posteriors and efficient optimization of TS acquisition functions in high dimensions are of interest. For this, subspace-based approaches~\\cite{Nayebi2019,ZhangRD2022gps} or trust-region methods~\\cite{Eriksson2019} present viable strategies.\nAdditionally, the application of $\\varepsilon$-greedy TS to non-Gaussian likelihood settings and the evolution of $\\varepsilon$ during the optimization process are still open problems. The former may be addressed using a Markov chain Monte Carlo algorithm (see e.g., \\cite{Mazumdar2020,ZhengH2024}) to sample a large set of function values or to sample the weight values $\\boldsymbol{\\beta}$ in \\cref{eqn8} from a non-Gaussian posterior.\n\n\\section*{Acknowledgments}\n\nWe thank the University of Houston for providing startup fund to support this research.\nWe thank Prof. Satoshi Yamada at the University of Tokyo and Prof. Makoto Ohsaki at Kyoto University for providing experimental results of the cantilever beam used in \\cref{sec:example-beam}.\n\n\\clearpage\n\\bibliographystyle{elsarticle-num-names}\n \n\n\\end{document}\n"}
