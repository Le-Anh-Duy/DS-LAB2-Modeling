{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2a2a194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to path: d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\23127011\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Add the project root to sys.path to import src\n",
    "# Adjust this path depending on where you run the notebook\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '23127011'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "print(f\"Added to path: {project_root}\")\n",
    "\n",
    "from src.parser import find_root_tex_file, LatexFlattener, LatexStructureBuilder, LatexContentProcessor\n",
    "from src.processing import ContentDeduplicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8af88986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bibtexparser in c:\\users\\duyla\\anaconda3\\lib\\site-packages (1.4.3)\n",
      "Requirement already satisfied: texsoup in c:\\users\\duyla\\anaconda3\\lib\\site-packages (0.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.3 in c:\\users\\duyla\\anaconda3\\lib\\site-packages (from bibtexparser) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install bibtexparser texsoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d230c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.parser import LatexFlattener, LatexStructureBuilder, LatexContentProcessor, find_root_tex_file\n",
    "from src.cleaner import ReferenceProcessor\n",
    "from src.deduplicator import ReferenceDeduplicator, ContentDeduplicator, replace_citations_in_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc0a8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61d1d5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data raw path: d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\data_raw_real\n",
      "‚úÖ Data raw path found.\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DATA_RAW_PATH = os.path.abspath(os.path.join(os.getcwd(), '..', 'data_raw_real'))\n",
    "\n",
    "print(f\"Data raw path: {DATA_RAW_PATH}\")\n",
    "\n",
    "if not os.path.exists(DATA_RAW_PATH):\n",
    "    print(\"‚ùå Warning: Data raw path does not exist. Please check the path.\")\n",
    "else:\n",
    "    print(\"‚úÖ Data raw path found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac17f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_base_id = '2403-00553'\n",
    "versions_to_process = '2403-00553v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2c84ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Testing directory: d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\data_raw_real\\2403-00553\\tex\\2403-00553v1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n cho Version c·ª• th·ªÉ\n",
    "ver_path = os.path.join(DATA_RAW_PATH, paper_base_id, 'tex', versions_to_process)\n",
    "print(f\"üìÇ Testing directory: {ver_path}\")\n",
    "\n",
    "# 2. B∆∞·ªõc ti√™n quy·∫øt: Flatten n·ªôi dung tr∆∞·ªõc (v√¨ ReferenceProcessor c·∫ßn text ph·∫≥ng)\n",
    "root_file = find_root_tex_file(ver_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59a238d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\data_raw_real\\2403-00553\\tex\\2403-00553v1 ['figures', 'tables'] ['colm2024_conference.bib', 'colm2024_conference.bst', 'colm2024_conference.sty', 'fancyhdr.sty', 'main.bbl', 'main.tex', 'math_commands.tex', 'natbib.sty', 'README.md']\n",
      "d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\data_raw_real\\2403-00553\\tex\\2403-00553v1\\figures ['img'] ['metric_correlations_cnn.tex', 'metric_correlations_news.tex', 'metric_correlations_r1_bert_cnn.tex', 'metric_correlations_xsum.tex', 'timing_ld.tex']\n",
      "d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\data_raw_real\\2403-00553\\tex\\2403-00553v1\\figures\\img [] []\n",
      "d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\data_raw_real\\2403-00553\\tex\\2403-00553v1\\tables [] ['alternative_example_patterns.tex', 'average_lengths_news.tex', 'correlation_all_metrics_length.tex', 'correlation_news.tex', 'correlation_truncs.tex', 'example_patterns.tex', 'example_patterns_instructions.tex', 'example_patterns_story.tex', 'ld_experiments_cnn.tex', 'ld_experiments_cnn_length_controlled.tex', 'ld_experiments_instructions.tex', 'ld_experiments_instructions_length_controlled.tex', 'ld_experiments_story.tex', 'ld_experiments_story_length_controlled.tex', 'ld_experiments_xsum.tex', 'ld_experiments_xsum_length_controlled.tex', 'lengths_instruction_datasets.tex']\n",
      "Found .tex files: ['d:\\\\Coding\\\\School\\\\Y3-K1\\\\Intro2DS\\\\DS - LAB 2\\\\Milestone2_Project\\\\data_raw_real\\\\2403-00553\\\\tex\\\\2403-00553v1\\\\main.tex', 'd:\\\\Coding\\\\School\\\\Y3-K1\\\\Intro2DS\\\\DS - LAB 2\\\\Milestone2_Project\\\\data_raw_real\\\\2403-00553\\\\tex\\\\2403-00553v1\\\\math_commands.tex', 'd:\\\\Coding\\\\School\\\\Y3-K1\\\\Intro2DS\\\\DS - LAB 2\\\\Milestone2_Project\\\\data_raw_real\\\\2403-00553\\\\tex\\\\2403-00553v1\\\\figures\\\\metric_correlations_cnn.tex', 'd:\\\\Coding\\\\School\\\\Y3-K1\\\\Intro2DS\\\\DS - LAB 2\\\\Milestone2_Project\\\\data_raw_real\\\\2403-00553\\\\tex\\\\2403-00553v1\\\\figures\\\\metric_correlations_news.tex', 'd:\\\\Coding\\\\School\\\\Y3-K1\\\\Intro2DS\\\\DS - LAB 2\\\\Milestone2_Project\\\\data_raw_real\\\\2403-00553\\\\tex\\\\2403-00553v1\\\\figures\\\\metric_correlations_r1_bert_cnn.tex', 'd:\\\\Coding\\\\School\\\\Y3-K1\\\\Intro2DS\\\\DS - LAB 2\\\\Milestone2_Project\\\\data_raw_real\\\\2403-00553\\\\tex\\\\2403-00553v1\\\\figures\\\\metric_correlations_xsum.tex', 'd:\\\\Coding\\\\School\\\\Y3-K1\\\\Intro2DS\\\\DS - LAB 2\\\\Milestone2_Project\\\\data_raw_real\\\\2403-00553\\\\tex\\\\2403-00553v1\\\\figures\\\\timing_ld.tex', 'd:\\\\Coding\\\\School\\\\Y3-K1\\\\Intro2DS\\\\DS - LAB 2\\\\Milestone2_Project\\\\data_raw_real\\\\2403-00553\\\\tex\\\\2403-00553v1\\\\tables\\\\alternative_example_patterns.tex', 'd:\\\\Coding\\\\School\\\\Y3-K1\\\\Intro2DS\\\\DS - LAB 2\\\\Milestone2_Project\\\\data_raw_real\\\\2403-00553\\\\tex\\\\2403-00553v1\\\\tables\\\\average_lengths_news.tex', 'd:\\\\Coding\\\\School\\\\Y3-K1\\\\Intro2DS\\\\DS - LAB 2\\\\Milestone2_Project\\\\data_raw_real\\\\2403-00553\\\\tex\\\\2403-00553v1\\\\tables\\\\correlation_all_metrics_length.tex', 'd:\\\\Coding\\\\School\\\\Y3-K1\\\\Intro2DS\\\\DS - LAB 2\\\\Milestone2_Project\\\\data_raw_real\\\\2403-00553\\\\tex\\\\2403-00553v1\\\\tables\\\\correlation_news.tex', 'd:\\\\Coding\\\\School\\\\Y3-K1\\\\Intro2DS\\\\DS - LAB 2\\\\Milestone2_Project\\\\data_raw_real\\\\2403-00553\\\\tex\\\\2403-00553v1\\\\tables\\\\correlation_truncs.tex', 'd:\\\\Coding\\\\School\\\\Y3-K1\\\\Intro2DS\\\\DS - LAB 2\\\\Milestone2_Project\\\\data_raw_real\\\\2403-00553\\\\tex\\\\2403-00553v1\\\\tables\\\\example_patterns.tex', 'd:\\\\Coding\\\\School\\\\Y3-K1\\\\Intro2DS\\\\DS - LAB 2\\\\Milestone2_Project\\\\data_raw_real\\\\2403-00553\\\\tex\\\\2403-00553v1\\\\tables\\\\example_patterns_instructions.tex', 'd:\\\\Coding\\\\School\\\\Y3-K1\\\\Intro2DS\\\\DS - LAB 2\\\\Milestone2_Project\\\\data_raw_real\\\\2403-00553\\\\tex\\\\2403-00553v1\\\\tables\\\\example_patterns_story.tex', 'd:\\\\Coding\\\\School\\\\Y3-K1\\\\Intro2DS\\\\DS - LAB 2\\\\Milestone2_Project\\\\data_raw_real\\\\2403-00553\\\\tex\\\\2403-00553v1\\\\tables\\\\ld_experiments_cnn.tex', 'd:\\\\Coding\\\\School\\\\Y3-K1\\\\Intro2DS\\\\DS - LAB 2\\\\Milestone2_Project\\\\data_raw_real\\\\2403-00553\\\\tex\\\\2403-00553v1\\\\tables\\\\ld_experiments_cnn_length_controlled.tex', 'd:\\\\Coding\\\\School\\\\Y3-K1\\\\Intro2DS\\\\DS - LAB 2\\\\Milestone2_Project\\\\data_raw_real\\\\2403-00553\\\\tex\\\\2403-00553v1\\\\tables\\\\ld_experiments_instructions.tex', 'd:\\\\Coding\\\\School\\\\Y3-K1\\\\Intro2DS\\\\DS - LAB 2\\\\Milestone2_Project\\\\data_raw_real\\\\2403-00553\\\\tex\\\\2403-00553v1\\\\tables\\\\ld_experiments_instructions_length_controlled.tex', 'd:\\\\Coding\\\\School\\\\Y3-K1\\\\Intro2DS\\\\DS - LAB 2\\\\Milestone2_Project\\\\data_raw_real\\\\2403-00553\\\\tex\\\\2403-00553v1\\\\tables\\\\ld_experiments_story.tex', 'd:\\\\Coding\\\\School\\\\Y3-K1\\\\Intro2DS\\\\DS - LAB 2\\\\Milestone2_Project\\\\data_raw_real\\\\2403-00553\\\\tex\\\\2403-00553v1\\\\tables\\\\ld_experiments_story_length_controlled.tex', 'd:\\\\Coding\\\\School\\\\Y3-K1\\\\Intro2DS\\\\DS - LAB 2\\\\Milestone2_Project\\\\data_raw_real\\\\2403-00553\\\\tex\\\\2403-00553v1\\\\tables\\\\ld_experiments_xsum.tex', 'd:\\\\Coding\\\\School\\\\Y3-K1\\\\Intro2DS\\\\DS - LAB 2\\\\Milestone2_Project\\\\data_raw_real\\\\2403-00553\\\\tex\\\\2403-00553v1\\\\tables\\\\ld_experiments_xsum_length_controlled.tex', 'd:\\\\Coding\\\\School\\\\Y3-K1\\\\Intro2DS\\\\DS - LAB 2\\\\Milestone2_Project\\\\data_raw_real\\\\2403-00553\\\\tex\\\\2403-00553v1\\\\tables\\\\lengths_instruction_datasets.tex']\n",
      "d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\data_raw_real\\2403-00553\\tex\\2403-00553v1\n"
     ]
    }
   ],
   "source": [
    "tex_files = []\n",
    "for root, dirs, files in os.walk(ver_path):\n",
    "    # dirs[:] = [d for d in dirs if d not in BLOCKLIST_DIRS]\n",
    "    print(root, dirs, files)\n",
    "    for f in files:\n",
    "        # all_files.add(f)\n",
    "        if f.lower().endswith('.tex'):\n",
    "            tex_files.append(os.path.join(root, f))\n",
    "\n",
    "print(\"Found .tex files:\", tex_files)\n",
    "print(ver_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3352c27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2678fe5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\data_raw_real\\2403-00553\\tex\\2403-00553v1\\main.tex\n"
     ]
    }
   ],
   "source": [
    "print(root_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "def6d8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Root file found: main.tex\n",
      "üìù Kh·ªüi t·∫°o LatexFlattener cho Paper: 2403-00553, Version: 2403-00553v1\n",
      "   Remove references: No\n",
      "üìÑ Flattened content size: 59831 characters\n",
      "\n",
      "--- START REFERENCE PROCESSING ---\n",
      "d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\data_raw_real\\2403-00553\\tex\\2403-00553v1\n",
      "   üîç Scanning references for 2403-00553v1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type online not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Found 41 cited keys in text.\n",
      "         -> Parsed 1 items from main.bbl\n",
      "      Matched 41 references.\n",
      "--- END REFERENCE PROCESSING ---\n",
      "\n",
      "üìä Total Valid References: 41\n",
      "\n",
      "üîé Sample Reference 1:\n",
      "{\n",
      "  \"key\": \"padmakumar2023does\",\n",
      "  \"raw_text\": \"@article{padmakumar2023does,\\n  year = {2023},\\n  journal = {arXiv preprint arXiv:2309.05196},\\n  author = {Padmakumar, Vishakh and He, He},\\n  title = {Does Writing with Language Models Reduce Content Diversity?},\\n}\",\n",
      "  \"type\": \"bib_article\",\n",
      "  \"source\": \"colm2024_conference.bib\"\n",
      "}\n",
      "\n",
      "Key: padmakumar2023does\n",
      "Source Type: bib_article\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# --- TEST RUNNER CHO REFERENCE PROCESSOR ---\n",
    "\n",
    "if not root_file:\n",
    "    print(\"‚ùå Error: Could not find root tex file.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Root file found: {os.path.basename(root_file)}\")\n",
    "    \n",
    "    # Flatten\n",
    "    flattener = LatexFlattener(root_file, paper_id=paper_base_id, version=versions_to_process, remove_references=False)\n",
    "    flat_result = flattener.flatten()\n",
    "    flat_content = flat_result['content']\n",
    "    print(f\"üìÑ Flattened content size: {len(flat_content)} characters\")\n",
    "    # print('\\\\bibliography{apssamp}' in flat_content)\n",
    "# \\bibliography{apssamp}% Produces the bibliography via BibTeX.\n",
    "# \n",
    "    # 3. Kh·ªüi t·∫°o v√† ch·∫°y ReferenceProcessor\n",
    "    print(\"\\n--- START REFERENCE PROCESSING ---\")\n",
    "    print(ver_path)\n",
    "    ref_processor = ReferenceProcessor(paper_base_id, versions_to_process, ver_path)\n",
    "    \n",
    "    # H√†m n√†y s·∫Ω in ra log chi ti·∫øt v·ªÅ vi·ªác t√¨m th·∫•y .bbl hay .bib v√† s·ªë l∆∞·ª£ng ref l·ªçc ƒë∆∞·ª£c\n",
    "    content_after_process, final_refs = ref_processor.process_references(flat_content)\n",
    "    \n",
    "    print(\"--- END REFERENCE PROCESSING ---\\n\")\n",
    "\n",
    "    # 4. Ki·ªÉm tra k·∫øt qu·∫£ ƒë·∫ßu ra\n",
    "    print(f\"üìä Total Valid References: {len(final_refs)}\")\n",
    "    \n",
    "    if len(final_refs) > 0:\n",
    "        print(\"\\nüîé Sample Reference 1:\")\n",
    "        print(json.dumps(final_refs[0], indent=2, ensure_ascii=False))\n",
    "        \n",
    "        # Ki·ªÉm tra xem key c·ªßa ref c√≥ ƒë√∫ng format kh√¥ng\n",
    "        print(f\"\\nKey: {final_refs[0]['key']}\")\n",
    "        print(f\"Source Type: {final_refs[0]['type']}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No references found. Check if the paper actually has citations or if regex needs adjustment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2f87dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'references-{paper_base_id}-v2.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(final_refs, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d759f3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\documentclass{article} \n",
      "\\usepackage{colm2024_conference}\n",
      "\n",
      "\\usepackage{microtype}\n",
      "\\usepackage{hyperref}\n",
      "\\usepackage{url}\n",
      "\\usepackage{booktabs}\n",
      "\\usepackage{graphicx}\n",
      "\\usepackage{multirow}\n",
      "\\usepackage{amsmath}\n",
      "\\usepackage[normalem]{ulem}\n",
      "\\useunder{\\uline}{\\ul}{}\n",
      "\\usepackage{subfigure}\n",
      "\\usepackage{subcaption}\n",
      "\n",
      "\\title{Standardizing the Measurement of Text Diversity: \\\\A Tool and a Comparative Analysis of Scores}\n",
      "\n",
      "\\newcommand*\\samethanks[1][\\value{footnote}]{\\footnotemark[#1]}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\author{\\textbf{Chantal Shaib$^1$\\thanks{Work completed while at Adobe Research.}}\\quad\\quad\n",
      "\\textbf{Joe Barrow$^3$\\samethanks}\\quad\\quad\n",
      "\\textbf{Jiuding Sun$^1$}\\quad\\quad\n",
      "\\textbf{Alexa F. Siu$^2$}\\quad\\quad\\\\\n",
      "\\textbf{Byron C. Wallace$^1$}\\quad\\quad\n",
      "\\textbf{Ani Nenkova$^2$}\\\\\n",
      "$^1$Northeastern University, $^2$Adobe Research, $^3$Pattern Data\\\\\n",
      "\\small\\texttt{\\{shaib.c, sun.jiu, b.wallace\\}@northeastern.edu}\\\\\n",
      "\\small\\texttt{\\{asiu, nenkova\\}@adobe.com} \\\\\n",
      "\\small\\texttt{joe.barrow@patterndataworks.com}\\\\\n",
      "}\n",
      "\n",
      "\\newcommand{\\fix}{\\marginpar{FIX}}\n",
      "\\newcommand{\\new}{\\marginpar{NEW}}\n",
      "\n",
      "\\colmfinalcopy \n",
      "\n",
      "\n",
      "\\begin{document}\n",
      "\\maketitle\n",
      "\n",
      "\\begin{abstract}\n",
      "The diversity across outputs generated by large language models shapes the perception of their quality and utility.  \n",
      "Prompt leaks, templated answer structure, and canned responses across different interactions are readily noticed by people, but there is no standard score to measure this aspect of model behavior. \n",
      "In this work we empirically investigate diversity scores on English texts. \n",
      "We find that computationally efficient compression algorithms capture information similar to what is measured by slow to compute $n$-gram overlap homogeneity scores. \n",
      "Further, a combination of measures---compression ratios, self-repetition of long $n$-grams and Self-BLEU and BERTScore---are sufficient to report, as they have low mutual correlation with each other. \n",
      "The applicability of scores extends beyond analysis of generative models; for example, we highlight applications on instruction-tuning datasets and human-produced texts. \n",
      "We release a diversity score package to facilitate research and invite consistency across reports.\n",
      "\\end{abstract}\n",
      "\n",
      "\\section{Introduction}\n",
      "\n",
      "Evaluation of LLM-generated texts is typically done with respect to accuracy or factuality, e.g., as measured via entailment \\citep{tang-etal-2023-understanding}, or text quality aspects such as coherence and fluency (e.g., estimated using LLMs as evaluators as in \\citealt{Liu2023GEvalNE}). \n",
      "For summarization tasks where references are available, the similarity of generated outputs to these is also often measured. \n",
      "A complementary dimension of model performance is \\emph{diversity}, i.e., how much ``boilerplate'' content is repeated \\emph{across} LLM outputs. \n",
      "\n",
      "The diversity of generated outputs is intuitively important: A model prone to repeating specific sentence constructions or boilerplate turns of phrase across its outputs will likely be deemed lower quality than an LLM capable of more diverse generations, all else being equal. \n",
      "Table~\\ref{table:patterns1} shows example repetitions over a news summarization dataset. We provide detailed examples including which model produced each set of examples in Appendix~\\ref{appendix:more_patterns}.\n",
      "In this work we first analyze commonly reported diversity scores over English language outputs from several LLMs, and identify a few practical, (mostly) independent scores that characterize repetition. \n",
      "We also release \\texttt{diversity}, an open-source Python package that can be used to explore and evaluate the diversity of a generated text datasets.\\footnote{\\hyperlink{https://pypi.org/project/diversity/}{https://pypi.org/project/diversity/} \\label{pkg}}\n",
      "\n",
      "\n",
      "\n",
      "% <BEGIN_FILE: tables/example_patterns.tex>\n",
      "\\begin{table}\n",
      "\\resizebox{\\textwidth}{!}{\n",
      "\\begin{tabular}{@{}lll@{}}\n",
      "\\toprule\n",
      "\\textbf{Model} &  \\textbf{Token Repetition Text}&\\textbf{Pattern-Matched Text}\\\\ \\midrule\n",
      "StableLM       &  \"The article also notes that...\" \\texttt{41/500}&\\textbf{DT NN VBZ DT JJ NN} \\texttt{84/500}\\\\\n",
      " & \"The article also mentions that...\" \\texttt{17/500}&\"The article discusses the recent debate...\" \\\\\n",
      "               &  \"The article notes that the...\" \\texttt{15/500}&\"The book provides a helpful guide...\"                   \\\\\n",
      "               &  \"The article concludes by stating...\" \\texttt{14/500}&\"The article discusses the controversial penalty...\"     \\\\\n",
      "               &  \"The author also notes that...\" \\texttt{11/500}&\"The article discusses the illicit market...\"            \\\\ \\midrule\n",
      "FlanT5         &  \"The boy, who cannot be...\" \\texttt{5/500}&\\textbf{NNP NNP DT JJ NN VBD} \\texttt{37/500}\\\\\n",
      " & \"The study, published in the...\" \\texttt{7/500}&\"Christopher Barry, a 53-year-old man, was...\" \\\\\n",
      "               &  \"The body of a man who...\" \\texttt{3/500}&\"Charles Collins, a 28-year-old man, saved...\"           \\\\\n",
      "               &  \"A man has been charged with...\" \\texttt{5/500}&\"Damian Parks, a 22-year-old student, went...\"           \\\\\n",
      "               &  \"... was last seen at a...\" \\texttt{4/500}&\"Lynn Fast, a 21-year-old mother, claimed...\"            \\\\ \\midrule\n",
      "Llama-2        &  \"The article discusses the...\" \\texttt{15/500}&\\textbf{NNP NN NNP NNP VBZ VBN} \\texttt{42/500}\\\\\n",
      " & \"The article also mentions that...\" \\texttt{28/500}&\"American conductor Marin Alsop has become...\" \\\\\n",
      "               &  \"The article concludes by noting...\" \\texttt{6/500}&\"England striker Andy Cole has warned...\"                \\\\\n",
      "               &  \"The article is about the...\" \\texttt{19/500}&\"Barcelona defender Dani Alves has announced...\"         \\\\\n",
      "               &  \"According to the article, a...\" \\texttt{16/500}&\"England economist Neil Haldane has said....\"            \\\\\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "}\n",
      "\\caption{Examples of exact text-match and repeating part-of-speech patterns in 500 model generated summaries of news articles from the CNN/DM dataset. The number of times the pattern occurs is shown in parenthesis. These patterns appear at a higher frequency in the generated outputs than in the original input data.  Different models are characterized by a different set of repeated patterns. }\n",
      "\\label{table:patterns1}\n",
      "\\end{table}\n",
      "% <END_FILE: tables/example_patterns.tex>\n",
      "\n",
      "\\section{Background}\n",
      "\n",
      "We investigate automatic approaches for characterizing the diversity of outputs from LLMs.\n",
      "Our work is motivated by qualitative observations of such outputs, in which it is easy to notice undesirable repetition of formulaic responses. \n",
      "\n",
      "Lack of diversity may result from repetition of long stretches of text or be due to subtle distributional patterns \n",
      "\\citep{holtzman2019curious,DBLP:journals/tacl/MeisterWC22,DBLP:journals/tacl/MeisterPWC23}. \n",
      "We focus on scores that are likely to capture overt repetition across outputs, and leave for future work similar analysis of semantic and structural diversity scores \\citep{bar-etal-2012-text}.\n",
      "\n",
      "Image captioning and dialog are text generation tasks with a robust body of work quantifying the diversity of produced texts. \n",
      "In both domains, prior work has documented that models tend to produce the same text for different contexts.  \\citet{li-etal-2016-diversity} find that four phrases account for about a third of all turns produced by a conversational agent. \\citet{devlin-etal-2015-language} report that more than half of the automatic captions are repeated verbatim for  different images and most of these captions are seen as exact strings in training. Self-repetition~\\citep{salkar2022self}, i.e., an exact repetition of the same $n$-gram longer than four across different summaries is a way to adapt for a similar analysis in tasks where the generated text is longer, so exact matches rarely occur but repetition is common,  especially relative to the training data \\citep{wang-etal-2023-automated}. \n",
      "\n",
      "The above observations speak to the need for a standardized, easy to use method to quantify diversity. \n",
      "In this work we propose such standardization and show that \\emph{compression ratio} is a fast, convenient to compute score which is sufficient to capture the information in all token/type ratio related alternatives. \n",
      "However, we also find that compression ratios---and all scores considered---are moderately to strongly correlated with the length of texts, complicating interpretation.  \n",
      "Many comparisons remain meaningful when accompanied with information about length, but in others no conclusions can be reliably drawn.\n",
      "\n",
      "The association between length and measures of diversity is well-established in corpus linguistics (see detailed discussion in \\citealt{Brysbaert2016HowMW}). \n",
      "The number of unique words in a corpus is a power function of the total words seen, where the power is less than 1. \n",
      "The number of total words grows linearly, while the number of unique words is sublinear, so longer texts have more repetitions of unique words and $n$-grams than shorter texts \\citep{Covington2010CuttingTG, McCarthy2010MTLDVA}.\n",
      "\n",
      "LLM generations can lead to reduced text diversity in a few ways. \n",
      "\\citet{DBLP:journals/corr/abs-2311-09807} study the effect of consecutive rounds of distillation, in which a language model produces the data on which the next language model is trained. \n",
      "They report dramatic reduction in diversity over 10 iterations. \n",
      "However, this study did not report the length of text produced in consecutive distillation rounds. Given our findings, \n",
      "it would be prudent to check if output lengths remain comparable (or shorter). \n",
      "\\citet{Padmakumar2023DoesWW} show that when people write with the aid of an LLM (e.g., instructGPT) they produce less diverse writing than when they do not. \n",
      "Here we ascertain that these results are independent of length, indicating that there is a genuine reduction in diversity (rather than merely affecting lengths which in turn influence diversity measures). \n",
      "\n",
      "Work across use-cases has shown that reducing the repetition---or,  \n",
      "equivalently, increasing the diversity---in training data yields higher quality models. \n",
      "De-duplicating pre-training data leads to more efficient pre-training and better \n",
      "models that do not repeat pre-training data directly \\citep{lee-etal-2022-deduplicating,DBLP:journals/corr/abs-2303-09540}. \n",
      "Removing fine-tuning summaries with repeated content improves summarization performance \\citep{choubey2023lexical}. \n",
      "Given this, instruction diversity used for instruction tuning LLMs will likely also have implications for mode performance. \n",
      "We assess the diversity of instructions in common instruction tuning datasets. \n",
      "\n",
      "\\section{A Smorgasbord of Text Diversity Scores}\n",
      "\\label{sec:ld}\n",
      "The variety of scores used to measure diversity across a corpus of texts derive from two core ideas: Computing average similarity between pairs of outputs produced by the same model for different inputs, and computing variants of token/type ratio. \n",
      "The former are adapted from common approaches to text generation evaluation by comparing with references, using \n",
      "standard  \n",
      "measures of pairwise similarity; the latter track the diversity of vocabulary measured as the ratio of unique words to total words produced, with the outputs from a model concatenated into a single text. \n",
      "\n",
      "We first describe each score, and then  \n",
      "present insights regarding their mutual redundancy. \n",
      "We also consider their required run-times, which are lengthy for some  metrics and may render them impractical for   \n",
      "analysis of a large number of outputs. \n",
      "All scores are defined for a set of generated texts $D$, each conditioned on its respective input.\n",
      "\n",
      "\n",
      "\\paragraph{Self-BLEU} The quality of text in machine translation, summarization, and image captioning is often reported in terms of overlap with a reference text.\n",
      "This idea can be adapted to measure diversity across different outputs by using one generated text as a reference and measuring the similarity of other outputs against this.\n",
      "Self-BLEU measures similarity between all text pairs in $D$ using BLEU as the similarity score \\citep{Zhu2018TexygenAB}. \n",
      "BLEU can be replaced with an arbitrary similarity score, e.g., ROUGE or BERTScore. \n",
      "These variants are called homogenization scores and have recently been used to compare the diversity of texts produced under several conditions \\citep{Padmakumar2023DoesWW}.\n",
      "\n",
      "\n",
      "\\paragraph{Homogenization Score (ROUGE-L)} \n",
      "Here the similarity score of choice is ROUGE-L (\\citealt{lin-och-2004-automatic}; Eq. \\ref{eq:homogeneity score}). \n",
      "This quantifies overlap in terms of longest common sub-sequences between all pairs of text in a  \n",
      "corpus instead of the fixed $n$-gram size used in other ROUGE variants:\n",
      "\n",
      "\\begin{equation}\n",
      "    \\text{hom}(D) = \\frac{1}{|D|-1} \\sum_{d, d' \\in D; \\: d \\neq d'} \\text{sim}(d, d')\n",
      "    \\label{eq:homogeneity score}\n",
      "\\end{equation}\n",
      "\n",
      "\\paragraph{Homogenization Score (BERTScore)} \n",
      "This homogenization score uses BERTScore to measure similarity between documents in Equation \\ref{eq:homogeneity score}. \n",
      "Unlike the other scores, it does not count the repetition of specific tokens, but instead uses BERT embeddings to (ideally) capture ``semantic'' similarity beyond verbatim $n$-gram matches. \n",
      "\n",
      "\\paragraph{Self-repetition Score} Self-repetition was introduced  \n",
      "to measure the tendency of LMs to repeat long $n$-grams across different outputs \\citep{salkar2022self}. \n",
      "\n",
      "\n",
      "\\begin{equation}\n",
      "\\text{SRS}(d) = \\log \\left ( \\sum\\limits_{i=1}^k N_i + 1 \\right)\n",
      "    \\label{eq:self-rep}\n",
      "\\end{equation}\n",
      "\n",
      "\\noindent Where $k$ is total number of 4-grams in a single document $d \\in D$, and $N_i$ the number of other summaries in which 4-gram $i$ appears. The final score is the sum of $\\text{SRS(}d\\text{)}$ divided by the number of documents in the corpus $D$.\n",
      "\n",
      "\n",
      "\\paragraph{Moving Average Token-Type Ratio}\n",
      "The token-type ratio for a text is  the \n",
      "unique token count divided by the total count of tokens. \n",
      "Moving Average Token Type Ratios (MATTRs) was introduced as a way to measure the lexical dynamics across a text which is insensitive to text length. \n",
      "The score captures the repetition of a given word in segments of text and does not explicitly account for longer repeated sequences  \\citep{Covington2010CuttingTG}.\n",
      "\n",
      "\\paragraph{$N$-Gram Diversity Score} NGD extends the idea of token-type ratio to longer $n$-grams \\citep{Padmakumar2023DoesWW, meister-etal-2023-locally, li-etal-2023-contrastive}. It is defined as a ratio of the unique $n$-gram counts to all $n$-gram counts:\n",
      "\n",
      "\\begin{equation}\n",
      "    \\text{NGD}(D) = \\sum\\limits_{n=1}^4\\frac{\\text{\\# unique }n\\text{-grams in } D\\oplus}{\\text{\\# }n\\text{-grams in } D\\oplus}\n",
      "    \\label{eq:ngram diversity score}\n",
      "\\end{equation}\n",
      "\n",
      "\\noindent Where $D\\oplus$ denotes the dataset $D$  concatenated into a single string. \n",
      "We use four as the maximum $n$-gram length. \n",
      "This method captures repeated \\emph{sequences} in addition to single token diversity. \n",
      "\n",
      "\n",
      "\\paragraph{Hypergeometric Distribution D}\n",
      "The probability of text under a Hypergeometric Distribution D (HD-D) is an another measure of lexical diversity \\citep{McCarthy2010MTLDVA}.\\footnote{For both HD-D and MATTR, we use the implementation provided in the \\texttt{lexical-diversity} package (\\url{https://pypi.org/project/lexical-diversity/}).} HD-D does not capture repetition of sub-sequences.\n",
      "\n",
      "\n",
      "\\paragraph{Compression Ratios}\n",
      "The diversity scores introduced so far are all a function of the number of repeated substrings across outputs. Some measure these over pairs of generated texts, others are computed for a concatenation of all outputs into a single text.  Text compression algorithms are designed to identify redundancy of sequences of variable length in text. \n",
      "\n",
      "\n",
      "We use gZip to compress the concatenated text of all outputs generated by a model. \n",
      "The compression ratio is then the  ratio between the size of the compressed file to that of the original file. \n",
      "High compression ratios imply more redundancy: \n",
      "\n",
      "\\begin{equation}\n",
      "    \\label{eq:compression}\n",
      "    \\text{CR}(D) = \\frac{\\text{size of } D\\oplus}{\\text{compressed size of } D\\oplus}\n",
      "\\end{equation}\n",
      "\n",
      "\\paragraph{Part-of-Speech Compression Ratio} To capture repeated syntactic patterns, we also compute compression ratios for part-of-speech (POS) tag sequences. We use the NLTK POS tagger \\footnote{\\url{https://www.nltk.org/api/nltk.tag.html}} and the Penn Treebank set of 36 tags.\n",
      "\n",
      "\\section{Data and Models}\n",
      "\n",
      "We compute diversity scores for the outputs of six instruction tuned models on the CNN/DailyMail \\citep{Hermann2015TeachingMT} and XSUM \\citep{Narayan2018DontGM} English news summarization datasets. \n",
      "The models are: Llama-2 \\citep{Touvron2023LLaMAOA}, GPT-4 \\citep{OpenAI2023GPT4TR}, FlanT5-XXL \\citep{Longpre2023TheFC}, StableLM \\citep{alpaca, vicuna2023,gpt4all}, Mistral \\citep{Jiang2023Mistral7}, and StableBeluga \\citep{touvron2023llama, mukherjee2023orca}.\\footnote{All models---except GPT-4---downloaded from {\\sc HuggingFace} (\\url{https://huggingface.co/models}).} \n",
      "We selected these models to cover a range of availability (open- and closed-source), and architectures (encoder-decoder, decoder-only).\\footnote{We use prompts for summarization provided by each model, where available. See Appendix~\\ref{appendix:prompts}.}\n",
      "We also measure the diversity of the input news articles, the first three sentences of the articles, and the reference summaries. \n",
      "The lengths of texts vary considerably by source, for reference and model-produced text alike, so we also note average lengths when reporting diversity.\n",
      "\n",
      "\\section{Text Length as Confounder}\n",
      "\n",
      "To keep computational time and costs  \n",
      "manageable, we randomly sample 500 inputs from  \n",
      "CNN/DailyMail and XSUM \n",
      "for analysis. \n",
      "Table~\\ref{table:cnn_diversity} reports  \n",
      "diversity scores for the outputs generated by the six zero-shot \n",
      "LLMs for these inputs.  \n",
      "\n",
      "\n",
      "% <BEGIN_FILE: tables/correlation_news.tex>\n",
      "\n",
      "\\begin{table}\n",
      "    \\centering\n",
      "    \\resizebox{0.75\\textwidth}{!}{\n",
      "\\begin{tabular}{@{}lllllllll@{}}\n",
      "\\toprule\n",
      "\\textbf{CR} & \\textbf{\\begin{tabular}[c]{@{}l@{}}CR: POS\\end{tabular}}& \\textbf{NGD} & \\textbf{\\begin{tabular}[c]{@{}l@{}}Self-\\\\Rep.\\end{tabular}}& \\textbf{\\begin{tabular}[c]{@{}l@{}}Hom. \\\\(R-L)\\end{tabular}}& \\textbf{\\begin{tabular}[c]{@{}l@{}}Hom. \\\\(BERT)\\end{tabular}}& \\textbf{\\begin{tabular}[c]{@{}l@{}}Self-\\\\BLEU\\end{tabular}} & \\textbf{MATTR} & \\textbf{HD-D} \\\\ \\midrule\n",
      "0.83        & 0.695& 0.885        & 0.87               & 0.841               & 0.921                & 0.991              & 0.799          & 0.654         \\\\ \\bottomrule\n",
      "\\end{tabular}\n",
      "}\n",
      "    \\caption{Score correlations for each text diversity score between the CNN/DM and XSUM datasets.}\n",
      "    \\label{table:correlations_news}\n",
      "\\end{table}\n",
      "% <END_FILE: tables/correlation_news.tex>\n",
      "\n",
      "\n",
      "% <BEGIN_FILE: figures/metric_correlations_cnn.tex>\n",
      "\\begin{figure}\n",
      "\\centering\n",
      "    \\includegraphics[width=0.6\\textwidth]{figures/img/corr_cnn.pdf}\n",
      "    \\caption{Correlations between text diversity scores on CNN/DM. Compression ratio correlates strongly with most other diversity metrics.}\n",
      "    \\label{fig:corr_cnn}\n",
      "\\end{figure}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "% <END_FILE: figures/metric_correlations_cnn.tex>\n",
      "\n",
      "\n",
      "% <BEGIN_FILE: tables/ld_experiments_cnn.tex>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\begin{table}\n",
      "\\begin{centering}\n",
      "\\resizebox{\\textwidth}{!}{\n",
      "\\begin{tabular}{@{}lllllllllll}\n",
      "\\toprule\n",
      "\\textbf{Model}                                                     &   \\textbf{\\begin{tabular}[c]{@{}l@{}}Avg.\\\\Length\\end{tabular}}&\\textbf{\\begin{tabular}[c]{@{}l@{}}CR\\\\(‚Üì) \\end{tabular}}& \\textbf{\\begin{tabular}[c]{@{}l@{}}CR: POS\\\\(‚Üì) \\end{tabular}}& \\textbf{\\begin{tabular}[c]{@{}l@{}}NGD\\\\(‚Üë) \\end{tabular}} &  \\textbf{\\begin{tabular}[c]{@{}l@{}}Self-\\\\Rep. (‚Üì) \\end{tabular}}&\\textbf{\\begin{tabular}[c]{@{}l@{}}Hom. \\\\(R-L) (‚Üì)\\end{tabular}}& \\textbf{\\begin{tabular}[c]{@{}l@{}}Hom. \\\\(BERT) (‚Üì)\\end{tabular}}& \\textbf{\\begin{tabular}[c]{@{}l@{}}Self-\\\\BLEU (‚Üì)\\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}l@{}}MATTR\\\\(‚Üë) \\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}l@{}}HD-D\\\\(‚Üë) \\end{tabular}} \\\\\\midrule\n",
      " Article                                                     &  452.25&2.615& 5.544& 2.637          & 6.216& 0.118                                                                  & 0.696                                                               & 0.003          & 0.837          &0.896           \\\\\n",
      " Article (Lead 3) &  75.87                                                                                      &2.369& 5.497& 3.041          & 4.276& 0.105                                                                  & 0.686                                                               & 0              & 0.856          &0.892           \\\\\n",
      " Reference                                                   &  51.78                                                                                      &2.277& 5.330& 3.164          & 3.842& 0.074                                                                  & 0.683                                                               & 0              & 0.875          &0.919           \\\\ \\midrule\n",
      "StableLM                                                    &   132.71&\\textbf{2.724}& \\textbf{5.940}&2.673 &  4.940&\\textbf{0.126}                                                         & 0.689                                                               & 0.002          & 0.792 & 0.867  \\\\\n",
      " Mistral                                                     & 114.88& 2.499& \\textbf{5.621}& 2.926          & 4.688& \\textbf{0.123}                                                                  & \\underline{\\emph{0.697}}                                                               & \\underline{\\emph{0.036}}          & 0.831          &0.880            \\\\\n",
      " Llama-2                                                     & 106.52                                                                                     & \\underline{\\emph{2.543}}& \\underline{\\textbf{5.684}}& \\underline{2.874}          & 4.159*& \\underline{\\textbf{0.125}}                                                                  & \\underline{\\emph{0.694}}                                                               & 0.001          & \\underline{0.820}           &\\underline{\\emph{0.873}}           \\\\\n",
      " StableBeluga                                                & 91.17                                                                                      & 2.452& \\underline{\\textbf{5.644}}& 3.028          & \\underline{4.467}& \\textbf{0.121}                                                                  & \\underline{\\emph{0.702}}                                                      & \\underline{\\emph{0.047}} & 0.846          &0.889           \\\\\n",
      " FlanT5                                                      & 63.84& \\underline{\\textbf{2.453}}& \\textbf{5.608}& \\underline{\\emph{2.939}}          & 3.608*& 0.084& 0.667& 0& \\underline{\\emph{0.833}}          &\\underline{\\textbf{0.887}}           \\\\\n",
      "GPT-4                                                       &   55.4                                                                                       &2.361& \\textbf{5.463}&\\textbf{3.124}&  \\underline{\\emph{3.909}}&\\underline{\\emph{0.098}}                                                                  & \\underline{\\emph{0.684}}                                                               & \\textbf{\\underline{\\emph{0.001}}}          & 0.853& \\textbf{0.891}\\\\\\midrule\n",
      "\\end{tabular}\n",
      "}\n",
      "\\caption{Diversity scores for the CNN/Daily Mail dataset. Arrows indicate direction of \\textit{more diversity}. Values indicating less diversity compared to at least one text source that produces longer human texts are bolded; models with scores that are less diverse than those from a model that produces longer summaries are underlined. An asterisk indicates a model more diverse than a shorter human text.}\n",
      "\\label{table:cnn_diversity}\n",
      "\\end{centering}\n",
      "\n",
      "\\end{table}\n",
      "% <END_FILE: tables/ld_experiments_cnn.tex>\n",
      "\n",
      "\n",
      "The top panel of the table shows scores for human-written texts: the original article given as input for summarization, the baseline summary consisting of the first three sentences of the news article and the human reference summary. These scores serve as a reference point with respect to which to interpret the scores for models. The expectation is that the human texts are more diverse than those produced by LLMs, with the caveat that the texts were scraped from the web, so may contain HTML, ads, and page layout artefacts which might be repetitive \\citep{salkar2022self}.   \n",
      "\n",
      "The human texts differ  by length and the sources of longer texts appear to be less diverse. \n",
      "The association between the length of the produced texts and their diversity is similarly pronounced in the XSUM dataset, as seen Table \\ref{appendix_table:xsum_diversity}.  \n",
      "Text length as a confounder for diversity has been reported in prior work \\citep{salkar2022self}, along with potential methods to adjust for this, e.g., sampling blocks of fixed size \\citep{Covington2010CuttingTG}.  \n",
      "\n",
      "Table~\\ref{table:metric_length_correlation}  \n",
      "reports correlations between the number of words produced by each model and\n",
      "diversity scores.  \n",
      "All scores of the token/type ratio family are highly correlated with length, while the pairwise similarity ones are only moderately correlated. Self-BLEU has low correlation with length.\n",
      "\n",
      "\n",
      "% <BEGIN_FILE: tables/ld_experiments_xsum.tex>\n",
      "\\begin{table*}\n",
      "\\begin{centering}\n",
      "\\resizebox{\\textwidth}{!}{\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\begin{tabular}{@{}lllllllllll}\n",
      "\\toprule\n",
      "\\textbf{Model}& \\textbf{\\begin{tabular}[c]{@{}l@{}}Avg.\\\\Length\\end{tabular}}&\\textbf{\\begin{tabular}[c]{@{}l@{}}CR\\\\(‚Üì) \\end{tabular}}& \\textbf{\\begin{tabular}[c]{@{}l@{}}CR: POS\\\\(‚Üì) \\end{tabular}}& \\textbf{\\begin{tabular}[c]{@{}l@{}}NGD\\\\(‚Üë) \\end{tabular}} &  \\textbf{\\begin{tabular}[c]{@{}l@{}}Self-\\\\Rep. (‚Üì) \\end{tabular}}&\\textbf{\\begin{tabular}[c]{@{}l@{}}Hom. \\\\(R-L) (‚Üì)\\end{tabular}}& \\textbf{\\begin{tabular}[c]{@{}l@{}}Hom. \\\\(BERT) (‚Üì)\\end{tabular}}& \\textbf{\\begin{tabular}[c]{@{}l@{}}Self-\\\\BLEU (‚Üì)\\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}l@{}}MATTR\\\\(‚Üë) \\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}l@{}}HD-D\\\\(‚Üë) \\end{tabular}} \\\\\\midrule\n",
      " Article    &  310.20&2.511            & 5.555& 2.756            & 5.643& 0.110                                                                            & 0.695                                                                        & 0.002                  & 0.838              &0.892              \\\\ \n",
      " Article (Lead-3) &  55.94&2.316            & 5.454& 3.107            & 3.999& 0.103                                                                           & 0.683                                                                        & 0                      & 0.860               &0.891              \\\\ \n",
      " Reference     &  21.04                                         &2.276            & 5.409& 3.211            & 2.914& 0.081                                                                           & 0.673                                                                        & 0                      & 0.877              &0.888              \\\\\\midrule\n",
      " StableLM  & 109.20& 2.745& 6.008& 2.636& 4.687& 0.130                                                                  & 0.695                                                                        & 0.002                  & 0.78               &0.854              \\\\\n",
      " Llama-2  & 102.48& 2.634            & 5.802& 2.795            & 4.618& 0.128                                                                           & 0.687                                                                        & 0.002                  & 0.795              &0.858              \\\\\n",
      " Mistral  & 95.18                                         & 2.531            & 5.708& 2.911            & 4.495& \\underline{\\emph{0.132}}& \\underline{\\emph{0.698}}& \\underline{\\emph{0.044}}                  & 0.819              &0.867              \\\\\n",
      " StableBeluga  & 88.46& 2.461            & 5.673& 2.992            & 4.418& 0.124                                                                           & \\underline{\\emph{0.698}}& \\underline{\\emph{0.046}}& 0.837              &0.88               \\\\ \n",
      "GPT-4   &  62.15&2.394            &5.531*& 3.079            &  4.041&0.104                                                                           & 0.682                                                                        & 0                      & 0.848              & 0.886              \\\\\n",
      "FlanT5    &  20.93&\\textbf{\\underline{\\emph{2.666}}}            &\\textbf{\\underline{\\emph{6.222}}}& \\textbf{\\underline{\\emph{2.743}}}            &  \\textbf{\\underline{\\emph{2.868}}}&\\textbf{\\underline{\\emph{0.114}}}                                                                           & 0.665                                                                        & 0.001                  & \\textbf{\\underline{\\emph{0.756}}}& \\textbf{0.842}\\\\ \n",
      "\\midrule\n",
      "\\end{tabular}\n",
      "}\n",
      "\\caption{Diversity scores for XSUM summaries. Arrow indicate the direction of more diverse texts for each score.}\n",
      "\\label{appendix_table:xsum_diversity}\n",
      "\\end{centering}\n",
      "\n",
      "\\end{table*}\n",
      "% <END_FILE: tables/ld_experiments_xsum.tex>\n",
      "\n",
      "\n",
      "% <BEGIN_FILE: tables/ld_experiments_xsum_length_controlled.tex>\n",
      "\\begin{table}\n",
      "\\begin{centering}\n",
      "\\resizebox{0.7\\textwidth}{!}{\n",
      "\\begin{tabular}{llllll}\n",
      "\\midrule\n",
      "\\textbf{Model}& \\multicolumn{1}{l}{\\textbf{\\begin{tabular}[c]{@{}l@{}}CR\\\\(‚Üì) \\end{tabular}}} &  \\textbf{\\begin{tabular}[c]{@{}l@{}}CR: POS\\\\(‚Üì) \\end{tabular}}&\\multicolumn{1}{l}{\\textbf{\\begin{tabular}[c]{@{}l@{}}Self-\\\\Rep. (‚Üì)\\end{tabular}}}  & \\multicolumn{1}{l}{\\textbf{\\begin{tabular}[c]{@{}l@{}}Hom. \\\\(BERT) (‚Üì)\\end{tabular}}} & \\multicolumn{1}{l}{\\textbf{\\begin{tabular}[c]{@{}l@{}}Self-\\\\BLEU (‚Üì)\\end{tabular}} }   \\\\\\midrule\n",
      " Article                                 & 2.162                                                       &  5.095&2.719& 0.666                                                                                                                    &0                                                                  \\\\\n",
      " Article (Lead 3)& 2.179&  5.093&2.719& 0.663&0\\\\\n",
      " Reference                               & 2.230                                                        &  5.314&2.663& 0.667                                                                                                                    &0                                                                  \\\\ \\midrule\n",
      "Llama-2                                 & 2.345                                                       &  5.636&2.919& 0.663                                                                                                                    & 0.002\\\\\n",
      "GPT-4                                   & 2.213                                                       &  5.425&2.666& 0.663                                                                                                                    & 0                                                                  \\\\\n",
      "FlanT5                                  & 2.490&  5.737&2.707& 0.665& 0.001                                                              \\\\\n",
      "StableLM                                & 2.342                                                       &  5.521&2.823& 0.664                                                                                                                    & 0.001                                                              \\\\\n",
      "Mistral                                 & 2.308                                                       &  5.689&2.736& 0.659                                                                                                                    & 0                                                                  \\\\\n",
      "StableBeluga                            & 2.210                                                        &  5.436&2.663& 0.659                                                                                                                    & 0                                                                  \\\\\\midrule\n",
      "\\end{tabular}\n",
      "}\n",
      "\\caption{Diversity metrics for XSUM summaries, with outputs from each model truncated to the length of the shortest. All scores are directly comparable.}\n",
      "\\label{appendix_table:xsum_diversity_length_controlled}\n",
      "\\end{centering}\n",
      "\n",
      "\\end{table}\n",
      "% <END_FILE: tables/ld_experiments_xsum_length_controlled.tex>\n",
      "\n",
      "\n",
      "\n",
      "% <BEGIN_FILE: tables/correlation_all_metrics_length.tex>\n",
      "\\begin{table*}\n",
      "\\begin{centering}\n",
      "\\resizebox{0.75\\textwidth}{!}{\n",
      "\n",
      "\\begin{tabular}{lllllllll}\n",
      "\\toprule\n",
      "  \\textbf{CR} & \\textbf{\\begin{tabular}[c]{@{}l@{}}CR: POS\\end{tabular}}& \\textbf{NGD} &  \\textbf{\\begin{tabular}[c]{@{}l@{}}Self-\\\\Rep.\\end{tabular}}&\\textbf{\\begin{tabular}[c]{@{}l@{}}Hom. \\\\(R-L)\\end{tabular}}& \\textbf{\\begin{tabular}[c]{@{}l@{}}Hom. \\\\(BERT)\\end{tabular}}& \\textbf{\\begin{tabular}[c]{@{}l@{}}Self-\\\\BLEU\\end{tabular}} & \\textbf{MATTR} & \\textbf{HD-D} \\\\\\midrule\n",
      "    0.867& 0.832& 0.81& 0.904& 0.875& 0.579& 0.235& 0.79&0.855\\\\\\midrule\n",
      "\\end{tabular}\n",
      "}\n",
      "\\caption{Correlation between each score and total number of words (concatenated text)  for CNN/Daily Mail.}\n",
      "\\label{table:metric_length_correlation}\n",
      "\\end{centering}\n",
      "\n",
      "\\end{table*}\n",
      "% <END_FILE: tables/correlation_all_metrics_length.tex>\n",
      "\n",
      "\n",
      "\\section{Diversity of Model Summaries}\n",
      "\n",
      "The confound of length complicates reporting. On both CNN/DM and XSUM (cf. Tables \\ref{table:cnn_diversity} and \\ref{appendix_table:xsum_diversity}), StableLM produces the longest summaries. All scores indicate that these are the least diverse, most likely due to the length confound. In both sets of results, we look for models that produce shorter summaries that are less diverse. These findings are notable and hold, despite length differences.\n",
      "\n",
      "Three types of differences are marked in the tables. Model summaries that are shorter but less diverse than human summaries are marked in bold. Human texts here are written by journalists, so the expectation is that they would be more diverse. More bold entries in a column indicate that the score captures difference between human and machine diversity, which is a desirable trait. Underlined entries mark models that are less diverse than other models that produce longer summaries. The more underlined entries there are for a model, the more indicators there are that its output is less diverse.\n",
      "Asterisks mark models that appear more diverse than a human text of shorter length. \n",
      "\n",
      "\n",
      "The most interesting diversity scores are ones that capture differences between human and automatically produced text, without necessarily committing to an interpretation of which source is preferable. Human evaluation in future work will address this question. On the CNN/DM dataset, homogenization with BERTScore and MATTR are the two scores that detect no differences between human and model texts. \n",
      "BERTScore does not detect such differences on the XSUM dataset either. Compression ratio for part of speech sequences is the score that identifies the most differences between human and model-generated text.  Self-repetition stands out as the only score that identifies model generated text as more diverse on the CNN/DM dataset. From this analysis, CR:POS and self-repetition emerge as prime candidates of reportable scores, while homogenization with BERTScore as perhaps not useful.\n",
      "\n",
      "All scores detect at least one difference for a pair from the  models we study. According to seven of the scores, Llama-2 generates texts that are less diverse than those from Mistral. FLAN-T5 is also marked as less diverse than StableBeluga according to four scores. Finally, four scores identify GPT-4 as less diverse than FLAN-T5; two of these are BERTScore homogenization, which we establish is perhaps not necessarily applicable and self-repetition, which marks human text as less diverse. \n",
      "\n",
      "The XSUM dataset results in fewer notable observations. The one consistent findings is that FLAN-T5 produces that shortest and least diverse summaries, less diverse than other models and less diverse than human text.  The type of input text clearly changes the behavior of the models and the diversity of text they produce. \n",
      "\n",
      "\\section{Correlation Analysis}\n",
      "Here we present three sets of correlation analyses between \\emph{(i)} different diversity scores, \\emph{(ii)} the same diversity score across datasets, and \\emph{(iii)} diversity scores and standard reference-based evaluations. \n",
      "\n",
      "Despite the large number of diversity scores in our list, they all revolve around $n$-gram repetition. It is of interest to know if they capture different or similar information. With this motivation in mind, we computed the correlations between every pair of scores, shown in Figure~\\ref{fig:corr_cnn}.\n",
      "\n",
      "Compression ratio is highly to moderately correlated with other $n$-gram scores.  The only weak correlations are with Self-BLEU and BERTScore homogenization.  BERTScore homogenization and Self-BLEU are moderately correlated with each. Given the degenerate behavior of BERTScore homogenization on the analysis of summaries, reporting self-BLEU only is advisable. Finally, self-repetition is only moderately correlated with with other scores, so is informative to report as a standard score for diversity. The correlations are similar on the XSUM summaries (see Appendix~\\ref{fig:corr_xsum}), reinforcing the recommendation for the set of scores that should be used to capture diversity.\n",
      "\n",
      "Diversity analysis on the CNN/DM and XSUM datasets did not indicate consistent system behavior. We further examine this mismatch, reporting in Table \\ref{table:correlations_news} the correlations between diversity score types values across the two datasets.  \n",
      "Self-BLEU scores are almost perfectly correlated between the two datasets; they appear to not be affected by text source. The other scores are still moderately to highly correlated but as already observed, models are ranked differently. When reporting diversity, source of analyzed data also has to be taken into account, in addition to length. \n",
      "\n",
      "Our guiding assumption is that output diversity and self-repetition are aspects of model behavior that are not captured by existing evaluation approaches. Here we directly test this assumption.\n",
      "We compute the system level correlation between the diversity scores and the traditional BERTScore and ROUGE evals, shown in Figure~\\ref{fig:corr_r1_bert}. The reference-based evaluations are only weakly correlated with the diversity metrics. Self-BLEU, however, is moderately anti-correlated with with both ROUGE-1 and BERTScore. \n",
      "\n",
      "\n",
      "% <BEGIN_FILE: figures/metric_correlations_r1_bert_cnn.tex>\n",
      "\\begin{figure}\n",
      "\\centering\n",
      "    \\includegraphics[width=0.5\\textwidth]{figures/img/corr_r1_bert_self_rep.pdf}\n",
      "    \\caption{Correlations between diversity metrics, BERTScore, and ROUGE-1. Both reference-based metrics are weakly correlated with CR and Hom. (BERT), and moderately anti-correlated with Self-BLEU.}\n",
      "    \\label{fig:corr_r1_bert}\n",
      "\\end{figure}\n",
      "\n",
      "% <END_FILE: figures/metric_correlations_r1_bert_cnn.tex>\n",
      "\n",
      "\n",
      "\\section{Truncating to Control Length}\n",
      "\n",
      "% <BEGIN_FILE: tables/ld_experiments_cnn_length_controlled.tex>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\begin{table}\n",
      "\\begin{centering}\n",
      "\\resizebox{0.7\\textwidth}{!}{\n",
      "\\begin{tabular}{llllll}\n",
      "\\toprule\n",
      "\\textbf{Model}                                                     &  \\textbf{\\begin{tabular}[c]{@{}l@{}}CR\\\\(‚Üì) \\end{tabular}}&    \\textbf{\\begin{tabular}[c]{@{}l@{}}CR: POS\\\\(‚Üì) \\end{tabular}}&\\textbf{\\begin{tabular}[c]{@{}l@{}}Self-\\\\Rep. (‚Üì)\\end{tabular}}& \\textbf{\\begin{tabular}[c]{@{}l@{}}Hom. \\\\(BERT) (‚Üì)\\end{tabular}}& \\textbf{\\begin{tabular}[c]{@{}l@{}}Self-\\\\BLEU (‚Üì)\\end{tabular}} \\\\\\midrule\n",
      " Article                                                              & 2.268           &  5.25&2.763& 0.676                                                                &0                      \\\\\n",
      " Article (Lead 3) & 2.274&  5.25&2.762& 0.658&0\\\\\n",
      " Reference                                                             & 2.189&  5.179&2.763& 0.674                                                                         &0                      \\\\ \\midrule\n",
      "Llama-2& 2.96&    5.627&2.847& 0.674& 0.001\\\\\n",
      "GPT-4& \\textbf{2.287}&    5.376&2.761& 0.672                                                                         & \\textbf{0}\\\\\n",
      "FlanT5& 2.288&    5.389&2.779& 0.673                                                                         & \\textbf{0}\\\\\n",
      "StableLM                                                                & 2.393&    5.537&2.884& 0.672                                                                         & 0.001\\\\\n",
      "Mistral                                                            & 2.32&    5.415&2.812& \\textbf{0.67}& \\textbf{0}\\\\\n",
      "StableBeluga                                                           & 2.288&    5.46&2.766& 0.671                                                                         & \\textbf{0}\\\\\\midrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "}\n",
      "\\caption{Scores on CNN/DM summaries truncated \n",
      "to the length of the shortest summary for a given input.}\n",
      "\\label{ c}\n",
      "\\end{centering}\n",
      "\n",
      "\\end{table}\n",
      "% <END_FILE: tables/ld_experiments_cnn_length_controlled.tex>\n",
      "\n",
      "\n",
      "For each input for summarization, we truncate all summaries to the length of the shortest one produced by any of the sources, as a crude method to remove the influence of length on scores.\n",
      "The resulting scores are directly comparable across sources, listed in Tables \\ref{fig:corr_xsum} and \\ref{appendix_table:xsum_diversity_length_controlled}.\n",
      "\n",
      "Compression ratio and Self-BLEU scores indicate that model-produced text is less diverse than human text. BERTScore homogenization scores barely vary across sources, further supporting the recommendation that this is not a useful score to report. \n",
      "\n",
      "On the CNN/DM dataset, Self-BLEU indicates that Llama-2 and StableLM are the most repetitive models. Compression ratio also ranks these two models as the least diverse. The results are consistent on XSUM, but for that dataset Flan-T5 is also highly ranked and the most repetitive. \n",
      "\n",
      "The truncation approach to control for length is not practical for published research or leaderboards. Introducing a new source of texts would require recomputing the scores for other sources one may want to compare with, which is impractical and sometimes impossible when the outputs from other sources are not available. Future research will have to search for more practical alternatives. \n",
      "\n",
      "\\section{Run-Time Considerations}\n",
      "\n",
      "When analyzing the diversity of large volumes of text, run-time considerations become relevant. \n",
      "Figure~\\ref{fig:timing_ld} provides insights about the feasibility of obtaining scores for large samples\\footnote{Run on a single NVIDIA Quadro RTX 8000 GPU.}. The compression ratio scores are fast, with text compression utilities specifically optimized for speed. Self-repetition takes longer but acceptable time. Self-BLEU and BERTScore homogenization are prohibitively slow.\n",
      "\n",
      "% <BEGIN_FILE: figures/timing_ld.tex>\n",
      "\\begin{figure}\n",
      "\\centering\n",
      "    \\includegraphics[width=0.82\\textwidth]{figures/img/timing_experiments.pdf}\n",
      "    \\caption{Mean run time \\textbf{(log-scale)} on CNN/DM summaries. Run times increase with the number of text for the analysis. Even for small datasets, Self-BLEU and BERTScore homogenisation are unpractically slow.}\n",
      "    \\label{fig:timing_ld}\n",
      "\\end{figure}\n",
      "\n",
      "% <END_FILE: figures/timing_ld.tex>\n",
      "\n",
      "\n",
      "\\section{Broader Applications}\n",
      "\n",
      "The guiding motivation for this work has been to develop standardized and informed approach to the analysis the diversity of text produced by LLMs. The standardization of scores will facilitate analysis in broader settings. Here we provide two examples: human writing, with and without facilitation from a LLM, and instruction tuning datasets. \n",
      "\n",
      "\\subsection{Human Story Writing}\n",
      "\\label{appendix:stories}\n",
      "\\citet{padmakumar-etal-2023-investigating} presented an analysis of human-written stories, where people wrote either by themselves or with the help of GPT-3 or GPT-3.5 Turbo. They find that using LLMs as writing partners leads to greater homogenization of the stories. \n",
      "\n",
      "As reported by \\citet{padmakumar-etal-2023-investigating}, we find that all diversity scores agree that people writing independently produce the more diverse texts (cf. Table~\\ref{appendix_table:stories_diversity}). Here, story length is not an issue because the average length of stories in each setting are comparable: 375 words for writing without help, 372 words when writing with GPT-3 and 370 when writing with GPT-3.5.\n",
      " \n",
      "\n",
      "% <BEGIN_FILE: tables/ld_experiments_story.tex>\n",
      "\\begin{table}\n",
      "\\begin{centering}\n",
      "\\resizebox{0.7\\textwidth}{!}{\n",
      "\\begin{tabular}{@{}llllll}\n",
      "\\toprule\n",
      "\\textbf{Dataset}& \\textbf{\\begin{tabular}[c]{@{}l@{}}CR\\\\(‚Üì) \\end{tabular}}& \\textbf{\\begin{tabular}[c]{@{}l@{}}CR: POS\\\\(‚Üì) \\end{tabular}}&\\textbf{\\begin{tabular}[c]{@{}l@{}}Self-\\\\Rep. (‚Üì)\\end{tabular}}& \\textbf{\\begin{tabular}[c]{@{}l@{}}Hom. \\\\(BERT) (‚Üì)\\end{tabular}}&  \\textbf{\\begin{tabular}[c]{@{}l@{}}Self-\\\\BLEU (‚Üì)\\end{tabular}} \\\\ \\midrule\n",
      "Solo& 2.901& 5.314&5.873& 0.604& 0.018\\\\\n",
      " GPT-3& 2.940&  5.371&5.911& 0.613&0.020               \\\\\n",
      "InstructGPT& 3.064& 5.462&5.966& 0.631& 0.022\\\\\\midrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "}\n",
      "\n",
      "\\caption{Diversity scores for the essays dataset. Working with the help of an LLM correlates with lower diversity.}\n",
      "\\label{appendix_table:stories_diversity}\n",
      "\\end{centering}\n",
      "\\end{table}\n",
      "% <END_FILE: tables/ld_experiments_story.tex>\n",
      " \n",
      "\n",
      "\\subsection{Instruction-tuning Datasets}\n",
      "\\label{appendix:instructions}\n",
      "\n",
      "The quality and diversity of instructions are likely to result in more robust and capable systems \\citep{DBLP:conf/iclr/SanhWRBSACSRDBX22,mishra-etal-2022-cross}. We analyze the diversity of five instruction-tuning datasets.\n",
      "\n",
      "\\paragraph{Open Assistant} is a collection of crowdsourced instructions \\citep{kopf2024openassistant}. The data was collected under detailed guidelines and includes questions that reflect real-life situations.\n",
      "\n",
      "\\paragraph{Super-NaturalInstructions} A corpus comprising crowdsourced instructions that transform 200 benchmarks and intermediate evaluation results into a set of instructions and demonstrations \\citep{wang-etal-2022-super}. \n",
      "\n",
      "\\paragraph{Unnatural Instructions} An (almost) automatically created dataset, using instructions from the SuperNatural-Instructions dataset to automatically generate new instructions \\citep{honovich-etal-2023-unnatural}. To increase diversity, each instruction was also paraphrased. \n",
      "\n",
      "\\citet{honovich-etal-2023-unnatural} compare the diversity of instructions in Unnatural and Super-Natural Instructions with pairwise BERTScore similarities (within each dataset), and find that the similarities are much higher in Super-NaturalInstructions. \n",
      "\n",
      "\\paragraph{Alpaca} This dataset is created following the Self-instruct dataset \\citep{wang-etal-2023-self-instruct}. GPT-3 was prompted to create instructions and demonstrations based on a seed of 175 human-written instructions. Crucially, the collection method includes a diversity filter, only including model-written instructions if their ROUGE-L similarity is less than 0.7 with an existing instruction. \n",
      "Length of instructions and demonstrations is also controlled for as a criterion for inclusion in the final instruction dataset.\n",
      "\n",
      "\\paragraph{Dolly} A set of human instructions and demonstrations, collected by Datrabricks employees \\citep{DatabricksBlog2023DollyV2}. By design, they cover only eight classes of popular tasks: creative writing, closed and open QA, summarization, information extraction, classification and brainstorming.\n",
      "\n",
      "In Table \\ref{table:instruction_diversity} we report diversity scores. \n",
      "Here datasets are ordered by size; we therefore expect that scores will be sorted in diminishing order in each column. Only deviations from this ordering are reportable. We provide details about the number of instructions and words in Appendix~\\ref{appendix:instruction_details}.\n",
      "\n",
      "Open Assistant instructions are remarkably diverse compared to the other datasets, and all diversity scores for it are more favorable than that for other datasets. Unnatural instructions are remarkable in the opposite direction, with outlier scores that are so much higher, they are likely not due to length entirely. We provide an analysis of the diversity scores with the length controlled in Appendix~\\ref{appendix:instructions_length}.\n",
      "\n",
      " Given the large dataset sizes, ranging from 15-80k data points, we do not compute the homogenization scores nor Self-BLEU, as the computation time is infeasible. For approximately 50k instructions, the estimated computation times ranged from 48 to 800 hours for these scores. This case study highlights the relevancy of the run-time analysis for computing score that we presented in the previous section. \n",
      "\n",
      "\n",
      "% <BEGIN_FILE: tables/ld_experiments_instructions.tex>\n",
      "\\begin{table}\n",
      "\\begin{centering}\n",
      "\\resizebox{0.7\\textwidth}{!}{\n",
      "\\begin{tabular}{@{}llll}\n",
      "\\toprule\n",
      "\\textbf{Dataset} & \\textbf{CR (‚Üì)} &\\textbf{CR: POS (‚Üì)}&  \\textbf{\\begin{tabular}[c]{@{}l@{}}Self-Rep. (‚Üì)\\end{tabular}}\\\\ \\midrule\n",
      " Open Assistant                          & 2.886            &6.731&3.969\\\\\n",
      " Unnatural Instructions                  & 4.191 &7.278&9.868\\\\\n",
      " Alpaca                                  & 3.119            &6.61&3.105\\\\\n",
      "  Super-NaturalInstructions& 2.675&  5.749&3.456\\\\\n",
      "Dolly                                   & 2.578 &6.214&  2.935\\\\\n",
      "\\midrule\n",
      "\\end{tabular}\n",
      "}\n",
      "\\caption{Diversity scores for instruction datasets. We do not include Self-BLEU nor Hom. (BERT) due to long run times. Datasets are ordered by size and differ vastly in length, so only scores for which a smaller dataset is less diverse are meaningfully interpretable.}\n",
      "\\label{table:instruction_diversity}\n",
      "\\end{centering}\n",
      "\\end{table}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "% <END_FILE: tables/ld_experiments_instructions.tex>\n",
      "\n",
      "\n",
      "\n",
      "\\section{Discussion and Recommendations}\n",
      "\n",
      "Our in-depth analyses reveal that compression ratio is an excellent score to report, easy to compute and strongly correlated with other scores used in past work. Compression ratio of part of speech sequences capture differences between human and model-generated text, so is also a good score to track. Self-repetition zeros in only on repetition of longer $n$-grams across generations and is only moderately correlated with compression ratios and is intuitively interpretable, ass desirable characteristics. Finally Self-BLEU is only weakly correlated with the previous three, so is a good complement score to report. In our analyses, we identified several drawbacks of BERTScore: it does not show differences between human and model-generated text and barely varies when adjusted for length. There is no good justification to report it. \n",
      "\n",
      "\n",
      "Length of the analyzed text has to be reported alongside all these scores. When length differs, scores are not meaningfully comparable. Truncating and downsampling text is one way to produce a set of results that are intuitively comparable. Different random draws of the sample chosen to represent a dataset will likely differ in diversity; the selection may lead to unwarranted conclusions. Truncating texts prevents any possibility of discovering repetitive behavior towards the end of longer text. \n",
      "Future research into a principled solution for this problem is urgently needed. \n",
      "\n",
      "Despite all this, we were able to glean meaningful insights about differences in diversity between human and model-produced text for summaries, essays and instructions. \n",
      "\n",
      "\\section{Limitations}\n",
      "\n",
      "In the work presented here we do not explore human approaches to evaluating the diversity of collections of text. These are straightforward when the produced text is fairly short, as in judging the diversity of a set of questions generated for a given document \\citep{sultan-etal-2020-importance} or the diversity of possible continuation of a conversation \\citep{tevet-berant-2021-evaluating}. Longer texts, as in the case of summaries, and larger collections, as in the case of instruction datasets are harder to judge for diversity.\n",
      "\n",
      "An interface allowing people to explore the data, organized by stretches of repeated text ordered either by the length of the repeated string or the number of times it has been repeated, can facilitate human evaluation. \n",
      "\n",
      "\\section*{Acknowledgements}\n",
      "We gratefully acknowledge the National Science Foundation (RI 2211954) for supporting this work. \n",
      "\n",
      "\\bibliography{colm2024_conference}\n",
      "\\bibliographystyle{colm2024_conference}\n",
      "\n",
      "\\clearpage\n",
      "\\appendix\n",
      "\\section{Appendix}\n",
      "\\label{sec:appendix}\n",
      "\n",
      "\\subsection{Examples of Repetitive Patterns}\n",
      "\\label{appendix:more_patterns}\n",
      "\n",
      "Table~\\ref{table:patterns2} show more examples of repeated sentence structures (using part-of-speech tags) from \\cite{padmakumar-etal-2023-investigating}.\n",
      "\n",
      "\n",
      "% <BEGIN_FILE: tables/example_patterns_story.tex>\n",
      "\\begin{table}\n",
      "\\resizebox{\\textwidth}{!}{\n",
      "\\begin{tabular}{@{}lll@{}}\n",
      "\\toprule\n",
      "\\textbf{Dataset}&  \\textbf{Token Repetition Text}&\\textbf{Pattern-Matched Text}\\\\ \\midrule\n",
      "GPT-3&  \"In my opinion...\" \\texttt{41/100}&\\textbf{PRP VBZ RB JJ TO VB} \\texttt{15/100}\\\\\n",
      " &  &\"It is also vital to discern...\"\\\\\n",
      "               &  &  \"It is very easy to construct...\"\\\\\n",
      " & &  \"It is largely inappropriate to try...\"\\\\\n",
      " & &\"It is morally acceptable to focus...\"\\\\\n",
      " & &\\textbf{PRP VBP IN DT NN IN} \\texttt{12/100}\\\\\n",
      " & &\"I don't like the damsel in...\"\\\\\n",
      " & &\"I fear that a cycle of...\"\\\\\n",
      " & &\"I feel that an acknowledgement of...\"\\\\\n",
      " & &  \"I find that the inflection of...\"\\\\ \\midrule\n",
      "Instruct-GPT&  \"In my opinion...\" \\texttt{25/100}&  \\textbf{MD VB DT JJ NN IN} \\texttt{20/100}\\\\\n",
      " &  \"It is important to...\" \\texttt{20/100}&  \"...can have a huge variety of...\"\\\\\n",
      "               &   \"Up with the news...\" \\texttt{15/100}&  \"...can have a negative effect on...\"\\\\\n",
      "               &   &  \"...can have a positive impact on...\"\\\\\n",
      "               &   &  \"...can have a sturdy framework for...\"\\\\\n",
      " & &  \\textbf{PRP VBZ RB JJ TO VB} \\texttt{12/100}\\\\\n",
      " & &  \"It is also important to realize...\"\\\\\n",
      " & &  \"It is fairly common to hear...\"\\\\\n",
      " & &  \"It is indeed surprising to hear...\"\\\\\n",
      " & &  \"It is probably impossible to keep...\"\\\\\n",
      " & &\"...it becomes very cringy to watch...\"\\\\ \\midrule\n",
      "Solo&   \"In my opinion...\" \\texttt{22/100}&\\textbf{PRP VBZ JJ TO VB IN} \\texttt{9/100}\\\\\n",
      " &   \"In my opinion, the...\" \\texttt{13/100}&\"It is crucial to recognize that...\"\\\\\n",
      "               &   \"When it comes to...\" \\texttt{11/100}&    \"It is crucial to remember that...\"\\\\\n",
      "               &   \"In my opinion, I...\" \\texttt{10/100}&    \"It is unjustifiable to assume that...\"\\\\\n",
      "               &   &    \"It is important to acknowledge that...\"\\\\\n",
      " & &\\textbf{PRP VBP IN DT JJ NN} \\texttt{10/100}\\\\\n",
      " & &\"I believe for the right person...\"\\\\\n",
      " & &  \"I do on a regular basis.\"\\\\\n",
      " & &  \"I fall into the second group.\"\\\\\n",
      " & &  \"I live in a small town...\"\\\\\n",
      "\\end{tabular}\n",
      "}\n",
      "\\caption{Examples of exact text-match and repeating part-of-speech patterns in essays from \\citet{padmakumar2023does}. The number of times the pattern occurs is shown in parenthesis.}\n",
      "\\label{table:patterns2}\n",
      "\\end{table}\n",
      "% <END_FILE: tables/example_patterns_story.tex>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\subsection{Summarization Prompts}\n",
      "\\label{appendix:prompts}\n",
      "\n",
      "Table~\\ref{appendix_table:prompts} details the prompts and format used to generate the summaries for the news datasets. We follow the formats recommended provided by each model, and insert the along with the instruction for summarization.\n",
      "\\begin{table}\n",
      "    \\centering\n",
      "    \\begin{tabular}{lll}\n",
      "    \\midrule\n",
      "         \\textbf{Model}&  \\textbf{Model Size}& \\textbf{Prompt} \\\\\\midrule\n",
      "         Llama-2&  7B& \\texttt{[TEXT]} [INST] Summarize the above text. [/INST]\\\\\n",
      "         GPT-4&  -& \\texttt{[TEXT]}. Summarize the above text.\\\\\n",
      "         Flan-T5&  11B& Summarize this article: \\texttt{[TEXT]}\\\\\n",
      "         StableLM&  7B& \\texttt{[TEXT]} $<|$USER$|>$Summarize the above text. \\\\ &&$<|$ASSISTANT$|>$\\\\\n",
      "         Mistral&  7B& \\#\\#\\# Instruction: Summarize the following: \\#\\#\\# Input: \\texttt{[TEXT]}.\\\\&&\\#\\#\\# Response:\\\\\n",
      "    \\end{tabular}\n",
      "    \\caption{Prompts used for each model to generate a summary. \\texttt{[TEXT]} is replaced with the input article.}\n",
      "    \\label{appendix_table:prompts}\n",
      "\\end{table}\n",
      "\n",
      "\\subsection{XSUM Metrics}\n",
      "Figure~\\ref{fig:corr_xsum} shows the correlations between all pairs of metrics for the XSUM dataset. The correlations show that  compression ratio is highly to moderately correlated with other n-gram scores, similar to the findings for the CNN/DM dataset.\n",
      "\n",
      "% <BEGIN_FILE: figures/metric_correlations_xsum.tex>\n",
      "\\begin{figure}\n",
      "\\centering\n",
      "    \\includegraphics[width=0.65\\textwidth]{figures/img/corr_xsum.pdf}\n",
      "    \\caption{Correlation table between scores on XSUM.}\n",
      "    \\label{fig:corr_xsum}\n",
      "\\end{figure}\n",
      "\n",
      "% <END_FILE: figures/metric_correlations_xsum.tex>\n",
      "\n",
      "\n",
      "\n",
      "\\subsection{Instruction Datasets, Details}\n",
      "\\label{appendix:instruction_details}\n",
      "Table~\\ref{appendix_table:instruction_sizes} shows the number of instructions, the typical length of an instruction and average number of words per instruction set. All vary, making it even harder to control for length. Truncating makes less sense here, and down-sampling the number per instructions is counter-productive given our goal to understand the diversity of the entire dataset. We do make use of these instruments given the lack of alternatives, but note that more meaningful solutions are urgently needed.\n",
      "\n",
      "% <BEGIN_FILE: tables/lengths_instruction_datasets.tex>\n",
      "\\begin{table}\n",
      "\\begin{centering}\n",
      "\\resizebox{0.75\\textwidth}{!}{\n",
      "\\begin{tabular}{@{}llll}\n",
      "\\toprule\n",
      "\\textbf{Dataset}& \\textbf{\\# Instructions}& \\textbf{Avg. \\# Words} &\\textbf{Total \\# Words}\\\\ \\midrule\n",
      "Open Assistant                          & 84,437& 78.10 &6,594,646\\\\\n",
      "Unnatural Instructions                  & 66,010& 38.05 &2,511,737\\\\\n",
      "Alpaca                                  & 52,002& 10.06 &523,329\\\\ \n",
      "Super-NaturalInstructions& 4550& 92.58&421,228\\\\\n",
      "Dolly                                   & 15,011& 12.37 &185,816\\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "}\n",
      "\\caption{Average number of words, and size of the instruction datasets. Numbers correspond to the training set available from Huggingface. For Super-NaturalInstructions, we filter for English-only instructions using the \\texttt{langdetect} library.}\n",
      "\\label{appendix_table:instruction_sizes}\n",
      "\\end{centering}\n",
      "\\end{table}\n",
      "% <END_FILE: tables/lengths_instruction_datasets.tex>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\subsection{Instruction Datasets, Length Controlled}\n",
      "\\label{appendix:instructions_length}\n",
      " Table \\ref{table:instruction_diversity_length_controlled} shows scores for instructions downsampled to the size of the smallest dataset, and truncated to the length of the shortest instructions in the remaining data. Again, the Open Assistant dataset stand out as most diverse, while the Unnatural Instructions dataset is markedly less diverse than the others. Self-repetition in the related Super-Natural and Unnatural instructions is notably high. The human instructions in Dolly compare favorably with automatic instructions, especially when bearing in mind that only eight tasks are covered in it. CR:POS points to Super-natural instructions as the most diverse. We do not have a convincing explanation of why it compares so favorably against others on this score.\n",
      "\n",
      "% <BEGIN_FILE: tables/ld_experiments_instructions_length_controlled.tex>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\begin{table}\n",
      "\\begin{centering}\n",
      "\\resizebox{0.75\\textwidth}{!}{\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\begin{tabular}{@{}llll}\n",
      "\\toprule\n",
      "\\textbf{Dataset} & \\textbf{CR (‚Üì)} &\\textbf{CR: POS (‚Üì)}&  \\textbf{\\begin{tabular}[c]{@{}l@{}}Self-Rep. (‚Üì)\\end{tabular}}\\\\ \\midrule\n",
      " Open Assistant                          & 2.370&5.402&1.741\\\\\n",
      " Unnatural Instructions                  & 6.036&8.421&5.595\\\\\n",
      " Alpaca                                  & 3.301&6.044&2.020\\\\\n",
      " Super-NaturalInstructions& 2.458&  1.844&4.859\\\\\n",
      "Dolly                                   & 2.832&5.504&  2.235\\\\\n",
      "\\midrule\n",
      "\\end{tabular}\n",
      "}\n",
      "\\caption{Truncated diversity scores for instruction datasets. }\n",
      "\\label{table:instruction_diversity_length_controlled}\n",
      "\\end{centering}\n",
      "\\end{table}\n",
      "% <END_FILE: tables/ld_experiments_instructions_length_controlled.tex>\n",
      "\n",
      "\n",
      "\\end{document}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(flat_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4f5630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e764e54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ STEP 1: GATHERING & REFERENCE EXTRACTION\n",
      "üìù Kh·ªüi t·∫°o LatexFlattener cho Paper: 2403-00530, Version: 2403-00530v2\n",
      "   Remove references: No\n",
      "   üîç Scanning references for 2403-00530v2...\n",
      "      Found 92 cited keys in text.\n",
      "         -> Parsed 92 items from main_rerev2025.bbl\n",
      "      Matched 92 references.\n",
      "\n",
      "üöÄ STEP 2: TEXT REPLACEMENT & PARSING\n",
      "   üìù Parsed 2403-00530v2 with 92 citation updates.\n",
      "üîç X·ª≠ l√Ω Preamble ƒë·ªÉ tr√≠ch xu·∫•t Title, Authors, Abstract...\n",
      "\n",
      "üöÄ STEP 3: EXPORT\n",
      "‚úÖ Exported refs.bib\n",
      "‚úÖ Exported hierarchy.json\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup\n",
    "paper_base_id = '2403-00530'\n",
    "versions = ['2403-00530v0', '2403-00530v2'] # List c√°c version\n",
    "ref_deduplicator = ReferenceDeduplicator()\n",
    "content_deduplicator = ContentDeduplicator()\n",
    "\n",
    "# L∆∞u tr·ªØ n·ªôi dung ph·∫≥ng t·∫°m th·ªùi ƒë·ªÉ parse sau khi dedup ref\n",
    "temp_flat_contents = {} \n",
    "\n",
    "print(\"üöÄ STEP 1: GATHERING & REFERENCE EXTRACTION\")\n",
    "for ver in versions:\n",
    "    ver_path = os.path.join(DATA_RAW_PATH, paper_base_id, 'tex', ver)\n",
    "    \n",
    "    # A. Flatten\n",
    "    root_file = find_root_tex_file(ver_path)\n",
    "    if not root_file: continue\n",
    "    \n",
    "    flattener = LatexFlattener(root_file, paper_id=paper_base_id, version=ver,remove_references=False)\n",
    "    flat_res = flattener.flatten()\n",
    "    flat_text = flat_res['content']\n",
    "    \n",
    "    # B. Reference Extract (D√πng class m·ªõi b·∫°n ƒë√£ test)\n",
    "    ref_proc = ReferenceProcessor(paper_base_id, ver, ver_path)\n",
    "    flat_text, refs = ref_proc.process_references(flat_text)\n",
    "    \n",
    "    # C. Add to Dedup\n",
    "    ref_deduplicator.add_references(ver, refs)\n",
    "    \n",
    "    # L∆∞u l·∫°i text g·ªëc ƒë·ªÉ t√≠ n·ªØa s·ª≠a\n",
    "    temp_flat_contents[ver] = flat_text\n",
    "\n",
    "print(\"\\nüöÄ STEP 2: TEXT REPLACEMENT & PARSING\")\n",
    "for ver in versions:\n",
    "    if ver not in temp_flat_contents: continue\n",
    "    \n",
    "    # A. L·∫•y map thay th·∫ø (Old -> New)\n",
    "    replacements = ref_deduplicator.get_replacements(ver)\n",
    "    raw_text = temp_flat_contents[ver]\n",
    "    \n",
    "    # B. S·ª≠a Key trong Text\n",
    "    # H√†m n√†y s·∫Ω ƒë·ªïi \\cite{hinton06} th√†nh \\cite{ref_0}\n",
    "    updated_text = replace_citations_in_text(raw_text, replacements)\n",
    "    \n",
    "    print(f\"   üìù Parsed {ver} with {len(replacements)} citation updates.\")\n",
    "    \n",
    "    # C. Parse Structure (D√πng text ƒê√É S·ª¨A)\n",
    "    builder = LatexStructureBuilder(updated_text, paper_base_id, ver)\n",
    "    root_node = builder.build_coarse_tree()\n",
    "    \n",
    "    # D. Clean Content\n",
    "    processor = LatexContentProcessor(paper_base_id, ver)\n",
    "    processor.process_tree(root_node)\n",
    "    \n",
    "    # E. Content Dedup\n",
    "    content_deduplicator.process_version(ver, root_node)\n",
    "\n",
    "print(\"\\nüöÄ STEP 3: EXPORT\")\n",
    "\n",
    "# 1. Export refs.bib\n",
    "bib_str = ref_deduplicator.export_bib_string()\n",
    "with open('refs.bib', 'w', encoding='utf-8') as f:\n",
    "    f.write(bib_str)\n",
    "print(\"‚úÖ Exported refs.bib\")\n",
    "\n",
    "# 2. Export hierarchy.json\n",
    "final_hier = content_deduplicator.get_final_json()\n",
    "with open('hierarchy.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(final_hier, f, indent=2, ensure_ascii=False)\n",
    "print(\"‚úÖ Exported hierarchy.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
