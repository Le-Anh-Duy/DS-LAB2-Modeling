{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4c41077",
   "metadata": {},
   "source": [
    "# This notebook is going to implement the parser for this lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2907ca7",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è C∆° ch·∫ø ho·∫°t ƒë·ªông: Thu·∫≠t to√°n t√¨m File G·ªëc (Root File Detection)\n",
    "\n",
    "ƒê·ªÉ x√°c ƒë·ªãnh ch√≠nh x√°c ƒë√¢u l√† file ch√≠nh (`main.tex`) trong m·ªôt r·ª´ng c√°c file m√£ ngu·ªìn LaTeX h·ªón ƒë·ªôn, ch√∫ng ta kh√¥ng d√πng c√°ch ƒëo√°n m√≤. Thu·∫≠t to√°n s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p **\"Ph√°p y k·ªπ thu·∫≠t s·ªë\" (Digital Forensics)** k·∫øt h·ª£p v·ªõi **Ph√¢n t√≠ch C√¢y ph·ª• thu·ªôc (Dependency Graph)**.\n",
    "\n",
    "Quy tr√¨nh x·ª≠ l√Ω ƒëi qua 3 m√†ng l·ªçc ch√≠nh:\n",
    "\n",
    "### 1. M√†ng l·ªçc C·∫•u tr√∫c (The Gatekeeper)\n",
    "Tr∆∞·ªõc ti√™n, m·ªçi file `.tex` ph·∫£i ch·ª©ng minh n√≥ l√† m·ªôt vƒÉn b·∫£n ƒë·ªôc l·∫≠p, kh√¥ng ph·∫£i l√† ƒëo·∫°n code r·ªùi r·∫°c.\n",
    "* **ƒêi·ªÅu ki·ªán ti√™n quy·∫øt:** File b·∫Øt bu·ªôc ph·∫£i ch·ª©a l·ªánh `\\documentclass`.\n",
    "* **Lo·∫°i b·ªè:** C√°c file Plain TeX c≈©, file data, ho·∫∑c c√°c file ch∆∞∆°ng h·ªìi (chapter) kh√¥ng c√≥ khai b√°o m√¥i tr∆∞·ªùng.\n",
    "\n",
    "### 2. Ph√¢n t√≠ch C√¢y ph·ª• thu·ªôc (Dependency Graph Analysis)\n",
    "ƒê√¢y l√† b∆∞·ªõc quan tr·ªçng nh·∫•t ƒë·ªÉ gi·∫£i quy·∫øt xung ƒë·ªôt khi c√≥ nhi·ªÅu file c√πng ch·∫°y ƒë∆∞·ª£c (v√≠ d·ª• d√πng g√≥i `subfiles`).\n",
    "* **Nguy√™n l√Ω:** \"K·∫ª ra l·ªánh l√† Vua\".\n",
    "* H·ªá th·ªëng qu√©t to√†n b·ªô m√£ ngu·ªìn ƒë·ªÉ v·∫Ω s∆° ƒë·ªì xem **File n√†o g·ªçi File n√†o** (th√¥ng qua l·ªánh `\\input`, `\\include`).\n",
    "* **Quy t·∫Øc:** N·∫øu File A ƒë∆∞·ª£c g·ªçi b·ªüi File B $\\rightarrow$ File A l√† **Con**, File B l√† **Cha**. File Con s·∫Ω b·ªã tr·ª´ ƒëi·ªÉm r·∫•t n·∫∑ng v√¨ n√≥ kh√¥ng th·ªÉ l√† Root.\n",
    "\n",
    "### 3. H·ªá th·ªëng Ch·∫•m ƒëi·ªÉm ƒêa ti√™u ch√≠ (Multi-factor Scoring)\n",
    "\n",
    "M·ªói ·ª©ng vi√™n v∆∞·ª£t qua v√≤ng lo·∫°i s·∫Ω ƒë∆∞·ª£c ch·∫•m ƒëi·ªÉm d·ª±a tr√™n b·∫£ng quy t·∫Øc sau:\n",
    "\n",
    "| Ti√™u ch√≠ (Criteria) | ƒêi·ªÉm s·ªë | L√Ω gi·∫£i |\n",
    "| :--- | :--- | :--- |\n",
    "| **D·∫•u v·∫øt bi√™n d·ªãch (.bbl)** | `+60` | N·∫øu t·ªìn t·∫°i file `.bbl` tr√πng t√™n, ch·ª©ng t·ªè t√°c gi·∫£ ƒë√£ t·ª´ng compile file n√†y. ƒê√¢y l√† b·∫±ng ch·ª©ng th√©p. |\n",
    "| **B·ªã file kh√°c g·ªçi (Is Child)** | `-50` (m·ªói l·∫ßn) | File g·ªëc l√† file kh·ªüi x∆∞·ªõng, kh√¥ng bao gi·ªù b·ªã file kh√°c g·ªçi. |\n",
    "| **Log file (.log)** | `+30` | T∆∞∆°ng t·ª± .bbl, file log tr√πng t√™n l√† d·∫•u v·∫øt c·ªßa l·∫ßn ch·∫°y tr∆∞·ªõc. |\n",
    "| **C·∫•u tr√∫c (`\\begin{document}`)** | `+20` | ƒê·∫£m b·∫£o file c√≥ n·ªôi dung ho√†n ch·ªânh. |\n",
    "| **Ch·ª©a Bibliography** | `+15` | B√†i b√°o khoa h·ªçc lu√¥n ph·∫£i c√≥ danh m·ª•c tham kh·∫£o. |\n",
    "| **G·ªçi file kh√°c (Is Parent)** | `+3` (m·ªói file) | File g·ªëc th∆∞·ªùng ƒë√≥ng vai tr√≤ \"Nh·∫°c tr∆∞·ªüng\", k·∫øt n·ªëi c√°c file con. |\n",
    "| **T√™n file chu·∫©n** | `+10` | T√™n l√† `main`, `paper`, `ms`, `article`. |\n",
    "| **T√™n file r√°c/Slide** | `-50` | T√™n ch·ª©a `letter`, `response` ho·∫∑c d√πng `\\documentclass{beamer}`. |\n",
    "\n",
    "---\n",
    "\n",
    "### üß© C∆° ch·∫ø gi·∫£i quy·∫øt h√≤a (Tie-Breaker)\n",
    "\n",
    "Trong tr∆∞·ªùng h·ª£p hi·∫øm hoi c√≥ 2 file ƒëi·ªÉm cao ngang nhau (v√≠ d·ª•: `main_v1.tex` v√† `main_v2.tex`), thu·∫≠t to√°n s·∫Ω ch·ªçn d·ª±a tr√™n:\n",
    "1.  **ƒê·ªô ∆∞u ti√™n t√™n:** ∆Øu ti√™n `main.tex` h∆°n t√™n l·∫°.\n",
    "2.  **K√≠ch th∆∞·ªõc n·ªôi dung:** File n·∫∑ng h∆°n th∆∞·ªùng l√† b·∫£n ƒë·∫ßy ƒë·ªß h∆°n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c03a9195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fd94a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# C√°c th∆∞ m·ª•c kh√¥ng c·∫ßn qu√©t ƒë·ªÉ t·ªëi ∆∞u t·ªëc ƒë·ªô\n",
    "BLOCKLIST_DIRS = {'.git', 'images', 'figures', '__pycache__', 'node_modules', 'media'}\n",
    "\n",
    "def build_dependency_map(folder_path):\n",
    "    \"\"\"\n",
    "    X√¢y d·ª±ng b·∫£n ƒë·ªì ph·ª• thu·ªôc: File n√†o b·ªã file n√†o g·ªçi?\n",
    "    Tr·∫£ v·ªÅ dict: { 'child_full_path': ['parent_full_path'] }\n",
    "    \"\"\"\n",
    "    dependency_map = {}\n",
    "    name_to_path = {}\n",
    "    \n",
    "    # 1. Map to√†n b·ªô file .tex trong folder n√†y ƒë·ªÉ tra c·ª©u nhanh\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        dirs[:] = [d for d in dirs if d not in BLOCKLIST_DIRS]\n",
    "        for f in files:\n",
    "            if f.lower().endswith('.tex'):\n",
    "                full_path = os.path.join(root, f)\n",
    "                name_to_path[f] = full_path\n",
    "\n",
    "    # 2. Qu√©t n·ªôi dung ƒë·ªÉ t√¨m quan h·ªá cha-con\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        dirs[:] = [d for d in dirs if d not in BLOCKLIST_DIRS]\n",
    "        for file in files:\n",
    "            if not file.lower().endswith('.tex'): continue\n",
    "            \n",
    "            parent_path = os.path.join(root, file)\n",
    "            try:\n",
    "                with open(parent_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "                    # T√¨m l·ªánh \\input, \\include, \\subfile\n",
    "                    # Regex b·∫Øt: \\input{filename}\n",
    "                    matches = re.findall(r'\\\\(?:input|include|subfile)(?:\\[.*?\\])?\\{([^}]+)\\}', content)\n",
    "                    \n",
    "                    for match in matches:\n",
    "                        child_name = os.path.basename(match.strip())\n",
    "                        if not child_name.lower().endswith('.tex'): \n",
    "                            child_name += '.tex'\n",
    "                        \n",
    "                        # N·∫øu file con t·ªìn t·∫°i trong danh s√°ch file c·ªßa folder\n",
    "                        if child_name in name_to_path:\n",
    "                            child_path = name_to_path[child_name]\n",
    "                            \n",
    "                            # Ghi nh·∫≠n: child_path b·ªã g·ªçi b·ªüi parent_path\n",
    "                            if child_path not in dependency_map:\n",
    "                                dependency_map[child_path] = []\n",
    "                            dependency_map[child_path].append(parent_path)\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "    return dependency_map\n",
    "\n",
    "def get_score(file_path, content, context_files, dependency_map):\n",
    "    \"\"\"H√†m ch·∫•m ƒëi·ªÉm logic LaTeX chu·∫©n\"\"\"\n",
    "    filename = os.path.basename(file_path)\n",
    "    base_name = os.path.splitext(filename)[0]\n",
    "    clean_content = re.sub(r'%.*', '', content) # X√≥a comment\n",
    "    \n",
    "    score = 0\n",
    "    \n",
    "    # --- 1. C·∫§U TR√öC B·∫ÆT BU·ªòC (GATEKEEPER) ---\n",
    "    # N·∫øu kh√¥ng c√≥ documentclass -> Lo·∫°i ngay (Theo y√™u c·∫ßu b·ªè Plain TeX)\n",
    "    if r'\\documentclass' not in clean_content:\n",
    "        return -1000 \n",
    "    \n",
    "    # C√≥ documentclass l√† ƒë·∫°t y√™u c·∫ßu c∆° b·∫£n\n",
    "    score += 20\n",
    "    \n",
    "    if r'\\begin{document}' in clean_content:\n",
    "        score += 20\n",
    "        \n",
    "    # --- 2. DEPENDENCY CHECK (QUAN TR·ªåNG) ---\n",
    "    # N·∫øu file n√†y b·ªã file kh√°c g·ªçi -> N√≥ l√† file con -> Tr·ª´ ƒëi·ªÉm c·ª±c n·∫∑ng\n",
    "    if file_path in dependency_map:\n",
    "        # Tr·ª´ 50 ƒëi·ªÉm cho m·ªói l·∫ßn b·ªã g·ªçi\n",
    "        score -= 50 * len(dependency_map[file_path])\n",
    "\n",
    "    # --- 3. D·∫§U HI·ªÜU \"V·ªÜ TINH\" (FORENSICS) ---\n",
    "    # File .bbl sinh ra tr√πng t√™n file g·ªëc -> D·∫•u hi·ªáu m·∫°nh nh·∫•t\n",
    "    if f\"{base_name}.bbl\" in context_files: score += 60\n",
    "    if f\"{base_name}.bib\" in context_files: score += 20\n",
    "    if f\"{base_name}.log\" in context_files: score += 30\n",
    "    \n",
    "    # --- 4. N·ªòI DUNG ---\n",
    "    if r'\\bibliography' in clean_content or r'\\begin{thebibliography}' in clean_content: score += 15\n",
    "    if r'\\maketitle' in clean_content: score += 10\n",
    "    if r'\\begin{abstract}' in clean_content: score += 10\n",
    "    \n",
    "    # ƒê·∫øm s·ªë l∆∞·ª£ng file con m√† n√≥ g·ªçi (Nh·∫°c tr∆∞·ªüng th∆∞·ªùng g·ªçi nhi·ªÅu nh·∫°c c√¥ng)\n",
    "    input_count = len(re.findall(r'\\\\(input|include|subfile)\\{', clean_content))\n",
    "    score += min(input_count * 3, 30) # Max c·ªông 30 ƒëi·ªÉm\n",
    "\n",
    "    # --- 5. HEURISTIC T√äN FILE ---\n",
    "    lower_name = filename.lower()\n",
    "    \n",
    "    # ƒêi·ªÉm c·ªông t√™n chu·∫©n\n",
    "    if lower_name in ['main.tex', 'ms.tex', 'paper.tex', 'article.tex']: \n",
    "        score += 10\n",
    "    \n",
    "    # ƒêi·ªÉm tr·ª´ t√™n file r√°c/template\n",
    "    if any(x in lower_name for x in ['template', 'sample', 'example']) and score < 60:\n",
    "        score -= 10\n",
    "    if 'response' in lower_name or 'reply' in lower_name or 'letter' in lower_name:\n",
    "        score -= 50\n",
    "        \n",
    "    # Tr·ª´ ƒëi·ªÉm file slide, standalone\n",
    "    if r'\\documentclass{beamer}' in clean_content: score -= 50\n",
    "    if r'\\documentclass{standalone}' in clean_content: score -= 20\n",
    "    if r'\\documentclass{letter}' in clean_content: score -= 50\n",
    "\n",
    "    return score\n",
    "\n",
    "def find_root_tex_file(version_folder_path):\n",
    "    \"\"\"\n",
    "    H√†m ch√≠nh: T√¨m file LaTeX g·ªëc trong th∆∞ m·ª•c version.\n",
    "    Input: ƒê∆∞·ªùng d·∫´n t·ªõi folder ch·ª©a code (vd: .../tex/version1)\n",
    "    Output: ƒê∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi t·ªõi file main.tex (ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y)\n",
    "    \"\"\"\n",
    "    # 1. X√¢y d·ª±ng b·∫£n ƒë·ªì ph·ª• thu·ªôc tr∆∞·ªõc\n",
    "    dep_map = build_dependency_map(version_folder_path)\n",
    "    \n",
    "    # 2. L·∫•y danh s√°ch to√†n b·ªô file ƒë·ªÉ check v·ªá tinh (.bbl, .bib)\n",
    "    all_files = set()\n",
    "    tex_files = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(version_folder_path):\n",
    "        dirs[:] = [d for d in dirs if d not in BLOCKLIST_DIRS]\n",
    "        for f in files:\n",
    "            all_files.add(f)\n",
    "            if f.lower().endswith('.tex'):\n",
    "                tex_files.append(os.path.join(root, f))\n",
    "    \n",
    "    if not tex_files:\n",
    "        return None\n",
    "\n",
    "    # 3. Ch·∫•m ƒëi·ªÉm t·ª´ng ·ª©ng vi√™n\n",
    "    candidates = []\n",
    "    for path in tex_files:\n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "                score = get_score(path, content, all_files, dep_map)\n",
    "                \n",
    "                # Ch·ªâ l·∫•y ·ª©ng vi√™n c√≥ ƒëi·ªÉm d∆∞∆°ng ho·∫∑c √≠t nh·∫•t kh√¥ng b·ªã lo·∫°i (-1000)\n",
    "                if score > -100:\n",
    "                    candidates.append({\n",
    "                        'path': path, \n",
    "                        'name': os.path.basename(path),\n",
    "                        'score': score, \n",
    "                        'len': len(content)\n",
    "                    })\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if not candidates:\n",
    "        return None\n",
    "    \n",
    "    # 4. S·∫Øp x·∫øp: ∆Øu ti√™n ƒêi·ªÉm cao -> Sau ƒë√≥ ƒë·∫øn ƒë·ªô d√†i n·ªôi dung\n",
    "    candidates.sort(key=lambda x: (x['score'], x['len']), reverse=True)\n",
    "    \n",
    "    # 5. X·ª≠ l√Ω Tie-breaker (n·∫øu Top 1 v√† Top 2 ƒëi·ªÉm b·∫±ng nhau)\n",
    "    if len(candidates) >= 2:\n",
    "        top1 = candidates[0]\n",
    "        top2 = candidates[1]\n",
    "        \n",
    "        # N·∫øu ƒëi·ªÉm ch√™nh l·ªách kh√¥ng ƒë√°ng k·ªÉ (< 5)\n",
    "        if (top1['score'] - top2['score']) < 5:\n",
    "            # ∆Øu ti√™n file c√≥ t√™n chu·∫©n\n",
    "            prio_names = ['main.tex', 'ms.tex', 'paper.tex', 'article.tex']\n",
    "            if top1['name'].lower() not in prio_names and top2['name'].lower() in prio_names:\n",
    "                return top2['path']\n",
    "            \n",
    "            # N·∫øu t√™n c≈©ng kh√¥ng gi√∫p √≠ch, l·∫•y file N·∫∂NG H∆†N ƒê√ÅNG K·ªÇ\n",
    "            if top2['len'] > top1['len'] * 1.5: \n",
    "                return top2['path']\n",
    "\n",
    "    # Tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n c·ªßa ·ª©ng vi√™n s·ªë 1\n",
    "    return candidates[0]['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce8abdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T√¨m th·∫•y 11 b√†i b√°o. ƒêang qu√©t file g·ªëc...\n",
      "\n",
      "../data_raw\\2403-00530\\tex\n",
      "  Phi√™n b·∫£n: 2403-00530v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 2403-00530: ../data_raw\\2403-00530\\tex\\2403-00530v1\\main.tex\n",
      "  Phi√™n b·∫£n: 2403-00530v2\n",
      "‚úÖ 2403-00530: ../data_raw\\2403-00530\\tex\\2403-00530v2\\main_rerev2025.tex\n",
      "../data_raw\\2403-00531\\tex\n",
      "  Phi√™n b·∫£n: 2403-00531v1\n",
      "‚úÖ 2403-00531: ../data_raw\\2403-00531\\tex\\2403-00531v1\\main.tex\n",
      "  Phi√™n b·∫£n: 2403-00531v2\n",
      "‚úÖ 2403-00531: ../data_raw\\2403-00531\\tex\\2403-00531v2\\apssamp.tex\n",
      "../data_raw\\2403-00532\\tex\n",
      "  Phi√™n b·∫£n: 2403-00532v1\n",
      "‚úÖ 2403-00532: ../data_raw\\2403-00532\\tex\\2403-00532v1\\file_arxiv.tex\n",
      "  Phi√™n b·∫£n: 2403-00532v2\n",
      "‚úÖ 2403-00532: ../data_raw\\2403-00532\\tex\\2403-00532v2\\file_arxiv2.tex\n",
      "../data_raw\\2403-00533\\tex\n",
      "  Phi√™n b·∫£n: 2403-00533v1\n",
      "‚úÖ 2403-00533: ../data_raw\\2403-00533\\tex\\2403-00533v1\\main.tex\n",
      "  Phi√™n b·∫£n: 2403-00533v2\n",
      "‚úÖ 2403-00533: ../data_raw\\2403-00533\\tex\\2403-00533v2\\JCAP.tex\n",
      "../data_raw\\2403-00534\\tex\n",
      "  Phi√™n b·∫£n: 2403-00534v1\n",
      "‚úÖ 2403-00534: ../data_raw\\2403-00534\\tex\\2403-00534v1\\1rdm.tex\n",
      "../data_raw\\2403-00535\\tex\n",
      "  Phi√™n b·∫£n: 2403-00535v1\n",
      "‚úÖ 2403-00535: ../data_raw\\2403-00535\\tex\\2403-00535v1\\Winnberg-etal-jpg.tex\n",
      "../data_raw\\2403-00536\\tex\n",
      "  Phi√™n b·∫£n: 2403-00536v1\n",
      "‚úÖ 2403-00536: ../data_raw\\2403-00536\\tex\\2403-00536v1\\arxiv-dynamic-knapsack.tex\n",
      "../data_raw\\2403-00537\\tex\n",
      "  Phi√™n b·∫£n: 2403-00537v1\n",
      "‚úÖ 2403-00537: ../data_raw\\2403-00537\\tex\\2403-00537v1\\Paper.tex\n",
      "  Phi√™n b·∫£n: 2403-00537v2\n",
      "‚úÖ 2403-00537: ../data_raw\\2403-00537\\tex\\2403-00537v2\\Paper.tex\n",
      "../data_raw\\2403-00538\\tex\n",
      "  Phi√™n b·∫£n: 2403-00538v1\n",
      "‚úÖ 2403-00538: ../data_raw\\2403-00538\\tex\\2403-00538v1\\main.tex\n",
      "  Phi√™n b·∫£n: 2403-00538v2\n",
      "‚úÖ 2403-00538: ../data_raw\\2403-00538\\tex\\2403-00538v2\\main_new.tex\n",
      "../data_raw\\2403-00539\\tex\n",
      "  Phi√™n b·∫£n: 2403-00539v1\n",
      "‚úÖ 2403-00539: ../data_raw\\2403-00539\\tex\\2403-00539v1\\main.tex\n",
      "../data_raw\\2403-00540\\tex\n",
      "  Phi√™n b·∫£n: 2403-00540v1\n",
      "‚úÖ 2403-00540: ../data_raw\\2403-00540\\tex\\2403-00540v1\\EpsilonGreedy.tex\n",
      "  Phi√™n b·∫£n: 2403-00540v2\n",
      "‚úÖ 2403-00540: ../data_raw\\2403-00540\\tex\\2403-00540v2\\GreedyBO.tex\n",
      "  Phi√™n b·∫£n: 2403-00540v3\n",
      "‚úÖ 2403-00540: ../data_raw\\2403-00540\\tex\\2403-00540v3\\GreedyBO.tex\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- CH·∫†Y TH·ª¨ NGHI·ªÜM TR√äN FOLDER DATA_RAW ---\n",
    "\n",
    "# L∆∞u √Ω: V√¨ notebook n·∫±m trong folder 'notebooks', \n",
    "# n√™n ƒë∆∞·ªùng d·∫´n sang data ph·∫£i l√πi l·∫°i 1 c·∫•p (..)\n",
    "data_raw_path = '../data_raw' \n",
    "\n",
    "all_root_files = []\n",
    "\n",
    "if os.path.exists(data_raw_path):\n",
    "    # L·∫•y danh s√°ch c√°c folder con (v√≠ d·ª•: 2403-00530)\n",
    "    paper_folders = [f for f in os.listdir(data_raw_path) \n",
    "                     if os.path.isdir(os.path.join(data_raw_path, f))]\n",
    "    \n",
    "    print(f\"T√¨m th·∫•y {len(paper_folders)} b√†i b√°o. ƒêang qu√©t file g·ªëc...\\n\")\n",
    "    \n",
    "    for folder_id in paper_folders: # Ch·ªâ ch·∫°y th·ª≠ 5 b√†i ƒë·∫ßu ti√™n\n",
    "        folder_path = os.path.join(data_raw_path, folder_id, 'tex')\n",
    "        print(folder_path)\n",
    "\n",
    "        for version in os.listdir(folder_path):\n",
    "            version_path =  os.path.join(folder_path, version)\n",
    "            if not os.path.isdir(version_path):\n",
    "                continue\n",
    "            \n",
    "            print(f\"  Phi√™n b·∫£n: {version}\")\n",
    "            \n",
    "            main_file = find_root_tex_file(version_path)\n",
    "\n",
    "            root_file_info = {\"paper_id\": folder_id, \"version\": version, \"path\": main_file}\n",
    "            \n",
    "            if main_file:\n",
    "                print(f\"‚úÖ {folder_id}: {main_file}\")\n",
    "                all_root_files.append(root_file_info)\n",
    "            else:\n",
    "                print(f\"‚ùå {folder_id}: Kh√¥ng t√¨m th·∫•y file g·ªëc!\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y ƒë∆∞·ªùng d·∫´n: {data_raw_path}\")\n",
    "    print(\"H√£y ki·ªÉm tra l·∫°i v·ªã tr√≠ file notebook so v·ªõi folder data_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3f81f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'paper_id': '2403-00530', 'version': '2403-00530v1', 'path': '../data_raw\\\\2403-00530\\\\tex\\\\2403-00530v1\\\\main.tex'}, {'paper_id': '2403-00530', 'version': '2403-00530v2', 'path': '../data_raw\\\\2403-00530\\\\tex\\\\2403-00530v2\\\\main_rerev2025.tex'}, {'paper_id': '2403-00531', 'version': '2403-00531v1', 'path': '../data_raw\\\\2403-00531\\\\tex\\\\2403-00531v1\\\\main.tex'}, {'paper_id': '2403-00531', 'version': '2403-00531v2', 'path': '../data_raw\\\\2403-00531\\\\tex\\\\2403-00531v2\\\\apssamp.tex'}, {'paper_id': '2403-00532', 'version': '2403-00532v1', 'path': '../data_raw\\\\2403-00532\\\\tex\\\\2403-00532v1\\\\file_arxiv.tex'}, {'paper_id': '2403-00532', 'version': '2403-00532v2', 'path': '../data_raw\\\\2403-00532\\\\tex\\\\2403-00532v2\\\\file_arxiv2.tex'}, {'paper_id': '2403-00533', 'version': '2403-00533v1', 'path': '../data_raw\\\\2403-00533\\\\tex\\\\2403-00533v1\\\\main.tex'}, {'paper_id': '2403-00533', 'version': '2403-00533v2', 'path': '../data_raw\\\\2403-00533\\\\tex\\\\2403-00533v2\\\\JCAP.tex'}, {'paper_id': '2403-00534', 'version': '2403-00534v1', 'path': '../data_raw\\\\2403-00534\\\\tex\\\\2403-00534v1\\\\1rdm.tex'}, {'paper_id': '2403-00535', 'version': '2403-00535v1', 'path': '../data_raw\\\\2403-00535\\\\tex\\\\2403-00535v1\\\\Winnberg-etal-jpg.tex'}, {'paper_id': '2403-00536', 'version': '2403-00536v1', 'path': '../data_raw\\\\2403-00536\\\\tex\\\\2403-00536v1\\\\arxiv-dynamic-knapsack.tex'}, {'paper_id': '2403-00537', 'version': '2403-00537v1', 'path': '../data_raw\\\\2403-00537\\\\tex\\\\2403-00537v1\\\\Paper.tex'}, {'paper_id': '2403-00537', 'version': '2403-00537v2', 'path': '../data_raw\\\\2403-00537\\\\tex\\\\2403-00537v2\\\\Paper.tex'}, {'paper_id': '2403-00538', 'version': '2403-00538v1', 'path': '../data_raw\\\\2403-00538\\\\tex\\\\2403-00538v1\\\\main.tex'}, {'paper_id': '2403-00538', 'version': '2403-00538v2', 'path': '../data_raw\\\\2403-00538\\\\tex\\\\2403-00538v2\\\\main_new.tex'}, {'paper_id': '2403-00539', 'version': '2403-00539v1', 'path': '../data_raw\\\\2403-00539\\\\tex\\\\2403-00539v1\\\\main.tex'}, {'paper_id': '2403-00540', 'version': '2403-00540v1', 'path': '../data_raw\\\\2403-00540\\\\tex\\\\2403-00540v1\\\\EpsilonGreedy.tex'}, {'paper_id': '2403-00540', 'version': '2403-00540v2', 'path': '../data_raw\\\\2403-00540\\\\tex\\\\2403-00540v2\\\\GreedyBO.tex'}, {'paper_id': '2403-00540', 'version': '2403-00540v3', 'path': '../data_raw\\\\2403-00540\\\\tex\\\\2403-00540v3\\\\GreedyBO.tex'}]\n"
     ]
    }
   ],
   "source": [
    "print(all_root_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0f99f7",
   "metadata": {},
   "source": [
    "## G·ªôp c√°c file tex l·∫°i\n",
    "\n",
    "Sau khi t√¨m ƒë∆∞·ª£c file root, th√¨ ta ti·∫øp t·ª•c d√πng regex ƒë·ªÉ g·ªôp c√°c file con v√†o file g·ªëc, t·∫°o th√†nh m·ªôt file LaTeX ho√†n ch·ªânh. Qu√° tr√¨nh n√†y s·∫Ω l·∫∑p l·∫°i ƒë·ªá quy cho ƒë·∫øn khi kh√¥ng c√≤n l·ªánh `\\input` hay `\\include` n√†o n·ªØa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556acf5b",
   "metadata": {},
   "source": [
    "Sau khi ƒë√£ x√°c ƒë·ªãnh ƒë∆∞·ª£c c√°c file, ta s·∫Ω g·ªôp l·∫°i b·∫±ng c√°ch dfs ƒë·ªÉ g·ªôp s√¢u h∆°n, sau khi duy·ªát h·∫øt 1 file th√¨ ta k·∫πp gi·ªØa 2 flag l√† \n",
    "\n",
    "```\n",
    "%--- BEGIN FILE: filename.tex ---\n",
    "%--- END FILE: filename.tex ---\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a38addd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LatexFlattener:\n",
    "    def __init__(self, root_file_path, paper_id, version):\n",
    "        self.root_path = os.path.abspath(root_file_path)\n",
    "        self.root_dir = os.path.dirname(self.root_path)\n",
    "        self.paper_id = paper_id\n",
    "        self.version = version\n",
    "        print(f\"üìù Kh·ªüi t·∫°o LatexFlattener cho Paper: {self.paper_id}, Version: {self.version}\")\n",
    "        self.merged_files = [] # Danh s√°ch c√°c file ƒë√£ g·ªôp th√†nh c√¥ng\n",
    "        self.missing_files = [] # Danh s√°ch c√°c file b·ªã thi·∫øu\n",
    "\n",
    "    def flatten(self):\n",
    "        \"\"\"\n",
    "        H√†m ch√≠nh: Th·ª±c hi·ªán g·ªôp v√† tr·∫£ v·ªÅ c·∫•u tr√∫c Dictionary (JSON object)\n",
    "        \"\"\"\n",
    "        # B·∫Øt ƒë·∫ßu ƒë·ªá quy t·ª´ root\n",
    "        full_content = self._process_file(self.root_path)\n",
    "        \n",
    "        # T·∫°o object k·∫øt qu·∫£\n",
    "        result_object = {\n",
    "            \"paper_id\": self.paper_id,\n",
    "            \"version\": self.version,\n",
    "            \"root_file_path\": self.root_path,\n",
    "            \"metadata\": {\n",
    "                \"total_length\": len(full_content),\n",
    "                \"merged_count\": len(self.merged_files),\n",
    "                \"merged_files\": self.merged_files,\n",
    "                \"missing_files\": self.missing_files\n",
    "            },\n",
    "            \"content\": full_content\n",
    "        }\n",
    "        return result_object\n",
    "\n",
    "    def _read_file(self, path):\n",
    "        if not os.path.exists(path): return None\n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                return f.read()\n",
    "        except: return None\n",
    "\n",
    "    def _remove_comments(self, text):\n",
    "        \"\"\"X√≥a comment g·ªëc c·ªßa t√°c gi·∫£ ƒë·ªÉ gi·∫£m nhi·ªÖu, nh∆∞ng gi·ªØ l·∫°i marker c·ªßa m√¨nh sau n√†y\"\"\"\n",
    "        # Regex: T√¨m k√Ω t·ª± % kh√¥ng ƒëi sau d·∫•u \\\n",
    "        return re.sub(r'(?<!\\\\)%.*', '', text)\n",
    "\n",
    "    def _remove_bibliography(self, text):\n",
    "        \"\"\"Lo·∫°i b·ªè ph·∫ßn t√†i li·ªáu tham kh·∫£o theo y√™u c·∫ßu\"\"\"\n",
    "        text = re.sub(r'\\\\bibliography\\{[^}]+\\}', '', text)\n",
    "        text = re.sub(r'\\\\printbibliography', '', text)\n",
    "        text = re.sub(r'\\\\begin\\{thebibliography\\}.*?\\\\end\\{thebibliography\\}', '', text, flags=re.DOTALL)\n",
    "        return text\n",
    "\n",
    "    def _process_file(self, current_path, visited=None):\n",
    "        if visited is None: visited = set()\n",
    "        \n",
    "        abs_path = os.path.abspath(current_path)\n",
    "        rel_path = os.path.relpath(abs_path, self.root_dir).replace('\\\\', '/') # Chu·∫©n h√≥a ƒë∆∞·ªùng d·∫´n\n",
    "        \n",
    "        # 1. Check v√≤ng l·∫∑p\n",
    "        if abs_path in visited:\n",
    "            return f\"\\n% <WARNING: Circular dependency detected for {rel_path}>\\n\"\n",
    "        visited.add(abs_path)\n",
    "\n",
    "        # 2. ƒê·ªçc n·ªôi dung\n",
    "        raw_content = self._read_file(abs_path)\n",
    "        if raw_content is None:\n",
    "            self.missing_files.append(rel_path)\n",
    "            return f\"\\n% <WARNING: File not found: {rel_path}>\\n\"\n",
    "        \n",
    "        self.merged_files.append(rel_path)\n",
    "\n",
    "        # 3. L√†m s·∫°ch s∆° b·ªô (X√≥a comment g·ªëc + X√≥a Bib)\n",
    "        content = self._remove_comments(raw_content)\n",
    "        content = self._remove_bibliography(content)\n",
    "\n",
    "        # 4. T√¨m v√† thay th·∫ø ƒë·ªá quy c√°c file con\n",
    "        # Regex h·ªó tr·ª£: \\input{file}, \\include{file}, \\subfile{file}, \\input file\n",
    "        pattern = re.compile(r'\\\\(?:input|include|subfile)(?:(?:\\s*\\{([^}]+)\\})|(?:\\s+([^\\s%]+)))')\n",
    "\n",
    "        def replace_match(match):\n",
    "            fname = match.group(1) or match.group(2)\n",
    "            if not fname: return \"\"\n",
    "            fname = fname.strip()\n",
    "            if not fname.lower().endswith('.tex'): fname += '.tex'\n",
    "            \n",
    "            # Resolve path\n",
    "            child_path = os.path.join(self.root_dir, fname)\n",
    "            if not os.path.exists(child_path):\n",
    "                child_path = os.path.join(os.path.dirname(abs_path), fname)\n",
    "            \n",
    "            # ƒê·ªá quy\n",
    "            child_content = self._process_file(child_path, visited)\n",
    "            \n",
    "            # QUAN TR·ªåNG: K·∫πp n·ªôi dung gi·ªØa 2 Marker\n",
    "            return (f\"\\n% <BEGIN_FILE: {fname}>\\n\"\n",
    "                    f\"{child_content}\"\n",
    "                    f\"\\n% <END_FILE: {fname}>\\n\")\n",
    "\n",
    "        flattened_content = pattern.sub(replace_match, content)\n",
    "        \n",
    "        return flattened_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d93c904",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- H√ÄM T√çCH H·ª¢P CHO TO√ÄN B·ªò DATASET ---\n",
    "def process_and_save_dataset(root_file_info_list, output_json_path):\n",
    "    \"\"\"\n",
    "    Input: Danh s√°ch c√°c root file t√¨m ƒë∆∞·ª£c t·ª´ b∆∞·ªõc tr∆∞·ªõc (list of dict)\n",
    "    Output: L∆∞u 1 file JSONL (m·ªói d√≤ng l√† 1 json object c·ªßa 1 b√†i b√°o)\n",
    "    \"\"\"\n",
    "    print(f\"üöÄ B·∫Øt ƒë·∫ßu Flattening {len(root_file_info_list)} versions...\")\n",
    "    \n",
    "    with open(output_json_path, 'w', encoding='utf-8') as f_out:\n",
    "        for info in tqdm(root_file_info_list, desc=\"Flattening\"):\n",
    "            root_path = info['path']\n",
    "            # print(info)\n",
    "            try:\n",
    "                flattener = LatexFlattener(root_path, paper_id=info['paper_id'], version=info['version'])\n",
    "                result = flattener.flatten()                \n",
    "                f_out.write(json.dumps(result, ensure_ascii=False) + \"\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"L·ªói khi x·ª≠ l√Ω {root_path}: {e}\")\n",
    "\n",
    "    print(f\"‚úÖ Ho√†n t·∫•t! D·ªØ li·ªáu ƒë√£ l∆∞u t·∫°i: {output_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d72aba03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ B·∫Øt ƒë·∫ßu Flattening 19 versions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:00<00:00, 431.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Kh·ªüi t·∫°o LatexFlattener cho Paper: 2403-00530, Version: 2403-00530v1\n",
      "üìù Kh·ªüi t·∫°o LatexFlattener cho Paper: 2403-00530, Version: 2403-00530v2\n",
      "üìù Kh·ªüi t·∫°o LatexFlattener cho Paper: 2403-00531, Version: 2403-00531v1\n",
      "üìù Kh·ªüi t·∫°o LatexFlattener cho Paper: 2403-00531, Version: 2403-00531v2\n",
      "üìù Kh·ªüi t·∫°o LatexFlattener cho Paper: 2403-00532, Version: 2403-00532v1\n",
      "üìù Kh·ªüi t·∫°o LatexFlattener cho Paper: 2403-00532, Version: 2403-00532v2\n",
      "üìù Kh·ªüi t·∫°o LatexFlattener cho Paper: 2403-00533, Version: 2403-00533v1\n",
      "üìù Kh·ªüi t·∫°o LatexFlattener cho Paper: 2403-00533, Version: 2403-00533v2\n",
      "üìù Kh·ªüi t·∫°o LatexFlattener cho Paper: 2403-00534, Version: 2403-00534v1\n",
      "üìù Kh·ªüi t·∫°o LatexFlattener cho Paper: 2403-00535, Version: 2403-00535v1\n",
      "üìù Kh·ªüi t·∫°o LatexFlattener cho Paper: 2403-00536, Version: 2403-00536v1\n",
      "üìù Kh·ªüi t·∫°o LatexFlattener cho Paper: 2403-00537, Version: 2403-00537v1\n",
      "üìù Kh·ªüi t·∫°o LatexFlattener cho Paper: 2403-00537, Version: 2403-00537v2\n",
      "üìù Kh·ªüi t·∫°o LatexFlattener cho Paper: 2403-00538, Version: 2403-00538v1\n",
      "üìù Kh·ªüi t·∫°o LatexFlattener cho Paper: 2403-00538, Version: 2403-00538v2\n",
      "üìù Kh·ªüi t·∫°o LatexFlattener cho Paper: 2403-00539, Version: 2403-00539v1\n",
      "üìù Kh·ªüi t·∫°o LatexFlattener cho Paper: 2403-00540, Version: 2403-00540v1\n",
      "üìù Kh·ªüi t·∫°o LatexFlattener cho Paper: 2403-00540, Version: 2403-00540v2\n",
      "üìù Kh·ªüi t·∫°o LatexFlattener cho Paper: 2403-00540, Version: 2403-00540v3\n",
      "‚úÖ Ho√†n t·∫•t! D·ªØ li·ªáu ƒë√£ l∆∞u t·∫°i: flattened_papers.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Gi·∫£ s·ª≠ 'all_root_files' l√† list ch·ª©a k·∫øt qu·∫£ t√¨m ƒë∆∞·ª£c t·ª´ b∆∞·ªõc tr∆∞·ªõc\n",
    "# V√≠ d·ª•: all_root_files = [{'path': 'data/2403-00611/tex/v1/main.tex', ...}, ...]\n",
    "\n",
    "# N·∫øu b·∫°n ch∆∞a c√≥ list n√†y, h√£y ch·∫°y l·∫°i h√†m find_root_in_version cho to√†n b·ªô dataset v√† gom v√†o 1 list\n",
    "\n",
    "OUTPUT_FILE = \"flattened_papers.jsonl\"\n",
    "\n",
    "# Ch·∫°y x·ª≠ l√Ω h√†ng lo·∫°t\n",
    "if 'all_root_files' in globals() and all_root_files:\n",
    "    process_and_save_dataset(all_root_files, OUTPUT_FILE)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è H√£y ƒë·∫£m b·∫£o b·∫°n ƒë√£ c√≥ bi·∫øn 'all_root_files' ch·ª©a danh s√°ch c√°c file g·ªëc.\")\n",
    "    \n",
    "    # Test th·ª≠ v·ªõi 1 file c·ª• th·ªÉ\n",
    "    # sample_root = r\"ƒê∆Ø·ªúNG_D·∫™N_ƒê·∫æN_FILE_ROOT_TH·∫¨T_C·ª¶A_B·∫†N\"\n",
    "    # flattener = LatexFlattener(sample_root)\n",
    "    # print(json.dumps(flattener.flatten(), indent=2)[:1000]) # In th·ª≠ 1000 k√Ω t·ª± ƒë·∫ßu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd5c6b3",
   "metadata": {},
   "source": [
    "## Ch·∫°y th·ª≠ nghi·ªám t√≠nh ƒë√∫ng ƒë·∫Øn c·ªßa thu·∫≠t to√°n g·ªôp file tr√™n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6afe2fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ ƒêang kh·ªüi t·∫°o m√¥i tr∆∞·ªùng test t·∫°i: C:\\Users\\duyla\\AppData\\Local\\Temp\\tmpjbn3gwu0\n",
      "üìù Kh·ªüi t·∫°o LatexFlattener cho Paper: test-id, Version: test-version\n",
      "\n",
      "‚úÖ K·∫æT QU·∫¢ FLATTEN JSON:\n",
      "- Root File: C:\\Users\\duyla\\AppData\\Local\\Temp\\tmpjbn3gwu0\\main.tex\n",
      "- T·ªïng ƒë·ªô d√†i: 900 chars\n",
      "- Files ƒë√£ g·ªôp: [\n",
      "  \"main.tex\",\n",
      "  \"chapters/intro.tex\",\n",
      "  \"chapters/deep_logic/theory.tex\",\n",
      "  \"common_macros.tex\",\n",
      "  \"chapters/deep_logic/components/equation.tex\"\n",
      "]\n",
      "- Kh√¥ng c√≥ file n√†o b·ªã thi·∫øu.\n",
      "‚úÖ PASS: Comment ƒë√£ ƒë∆∞·ª£c x√≥a.\n",
      "‚úÖ PASS: Bibliography ƒë√£ ƒë∆∞·ª£c x√≥a.\n",
      "\n",
      "üå≥ VISUALIZATION (C·∫§U TR√öC SAU KHI G·ªòP):\n",
      "==================================================\n",
      "|  \"\\documentclass{article}\"\n",
      "|  \"\\usepackage{import}\"\n",
      "|  \"\\begin{document}\"\n",
      "|  \"\\section{Main Title}\"\n",
      "|  \"This is the root file.\"\n",
      "‚îú‚îÄ‚îÄ üìÇ M·ªû FILE: chapters/intro.tex\n",
      "|   |  \"\\section{Introduction}\"\n",
      "|   |  \"Intro content here.\"\n",
      "|   ‚îú‚îÄ‚îÄ üìÇ M·ªû FILE: deep_logic/theory.tex\n",
      "|   |   |  \"\\subsection{Theoretical Framework}\"\n",
      "|   |   |  \"Theory content.\"\n",
      "|   |   ‚îú‚îÄ‚îÄ üìÇ M·ªû FILE: ../../common_macros.tex\n",
      "|   |   |   |  \"\\def\\mycommand{Macro Loaded Successfully...\"\n",
      "|   |   ‚îî‚îÄ‚îÄ üèÅ ƒê√ìNG FILE: ../../common_macros.tex\n",
      "|   |   ‚îú‚îÄ‚îÄ üìÇ M·ªû FILE: components/equation.tex\n",
      "|   |   |   |  \"\\begin{equation}\"\n",
      "|   |   |   |  \"E = mc^2\"\n",
      "|   |   |   |  \"\\end{equation}\"\n",
      "|   |   |   |  \"Final deepest content.\"\n",
      "|   |   ‚îî‚îÄ‚îÄ üèÅ ƒê√ìNG FILE: components/equation.tex\n",
      "|   ‚îî‚îÄ‚îÄ üèÅ ƒê√ìNG FILE: deep_logic/theory.tex\n",
      "‚îî‚îÄ‚îÄ üèÅ ƒê√ìNG FILE: chapters/intro.tex\n",
      "|  \"\\section{Conclusion}\"\n",
      "|  \"End of main.\"\n",
      "|  \"\\end{document}\"\n",
      "==================================================\n",
      "\n",
      "üßπ ƒê√£ d·ªçn d·∫πp m√¥i tr∆∞·ªùng test.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import json\n",
    "import re\n",
    "\n",
    "# ==========================================\n",
    "# 1. SETUP: T·∫†O M√ä CUNG FILE (COMPLEX STRUCTURE)\n",
    "# ==========================================\n",
    "\n",
    "def create_complex_test_env():\n",
    "    # T·∫°o th∆∞ m·ª•c t·∫°m\n",
    "    base_dir = tempfile.mkdtemp()\n",
    "    \n",
    "    # C·∫•u tr√∫c file s·∫Ω t·∫°o:\n",
    "    # base_dir/\n",
    "    # ‚îú‚îÄ‚îÄ main.tex                  (ROOT)\n",
    "    # ‚îú‚îÄ‚îÄ common_macros.tex         (ƒê∆∞·ª£c g·ªçi t·ª´ folder con b·∫±ng ../)\n",
    "    # ‚îú‚îÄ‚îÄ references.bib            (File r√°c, c·∫ßn b·ªã ignore)\n",
    "    # ‚îî‚îÄ‚îÄ chapters/\n",
    "    #     ‚îú‚îÄ‚îÄ intro.tex             (Level 1)\n",
    "    #     ‚îî‚îÄ‚îÄ deep_logic/\n",
    "    #         ‚îú‚îÄ‚îÄ theory.tex        (Level 2)\n",
    "    #         ‚îî‚îÄ‚îÄ components/\n",
    "    #             ‚îî‚îÄ‚îÄ equation.tex  (Level 3 - S√¢u nh·∫•t)\n",
    "\n",
    "    files = {\n",
    "        # ROOT FILE\n",
    "        \"main.tex\": r\"\"\"\n",
    "        \\documentclass{article}\n",
    "        \\usepackage{import}\n",
    "        \\begin{document}\n",
    "        \\section{Main Title}\n",
    "        This is the root file.\n",
    "        \n",
    "        % Test x√≥a comment: D√≤ng n√†y kh√¥ng ƒë∆∞·ª£c xu·∫•t hi·ªán trong output.\n",
    "        \n",
    "        % Trap 1: G·ªçi file trong folder con\n",
    "        \\input{chapters/intro}\n",
    "        \n",
    "        \\section{Conclusion}\n",
    "        End of main.\n",
    "        \\bibliography{references} % Trap 2: Ph·∫£i x√≥a d√≤ng n√†y\n",
    "        \\end{document}\n",
    "        \"\"\",\n",
    "\n",
    "        # FILE N·∫∞M NGO√ÄI (D√πng ƒë·ªÉ test ../)\n",
    "        \"common_macros.tex\": r\"\"\"\n",
    "        \\def\\mycommand{Macro Loaded Successfully}\n",
    "        \"\"\",\n",
    "\n",
    "        # LEVEL 1\n",
    "        \"chapters/intro.tex\": r\"\"\"\n",
    "        \\section{Introduction}\n",
    "        Intro content here.\n",
    "        \n",
    "        % Trap 3: G·ªçi s√¢u h∆°n (Level 2)\n",
    "        \\include{deep_logic/theory}\n",
    "        \"\"\",\n",
    "\n",
    "        # LEVEL 2\n",
    "        \"chapters/deep_logic/theory.tex\": r\"\"\"\n",
    "        \\subsection{Theoretical Framework}\n",
    "        Theory content.\n",
    "        \n",
    "        % Trap 4: G·ªçi ng∆∞·ª£c l√™n th∆∞ m·ª•c cha c·ªßa root (Relative path tricky)\n",
    "        \\input{../../common_macros.tex}\n",
    "        \n",
    "        % Trap 5: G·ªçi s√¢u n·ªØa (Level 3)\n",
    "        \\subfile{components/equation}\n",
    "        \"\"\",\n",
    "\n",
    "        # LEVEL 3\n",
    "        \"chapters/deep_logic/components/equation.tex\": r\"\"\"\n",
    "        \\begin{equation}\n",
    "            E = mc^2\n",
    "        \\end{equation}\n",
    "        Final deepest content.\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "    print(f\"üìÇ ƒêang kh·ªüi t·∫°o m√¥i tr∆∞·ªùng test t·∫°i: {base_dir}\")\n",
    "    for rel_path, content in files.items():\n",
    "        abs_path = os.path.join(base_dir, rel_path)\n",
    "        os.makedirs(os.path.dirname(abs_path), exist_ok=True)\n",
    "        with open(abs_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(content.strip())\n",
    "            \n",
    "    return base_dir, os.path.join(base_dir, \"main.tex\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. EXECUTION: CH·∫†Y FLATTENER\n",
    "# ==========================================\n",
    "\n",
    "def visualize_structure(content):\n",
    "    \"\"\"\n",
    "    H√†m n√†y parse c√°c marker <BEGIN_FILE...> ƒë·ªÉ v·∫Ω l·∫°i c√¢y c·∫•u tr√∫c\n",
    "    Gi√∫p m·∫Øt th∆∞·ªùng ki·ªÉm tra xem vi·ªác l·ªìng gh√©p c√≥ ƒë√∫ng th·ª© t·ª± kh√¥ng.\n",
    "    \"\"\"\n",
    "    lines = content.split('\\n')\n",
    "    indent_level = 0\n",
    "    tree_visualization = []\n",
    "    \n",
    "    print(\"\\nüå≥ VISUALIZATION (C·∫§U TR√öC SAU KHI G·ªòP):\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for line in lines:\n",
    "        # N·∫øu g·∫∑p Marker Begin -> TƒÉng th·ª•t ƒë·∫ßu d√≤ng\n",
    "        match_begin = re.search(r'% <BEGIN_FILE: (.*?)>', line)\n",
    "        if match_begin:\n",
    "            fname = match_begin.group(1)\n",
    "            print(f\"{'|   ' * indent_level}‚îú‚îÄ‚îÄ üìÇ M·ªû FILE: {fname}\")\n",
    "            indent_level += 1\n",
    "            continue\n",
    "            \n",
    "        # N·∫øu g·∫∑p Marker End -> Gi·∫£m th·ª•t ƒë·∫ßu d√≤ng\n",
    "        match_end = re.search(r'% <END_FILE: (.*?)>', line)\n",
    "        if match_end:\n",
    "            indent_level -= 1\n",
    "            fename = match_end.group(1)\n",
    "            print(f\"{'|   ' * indent_level}‚îî‚îÄ‚îÄ üèÅ ƒê√ìNG FILE: {fename}\")\n",
    "            continue\n",
    "            \n",
    "        # In n·ªôi dung text (ƒë·∫°i di·ªán)\n",
    "        if line.strip() and not line.strip().startswith('%'):\n",
    "            # C·∫Øt ng·∫Øn n·ªôi dung ƒë·ªÉ d·ªÖ nh√¨n\n",
    "            preview = (line.strip()[:40] + '...') if len(line.strip()) > 40 else line.strip()\n",
    "            print(f\"{'|   ' * indent_level}|  \\\"{preview}\\\"\")\n",
    "\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# ==========================================\n",
    "# 3. MAIN RUNNER\n",
    "# ==========================================\n",
    "\n",
    "try:\n",
    "    # 1. T·∫°o Lab\n",
    "    temp_dir, root_file = create_complex_test_env()\n",
    "    \n",
    "    # 2. G·ªçi Class c·ªßa b·∫°n (Gi·∫£ s·ª≠ class LatexFlattener ƒë√£ ƒë∆∞·ª£c define ·ªü cell tr∆∞·ªõc)\n",
    "    # N·∫øu ch∆∞a c√≥ class, code s·∫Ω b√°o l·ªói NameError\n",
    "    flattener = LatexFlattener(root_file, 'test-id', 'test-version')\n",
    "    result = flattener.flatten()\n",
    "    \n",
    "    # 3. Ki·ªÉm tra k·∫øt qu·∫£\n",
    "    print(\"\\n‚úÖ K·∫æT QU·∫¢ FLATTEN JSON:\")\n",
    "    print(f\"- Root File: {result['root_file_path']}\")\n",
    "    print(f\"- T·ªïng ƒë·ªô d√†i: {result['metadata']['total_length']} chars\")\n",
    "    print(f\"- Files ƒë√£ g·ªôp: {json.dumps(result['metadata']['merged_files'], indent=2)}\")\n",
    "    \n",
    "    if result['metadata']['missing_files']:\n",
    "        print(f\"‚ö†Ô∏è Files b·ªã thi·∫øu: {result['metadata']['missing_files']}\")\n",
    "    else:\n",
    "        print(\"- Kh√¥ng c√≥ file n√†o b·ªã thi·∫øu.\")\n",
    "\n",
    "    # 4. Ki·ªÉm tra logic lo·∫°i b·ªè r√°c\n",
    "    content = result['content']\n",
    "    if \"Test x√≥a comment\" in content:\n",
    "        print(\"‚ùå L·ªñI: Comment ch∆∞a ƒë∆∞·ª£c x√≥a!\")\n",
    "    else:\n",
    "        print(\"‚úÖ PASS: Comment ƒë√£ ƒë∆∞·ª£c x√≥a.\")\n",
    "        \n",
    "    if \"\\\\bibliography\" in content:\n",
    "        print(\"‚ùå L·ªñI: Bibliography ch∆∞a ƒë∆∞·ª£c x√≥a!\")\n",
    "    else:\n",
    "        print(\"‚úÖ PASS: Bibliography ƒë√£ ƒë∆∞·ª£c x√≥a.\")\n",
    "\n",
    "    # 5. V·∫Ω c√¢y ƒë·ªÉ ng∆∞·ªùi d√πng verify\n",
    "    visualize_structure(content)\n",
    "\n",
    "finally:\n",
    "    # D·ªçn d·∫πp\n",
    "    if 'temp_dir' in globals() and os.path.exists(temp_dir):\n",
    "        shutil.rmtree(temp_dir)\n",
    "        print(\"\\nüßπ ƒê√£ d·ªçn d·∫πp m√¥i tr∆∞·ªùng test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c0d70e",
   "metadata": {},
   "source": [
    "K·∫øt lu·∫≠n: Thu·∫≠t to√°n g·ªôp file ho·∫°t ƒë·ªông ch√≠nh x√°c, c√°c file con ƒë∆∞·ª£c ch√®n ƒë√∫ng v·ªã tr√≠ trong file g·ªëc, v√† c·∫•u tr√∫c t√†i li·ªáu LaTeX ƒë∆∞·ª£c duy tr√¨ nguy√™n v·∫πn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae934acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import uuid\n",
    "\n",
    "class LatexStructureBuilder:\n",
    "    def __init__(self, flattened_content, paper_id, version):\n",
    "        self.content = flattened_content\n",
    "        self.paper_id = paper_id\n",
    "        self.version = version\n",
    "        # ƒê·ªãnh nghƒ©a th·ª© t·ª± c·∫•p b·∫≠c (nh·ªè h∆°n l√† c·∫•p cao h∆°n/cha)\n",
    "        self.HIERARCHY_LEVELS = {\n",
    "            'document': 0,      # Root\n",
    "            'part': 1,\n",
    "            'chapter': 2,\n",
    "            'section': 3,\n",
    "            'subsection': 4,\n",
    "            'subsubsection': 5,\n",
    "            'paragraph': 6,\n",
    "            'subparagraph': 7\n",
    "        }\n",
    "        # Regex ƒë·ªÉ b·∫Øt c√°c header: \\section{Title}, \\section*{Title}, \\chapter{...}\n",
    "        # Group 1: command (section, chapter...)\n",
    "        # Group 2: * (n·∫øu c√≥)\n",
    "        # Group 3: Title\n",
    "        self.SECTION_REGEX = re.compile(\n",
    "            r'\\\\(part|chapter|section|subsection|subsubsection|paragraph|subparagraph)(\\*?)\\s*\\{([^}]+)\\}',\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "\n",
    "    def build_coarse_tree(self):\n",
    "        \"\"\"\n",
    "        X√¢y d·ª±ng c√¢y c·∫•u tr√∫c th√¥.\n",
    "        Tr·∫£ v·ªÅ node g·ªëc (Document) ch·ª©a to√†n b·ªô c√¢y con.\n",
    "        \"\"\"\n",
    "        # 1. T·∫°o Node g·ªëc (Document) [cite: 584]\n",
    "        root = {\n",
    "            'id': f'{self.paper_id}-{version}-document-{uuid.uuid4()}',\n",
    "            'type': 'document',\n",
    "            'title': 'Root Document',\n",
    "            'level': 0,\n",
    "            'raw_content': \"\", # N·ªôi dung m·ªü ƒë·∫ßu tr∆∞·ªõc khi v√†o section ƒë·∫ßu ti√™n (v√≠ d·ª• abstract)\n",
    "            'children': []\n",
    "        }\n",
    "\n",
    "        # Stack ƒë·ªÉ theo d√µi cha hi·ªán t·∫°i. Kh·ªüi ƒë·∫ßu ch·ªâ c√≥ root.\n",
    "        # Stack store: reference t·ªõi c√°c node dictionary\n",
    "        stack = [root]\n",
    "\n",
    "        # 2. T√¨m t·∫•t c·∫£ c√°c v·ªã tr√≠ section headers\n",
    "        matches = list(self.SECTION_REGEX.finditer(self.content))\n",
    "        \n",
    "        # Bi·∫øn cursor ƒë·ªÉ theo d√µi v·ªã tr√≠ c·∫Øt text\n",
    "        cursor = 0\n",
    "\n",
    "        for match in matches:\n",
    "            start_idx = match.start()\n",
    "            end_idx = match.end()\n",
    "            \n",
    "            command = match.group(1)\n",
    "            is_starred = match.group(2) == '*'\n",
    "            title = match.group(3).strip()\n",
    "            \n",
    "            # L·∫•y level c·ªßa command hi·ªán t·∫°i\n",
    "            current_level = self.HIERARCHY_LEVELS.get(command, 100)\n",
    "\n",
    "            # --- B∆Ø·ªöC A: G√ÅN N·ªòI DUNG CHO NODE ƒêANG ACTIVE ---\n",
    "            # Text t·ª´ con tr·ªè c≈© ƒë·∫øn ƒë·∫ßu header m·ªõi thu·ªôc v·ªÅ node ƒëang n·∫±m tr√™n ƒë·ªânh stack\n",
    "            # (t·ª©c l√† node cha ho·∫∑c anh em li·ªÅn tr∆∞·ªõc)\n",
    "            text_segment = self.content[cursor:start_idx]\n",
    "            if text_segment.strip():\n",
    "                # G√°n v√†o content c·ªßa node hi·ªán t·∫°i tr√™n ƒë·ªânh stack\n",
    "                # L∆∞u √Ω: ·ªû Part 1, ta g·ªôp th√¥, ch∆∞a chia nh·ªè sentences\n",
    "                if 'raw_content' not in stack[-1]:\n",
    "                    stack[-1]['raw_content'] = \"\"\n",
    "                stack[-1]['raw_content'] += text_segment\n",
    "\n",
    "            # --- B∆Ø·ªöC B: T√åM CHA CHO NODE M·ªöI (Adjust Stack) ---\n",
    "            # Logic: Pop stack cho ƒë·∫øn khi g·∫∑p node c√≥ level nh·ªè h∆°n (cao h∆°n) node hi·ªán t·∫°i\n",
    "            # V√≠ d·ª•: ƒêang ·ªü Subsection (4), g·∫∑p Section (3) -> Pop Subsection ra, ƒë·ªÉ Section m·ªõi l√†m con c·ªßa Chapter (2)\n",
    "            # N·∫øu g·∫∑p Subsection kh√°c (4) -> Pop Subsection c≈© ra, Subsection m·ªõi thay th·∫ø v·ªã tr√≠ ƒë√≥ (anh em).\n",
    "            \n",
    "            while len(stack) > 1 and stack[-1]['level'] >= current_level:\n",
    "                stack.pop()\n",
    "            \n",
    "            # Parent l√† node ƒëang n·∫±m tr√™n ƒë·ªânh stack sau khi pop\n",
    "            parent = stack[-1]\n",
    "\n",
    "            # --- B∆Ø·ªöC C: T·∫†O NODE M·ªöI ---\n",
    "            # L∆∞u √Ω: B·ªè qua ph·∫ßn References/Bibliography n·∫øu ti√™u ƒë·ªÅ match \n",
    "            if title.lower() in ['references', 'bibliography', 't√†i li·ªáu tham kh·∫£o']:\n",
    "                # Update cursor ƒë·ªÉ b·ªè qua header n√†y, nh∆∞ng c·∫ßn c·∫©n th·∫≠n ph·∫ßn content b√™n d∆∞·ªõi\n",
    "                # V·ªõi logic n√†y, content b√™n d∆∞·ªõi References s·∫Ω b·ªã g√°n v√†o node cha c·ªßa references\n",
    "                # T·∫°m th·ªùi ta v·∫´n t·∫°o node ƒë·ªÉ ch·ª©a, sau n√†y filter b·ªè sau.\n",
    "                pass \n",
    "\n",
    "            new_node = {\n",
    "                # 'id': self.paper_id + f'-{command}-' + str(uuid.uuid4()),\n",
    "                'id': f'{self.paper_id}-{version}-{command}-{uuid.uuid4()}',\n",
    "                'type': command,\n",
    "                'title': title,\n",
    "                'level': current_level,\n",
    "                'raw_content': \"\", # S·∫Ω ƒë∆∞·ª£c ƒëi·ªÅn ·ªü v√≤ng l·∫∑p k·∫ø ti·∫øp ho·∫∑c sau v√≤ng l·∫∑p\n",
    "                'children': []\n",
    "            }\n",
    "\n",
    "            # Link v√†o parent\n",
    "            parent['children'].append(new_node)\n",
    "            \n",
    "            # Push node m·ªõi v√†o stack ƒë·ªÉ n√≥ h·ª©ng content ti·∫øp theo\n",
    "            stack.append(new_node)\n",
    "            \n",
    "            # C·∫≠p nh·∫≠t cursor = v·ªã tr√≠ k·∫øt th√∫c c·ªßa header v·ª´a x·ª≠ l√Ω\n",
    "            cursor = end_idx\n",
    "\n",
    "        # --- B∆Ø·ªöC D: X·ª¨ L√ù PH·∫¶N D∆Ø CU·ªêI C√ôNG ---\n",
    "        # Text t·ª´ header cu·ªëi c√πng ƒë·∫øn h·∫øt file thu·ªôc v·ªÅ node ƒëang n·∫±m tr√™n ƒë·ªânh stack\n",
    "        remaining_text = self.content[cursor:]\n",
    "        if remaining_text.strip():\n",
    "             if 'raw_content' not in stack[-1]:\n",
    "                    stack[-1]['raw_content'] = \"\"\n",
    "             stack[-1]['raw_content'] += remaining_text\n",
    "\n",
    "        return root\n",
    "\n",
    "    def print_tree(self, node, indent=0):\n",
    "        \"\"\"H√†m helper ƒë·ªÉ in c√¢y ra console ki·ªÉm tra\"\"\"\n",
    "        prefix = \"  \" * indent\n",
    "        preview = (node.get('raw_content', '')[:50] + '...') if node.get('raw_content') else \"[Empty]\"\n",
    "        print(f\"{prefix}- [{node['type'].upper()}] {node['title']} (ID: {node['id'][:8]})\")\n",
    "        print(f\"{prefix}  Content Preview: {preview}\")\n",
    "        \n",
    "        for child in node['children']:\n",
    "            self.print_tree(child, indent + 1)\n",
    "    \n",
    "    def print_tree_to_file(self, root_node, output_path):\n",
    "        \"\"\"\n",
    "        In c√¢y ra file JSON v·ªõi danh s√°ch nodes v√† edges\n",
    "        \n",
    "        Args:\n",
    "            root_node: Node g·ªëc c·ªßa c√¢y\n",
    "            output_path: ƒê∆∞·ªùng d·∫´n file JSON ƒë·ªÉ l∆∞u\n",
    "        \n",
    "        Output format:\n",
    "        {\n",
    "            \"nodes\": [\n",
    "                {\n",
    "                    \"id\": \"uuid\",\n",
    "                    \"type\": \"section\",\n",
    "                    \"title\": \"Section Title\",\n",
    "                    \"level\": 3,\n",
    "                    \"content\": \"raw content...\"\n",
    "                }\n",
    "            ],\n",
    "            \"edges\": [\n",
    "                {\n",
    "                    \"from\": \"parent_id\",\n",
    "                    \"to\": \"child_id\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \"\"\"\n",
    "        nodes = []\n",
    "        edges = []\n",
    "        \n",
    "        def traverse(node, parent_id=None):\n",
    "            # Th√™m node hi·ªán t·∫°i v√†o danh s√°ch\n",
    "            node_data = {\n",
    "                \"id\": node['id'],\n",
    "                \"type\": node['type'],\n",
    "                \"title\": node['title'],\n",
    "                \"level\": node['level'],\n",
    "                \"content\": node.get('raw_content', '')\n",
    "            }\n",
    "            nodes.append(node_data)\n",
    "            \n",
    "            # N·∫øu c√≥ parent, t·∫°o edge\n",
    "            if parent_id is not None:\n",
    "                edges.append({\n",
    "                    \"from\": parent_id,\n",
    "                    \"to\": node['id']\n",
    "                })\n",
    "            \n",
    "            # ƒê·ªá quy cho c√°c children\n",
    "            for child in node.get('children', []):\n",
    "                traverse(child, node['id'])\n",
    "        \n",
    "        # B·∫Øt ƒë·∫ßu traverse t·ª´ root\n",
    "        traverse(root_node)\n",
    "        \n",
    "        # T·∫°o c·∫•u tr√∫c d·ªØ li·ªáu cu·ªëi c√πng\n",
    "        output_data = {\n",
    "            \"nodes\": nodes,\n",
    "            \"edges\": edges,\n",
    "            \"metadata\": {\n",
    "                \"total_nodes\": len(nodes),\n",
    "                \"total_edges\": len(edges)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Ghi ra file\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"‚úÖ ƒê√£ l∆∞u c·∫•u tr√∫c c√¢y v√†o: {output_path}\")\n",
    "        print(f\"   - T·ªïng s·ªë nodes: {len(nodes)}\")\n",
    "        print(f\"   - T·ªïng s·ªë edges: {len(edges)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "134e341f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "% <WARNING: File not found: main.tex>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- V√ç D·ª§ S·ª¨ D·ª§NG V·ªöI CLASS C≈® ---\n",
    "# Gi·∫£ s·ª≠ b·∫°n ƒë√£ c√≥ bi·∫øn `full_content` t·ª´ LatexFlattener\n",
    "flattened_content = flattener.flatten()['content']\n",
    "print(flattened_content)\n",
    "# builder = LatexStructureBuilder(flattened_content)\n",
    "# root_tree = builder.build_coarse_tree()\n",
    "# builder.print_tree(root_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3158d9e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d98a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f309f5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ load 19 papers t·ª´ flattened_papers.jsonl\n",
      "\n",
      "üìä TH√îNG TIN T·ªîNG QUAN:\n",
      "============================================================\n",
      "1. Paper ID: 2403-00530 - Version: 2403-00530v1\n",
      "   - Content length: 41,291 chars\n",
      "   - Merged files: 1\n",
      "2. Paper ID: 2403-00530 - Version: 2403-00530v2\n",
      "   - Content length: 51,454 chars\n",
      "   - Merged files: 1\n",
      "3. Paper ID: 2403-00531 - Version: 2403-00531v1\n",
      "   - Content length: 62,735 chars\n",
      "   - Merged files: 1\n",
      "4. Paper ID: 2403-00531 - Version: 2403-00531v2\n",
      "   - Content length: 72,185 chars\n",
      "   - Merged files: 1\n",
      "5. Paper ID: 2403-00532 - Version: 2403-00532v1\n",
      "   - Content length: 179,231 chars\n",
      "   - Merged files: 1\n",
      "6. Paper ID: 2403-00532 - Version: 2403-00532v2\n",
      "   - Content length: 180,119 chars\n",
      "   - Merged files: 1\n",
      "7. Paper ID: 2403-00533 - Version: 2403-00533v1\n",
      "   - Content length: 64,852 chars\n",
      "   - Merged files: 1\n",
      "8. Paper ID: 2403-00533 - Version: 2403-00533v2\n",
      "   - Content length: 64,048 chars\n",
      "   - Merged files: 1\n",
      "9. Paper ID: 2403-00534 - Version: 2403-00534v1\n",
      "   - Content length: 59,864 chars\n",
      "   - Merged files: 2\n",
      "10. Paper ID: 2403-00535 - Version: 2403-00535v1\n",
      "   - Content length: 231,271 chars\n",
      "   - Merged files: 2\n",
      "11. Paper ID: 2403-00536 - Version: 2403-00536v1\n",
      "   - Content length: 219,369 chars\n",
      "   - Merged files: 1\n",
      "12. Paper ID: 2403-00537 - Version: 2403-00537v1\n",
      "   - Content length: 38,632 chars\n",
      "   - Merged files: 1\n",
      "13. Paper ID: 2403-00537 - Version: 2403-00537v2\n",
      "   - Content length: 37,469 chars\n",
      "   - Merged files: 1\n",
      "14. Paper ID: 2403-00538 - Version: 2403-00538v1\n",
      "   - Content length: 31,880 chars\n",
      "   - Merged files: 1\n",
      "15. Paper ID: 2403-00538 - Version: 2403-00538v2\n",
      "   - Content length: 39,480 chars\n",
      "   - Merged files: 1\n",
      "16. Paper ID: 2403-00539 - Version: 2403-00539v1\n",
      "   - Content length: 61,065 chars\n",
      "   - Merged files: 1\n",
      "17. Paper ID: 2403-00540 - Version: 2403-00540v1\n",
      "   - Content length: 39,675 chars\n",
      "   - Merged files: 1\n",
      "18. Paper ID: 2403-00540 - Version: 2403-00540v2\n",
      "   - Content length: 54,726 chars\n",
      "   - Merged files: 1\n",
      "19. Paper ID: 2403-00540 - Version: 2403-00540v3\n",
      "   - Content length: 57,622 chars\n",
      "   - Merged files: 1\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ƒê·ªçc file flattened_papers.jsonl v√† test LatexStructureBuilder\n",
    "\n",
    "import json\n",
    "\n",
    "def load_flattened_papers(jsonl_path):\n",
    "    \"\"\"\n",
    "    ƒê·ªçc file JSONL v√† tr·∫£ v·ªÅ list c√°c paper objects\n",
    "    \"\"\"\n",
    "    papers = []\n",
    "    try:\n",
    "        with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.strip():  # B·ªè qua d√≤ng tr·ªëng\n",
    "                    paper = json.loads(line)\n",
    "                    papers.append(paper)\n",
    "        print(f\"‚úÖ ƒê√£ load {len(papers)} papers t·ª´ {jsonl_path}\")\n",
    "        return papers\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Kh√¥ng t√¨m th·∫•y file: {jsonl_path}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói khi ƒë·ªçc file: {e}\")\n",
    "        return []\n",
    "\n",
    "# Load papers\n",
    "papers = load_flattened_papers(\"flattened_papers.jsonl\")\n",
    "\n",
    "# Hi·ªÉn th·ªã th√¥ng tin t·ªïng quan\n",
    "if papers:\n",
    "    print(\"\\nüìä TH√îNG TIN T·ªîNG QUAN:\")\n",
    "    print(\"=\"*60)\n",
    "    for i, paper in enumerate(papers, 1):\n",
    "        print(f\"{i}. Paper ID: {paper['paper_id']} - Version: {paper['version']}\")\n",
    "        print(f\"   - Content length: {paper['metadata']['total_length']:,} chars\")\n",
    "        print(f\"   - Merged files: {paper['metadata']['merged_count']}\")\n",
    "        if paper['metadata']['missing_files']:\n",
    "            print(f\"   - Missing files: {len(paper['metadata']['missing_files'])}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3178177d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c88500d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing v·ªõi Paper: 2403-00530 - 2403-00530v1\n",
      "üìÑ Content length: 41,291 characters\n",
      "\n",
      "\n",
      "üå≥ C·∫§U TR√öC C√ÇY (TREE STRUCTURE):\n",
      "======================================================================\n",
      "- [DOCUMENT] Root Document (ID: 2403-005)\n",
      "  Content Preview: \\documentclass[\n",
      "twocolumn,\n",
      "superscriptaddress,\n",
      "ams...\n",
      "  - [SECTION] Introduction (ID: 2403-005)\n",
      "    Content Preview: \\label{intro}\n",
      "\n",
      "Nearly flat bands confined within a...\n",
      "  - [SECTION] Extended Hubbard corrected tight-binding model (ID: 2403-005)\n",
      "    Content Preview: \\label{sec2}\n",
      "...\n",
      "    - [SUBSECTION] Interacting correction (ID: 2403-005)\n",
      "      Content Preview: \\label{sec2-1}\n",
      "\n",
      "\\begin{figure}[tb!] \n",
      "\\includegraph...\n",
      "    - [SUBSECTION] Rhombohedral stacked N-layer graphene (ID: 2403-005)\n",
      "      Content Preview: \\label{rhomboG}\n",
      "\n",
      "\\begin{table}[tb!] \n",
      "\\begin{threep...\n",
      "  - [SECTION] Realistic gapped states and Flatbands (ID: 2403-005)\n",
      "    Content Preview: \\label{sec3}\n",
      "\n",
      "\\begin{figure*}[bt!] \n",
      "\\subfloat[\\lab...\n",
      "  - [SECTION] Conclusion (ID: 2403-005)\n",
      "    Content Preview: \\label{conclusion}\n",
      "\n",
      "In this paper, we introduced t...\n",
      "  - [SECTION] Non-corrected Tight-binding Hamiltonian (ID: 2403-005)\n",
      "    Content Preview: \\label{Appendix1}\n",
      "\n",
      "In this appendix, we list the F...\n",
      "======================================================================\n",
      "\n",
      "üìå Paper Info:\n",
      "   - Paper ID: 2403-00530\n",
      "   - Version: 2403-00530v1\n",
      "‚úÖ ƒê√£ l∆∞u c·∫•u tr√∫c c√¢y v√†o: structure_2403-00530_2403-00530v1.json\n",
      "   - T·ªïng s·ªë nodes: 8\n",
      "   - T·ªïng s·ªë edges: 7\n",
      "\n",
      "üìä T·ªïng s·ªë nodes: 8\n",
      "üìä S·ªë children c·ªßa root: 5\n"
     ]
    }
   ],
   "source": [
    "# Test LatexStructureBuilder v·ªõi paper ƒë·∫ßu ti√™n\n",
    "\n",
    "if papers:\n",
    "    # L·∫•y paper ƒë·∫ßu ti√™n\n",
    "    test_paper = papers[0]\n",
    "    print(f\"üß™ Testing v·ªõi Paper: {test_paper['paper_id']} - {test_paper['version']}\")\n",
    "    print(f\"üìÑ Content length: {len(test_paper['content']):,} characters\\n\")\n",
    "    \n",
    "    # Build c√¢y c·∫•u tr√∫c v·ªõi paper_id v√† version\n",
    "    builder = LatexStructureBuilder(test_paper['content'], \n",
    "                                   paper_id=test_paper['paper_id'],\n",
    "                                   version=test_paper['version'])\n",
    "    root_tree = builder.build_coarse_tree()\n",
    "    \n",
    "    print(\"\\nüå≥ C·∫§U TR√öC C√ÇY (TREE STRUCTURE):\")\n",
    "    print(\"=\"*70)\n",
    "    builder.print_tree(root_tree)\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "\n",
    "    print(f\"\\nüìå Paper Info:\")\n",
    "    print(f\"   - Paper ID: {test_paper['paper_id']}\")\n",
    "    print(f\"   - Version: {test_paper['version']}\")\n",
    "    \n",
    "    # L∆∞u c·∫•u tr√∫c c√¢y v√†o file JSON\n",
    "    output_tree_path = f\"structure_{test_paper['paper_id']}_{test_paper['version']}.json\"\n",
    "    builder.print_tree_to_file(root_tree, output_tree_path)\n",
    "    \n",
    "    # ƒê·∫øm s·ªë l∆∞·ª£ng sections\n",
    "\n",
    "\n",
    "    def count_nodes(node):\n",
    "        count = 1\n",
    "        for child in node.get('children', []):\n",
    "            count += count_nodes(child)\n",
    "        return count\n",
    "    \n",
    "    total_nodes = count_nodes(root_tree)\n",
    "    print(f\"\\nüìä T·ªïng s·ªë nodes: {total_nodes}\")\n",
    "    print(f\"üìä S·ªë children c·ªßa root: {len(root_tree['children'])}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Kh√¥ng c√≥ papers ƒë·ªÉ test. H√£y ch·∫°y l·∫°i c√°c cell tr∆∞·ªõc ƒë·ªÉ t·∫°o file flattened_papers.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7527a653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä SO S√ÅNH C·∫§U TR√öC GI·ªÆA C√ÅC PAPERS:\n",
      "======================================================================\n",
      "\n",
      "1. Paper 2403-00530 - 2403-00530v1\n",
      "   Content: 41,291 chars\n",
      "   - Section: 5\n",
      "   - Subsection: 2\n",
      "\n",
      "2. Paper 2403-00530 - 2403-00530v2\n",
      "   Content: 51,454 chars\n",
      "   - Section: 8\n",
      "   - Subsection: 3\n",
      "\n",
      "3. Paper 2403-00531 - 2403-00531v1\n",
      "   Content: 62,735 chars\n",
      "   - Section: 6\n",
      "   - Subsection: 3\n",
      "   - Subsubsection: 2\n",
      "\n",
      "4. Paper 2403-00531 - 2403-00531v2\n",
      "   Content: 72,185 chars\n",
      "   - Section: 6\n",
      "   - Subsection: 5\n",
      "   - Subsubsection: 2\n",
      "\n",
      "5. Paper 2403-00532 - 2403-00532v1\n",
      "   Content: 179,231 chars\n",
      "   - Section: 11\n",
      "   - Subsection: 10\n",
      "   - Subsubsection: 8\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test v·ªõi nhi·ªÅu papers v√† so s√°nh c·∫•u tr√∫c\n",
    "\n",
    "if len(papers) > 1:\n",
    "    print(\"üìä SO S√ÅNH C·∫§U TR√öC GI·ªÆA C√ÅC PAPERS:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for i, paper in enumerate(papers[:5], 1):  # Test 5 papers ƒë·∫ßu ti√™n\n",
    "        print(f\"\\n{i}. Paper {paper['paper_id']} - {paper['version']}\")\n",
    "        print(f\"   Content: {len(paper['content']):,} chars\")\n",
    "        \n",
    "        builder = LatexStructureBuilder(paper['content'],\n",
    "                                       paper_id=paper['paper_id'],\n",
    "                                       version=paper['version'])\n",
    "        root_tree = builder.build_coarse_tree()\n",
    "        \n",
    "        # ƒê·∫øm t·ª´ng lo·∫°i section\n",
    "        section_counts = {}\n",
    "        def count_sections(node):\n",
    "            node_type = node['type']\n",
    "            section_counts[node_type] = section_counts.get(node_type, 0) + 1\n",
    "            for child in node.get('children', []):\n",
    "                count_sections(child)\n",
    "        \n",
    "        count_sections(root_tree)\n",
    "        \n",
    "        # Hi·ªÉn th·ªã th·ªëng k√™\n",
    "        for section_type in ['part', 'chapter', 'section', 'subsection', 'subsubsection']:\n",
    "            if section_type in section_counts:\n",
    "                print(f\"   - {section_type.capitalize()}: {section_counts[section_type]}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Ch·ªâ c√≥ 1 paper, b·ªè qua so s√°nh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50ae64fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import uuid\n",
    "\n",
    "class LatexContentProcessor:\n",
    "    def __init__(self, paper_id, version):\n",
    "        self.paper_id = paper_id\n",
    "        self.version = version\n",
    "        \n",
    "        # --- REGEX PATTERNS ---\n",
    "        \n",
    "        # 1. Block Math: $$...$$, \\[...\\], \\begin{equation}...\n",
    "        self.REGEX_MATH_BLOCK = re.compile(\n",
    "            r'(\\\\begin\\{equation\\*?\\}.*?\\\\end\\{equation\\*?\\}|\\\\\\[.*?\\\\\\]|\\$\\$.*?\\$\\$)', \n",
    "            re.DOTALL\n",
    "        )\n",
    "        \n",
    "        # 2. Figures/Tables: \\begin{figure/table}...\n",
    "        self.REGEX_FIGURE = re.compile(\n",
    "            r'(\\\\begin\\{(figure|table)\\*?\\}.*?\\\\end\\{(figure|table)\\*?\\})', \n",
    "            re.DOTALL | re.IGNORECASE\n",
    "        )\n",
    "        \n",
    "        # 3. Lists: \\begin{itemize/enumerate}...\n",
    "        self.REGEX_LIST = re.compile(\n",
    "            r'(\\\\begin\\{(itemize|enumerate)\\}.*?\\\\end\\{(itemize|enumerate)\\})', \n",
    "            re.DOTALL\n",
    "        )\n",
    "        \n",
    "        # 4. Sentence Splitter: T√¨m d·∫•u ch·∫•m/h·ªèi/th√°n k·∫øt th√∫c c√¢u\n",
    "        # X·ª≠ l√Ω c√°c tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát: abbreviations, s·ªë th·∫≠p ph√¢n, tr√≠ch d·∫´n\n",
    "        \n",
    "        # Danh s√°ch abbreviations ph·ªï bi·∫øn trong paper khoa h·ªçc\n",
    "        abbrev_pattern = r'(?:Fig|Eq|Eqs|Tab|Sec|Ref|Vol|No|Ch|Dr|Prof|Ph\\.D|' \\\n",
    "                        r'et al|i\\.e|e\\.g|vs|cf|etc|approx|ca|viz)'\n",
    "        \n",
    "        # Pattern ch√≠nh:\n",
    "        # - Kh√¥ng ph·∫£i sau abbreviations\n",
    "        # - Kh√¥ng ph·∫£i gi·ªØa ch·ªØ c√°i ƒë∆°n (U.S.)\n",
    "        # - Kh√¥ng ph·∫£i s·ªë th·∫≠p ph√¢n (3.14)\n",
    "        # - Cho ph√©p d·∫•u ngo·∫∑c k√©p/ƒë∆°n sau d·∫•u c√¢u\n",
    "        self.REGEX_SENTENCE = re.compile(\n",
    "                    r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s+(?=[A-Z\\(])'\n",
    "                )\n",
    "\n",
    "    def process_tree(self, node):\n",
    "        \"\"\"\n",
    "        Duy·ªát ƒë·ªá quy c√¢y c·∫•u tr√∫c th√¥ ƒë·ªÉ \"m·ªï x·∫ª\" raw_content th√†nh c√°c elements.\n",
    "        \"\"\"\n",
    "        # 1. X·ª≠ l√Ω raw_content c·ªßa node hi·ªán t·∫°i (n·∫øu c√≥)\n",
    "        if node.get('raw_content') and node['raw_content'].strip():\n",
    "            # T√°ch n·ªôi dung th√†nh c√°c node con chi ti·∫øt (c√¢u, h√¨nh, c√¥ng th·ª©c...)\n",
    "            fine_grained_nodes = self.parse_content_blocks(node['raw_content'])\n",
    "            \n",
    "            # QUAN TR·ªåNG: Ch√®n c√°c node n·ªôi dung v√†o ƒê·∫¶U danh s√°ch children\n",
    "            # L√Ω do: Trong LaTeX, text c·ªßa Section lu√¥n n·∫±m tr∆∞·ªõc Subsection con.\n",
    "            node['children'] = fine_grained_nodes + node['children']\n",
    "            \n",
    "            # X√≥a raw_content ƒë·ªÉ gi·∫£i ph√≥ng b·ªô nh·ªõ v√† ƒë√°nh d·∫•u l√† ƒë√£ x·ª≠ l√Ω\n",
    "            del node['raw_content']\n",
    "\n",
    "        # 2. ƒê·ªá quy x·ª≠ l√Ω c√°c con (bao g·ªìm c·∫£ c√°c Subsection c≈© v√† c√°c List m·ªõi t·∫°o)\n",
    "        # L∆∞u √Ω: Ta ch·ªâ ƒë·ªá quy v√†o c√°c node c·∫•u tr√∫c (part, chapter, section...) \n",
    "        # ho·∫∑c list, kh√¥ng c·∫ßn ƒë·ªá quy v√†o sentence/equation (node l√°).\n",
    "        for child in node['children']:\n",
    "            # Ch·ªâ ƒë·ªá quy n·∫øu node con ƒë√≥ c√≥ th·ªÉ ch·ª©a content con (v√≠ d·ª• List ho·∫∑c Section con)\n",
    "            if child['type'] not in ['sentence', 'equation', 'figure', 'list_item']:\n",
    "                self.process_tree(child)\n",
    "\n",
    "    def parse_content_blocks(self, text):\n",
    "        \"\"\"\n",
    "        C·∫Øt chu·ªói text h·ªón h·ª£p th√†nh danh s√°ch c√°c Node Elements\n",
    "        \"\"\"\n",
    "        elements = []\n",
    "        \n",
    "        # Pattern t·ªïng h·ª£p ƒë·ªÉ split: Math OR Figure OR List\n",
    "        pattern = re.compile(\n",
    "            f\"({self.REGEX_MATH_BLOCK.pattern}|{self.REGEX_FIGURE.pattern}|{self.REGEX_LIST.pattern})\",\n",
    "            re.DOTALL | re.IGNORECASE\n",
    "        )\n",
    "        \n",
    "        # Split text, gi·ªØ l·∫°i delimiter (ch√≠nh l√† n·ªôi dung block)\n",
    "        parts = pattern.split(text)\n",
    "        \n",
    "        for part in parts:\n",
    "            if not part: continue\n",
    "            part = part.strip()\n",
    "            if not part: continue\n",
    "\n",
    "            # --- PH√ÇN LO·∫†I & T·∫†O NODE ---\n",
    "            \n",
    "            # 1. Math Block\n",
    "            if self.REGEX_MATH_BLOCK.fullmatch(part):\n",
    "                elements.append(self._create_node(\n",
    "                    type_name='equation',\n",
    "                    title='Equation Block',\n",
    "                    raw_content=self._normalize_math(part)\n",
    "                ))\n",
    "            \n",
    "            # 2. Figure/Table\n",
    "            elif self.REGEX_FIGURE.fullmatch(part):\n",
    "                elements.append(self._create_node(\n",
    "                    type_name='figure',\n",
    "                    title='Figure/Table',\n",
    "                    raw_content=self._clean_latex(part)\n",
    "                ))\n",
    "            \n",
    "            # 3. List (Itemize/Enumerate) -> T·∫°o c·∫•u tr√∫c l·ªìng nhau\n",
    "            elif self.REGEX_LIST.fullmatch(part):\n",
    "                list_node = self._process_list_block(part)\n",
    "                elements.append(list_node)\n",
    "            \n",
    "            # 4. Text thu·∫ßn -> T√°ch th√†nh Sentence Nodes\n",
    "            else:\n",
    "                sentences = self._split_sentences(part)\n",
    "                for sent in sentences:\n",
    "                    elements.append(self._create_node(\n",
    "                        type_name='sentence',\n",
    "                        title=sent[:30] + \"...\", # Title xem tr∆∞·ªõc\n",
    "                        raw_content=self._clean_latex(sent)\n",
    "                    ))\n",
    "                    \n",
    "        return elements\n",
    "\n",
    "    def _process_list_block(self, list_content):\n",
    "        \"\"\"X·ª≠ l√Ω ri√™ng cho Itemize/Enumerate ƒë·ªÉ t√°ch c√°c \\item\"\"\"\n",
    "        # 1. X√°c ƒë·ªãnh lo·∫°i list (itemize hay enumerate)\n",
    "        # Group 1 s·∫Ω b·∫Øt ƒë∆∞·ª£c t√™n m√¥i tr∆∞·ªùng (itemize/enumerate)\n",
    "        match = re.match(r'\\\\begin\\{(itemize|enumerate)\\}', list_content, re.IGNORECASE)\n",
    "        list_type = match.group(1) if match else \"itemize\"\n",
    "\n",
    "        list_node = self._create_node(\n",
    "            type_name='list',\n",
    "            title=f'List ({list_type})',\n",
    "            raw_content=\"\" \n",
    "        )\n",
    "        \n",
    "        # 2. B√≥c v·ªè (Unwrap) an to√†n\n",
    "        # X√≥a th·∫ª m·ªü ƒë·∫ßu ti√™n (ch·ªâ x√≥a 1 l·∫ßn - count=1)\n",
    "        # Regex b·∫Øt: \\begin{type} theo sau c√≥ th·ªÉ l√† [options]\n",
    "        content_inner = re.sub(r'^\\\\begin\\{' + list_type + r'\\}(\\[.*?\\])?', '', list_content, count=1, flags=re.IGNORECASE).strip()\n",
    "        \n",
    "        # X√≥a th·∫ª ƒë√≥ng cu·ªëi c√πng (Neo v√†o cu·ªëi chu·ªói $)\n",
    "        content_inner = re.sub(r'\\\\end\\{' + list_type + r'\\}\\s*$', '', content_inner, count=1, flags=re.IGNORECASE).strip()\n",
    "\n",
    "        # 3. T√°ch c√°c \\item\n",
    "        # L√∫c n√†y content_inner ch·ªâ c√≤n n·ªôi dung ru·ªôt, c√°c list con (n·∫øu c√≥) v·∫´n nguy√™n v·∫πn\n",
    "        items = re.split(r'\\\\item\\s+', content_inner)\n",
    "        \n",
    "        for item in items:\n",
    "            # B·ªè qua ph·∫ßn text r√°c tr∆∞·ªõc \\item ƒë·∫ßu ti√™n (th∆∞·ªùng l√† kho·∫£ng tr·∫Øng)\n",
    "            if not item.strip(): \n",
    "                continue\n",
    "                \n",
    "            # Clean n·ªôi dung item\n",
    "            # L∆∞u √Ω: KH√îNG d√πng replace \\end n·ªØa v√¨ ta ƒë√£ b√≥c v·ªè ·ªü b∆∞·ªõc 2 r·ªìi\n",
    "            clean_content = self._clean_latex(item)\n",
    "            \n",
    "            if clean_content:\n",
    "                item_node = self._create_node(\n",
    "                    type_name='list_item',\n",
    "                    title='List Item',\n",
    "                    raw_content=clean_content\n",
    "                )\n",
    "                list_node['children'].append(item_node)\n",
    "                \n",
    "        return list_node\n",
    "    def _create_node(self, type_name, title, raw_content):\n",
    "        \"\"\"Helper t·∫°o node chu·∫©n theo format ID c·ªßa b·∫°n\"\"\"\n",
    "        return {\n",
    "            'id': f'{self.paper_id}-{self.version}-{type_name}-{uuid.uuid4()}',\n",
    "            'type': type_name,\n",
    "            'title': title,\n",
    "            'level': 99, # Level th·∫•p nh·∫•t (l√°)\n",
    "            'raw_content': raw_content,\n",
    "            'children': []\n",
    "        }\n",
    "\n",
    "    def _split_sentences(self, text):\n",
    "        \"\"\"T√°ch c√¢u\"\"\"\n",
    "        text = re.sub(r'\\s+', ' ', text) # G·ªôp newline th√†nh space\n",
    "        sentences = self.REGEX_SENTENCE.split(text)\n",
    "        return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "    def _normalize_math(self, content):\n",
    "        \"\"\"Chu·∫©n h√≥a to√°n h·ªçc: Convert $$ -> equation\"\"\"\n",
    "        if content.startswith('$$') and content.endswith('$$'):\n",
    "            inner = content[2:-2].strip()\n",
    "            return f\"\\\\begin{{equation}}{inner}\\\\end{{equation}}\"\n",
    "        elif content.startswith(r'\\['):\n",
    "            inner = content[2:-2].strip()\n",
    "            return f\"\\\\begin{{equation}}{inner}\\\\end{{equation}}\"\n",
    "        return content\n",
    "\n",
    "    def _clean_latex(self, text):\n",
    "        \"\"\"X√≥a c√°c l·ªánh format r√°c\"\"\"\n",
    "        # X√≥a \\centering, \\hfill, label, cite, ref... t√πy nhu c·∫ßu\n",
    "        text = re.sub(r'\\\\(centering|hfill|vfill|noindent|small|tiny|large)', '', text)\n",
    "        # X√≥a optional params [htbp] c·ªßa figure\n",
    "        text = re.sub(r'\\\\begin\\{(figure|table)\\}\\[.*?\\]', r'\\\\begin{\\1}', text)\n",
    "        return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "538cf88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing LatexContentProcessor\n",
      "üìÑ Paper: 2403-00530 - 2403-00530v2\n",
      "üìè Content length: 51,454 characters\n",
      "\n",
      "======================================================================\n",
      "B∆Ø·ªöC 1: X√ÇY D·ª∞NG C√ÇY C·∫§U TR√öC TH√î\n",
      "======================================================================\n",
      "‚úÖ S·ªë nodes ban ƒë·∫ßu (ch·ªâ sections): 12\n",
      "\n",
      "======================================================================\n",
      "B∆Ø·ªöC 2: X·ª¨ L√ù N·ªòI DUNG CHI TI·∫æT\n",
      "======================================================================\n",
      "‚úÖ S·ªë nodes sau x·ª≠ l√Ω: 249\n",
      "üìä TƒÉng th√™m: 237 nodes (sentences, equations, figures...)\n",
      "\n",
      "======================================================================\n",
      "B∆Ø·ªöC 3: TH·ªêNG K√ä C√ÅC LO·∫†I NODE\n",
      "======================================================================\n",
      "   sentence            :  201\n",
      "   figure              :   20\n",
      "   equation            :   16\n",
      "   section             :    8\n",
      "   subsection          :    3\n",
      "   document            :    1\n",
      "\n",
      "======================================================================\n",
      "B∆Ø·ªöC 4: XEM TR∆Ø·ªöC C·∫§U TR√öC C√ÇY (5 M·ª®C ƒê·∫¶U)\n",
      "======================================================================\n",
      "üß™ Testing LatexContentProcessor\n",
      "üìÑ Paper: 2403-00530 - 2403-00530v2\n",
      "üìä Content length: 51,454 characters\n",
      "\n",
      "======================================================================\n",
      "\n",
      "üìå B∆Ø·ªöC 1: X√¢y d·ª±ng c√¢y c·∫•u tr√∫c th√¥...\n",
      "‚úÖ C√¢y th√¥ ƒë√£ ƒë∆∞·ª£c x√¢y d·ª±ng\n",
      "   S·ªë l∆∞·ª£ng nodes theo lo·∫°i:\n",
      "   - document: 1\n",
      "   - section: 8\n",
      "   - subsection: 3\n",
      "\n",
      "üìå B∆Ø·ªöC 2: X·ª≠ l√Ω content chi ti·∫øt (t√°ch c√¢u, c√¥ng th·ª©c, h√¨nh...)...\n",
      "‚úÖ ƒê√£ x·ª≠ l√Ω xong!\n",
      "   S·ªë l∆∞·ª£ng nodes sau khi x·ª≠ l√Ω:\n",
      "   - document: 1\n",
      "   - equation: 16\n",
      "   - figure: 20\n",
      "   - section: 8\n",
      "   - sentence: 201\n",
      "   - subsection: 3\n",
      "\n",
      "üìä TH·ªêNG K√ä THAY ƒê·ªîI:\n",
      "   C√°c lo·∫°i node m·ªõi ƒë∆∞·ª£c t·∫°o:\n",
      "   - equation: 16 nodes\n",
      "   - figure: 20 nodes\n",
      "   - sentence: 201 nodes\n",
      "\n",
      "üìå B∆Ø·ªöC 3: L∆∞u k·∫øt qu·∫£ v√†o file...\n",
      "‚úÖ ƒê√£ l∆∞u c·∫•u tr√∫c c√¢y v√†o: detailed_structure_2403-00530_2403-00530v2.json\n",
      "   - T·ªïng s·ªë nodes: 249\n",
      "   - T·ªïng s·ªë edges: 248\n",
      "\n",
      "üå≥ PREVIEW C·∫§U TR√öC C√ÇY CHI TI·∫æT (2 levels ƒë·∫ßu):\n",
      "======================================================================\n",
      "‚îú‚îÄ [DOCUMENT] Root Document\n",
      "   (14 children)\n",
      "  ‚îú‚îÄ [SENTENCE] \\documentclass[ twocolumn, sup... | Content: \\documentclass[ twocolumn, superscriptaddress, ams...\n",
      "  ‚îú‚îÄ [SENTENCE] We used on-site and inter-site... | Content: We used on-site and inter-site Hubbard interaction...\n",
      "  ‚îú‚îÄ [SENTENCE] Our calculations for systems u... | Content: Our calculations for systems up to eight layers gi...\n",
      "   ... v√† 11 children kh√°c\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test LatexContentProcessor v·ªõi paper ƒë·∫ßu ti√™n\n",
    "paper_to_test = 1\n",
    "\n",
    "\n",
    "if papers:\n",
    "    # L·∫•y paper ƒë·∫ßu ti√™n ƒë·ªÉ test\n",
    "    test_paper = papers[paper_to_test]\n",
    "    paper_id = test_paper['paper_id']\n",
    "    version = test_paper['version']\n",
    "    \n",
    "    print(f\"üß™ Testing LatexContentProcessor\")\n",
    "    print(f\"üìÑ Paper: {paper_id} - {version}\")\n",
    "    print(f\"üìè Content length: {len(test_paper['content']):,} characters\\n\")\n",
    "    \n",
    "    # B∆Ø·ªöC 1: Build c√¢y th√¥ (Structure)\n",
    "    print(\"=\"*70)\n",
    "    print(\"B∆Ø·ªöC 1: X√ÇY D·ª∞NG C√ÇY C·∫§U TR√öC TH√î\")\n",
    "    print(\"=\"*70)\n",
    "    builder = LatexStructureBuilder(test_paper['content'], paper_id, version)\n",
    "    root_tree = builder.build_coarse_tree()\n",
    "    \n",
    "    # ƒê·∫øm s·ªë node tr∆∞·ªõc khi x·ª≠ l√Ω\n",
    "    def count_nodes(node):\n",
    "        count = 1\n",
    "        for child in node.get('children', []):\n",
    "            count += count_nodes(child)\n",
    "        return count\n",
    "    \n",
    "    nodes_before = count_nodes(root_tree)\n",
    "    print(f\"‚úÖ S·ªë nodes ban ƒë·∫ßu (ch·ªâ sections): {nodes_before}\\n\")\n",
    "    \n",
    "    # B∆Ø·ªöC 2: Process n·ªôi dung (T√°ch c√¢u, h√¨nh, to√°n)\n",
    "    print(\"=\"*70)\n",
    "    print(\"B∆Ø·ªöC 2: X·ª¨ L√ù N·ªòI DUNG CHI TI·∫æT\")\n",
    "    print(\"=\"*70)\n",
    "    processor = LatexContentProcessor(paper_id, version)\n",
    "    processor.process_tree(root_tree)\n",
    "    \n",
    "    nodes_after = count_nodes(root_tree)\n",
    "    print(f\"‚úÖ S·ªë nodes sau x·ª≠ l√Ω: {nodes_after}\")\n",
    "    print(f\"üìä TƒÉng th√™m: {nodes_after - nodes_before} nodes (sentences, equations, figures...)\\n\")\n",
    "    \n",
    "    # B∆Ø·ªöC 3: Th·ªëng k√™ c√°c lo·∫°i node\n",
    "    print(\"=\"*70)\n",
    "    print(\"B∆Ø·ªöC 3: TH·ªêNG K√ä C√ÅC LO·∫†I NODE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    node_types = {}\n",
    "    def count_by_type(node):\n",
    "        node_type = node['type']\n",
    "        node_types[node_type] = node_types.get(node_type, 0) + 1\n",
    "        for child in node.get('children', []):\n",
    "            count_by_type(child)\n",
    "    \n",
    "    count_by_type(root_tree)\n",
    "    \n",
    "    # S·∫Øp x·∫øp v√† hi·ªÉn th·ªã\n",
    "    for node_type, count in sorted(node_types.items(), key=lambda x: -x[1]):\n",
    "        print(f\"   {node_type:20s}: {count:4d}\")\n",
    "    \n",
    "    # B∆Ø·ªöC 4: In ra m·ªôt ph·∫ßn c√¢y ƒë·ªÉ ki·ªÉm tra\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"B∆Ø·ªöC 4: XEM TR∆Ø·ªöC C·∫§U TR√öC C√ÇY (5 M·ª®C ƒê·∫¶U)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    def print_tree_limited(node, indent=0, max_depth=5):\n",
    "        if indent >= max_depth:\n",
    "            return\n",
    "        \n",
    "        prefix = \"  \" * indent\n",
    "        content_preview = \"\"\n",
    "        if node.get('raw_content'):\n",
    "            content_preview = node['raw_content'][:50].replace('\\n', ' ')\n",
    "            content_preview = f\" ‚Üí \\\"{content_preview}...\\\"\"\n",
    "        \n",
    "        print(f\"{prefix}‚îú‚îÄ [{node['type']}] {node['title']}{content_preview}\")\n",
    "        \n",
    "        # Ch·ªâ in t·ªëi ƒëa 3 children ƒë·∫ßu ti√™n ƒë·ªÉ tr√°nh qu√° d√†i\n",
    "        children_to_show = node.get('children', [])[:3]\n",
    "        for child in children_to_show:\n",
    "            print_tree_limited(child, indent + 1, max_depth)\n",
    "        \n",
    "        if len(node.get('children', [])) > 3:\n",
    "            print(f\"{prefix}  ... ({len(node['children']) - 3} more children```python)\")\n",
    "# Test LatexContentProcessor v·ªõi paper ƒë·∫ßu ti√™n\n",
    "\n",
    "if papers:\n",
    "    # L·∫•y paper ƒë·∫ßu ti√™n ƒë·ªÉ test\n",
    "    test_paper = papers[paper_to_test]\n",
    "    paper_id = test_paper['paper_id']\n",
    "    version = test_paper['version']\n",
    "    flattened_content = test_paper['content']\n",
    "    \n",
    "    print(f\"üß™ Testing LatexContentProcessor\")\n",
    "    print(f\"üìÑ Paper: {paper_id} - {version}\")\n",
    "    print(f\"üìä Content length: {len(flattened_content):,} characters\\n\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # B∆∞·ªõc 1: Build c√¢y c·∫•u tr√∫c th√¥\n",
    "    print(\"\\nüìå B∆Ø·ªöC 1: X√¢y d·ª±ng c√¢y c·∫•u tr√∫c th√¥...\")\n",
    "    builder = LatexStructureBuilder(flattened_content, paper_id, version)\n",
    "    root_tree = builder.build_coarse_tree()\n",
    "    \n",
    "    # ƒê·∫øm nodes tr∆∞·ªõc khi process\n",
    "    def count_nodes_by_type(node, counts=None):\n",
    "        if counts is None:\n",
    "            counts = {}\n",
    "        node_type = node['type']\n",
    "        counts[node_type] = counts.get(node_type, 0) + 1\n",
    "        for child in node.get('children', []):\n",
    "            count_nodes_by_type(child, counts)\n",
    "        return counts\n",
    "    \n",
    "    before_counts = count_nodes_by_type(root_tree)\n",
    "    print(f\"‚úÖ C√¢y th√¥ ƒë√£ ƒë∆∞·ª£c x√¢y d·ª±ng\")\n",
    "    print(f\"   S·ªë l∆∞·ª£ng nodes theo lo·∫°i:\")\n",
    "    for node_type, count in sorted(before_counts.items()):\n",
    "        print(f\"   - {node_type}: {count}\")\n",
    "    \n",
    "    # B∆∞·ªõc 2: Process content chi ti·∫øt\n",
    "    print(f\"\\nüìå B∆Ø·ªöC 2: X·ª≠ l√Ω content chi ti·∫øt (t√°ch c√¢u, c√¥ng th·ª©c, h√¨nh...)...\")\n",
    "    processor = LatexContentProcessor(paper_id, version)\n",
    "    processor.process_tree(root_tree)\n",
    "    \n",
    "    # ƒê·∫øm nodes sau khi process\n",
    "    after_counts = count_nodes_by_type(root_tree)\n",
    "    print(f\"‚úÖ ƒê√£ x·ª≠ l√Ω xong!\")\n",
    "    print(f\"   S·ªë l∆∞·ª£ng nodes sau khi x·ª≠ l√Ω:\")\n",
    "    for node_type, count in sorted(after_counts.items()):\n",
    "        print(f\"   - {node_type}: {count}\")\n",
    "    \n",
    "    # So s√°nh s·ª± thay ƒë·ªïi\n",
    "    print(f\"\\nüìä TH·ªêNG K√ä THAY ƒê·ªîI:\")\n",
    "    new_types = set(after_counts.keys()) - set(before_counts.keys())\n",
    "    if new_types:\n",
    "        print(f\"   C√°c lo·∫°i node m·ªõi ƒë∆∞·ª£c t·∫°o:\")\n",
    "        for node_type in sorted(new_types):\n",
    "            print(f\"   - {node_type}: {after_counts[node_type]} nodes\")\n",
    "    \n",
    "    # B∆∞·ªõc 3: L∆∞u k·∫øt qu·∫£\n",
    "    output_file = f\"detailed_structure_{paper_id}_{version}.json\"\n",
    "    print(f\"\\nüìå B∆Ø·ªöC 3: L∆∞u k·∫øt qu·∫£ v√†o file...\")\n",
    "    builder.print_tree_to_file(root_tree, output_file)\n",
    "    \n",
    "    # Hi·ªÉn th·ªã preview c√¢y chi ti·∫øt (ch·ªâ 2 level ƒë·∫ßu ƒë·ªÉ kh·ªèi qu√° d√†i)\n",
    "    print(f\"\\nüå≥ PREVIEW C·∫§U TR√öC C√ÇY CHI TI·∫æT (2 levels ƒë·∫ßu):\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    def print_limited_tree(node, indent=0, max_depth=2):\n",
    "        if indent >= max_depth:\n",
    "            return\n",
    "        prefix = \"  \" * indent\n",
    "        content_preview = \"\"\n",
    "        if node.get('raw_content'):\n",
    "            preview_text = node['raw_content'][:50].replace('\\n', ' ')\n",
    "            content_preview = f\" | Content: {preview_text}...\"\n",
    "        \n",
    "        print(f\"{prefix}‚îú‚îÄ [{node['type'].upper()}] {node['title']}{content_preview}\")\n",
    "        \n",
    "        # Hi·ªÉn th·ªã s·ªë l∆∞·ª£ng children\n",
    "        if node['children']:\n",
    "            print(f\"{prefix}   ({len(node['children'])} children)\")\n",
    "            for i, child in enumerate(node['children'][:3]):  # Ch·ªâ show 3 children ƒë·∫ßu\n",
    "                print_limited_tree(child, indent + 1, max_depth)\n",
    "            if len(node['children']) > 3:\n",
    "                print(f\"{prefix}   ... v√† {len(node['children']) - 3} children kh√°c\")\n",
    "    \n",
    "    print_limited_tree(root_tree)\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Kh√¥ng c√≥ papers ƒë·ªÉ test!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37448349",
   "metadata": {},
   "source": [
    "detailed_structure_2403-00530_2403-00530v1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
