{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2: Model Evaluation & Ranking Generation\n",
    "**M·ª•c ti√™u:**\n",
    "1. ƒê·ªçc d·ªØ li·ªáu `labels.json` (ch·ª©a input BibTeX v√† Ground Truth).\n",
    "2. T√°i c·∫•u tr√∫c d·ªØ li·ªáu theo t·ª´ng b√†i b√°o (Paper-based grouping).\n",
    "3. S·ª≠ d·ª•ng m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán ƒë·ªÉ x·∫øp h·∫°ng (Ranking) c√°c reference candidates.\n",
    "4. T√≠nh ch·ªâ s·ªë **MRR (Mean Reciprocal Rank)**.\n",
    "5. Xu·∫•t file `pred.json` theo ƒë√∫ng ƒë·ªãnh d·∫°ng y√™u c·∫ßu n·ªôp b√†i.\n",
    "\n",
    "**Y√™u c·∫ßu ƒë·∫ßu v√†o:**\n",
    "* `labels.json`: File d·ªØ li·ªáu test.\n",
    "* `models/best_matcher.pkl`: Model ƒë√£ train.\n",
    "* `models/feature_names.pkl`: List t√™n features t∆∞∆°ng ·ª©ng.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup path ƒë·ªÉ import src module\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Import t·ª´ src.ml module\n",
    "from src.ml import (\n",
    "    load_json,\n",
    "    save_json,\n",
    "    normalize_text_basic,\n",
    "    get_tokens,\n",
    "    safe_year_diff,\n",
    "    compute_pairwise_features,\n",
    "    compute_tfidf_cosine_single,\n",
    "    parse_bibtex_smart,\n",
    "    clean_latex,\n",
    "    normalize_id,\n",
    "    transform_to_paper_based\n",
    ")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"‚úÖ Libraries v√† src.ml modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N ---\n",
    "# H√£y thay ƒë·ªïi ƒë∆∞·ªùng d·∫´n ph√π h·ª£p v·ªõi m√¥i tr∆∞·ªùng c·ªßa b·∫°n\n",
    "TEST_FILE_PATH = '../../dataset_final/test/labels.json'           # File d·ªØ li·ªáu test\n",
    "MODEL_PATH = '../../dataset_final/models/best_matcher.pkl'   # File model (ƒë√£ train ·ªü b∆∞·ªõc tr∆∞·ªõc)\n",
    "FEATURE_NAME_PATH = '../../dataset_final/models/feature_names.pkl' # Danh s√°ch features\n",
    "OUTPUT_DIR = 'submission_output'         # Th∆∞ m·ª•c ch·ª©a k·∫øt qu·∫£ pred.json\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    print(f\"Created output directory: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√°c h√†m helper ƒë√£ ƒë∆∞·ª£c import t·ª´ src.ml:\n",
    "# - normalize_text_basic(text) -> str\n",
    "# - get_tokens(text_or_list) -> set\n",
    "# - safe_year_diff(y1, y2) -> int\n",
    "\n",
    "print(\"üìö Helper functions ƒë√£ import t·ª´ src.ml.features:\")\n",
    "print(\"   - normalize_text_basic\")\n",
    "print(\"   - get_tokens\")  \n",
    "print(\"   - safe_year_diff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BibTeX Parser ƒë√£ ƒë∆∞·ª£c import t·ª´ src.ml.bibtex_parser:\n",
    "# - parse_bibtex_smart(bib_string) -> dict\n",
    "# - clean_latex(text) -> str\n",
    "# - normalize_id(text) -> str\n",
    "\n",
    "print(\"üìö BibTeX Parser ƒë√£ import t·ª´ src.ml.bibtex_parser:\")\n",
    "print(\"   - parse_bibtex_smart: Parser V3 (Title, Authors, Year, ID)\")\n",
    "print(\"   - clean_latex: X·ª≠ l√Ω LaTeX markup\")\n",
    "print(\"   - normalize_id: Chu·∫©n h√≥a DOI/arXiv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Engineering\n",
    "**L∆∞u √Ω quan tr·ªçng:** C√°c h√†m d∆∞·ªõi ƒë√¢y ph·∫£i **gi·ªëng h·ªát** logic b·∫°n ƒë√£ s·ª≠ d·ª•ng khi hu·∫•n luy·ªán m√¥ h√¨nh (Training Phase). N·∫øu thay ƒë·ªïi, model s·∫Ω nh·∫≠n di·ªán sai ƒë·∫∑c tr∆∞ng.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions ƒë√£ import t·ª´ module\n",
    "# Kh√¥ng c·∫ßn define l·∫°i\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Wrapper for normalize_text_basic.\"\"\"\n",
    "    return normalize_text_basic(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# S·ª≠ d·ª•ng compute_pairwise_features t·ª´ module\n",
    "# Th√™m TF-IDF cosine cho single pairs\n",
    "\n",
    "def compute_features_with_tfidf(row):\n",
    "    \"\"\"Compute features bao g·ªìm TF-IDF cosine.\"\"\"\n",
    "    feats = compute_pairwise_features(row)\n",
    "    \n",
    "    # Th√™m TF-IDF cosine cho single pair\n",
    "    q_tit = normalize_text_basic(row.get('bib_title', ''))\n",
    "    c_tit = normalize_text_basic(row.get('cand_title', ''))\n",
    "    feats['feat_title_tfidf_cosine'] = compute_tfidf_cosine_single(q_tit, c_tit)\n",
    "    \n",
    "    return feats\n",
    "\n",
    "print(\"‚úÖ Feature engineering function ready (s·ª≠ d·ª•ng src.ml.features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load & Transform Data\n",
    "Ch√∫ng ta c·∫ßn chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu t·ª´ danh s√°ch ph·∫≥ng (flat list) trong `manual.json` sang c·∫•u tr√∫c **Paper-based**.\n",
    "* M·ªói `source_paper_id` s·∫Ω l√† m·ªôt nh√≥m.\n",
    "* T·∫≠p **Candidates** c·ªßa nh√≥m ƒë√≥ l√† t·∫≠p h·ª£p t·∫•t c·∫£ c√°c `ground_truth` unique xu·∫•t hi·ªán trong paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 812 entries.\n",
      "‚úÖ Data transformed into 39 papers.\n"
     ]
    }
   ],
   "source": [
    "# --- DATA LOADING & TRANSFORMATION ---\n",
    "try:\n",
    "    raw_data = load_json(TEST_FILE_PATH)\n",
    "    print(f\"‚úÖ Loaded {len(raw_data)} entries.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: File not found.\")\n",
    "    raw_data = []\n",
    "\n",
    "# Transform sang Paper-based structure s·ª≠ d·ª•ng module\n",
    "papers_db = transform_to_paper_based(raw_data, parse_bibtex_smart)\n",
    "\n",
    "print(f\"‚úÖ Data transformed into {len(papers_db)} papers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Trained Model\n",
    "Load model SVM/RandomForest/XGBoost ƒë√£ l∆∞u t·ª´ b∆∞·ªõc Training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded: XGBClassifier\n",
      "üìù Expected features: 13\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model = joblib.load(MODEL_PATH)\n",
    "    feature_names = joblib.load(FEATURE_NAME_PATH)\n",
    "    print(f\"‚úÖ Model loaded: {type(model).__name__}\")\n",
    "    print(f\"üìù Expected features: {len(feature_names)}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è WARNING: Model file not found. Code will run with RANDOM SCORES for demonstration.\")\n",
    "    model = None\n",
    "    feature_names = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ranking Pipeline & Prediction\n",
    "V·ªõi m·ªói b√†i b√°o:\n",
    "1. T·∫°o c·∫∑p (Query, Candidate) cho t·∫•t c·∫£ ·ª©ng vi√™n.\n",
    "2. T√≠nh feature.\n",
    "3. D·ª± ƒëo√°n x√°c su·∫•t match.\n",
    "4. L·∫•y Top 5 ·ª©ng vi√™n c√≥ ƒëi·ªÉm cao nh·∫•t.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Ranking Pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ranking completed for all papers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "global_mrr_sum = 0\n",
    "global_query_count = 0\n",
    "\n",
    "print(\"üöÄ Starting Ranking Pipeline...\")\n",
    "\n",
    "for paper_id, data in papers_db.items():\n",
    "    queries = data['queries']\n",
    "    candidates_dict = data['candidates']\n",
    "    candidates_list = list(candidates_dict.values())\n",
    "    \n",
    "    if not candidates_list:\n",
    "        continue\n",
    "\n",
    "    # Init Output Structure\n",
    "    submission_data = {\n",
    "        \"partition\": \"test\", \n",
    "        \"groundtruth\": {},\n",
    "        \"prediction\": {}\n",
    "    }\n",
    "    \n",
    "    # Loop qua t·ª´ng query trong paper\n",
    "    for query in tqdm(queries, desc=f\"Paper {paper_id}\", leave=False):\n",
    "        bib_key = query['key']\n",
    "        true_id = query['true_id']\n",
    "        \n",
    "        # Groundtruth\n",
    "        submission_data['groundtruth'][bib_key] = true_id\n",
    "        \n",
    "        # Pairing & Feature Calc\n",
    "        pairs = []\n",
    "        for cand in candidates_list:\n",
    "            row = {}\n",
    "            row.update(query)\n",
    "            row.update(cand)\n",
    "            pairs.append(row)\n",
    "            \n",
    "        # Compute Features s·ª≠ d·ª•ng h√†m t·ª´ module\n",
    "        feats_list = [compute_features_with_tfidf(p) for p in pairs]\n",
    "        df_feats = pd.DataFrame(feats_list)\n",
    "        \n",
    "        # Predict\n",
    "        scores = []\n",
    "        if model:\n",
    "            # Ensure columns exist\n",
    "            for col in feature_names:\n",
    "                if col not in df_feats.columns: \n",
    "                    df_feats[col] = 0.0\n",
    "            \n",
    "            X_input = df_feats[feature_names]\n",
    "            scores = model.predict_proba(X_input)[:, 1]\n",
    "        else:\n",
    "            scores = np.random.rand(len(pairs))\n",
    "            \n",
    "        # Ranking\n",
    "        ranked_candidates = []\n",
    "        for idx, score in enumerate(scores):\n",
    "            ranked_candidates.append((candidates_list[idx]['cand_id'], score))\n",
    "        \n",
    "        ranked_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_5 = [x[0] for x in ranked_candidates[:5]]\n",
    "        \n",
    "        # Save Prediction\n",
    "        submission_data['prediction'][bib_key] = top_5\n",
    "        \n",
    "        # Calc MRR\n",
    "        if true_id in top_5:\n",
    "            rank = top_5.index(true_id) + 1\n",
    "            global_mrr_sum += 1.0 / rank\n",
    "        else:\n",
    "            global_mrr_sum += 0.0\n",
    "            \n",
    "        global_query_count += 1\n",
    "        \n",
    "    # Save JSON Output s·ª≠ d·ª•ng module\n",
    "    safe_pid = str(paper_id).replace('/', '_')\n",
    "    save_path = os.path.join(OUTPUT_DIR, f\"{safe_pid}_pred.json\")\n",
    "    save_json(submission_data, save_path)\n",
    "\n",
    "print(\"‚úÖ Ranking completed for all papers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation Report (Nh·∫≠n x√©t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "üìä FINAL EVALUATION REPORT\n",
      "========================================\n",
      "Papers Processed: 39\n",
      "Total Queries:    812\n",
      "Metric MRR:       0.8349\n",
      "========================================\n",
      "üåü Nh·∫≠n x√©t: Xu·∫•t s·∫Øc. Model x·∫øp h·∫°ng reference r·∫•t ch√≠nh x√°c.\n",
      "\n",
      "üìÅ File k·∫øt qu·∫£ (pred.json) l∆∞u t·∫°i: d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\23127011\\notebooks\\submission_output\n"
     ]
    }
   ],
   "source": [
    "final_mrr = global_mrr_sum / global_query_count if global_query_count > 0 else 0\n",
    "\n",
    "print(\"=\"*40)\n",
    "print(\"üìä FINAL EVALUATION REPORT\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Papers Processed: {len(papers_db)}\")\n",
    "print(f\"Total Queries:    {global_query_count}\")\n",
    "print(f\"Metric MRR:       {final_mrr:.4f}\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "if final_mrr > 0.8:\n",
    "    print(\"üåü Nh·∫≠n x√©t: Xu·∫•t s·∫Øc. Model x·∫øp h·∫°ng reference r·∫•t ch√≠nh x√°c.\")\n",
    "elif final_mrr > 0.5:\n",
    "    print(\"üëç Nh·∫≠n x√©t: Kh√°. Model ƒë∆∞a ra ·ª©ng vi√™n ƒë√∫ng trong top 5 th∆∞·ªùng xuy√™n.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nh·∫≠n x√©t: C·∫ßn c·∫£i thi·ªán. Ki·ªÉm tra l·∫°i Features ho·∫∑c Model.\")\n",
    "\n",
    "print(f\"\\nüìÅ File k·∫øt qu·∫£ (pred.json) l∆∞u t·∫°i: {os.path.abspath(OUTPUT_DIR)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
