{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2: Model Evaluation & Ranking Generation\n",
    "**M·ª•c ti√™u:**\n",
    "1. ƒê·ªçc d·ªØ li·ªáu `labels.json` (ch·ª©a input BibTeX v√† Ground Truth).\n",
    "2. T√°i c·∫•u tr√∫c d·ªØ li·ªáu theo t·ª´ng b√†i b√°o (Paper-based grouping).\n",
    "3. S·ª≠ d·ª•ng m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán ƒë·ªÉ x·∫øp h·∫°ng (Ranking) c√°c reference candidates.\n",
    "4. T√≠nh ch·ªâ s·ªë **MRR (Mean Reciprocal Rank)**.\n",
    "5. Xu·∫•t file `pred.json` theo ƒë√∫ng ƒë·ªãnh d·∫°ng y√™u c·∫ßu n·ªôp b√†i.\n",
    "\n",
    "**Y√™u c·∫ßu ƒë·∫ßu v√†o:**\n",
    "* `labels.json`: File d·ªØ li·ªáu test.\n",
    "* `models/best_matcher.pkl`: Model ƒë√£ train.\n",
    "* `models/feature_names.pkl`: List t√™n features t∆∞∆°ng ·ª©ng.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# C√†i ƒë·∫∑t hi·ªÉn th·ªã\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N ---\n",
    "# H√£y thay ƒë·ªïi ƒë∆∞·ªùng d·∫´n ph√π h·ª£p v·ªõi m√¥i tr∆∞·ªùng c·ªßa b·∫°n\n",
    "TEST_FILE_PATH = '../../dataset_final/test/labels.json'           # File d·ªØ li·ªáu test\n",
    "MODEL_PATH = '../../dataset_final/models/best_matcher.pkl'   # File model (ƒë√£ train ·ªü b∆∞·ªõc tr∆∞·ªõc)\n",
    "FEATURE_NAME_PATH = '../../dataset_final/models/feature_names.pkl' # Danh s√°ch features\n",
    "OUTPUT_DIR = 'submission_output'         # Th∆∞ m·ª•c ch·ª©a k·∫øt qu·∫£ pred.json\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    print(f\"Created output directory: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_text_basic(text):\n",
    "    \"\"\"Chu·∫©n h√≥a c∆° b·∫£n ƒë·ªÉ t√≠nh to√°n kho·∫£ng c√°ch\"\"\"\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    return str(text).lower().strip()\n",
    "\n",
    "def get_tokens(text_list_or_str):\n",
    "    \"\"\"Chuy·ªÉn ƒë·ªïi text ho·∫∑c list text th√†nh set c√°c t·ª´ ƒë∆°n (tokens)\"\"\"\n",
    "    if isinstance(text_list_or_str, list):\n",
    "        text = \" \".join([str(t) for t in text_list_or_str])\n",
    "    else:\n",
    "        text = str(text_list_or_str)\n",
    "    \n",
    "    # B·ªè d·∫•u c√¢u, gi·ªØ l·∫°i ch·ªØ s·ªë v√† ch·ªØ c√°i\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    return set(text.split())\n",
    "\n",
    "def safe_year_diff(y1, y2):\n",
    "    \"\"\"T√≠nh kho·∫£ng c√°ch nƒÉm, x·ª≠ l√Ω l·ªói n·∫øu thi·∫øu d·ªØ li·ªáu\"\"\"\n",
    "    try:\n",
    "        # L·∫•y 4 s·ªë ƒë·∫ßu ti√™n t√¨m th·∫•y l√†m nƒÉm\n",
    "        m1 = re.search(r'\\d{4}', str(y1))\n",
    "        m2 = re.search(r'\\d{4}', str(y2))\n",
    "        \n",
    "        if m1 and m2:\n",
    "            val1 = int(m1.group(0))\n",
    "            val2 = int(m2.group(0))\n",
    "            diff = abs(val1 - val2)\n",
    "            # Clip kho·∫£ng c√°ch ƒë·ªÉ tr√°nh outlier qu√° l·ªõn (v√≠ d·ª• sai s·ªë 100 nƒÉm)\n",
    "            return min(diff, 10) \n",
    "        return -1 # Gi√° tr·ªã missing indicator\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Engineering\n",
    "**L∆∞u √Ω quan tr·ªçng:** C√°c h√†m d∆∞·ªõi ƒë√¢y ph·∫£i **gi·ªëng h·ªát** logic b·∫°n ƒë√£ s·ª≠ d·ª•ng khi hu·∫•n luy·ªán m√¥ h√¨nh (Training Phase). N·∫øu thay ƒë·ªïi, model s·∫Ω nh·∫≠n di·ªán sai ƒë·∫∑c tr∆∞ng.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper: Text Processing ---\n",
    "def normalize_text(text):\n",
    "    if pd.isna(text) or text is None: return \"\"\n",
    "    text = str(text).lower().strip()\n",
    "    return text\n",
    "\n",
    "def get_tokens(text_list_or_str):\n",
    "    if isinstance(text_list_or_str, list):\n",
    "        text = \" \".join([str(t) for t in text_list_or_str])\n",
    "    else:\n",
    "        text = str(text_list_or_str)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    return set(text.split())\n",
    "def parse_bibtex_content(bib_content):\n",
    "    \"\"\"\n",
    "    Tr√≠ch xu·∫•t Title v√† Authors t·ª´ chu·ªói BibTeX raw.\n",
    "    ƒê√£ s·ª≠a l·ªói Regex pattern error.\n",
    "    \"\"\"\n",
    "    # Extract Title\n",
    "    # S·ª¨A L·ªñI: Thay (?<!\\) th√†nh (?<!\\\\)\n",
    "    title_match = re.search(r'title\\s*=\\s*[\\{\"](.*?)(?<!\\\\)[\\}\"]', bib_content, re.I | re.S)\n",
    "    title = title_match.group(1) if title_match else \"\"\n",
    "    title = normalize_text(title)\n",
    "    \n",
    "    # Extract Authors\n",
    "    # S·ª¨A L·ªñI: T∆∞∆°ng t·ª± cho ph·∫ßn authors\n",
    "    author_match = re.search(r'author\\s*=\\s*[\\{\"](.*?)(?<!\\\\)[\\}\"]', bib_content, re.I | re.S)\n",
    "    authors_str = author_match.group(1) if author_match else \"\"\n",
    "    # T√°ch author ƒë∆°n gi·∫£n b·∫±ng 'and'\n",
    "    authors = [normalize_text(a) for a in authors_str.split(' and ')] if authors_str else []\n",
    "    \n",
    "    return title, authors\n",
    "def compute_pairwise_features(row):\n",
    "    feats = {}\n",
    "    \n",
    "    # --- A. UNPACK DATA ---\n",
    "    q_tit = normalize_text_basic(row.get('bib_title', ''))\n",
    "    q_auth_list = row.get('bib_authors', [])\n",
    "    q_id = normalize_text_basic(row.get('bib_id', ''))\n",
    "    # L·∫•y nƒÉm v√† √©p ki·ªÉu v·ªÅ string ƒë·ªÉ x·ª≠ l√Ω an to√†n\n",
    "    q_year = str(row.get('bib_year', ''))\n",
    "    \n",
    "    c_tit = normalize_text_basic(row.get('cand_title', ''))\n",
    "    c_auth_list = row.get('cand_authors', [])\n",
    "    c_id = normalize_text_basic(row.get('cand_id', ''))\n",
    "    c_year = str(row.get('cand_year', '')) # Ground truth year\n",
    "    \n",
    "    # Chuy·ªÉn list author th√†nh string ƒë·ªÉ d√πng fuzzy match\n",
    "    q_auth_str = \" \".join(q_auth_list) if isinstance(q_auth_list, list) else str(q_auth_list)\n",
    "    c_auth_str = \" \".join(c_auth_list) if isinstance(c_auth_list, list) else str(c_auth_list)\n",
    "\n",
    "    # --- B. ID FEATURES (GOLDEN FEATURE) ---\n",
    "    id_score = 0.0\n",
    "    if q_id and c_id:\n",
    "        clean_q = re.sub(r'[^a-z0-9]', '', q_id)\n",
    "        clean_c = re.sub(r'[^a-z0-9]', '', c_id)\n",
    "        if clean_q == clean_c: id_score = 1.0\n",
    "        elif clean_q in clean_c or clean_c in clean_q: id_score = 0.8\n",
    "    feats['feat_id_match'] = id_score\n",
    "\n",
    "    # --- C. TITLE FEATURES ---\n",
    "    feats['feat_title_fuzzy'] = fuzz.ratio(q_tit, c_tit) / 100.0\n",
    "    feats['feat_title_sort'] = fuzz.token_sort_ratio(q_tit, c_tit) / 100.0\n",
    "    feats['feat_title_partial'] = fuzz.partial_ratio(q_tit, c_tit) / 100.0\n",
    "    \n",
    "    # Feature m·ªõi: Ki·ªÉm tra xem Title query c√≥ n·∫±m tr·ªçn trong Candidate kh√¥ng (v√† ng∆∞·ª£c l·∫°i)\n",
    "    # Gi√∫p b·∫Øt tr∆∞·ªùng h·ª£p BibTeX vi·∫øt t·∫Øt\n",
    "    feats['feat_title_contain'] = 1.0 if (q_tit and c_tit and (q_tit in c_tit or c_tit in q_tit)) else 0.0\n",
    "\n",
    "    len_q = len(q_tit)\n",
    "    len_c = len(c_tit)\n",
    "    feats['feat_title_len_diff'] = abs(len_q - len_c) / max(len_q, len_c, 1) # tr√°nh chia 0\n",
    "\n",
    "    # --- D. AUTHOR FEATURES (N√ÇNG C·∫§P) ---\n",
    "    q_tokens = get_tokens(q_auth_list)\n",
    "    c_tokens = get_tokens(c_auth_list)\n",
    "    \n",
    "    # 1. Jaccard (Gi·ªØ nguy√™n)\n",
    "    if q_tokens and c_tokens:\n",
    "        inter = len(q_tokens.intersection(c_tokens))\n",
    "        union = len(q_tokens.union(c_tokens))\n",
    "        feats['feat_auth_jaccard'] = inter / union\n",
    "        feats['feat_auth_overlap'] = inter\n",
    "    else:\n",
    "        feats['feat_auth_jaccard'] = 0.0\n",
    "        feats['feat_auth_overlap'] = 0\n",
    "        \n",
    "    # 2. Author Fuzzy Sort (M·ªöI - R·∫•t quan tr·ªçng)\n",
    "    # Gi√∫p x·ª≠ l√Ω: \"Bengio, Y.\" vs \"Yoshua Bengio\"\n",
    "    feats['feat_auth_token_sort'] = fuzz.token_sort_ratio(q_auth_str, c_auth_str) / 100.0\n",
    "        \n",
    "    # 3. First Author Match\n",
    "    try:\n",
    "        a1_q = str(q_auth_list[0]).split()[0].lower() if len(q_auth_list) > 0 else \"\"\n",
    "        a1_c = str(c_auth_list[0]).split()[0].lower() if len(c_auth_list) > 0 else \"\"\n",
    "        # So s√°nh 3 k√Ω t·ª± ƒë·∫ßu\n",
    "        feats['feat_first_auth_match'] = 1.0 if (a1_q and a1_c and a1_q[:3] == a1_c[:3]) else 0.0\n",
    "    except:\n",
    "        feats['feat_first_auth_match'] = 0.0\n",
    "        \n",
    "    # --- E. YEAR FEATURE (UNCOMMENT & FIX) ---\n",
    "    # NƒÉm l√† feature c·ª±c m·∫°nh ƒë·ªÉ l·ªçc nhi·ªÖu\n",
    "    feats['feat_year_diff'] = safe_year_diff(q_year, c_year)\n",
    "    # Th√™m feature binary: C√≥ kh·ªõp nƒÉm ch√≠nh x√°c kh√¥ng?\n",
    "    feats['feat_year_match'] = 1.0 if (feats['feat_year_diff'] == 0) else 0.0\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load & Transform Data\n",
    "Ch√∫ng ta c·∫ßn chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu t·ª´ danh s√°ch ph·∫≥ng (flat list) trong `manual.json` sang c·∫•u tr√∫c **Paper-based**.\n",
    "* M·ªói `source_paper_id` s·∫Ω l√† m·ªôt nh√≥m.\n",
    "* T·∫≠p **Candidates** c·ªßa nh√≥m ƒë√≥ l√† t·∫≠p h·ª£p t·∫•t c·∫£ c√°c `ground_truth` unique xu·∫•t hi·ªán trong paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 812 entries from ../../dataset_final/test/labels.json\n",
      "‚úÖ Data transformed into 39 papers.\n"
     ]
    }
   ],
   "source": [
    "# Load d·ªØ li·ªáu\n",
    "try:\n",
    "    with open(TEST_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "        raw_data = json.load(f)\n",
    "    print(f\"Loaded {len(raw_data)} entries from {TEST_FILE_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Error: File {TEST_FILE_PATH} not found.\")\n",
    "    raw_data = []\n",
    "\n",
    "# Grouping logic\n",
    "papers_db = defaultdict(lambda: {'queries': [], 'candidates': {}})\n",
    "\n",
    "for item in raw_data:\n",
    "    paper_id = item.get('source_paper_id', 'unknown_paper')\n",
    "    \n",
    "    # 1. Input Query (BibTeX)\n",
    "    bib_key = item.get('key')\n",
    "    bib_content = item.get('content')\n",
    "    q_title, q_authors = parse_bibtex_content(bib_content)\n",
    "    \n",
    "    query_obj = {\n",
    "        'key': bib_key,\n",
    "        'bib_title': q_title,\n",
    "        'bib_authors': q_authors,\n",
    "        'true_id': item.get('ground_truth', {}).get('id')\n",
    "    }\n",
    "    papers_db[paper_id]['queries'].append(query_obj)\n",
    "    \n",
    "    # 2. Candidate Pool (References)\n",
    "    gt = item.get('ground_truth', {})\n",
    "    cand_id = gt.get('id')\n",
    "    \n",
    "    if cand_id and cand_id not in papers_db[paper_id]['candidates']:\n",
    "        cand_authors = gt.get('authors', [])\n",
    "        papers_db[paper_id]['candidates'][cand_id] = {\n",
    "            'cand_id': cand_id,\n",
    "            'cand_title': normalize_text(gt.get('title', '')),\n",
    "            'cand_authors': [normalize_text(a) for a in cand_authors]\n",
    "        }\n",
    "\n",
    "print(f\"‚úÖ Data transformed into {len(papers_db)} papers.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Trained Model\n",
    "Load model SVM/RandomForest/XGBoost ƒë√£ l∆∞u t·ª´ b∆∞·ªõc Training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded: XGBClassifier\n",
      "üìù Expected features: 9\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model = joblib.load(MODEL_PATH)\n",
    "    feature_names = joblib.load(FEATURE_NAME_PATH)\n",
    "    print(f\"‚úÖ Model loaded: {type(model).__name__}\")\n",
    "    print(f\"üìù Expected features: {len(feature_names)}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è WARNING: Model file not found. Code will run with RANDOM SCORES for demonstration.\")\n",
    "    model = None\n",
    "    feature_names = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ranking Pipeline & Prediction\n",
    "V·ªõi m·ªói b√†i b√°o:\n",
    "1. T·∫°o c·∫∑p (Query, Candidate) cho t·∫•t c·∫£ ·ª©ng vi√™n.\n",
    "2. T√≠nh feature.\n",
    "3. D·ª± ƒëo√°n x√°c su·∫•t match.\n",
    "4. L·∫•y Top 5 ·ª©ng vi√™n c√≥ ƒëi·ªÉm cao nh·∫•t.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Ranking Pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ranking completed for all papers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "global_mrr_sum = 0\n",
    "global_query_count = 0\n",
    "\n",
    "print(\"üöÄ Starting Ranking Pipeline...\")\n",
    "\n",
    "for paper_id, data in papers_db.items():\n",
    "    queries = data['queries']\n",
    "    candidates_dict = data['candidates']\n",
    "    candidates_list = list(candidates_dict.values())\n",
    "    \n",
    "    if not candidates_list:\n",
    "        continue\n",
    "\n",
    "    # Init Output Structure\n",
    "    submission_data = {\n",
    "        \"partition\": \"test\", \n",
    "        \"groundtruth\": {},\n",
    "        \"prediction\": {}\n",
    "    }\n",
    "    \n",
    "    # Loop qua t·ª´ng query trong paper\n",
    "    for query in tqdm(queries, desc=f\"Paper {paper_id}\", leave=False):\n",
    "        bib_key = query['key']\n",
    "        true_id = query['true_id']\n",
    "        \n",
    "        # Groundtruth\n",
    "        submission_data['groundtruth'][bib_key] = true_id\n",
    "        \n",
    "        # Pairing & Feature Calc\n",
    "        pairs = []\n",
    "        for cand in candidates_list:\n",
    "            row = {}\n",
    "            row.update(query)\n",
    "            row.update(cand)\n",
    "            pairs.append(row)\n",
    "            \n",
    "        # Compute Features\n",
    "        feats_list = [compute_pairwise_features(p) for p in pairs]\n",
    "        df_feats = pd.DataFrame(feats_list)\n",
    "        \n",
    "        # Predict\n",
    "        scores = []\n",
    "        if model:\n",
    "            # Ensure columns exist\n",
    "            for col in feature_names:\n",
    "                if col not in df_feats.columns: df_feats[col] = 0.0\n",
    "            \n",
    "            X_input = df_feats[feature_names]\n",
    "            scores = model.predict_proba(X_input)[:, 1]\n",
    "        else:\n",
    "            scores = np.random.rand(len(pairs)) # Fallback\n",
    "            \n",
    "        # Ranking\n",
    "        ranked_candidates = []\n",
    "        for idx, score in enumerate(scores):\n",
    "            ranked_candidates.append((candidates_list[idx]['cand_id'], score))\n",
    "        \n",
    "        ranked_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_5 = [x[0] for x in ranked_candidates[:5]]\n",
    "        \n",
    "        # Save Prediction\n",
    "        submission_data['prediction'][bib_key] = top_5\n",
    "        \n",
    "        # Calc MRR\n",
    "        if true_id in top_5:\n",
    "            rank = top_5.index(true_id) + 1\n",
    "            global_mrr_sum += 1.0 / rank\n",
    "        else:\n",
    "            global_mrr_sum += 0.0\n",
    "            \n",
    "        global_query_count += 1\n",
    "        \n",
    "    # Save JSON Output\n",
    "    # T√™n file pred.json n√™n ƒë·∫∑t trong folder ri√™ng c·ªßa student/paper n·∫øu n·ªôp th·∫≠t\n",
    "    # ·ªû ƒë√¢y l∆∞u d·∫°ng prefix_pred.json ƒë·ªÉ d·ªÖ ki·ªÉm tra\n",
    "    safe_pid = str(paper_id).replace('/', '_')\n",
    "    save_path = os.path.join(OUTPUT_DIR, f\"{safe_pid}_pred.json\")\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(submission_data, f, indent=4)\n",
    "\n",
    "print(\"‚úÖ Ranking completed for all papers.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation Report (Nh·∫≠n x√©t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "üìä FINAL EVALUATION REPORT\n",
      "========================================\n",
      "Papers Processed: 39\n",
      "Total Queries:    812\n",
      "Metric MRR:       0.2893\n",
      "========================================\n",
      "‚ö†Ô∏è Nh·∫≠n x√©t: C·∫ßn c·∫£i thi·ªán. Ki·ªÉm tra l·∫°i Features ho·∫∑c Model.\n",
      "\n",
      "üìÅ File k·∫øt qu·∫£ (pred.json) l∆∞u t·∫°i: d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\23127011\\notebooks\\submission_output\n"
     ]
    }
   ],
   "source": [
    "final_mrr = global_mrr_sum / global_query_count if global_query_count > 0 else 0\n",
    "\n",
    "print(\"=\"*40)\n",
    "print(\"üìä FINAL EVALUATION REPORT\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Papers Processed: {len(papers_db)}\")\n",
    "print(f\"Total Queries:    {global_query_count}\")\n",
    "print(f\"Metric MRR:       {final_mrr:.4f}\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "if final_mrr > 0.8:\n",
    "    print(\"üåü Nh·∫≠n x√©t: Xu·∫•t s·∫Øc. Model x·∫øp h·∫°ng reference r·∫•t ch√≠nh x√°c.\")\n",
    "elif final_mrr > 0.5:\n",
    "    print(\"üëç Nh·∫≠n x√©t: Kh√°. Model ƒë∆∞a ra ·ª©ng vi√™n ƒë√∫ng trong top 5 th∆∞·ªùng xuy√™n.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nh·∫≠n x√©t: C·∫ßn c·∫£i thi·ªán. Ki·ªÉm tra l·∫°i Features ho·∫∑c Model.\")\n",
    "\n",
    "print(f\"\\nüìÅ File k·∫øt qu·∫£ (pred.json) l∆∞u t·∫°i: {os.path.abspath(OUTPUT_DIR)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
