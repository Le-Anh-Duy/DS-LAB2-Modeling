{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "216f4a32",
   "metadata": {},
   "source": [
    "# Notebook 02: Data Augmentation (Negative Sampling)\n",
    "\n",
    "**M·ª•c ti√™u:** Sinh d·ªØ li·ªáu hu·∫•n luy·ªán cho model Machine Learning (Pairwise Ranking).\n",
    "\n",
    "ƒê·ªÉ train model, ta c·∫ßn t·∫°o ra c√°c c·∫∑p d·ªØ li·ªáu `(Query, Candidate, Label)`:\n",
    "* **Query:** Th√¥ng tin tr√≠ch xu·∫•t t·ª´ BibTeX (ƒë√£ clean).\n",
    "* **Candidate:** Th√¥ng tin t·ª´ Ground Truth (Metadata).\n",
    "* **Label:** 1 (Match) ho·∫∑c 0 (Non-match).\n",
    "\n",
    "**Chi·∫øn l∆∞·ª£c sinh m·∫´u sai (Negative Sampling):**\n",
    "1.  **Local Negatives (Hard):** Ch·ªçn metadata sai n·∫±m *trong c√πng m·ªôt b√†i b√°o* (`paper_id` gi·ªëng nhau). V√≠ d·ª•: BibTeX ref #1 gh√©p v·ªõi GT ref #2 c·ªßa c√πng b√†i b√°o ƒë√≥. ƒê√¢y l√† m·∫´u kh√≥ v√¨ c√πng ch·ªß ƒë·ªÅ.\n",
    "2.  **Global Negatives (Easy):** Ch·ªçn metadata sai t·ª´ *b·∫•t k·ª≥ b√†i b√°o n√†o kh√°c*. Gi√∫p model h·ªçc ph√¢n bi·ªát s·ª± kh√°c bi·ªát r√µ r·ªát (kh√°c nƒÉm, kh√°c t√°c gi·∫£...).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e17cf670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Import modules th√†nh c√¥ng t·ª´ src.ml!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Setup path ƒë·ªÉ import src module\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Import t·ª´ src.ml module\n",
    "from src.ml import (\n",
    "    load_pickle,\n",
    "    save_pickle,\n",
    "    augment_dataset,\n",
    "    build_candidate_pools,\n",
    "    split_by_partition\n",
    ")\n",
    "\n",
    "# --- C·∫§U H√åNH ---\n",
    "INPUT_FILE = '../../dataset_final/clean_data/cleaned_data.pkl'\n",
    "OUTPUT_FILE = '../../dataset_final/clean_data/train_augmented.pkl'\n",
    "\n",
    "# T·ª∑ l·ªá Negative/Positive\n",
    "NUM_NEGATIVES = 4  \n",
    "NUM_HARD_NEGATIVES = 2 \n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"‚úÖ Import modules th√†nh c√¥ng t·ª´ src.ml!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b232e6b",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Load d·ªØ li·ªáu s·∫°ch (Cleaned Data)\n",
    "Ta load file `.pkl` ƒë∆∞·ª£c t·∫°o ra t·ª´ Notebook 01 ƒë·ªÉ t·∫≠n d·ª•ng c√°c tr∆∞·ªùng `clean_title`, `clean_authors`, `clean_id`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e930b575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T·ªïng s·ªë m·∫´u g·ªëc (Train + Validation): 13455\n",
      "C√°c c·ªôt c√≥ s·∫µn: ['partition', 'source_file', 'key', 'paper_id', 'bib_content', 'clean_title', 'clean_authors', 'clean_id', 'clean_year', 'parse_method', 'gt_id', 'gt_title', 'gt_authors', 'gt_year']\n"
     ]
    }
   ],
   "source": [
    "# Load d·ªØ li·ªáu s·∫°ch s·ª≠ d·ª•ng h√†m t·ª´ module\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    raise FileNotFoundError(f\"‚ùå Kh√¥ng t√¨m th·∫•y file: {INPUT_FILE}. H√£y ch·∫°y Notebook 01 tr∆∞·ªõc.\")\n",
    "\n",
    "df_all = load_pickle(INPUT_FILE)\n",
    "\n",
    "# L·∫•y c·∫£ train + validation ƒë·ªÉ augment\n",
    "df_train_src = df_all[df_all['partition'].isin(['train', 'validation'])].copy()\n",
    "\n",
    "print(f\"T·ªïng s·ªë m·∫´u g·ªëc (Train + Validation): {len(df_train_src)}\")\n",
    "print(\"C√°c c·ªôt c√≥ s·∫µn:\", df_train_src.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1f8f63",
   "metadata": {},
   "source": [
    "\n",
    "## 2. X√¢y d·ª±ng Candidate Pools\n",
    "Ta c·∫ßn t·∫°o kho ch·ª©a c√°c ·ª©ng vi√™n (Ground Truth) ƒë·ªÉ l·∫•y m·∫´u.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5399f127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Pool Size: 13455\n",
      "Number of Papers (Local Pools): 610\n"
     ]
    }
   ],
   "source": [
    "# S·ª≠ d·ª•ng h√†m build_candidate_pools t·ª´ module\n",
    "global_candidates, local_pool = build_candidate_pools(df_train_src)\n",
    "\n",
    "print(f\"Global Pool Size: {len(global_candidates)}\")\n",
    "print(f\"Number of Papers (Local Pools): {len(local_pool)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191f0369",
   "metadata": {},
   "source": [
    "## 3. Th·ª±c hi·ªán Negative Sampling\n",
    "Quy tr√¨nh cho m·ªói d√≤ng BibTeX (Query):\n",
    "1.  T·∫°o 1 c·∫∑p **Positive** (Ch√≠nh n√≥).\n",
    "2.  T·∫°o k c·∫∑p **Hard Negative** (L·∫•y t·ª´ `local_pool` c·ªßa c√πng `paper_id`).\n",
    "3.  T·∫°o m c·∫∑p **Easy Negative** (L·∫•y ng·∫´u nhi√™n t·ª´ `global_candidates` kh√°c `paper_id`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3661438a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Global Pool Size: 13455\n",
      "üìä Number of Papers: 610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üöÄ Generating samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13455/13455 [00:00<00:00, 21628.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Augmentation completed!\n",
      "   - Original samples: 13455\n",
      "   - Augmented samples: 67275\n",
      "   - Label distribution:\n",
      "label\n",
      "0    53820\n",
      "1    13455\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# S·ª≠ d·ª•ng h√†m augment_dataset t·ª´ module\n",
    "# H√†m n√†y ƒë√£ bao g·ªìm to√†n b·ªô logic sinh Positive + Negative samples\n",
    "\n",
    "df_augmented = augment_dataset(\n",
    "    df_source=df_train_src,\n",
    "    num_negatives=NUM_NEGATIVES,\n",
    "    num_hard_negatives=NUM_HARD_NEGATIVES,\n",
    "    random_seed=RANDOM_SEED,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3618d101",
   "metadata": {},
   "source": [
    "## 4. L∆∞u k·∫øt qu·∫£\n",
    "L∆∞u th√†nh DataFrame ƒë·ªÉ d√πng cho b∆∞·ªõc Feature Engineering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "017988e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- K·∫æT QU·∫¢ DATA AUGMENTATION ---\n",
      "S·ªë l∆∞·ª£ng m·∫´u ban ƒë·∫ßu: 13455\n",
      "S·ªë l∆∞·ª£ng m·∫´u sau khi sinh: 67275\n",
      "T·ª∑ l·ªá Positive/Negative:\n",
      "label\n",
      "0    53820\n",
      "1    13455\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚úÖ ƒê√£ l∆∞u file dataset hu·∫•n luy·ªán t·∫°i: d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\dataset_final\\clean_data\\train_augmented.pkl\n",
      "üëâ B∆Ø·ªöC TI·∫æP THEO: Ch·∫°y '03_feature_engineering.ipynb'\n"
     ]
    }
   ],
   "source": [
    "# L∆∞u file s·ª≠ d·ª•ng module\n",
    "print(\"\\n--- K·∫æT QU·∫¢ DATA AUGMENTATION ---\")\n",
    "print(f\"S·ªë l∆∞·ª£ng m·∫´u ban ƒë·∫ßu: {len(df_train_src)}\")\n",
    "print(f\"S·ªë l∆∞·ª£ng m·∫´u sau khi sinh: {len(df_augmented)}\")\n",
    "print(f\"T·ª∑ l·ªá Positive/Negative:\\n{df_augmented['label'].value_counts()}\")\n",
    "\n",
    "# L∆∞u file PKL\n",
    "df_augmented.to_pickle(OUTPUT_FILE)\n",
    "\n",
    "print(f\"\\n‚úÖ ƒê√£ l∆∞u file dataset hu·∫•n luy·ªán t·∫°i: {os.path.abspath(OUTPUT_FILE)}\")\n",
    "print(\"üëâ B∆Ø·ªöC TI·∫æP THEO: Ch·∫°y '03_feature_engineering.ipynb'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7172c4",
   "metadata": {},
   "source": [
    "## 5. Ki·ªÉm tra m·∫´u d·ªØ li·ªáu (Sanity Check)\n",
    "Ki·ªÉm tra xem m·∫´u Hard Negative tr√¥ng nh∆∞ th·∫ø n√†o.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0a6bea47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- M·∫™U POSITIVE ---\n",
      "Bib Title:  Ibrnet: Learning multi-view image-based rendering\n",
      "Cand Title: IBRNet: Learning Multi-View Image-Based Rendering\n",
      "Match ID:    == 2102-13090\n",
      "\n",
      "--- M·∫™U NEGATIVE (Check xem Title c√≥ kh√°c nhau kh√¥ng) ---\n",
      "Bib Title:  Graph States as a Resource for Quantum Metrology\n",
      "Cand Title: Emergent Bartering Behaviour in Multi-Agent Reinforcement Learning\n",
      "Label:      0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "# L·∫•y 1 m·∫´u Positive\n",
    "print(\"--- M·∫™U POSITIVE ---\")\n",
    "pos_sample = df_augmented[df_augmented['label'] == 1].iloc[0]\n",
    "print(f\"Bib Title:  {pos_sample['bib_title']}\")\n",
    "print(f\"Cand Title: {pos_sample['cand_title']}\")\n",
    "print(f\"Match ID:   {pos_sample['bib_id']} == {pos_sample['cand_id']}\")\n",
    "\n",
    "print(\"\\n--- M·∫™U NEGATIVE (Check xem Title c√≥ kh√°c nhau kh√¥ng) ---\")\n",
    "neg_sample = df_augmented[df_augmented['label'] == 0].iloc[0]\n",
    "print(f\"Bib Title:  {neg_sample['bib_title']}\")\n",
    "print(f\"Cand Title: {neg_sample['cand_title']}\")\n",
    "print(f\"Label:      {neg_sample['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f66ac093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l∆∞u json ƒë·ªÉ check\n",
    "\n",
    "import json\n",
    "with open('sample_positive.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(pos_sample.to_dict(), f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
