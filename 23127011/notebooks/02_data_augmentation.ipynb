{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "216f4a32",
   "metadata": {},
   "source": [
    "# Notebook 02: Data Augmentation (Negative Sampling)\n",
    "\n",
    "**M·ª•c ti√™u:** Sinh d·ªØ li·ªáu hu·∫•n luy·ªán cho model Machine Learning (Pairwise Ranking).\n",
    "\n",
    "ƒê·ªÉ train model, ta c·∫ßn t·∫°o ra c√°c c·∫∑p d·ªØ li·ªáu `(Query, Candidate, Label)`:\n",
    "* **Query:** Th√¥ng tin tr√≠ch xu·∫•t t·ª´ BibTeX (ƒë√£ clean).\n",
    "* **Candidate:** Th√¥ng tin t·ª´ Ground Truth (Metadata).\n",
    "* **Label:** 1 (Match) ho·∫∑c 0 (Non-match).\n",
    "\n",
    "**Chi·∫øn l∆∞·ª£c sinh m·∫´u sai (Negative Sampling):**\n",
    "1.  **Local Negatives (Hard):** Ch·ªçn metadata sai n·∫±m *trong c√πng m·ªôt b√†i b√°o* (`paper_id` gi·ªëng nhau). V√≠ d·ª•: BibTeX ref #1 gh√©p v·ªõi GT ref #2 c·ªßa c√πng b√†i b√°o ƒë√≥. ƒê√¢y l√† m·∫´u kh√≥ v√¨ c√πng ch·ªß ƒë·ªÅ.\n",
    "2.  **Global Negatives (Easy):** Ch·ªçn metadata sai t·ª´ *b·∫•t k·ª≥ b√†i b√°o n√†o kh√°c*. Gi√∫p model h·ªçc ph√¢n bi·ªát s·ª± kh√°c bi·ªát r√µ r·ªát (kh√°c nƒÉm, kh√°c t√°c gi·∫£...).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e17cf670",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- C·∫§U H√åNH ---\n",
    "INPUT_FILE = '../../dataset_final/clean_data/cleaned_data.pkl' # Load file PKL t·ª´ NB01\n",
    "OUTPUT_FILE = '../../dataset_final/clean_data/train_augmented.pkl' # L∆∞u d·∫°ng PKL cho nhanh\n",
    "\n",
    "# T·ª∑ l·ªá Negative/Positive\n",
    "# 1 Positive s·∫Ω ƒëi k√®m v·ªõi bao nhi√™u Negative?\n",
    "NUM_NEGATIVES = 4  \n",
    "# Trong ƒë√≥ bao nhi√™u l√† Hard Negative (c√πng b√†i b√°o)?\n",
    "NUM_HARD_NEGATIVES = 2 \n",
    "# S·ªë c√≤n l·∫°i s·∫Ω l√† Easy Negative (random b√†i kh√°c)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b232e6b",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Load d·ªØ li·ªáu s·∫°ch (Cleaned Data)\n",
    "Ta load file `.pkl` ƒë∆∞·ª£c t·∫°o ra t·ª´ Notebook 01 ƒë·ªÉ t·∫≠n d·ª•ng c√°c tr∆∞·ªùng `clean_title`, `clean_authors`, `clean_id`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e930b575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T·ªïng s·ªë m·∫´u g·ªëc (Partition = Train): 6720\n",
      "C√°c c·ªôt c√≥ s·∫µn: ['partition', 'source_type', 'key', 'paper_id', 'bib_content', 'gt_id', 'gt_title', 'gt_authors', 'clean_title', 'clean_authors', 'clean_id', 'clean_year', 'parse_method', 'gt_year']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    raise FileNotFoundError(f\"‚ùå Kh√¥ng t√¨m th·∫•y file: {INPUT_FILE}. H√£y ch·∫°y Notebook 01 tr∆∞·ªõc.\")\n",
    "\n",
    "df_all = pd.read_pickle(INPUT_FILE)\n",
    "\n",
    "# Ch·ªâ l·∫•y t·∫≠p Train ƒë·ªÉ Augment\n",
    "df_train_src = df_all[df_all['partition'] == 'train'].copy()\n",
    "\n",
    "print(f\"T·ªïng s·ªë m·∫´u g·ªëc (Partition = Train): {len(df_train_src)}\")\n",
    "print(\"C√°c c·ªôt c√≥ s·∫µn:\", df_train_src.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1f8f63",
   "metadata": {},
   "source": [
    "\n",
    "## 2. X√¢y d·ª±ng Candidate Pools\n",
    "Ta c·∫ßn t·∫°o kho ch·ª©a c√°c ·ª©ng vi√™n (Ground Truth) ƒë·ªÉ l·∫•y m·∫´u.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5399f127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Pool Size: 6720\n"
     ]
    }
   ],
   "source": [
    "global_candidates = df_train_src[['gt_id', 'gt_title', 'gt_authors', 'gt_year', 'paper_id']].to_dict('records')\n",
    "\n",
    "# 2. Local Pool: Gom nh√≥m theo Paper ID\n",
    "local_pool = {}\n",
    "for item in global_candidates:\n",
    "    pid = item['paper_id']\n",
    "    if pid not in local_pool:\n",
    "        local_pool[pid] = []\n",
    "    local_pool[pid].append(item)\n",
    "\n",
    "print(f\"Global Pool Size: {len(global_candidates)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191f0369",
   "metadata": {},
   "source": [
    "## 3. Th·ª±c hi·ªán Negative Sampling\n",
    "Quy tr√¨nh cho m·ªói d√≤ng BibTeX (Query):\n",
    "1.  T·∫°o 1 c·∫∑p **Positive** (Ch√≠nh n√≥).\n",
    "2.  T·∫°o k c·∫∑p **Hard Negative** (L·∫•y t·ª´ `local_pool` c·ªßa c√πng `paper_id`).\n",
    "3.  T·∫°o m c·∫∑p **Easy Negative** (L·∫•y ng·∫´u nhi√™n t·ª´ `global_candidates` kh√°c `paper_id`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb037562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3661438a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ ƒêang sinh d·ªØ li·ªáu training (ƒë√£ bao g·ªìm Year)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6720/6720 [00:00<00:00, 26678.07it/s]\n"
     ]
    }
   ],
   "source": [
    "augmented_rows = []\n",
    "\n",
    "print(\"üöÄ ƒêang sinh d·ªØ li·ªáu training (ƒë√£ bao g·ªìm Year)...\")\n",
    "\n",
    "for idx, row in tqdm(df_train_src.iterrows(), total=len(df_train_src)):\n",
    "    # --- A. L·∫§Y TH√îNG TIN QUERY (BIBTEX) ---\n",
    "    query_info = {\n",
    "        'bib_title': row['clean_title'],\n",
    "        'bib_authors': row['clean_authors'],\n",
    "        'bib_id': row['clean_id'],\n",
    "        'bib_year': row['clean_year'] # NƒÉm tr√≠ch xu·∫•t t·ª´ BibTeX\n",
    "    }\n",
    "    \n",
    "    true_gt_id = row['gt_id']\n",
    "    current_paper_id = row['paper_id']\n",
    "\n",
    "    # --- B. T·∫†O POSITIVE SAMPLE (LABEL = 1) ---\n",
    "    pos_row = query_info.copy()\n",
    "    pos_row.update({\n",
    "        'cand_id': row['gt_id'],\n",
    "        'cand_title': row['gt_title'],\n",
    "        'cand_authors': row['gt_authors'],\n",
    "        'cand_year': row['gt_year'],  # <--- QUAN TR·ªåNG: NƒÉm c·ªßa Ground Truth\n",
    "        'label': 1\n",
    "    })\n",
    "    augmented_rows.append(pos_row)\n",
    "\n",
    "    # --- C. T·∫†O NEGATIVE SAMPLES (LABEL = 0) ---\n",
    "    negatives_collected = 0\n",
    "    \n",
    "    # C.1: Hard Negatives (Local)\n",
    "    local_candidates = local_pool.get(current_paper_id, [])\n",
    "    valid_local_cands = [c for c in local_candidates if c['gt_id'] != true_gt_id]\n",
    "    \n",
    "    if valid_local_cands:\n",
    "        k_hard = min(NUM_HARD_NEGATIVES, len(valid_local_cands))\n",
    "        chosen_hard = random.sample(valid_local_cands, k_hard)\n",
    "        \n",
    "        for cand in chosen_hard:\n",
    "            neg_row = query_info.copy()\n",
    "            neg_row.update({\n",
    "                'cand_id': cand['gt_id'],\n",
    "                'cand_title': cand['gt_title'],\n",
    "                'cand_authors': cand['gt_authors'],\n",
    "                'cand_year': cand['gt_year'], # <--- L·∫•y nƒÉm c·ªßa ·ª©ng vi√™n sai\n",
    "                'label': 0\n",
    "            })\n",
    "            augmented_rows.append(neg_row)\n",
    "            negatives_collected += 1\n",
    "\n",
    "    # C.2: Easy Negatives (Global)\n",
    "    needed = NUM_NEGATIVES - negatives_collected\n",
    "    attempts = 0\n",
    "    while needed > 0 and attempts < 50:\n",
    "        attempts += 1\n",
    "        cand = random.choice(global_candidates)\n",
    "        \n",
    "        if cand['gt_id'] != true_gt_id and cand['paper_id'] != current_paper_id:\n",
    "            neg_row = query_info.copy()\n",
    "            neg_row.update({\n",
    "                'cand_id': cand['gt_id'],\n",
    "                'cand_title': cand['gt_title'],\n",
    "                'cand_authors': cand['gt_authors'],\n",
    "                'cand_year': cand['gt_year'], # <--- L·∫•y nƒÉm c·ªßa ·ª©ng vi√™n sai\n",
    "                'label': 0\n",
    "            })\n",
    "            augmented_rows.append(neg_row)\n",
    "            needed -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3618d101",
   "metadata": {},
   "source": [
    "## 4. L∆∞u k·∫øt qu·∫£\n",
    "L∆∞u th√†nh DataFrame ƒë·ªÉ d√πng cho b∆∞·ªõc Feature Engineering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "017988e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- K·∫æT QU·∫¢ DATA AUGMENTATION ---\n",
      "S·ªë l∆∞·ª£ng m·∫´u ban ƒë·∫ßu: 6720\n",
      "S·ªë l∆∞·ª£ng m·∫´u sau khi sinh: 33600\n",
      "T·ª∑ l·ªá Positive/Negative:\n",
      "label\n",
      "0    26880\n",
      "1     6720\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚úÖ ƒê√£ l∆∞u file dataset hu·∫•n luy·ªán t·∫°i: d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\dataset_final\\clean_data\\train_augmented.pkl\n",
      "üëâ B∆Ø·ªöC TI·∫æP THEO: Ch·∫°y '03_feature_engineering.ipynb' ƒë·ªÉ t·∫°o vector ƒë·∫∑c tr∆∞ng t·ª´ file n√†y.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_augmented = pd.DataFrame(augmented_rows)\n",
    "\n",
    "print(\"\\n--- K·∫æT QU·∫¢ DATA AUGMENTATION ---\")\n",
    "print(f\"S·ªë l∆∞·ª£ng m·∫´u ban ƒë·∫ßu: {len(df_train_src)}\")\n",
    "print(f\"S·ªë l∆∞·ª£ng m·∫´u sau khi sinh: {len(df_augmented)}\")\n",
    "print(f\"T·ª∑ l·ªá Positive/Negative:\\n{df_augmented['label'].value_counts()}\")\n",
    "\n",
    "# Shuffle d·ªØ li·ªáu\n",
    "df_augmented = df_augmented.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "# L∆∞u file PKL (Gi·ªØ nguy√™n ki·ªÉu d·ªØ li·ªáu List cho Authors)\n",
    "df_augmented.to_pickle(OUTPUT_FILE)\n",
    "\n",
    "print(f\"\\n‚úÖ ƒê√£ l∆∞u file dataset hu·∫•n luy·ªán t·∫°i: {os.path.abspath(OUTPUT_FILE)}\")\n",
    "print(\"üëâ B∆Ø·ªöC TI·∫æP THEO: Ch·∫°y '03_feature_engineering.ipynb' ƒë·ªÉ t·∫°o vector ƒë·∫∑c tr∆∞ng t·ª´ file n√†y.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7172c4",
   "metadata": {},
   "source": [
    "## 5. Ki·ªÉm tra m·∫´u d·ªØ li·ªáu (Sanity Check)\n",
    "Ki·ªÉm tra xem m·∫´u Hard Negative tr√¥ng nh∆∞ th·∫ø n√†o.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a6bea47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- M·∫™U POSITIVE ---\n",
      "Bib Title:  Adding conditional control to text-to-image diffusion models\n",
      "Cand Title: Adding Conditional Control to Text-to-Image Diffusion Models\n",
      "Match ID:    == 2302-05543\n",
      "\n",
      "--- M·∫™U NEGATIVE (Check xem Title c√≥ kh√°c nhau kh√¥ng) ---\n",
      "Bib Title:  Personalized federated learning via variational bayesian inference.\n",
      "Cand Title: Narrow-Line Cooling and Imaging of Ytterbium Atoms in an Optical Tweezer Array.\n",
      "Label:      0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "# L·∫•y 1 m·∫´u Positive\n",
    "print(\"--- M·∫™U POSITIVE ---\")\n",
    "pos_sample = df_augmented[df_augmented['label'] == 1].iloc[0]\n",
    "print(f\"Bib Title:  {pos_sample['bib_title']}\")\n",
    "print(f\"Cand Title: {pos_sample['cand_title']}\")\n",
    "print(f\"Match ID:   {pos_sample['bib_id']} == {pos_sample['cand_id']}\")\n",
    "\n",
    "print(\"\\n--- M·∫™U NEGATIVE (Check xem Title c√≥ kh√°c nhau kh√¥ng) ---\")\n",
    "neg_sample = df_augmented[df_augmented['label'] == 0].iloc[0]\n",
    "print(f\"Bib Title:  {neg_sample['bib_title']}\")\n",
    "print(f\"Cand Title: {neg_sample['cand_title']}\")\n",
    "print(f\"Label:      {neg_sample['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66ac093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l∆∞u json ƒë·ªÉ check\n",
    "\n",
    "import json\n",
    "with open('sample_positive.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(pos_sample.to_dict(), f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
