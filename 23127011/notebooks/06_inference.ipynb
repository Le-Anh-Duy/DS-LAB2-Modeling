{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa83b48",
   "metadata": {},
   "source": [
    "# Notebook 06: Full Inference for Submission\n",
    "**M·ª•c ti√™u:**\n",
    "1. Ch·∫°y pipeline d·ª± ƒëo√°n tr√™n **T·∫§T C·∫¢** c√°c t·∫≠p d·ªØ li·ªáu: Train, Validation, v√† Test.\n",
    "2. Sinh ra file `pred.json` cho t·ª´ng b√†i b√°o.\n",
    "3. S·∫Øp x·∫øp k·∫øt qu·∫£ v√†o ƒë√∫ng c·∫•u tr√∫c th∆∞ m·ª•c ƒë·ªÉ n·ªôp b√†i (Submission Ready).\n",
    "\n",
    "**Model:** Decision Tree/XGBoost v·ªõi **6 selected features**\n",
    "\n",
    "**C·∫•u tr√∫c Output:**\n",
    "```bash\n",
    "submission_final/\n",
    "‚îú‚îÄ‚îÄ <Student_ID>/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ <paper_id_1>/\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pred.json\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ <paper_id_2>/\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pred.json\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cb4b4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries v√† src.ml modules imported successfully!\n",
      "üìù S·ª≠ d·ª•ng 6 selected features\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup path ƒë·ªÉ import src module\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Import t·ª´ src.ml module (S·ª≠ d·ª•ng l·∫°i c√°c h√†m ƒë√£ refactor)\n",
    "from src.ml import (\n",
    "    load_json,\n",
    "    save_json,\n",
    "    normalize_text_basic,\n",
    "    compute_pairwise_features,\n",
    "    compute_tfidf_cosine_single,\n",
    "    parse_bibtex_smart,\n",
    "    transform_to_paper_based\n",
    ")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"‚úÖ Libraries v√† src.ml modules imported successfully!\")\n",
    "\n",
    "# --- DANH S√ÅCH 6 FEATURES ƒê√É CH·ªåN (ph·∫£i kh·ªõp v·ªõi Notebook 03/04) ---\n",
    "SELECTED_FEATURES = [\n",
    "    'feat_title_tfidf_cosine',\n",
    "    'feat_title_len_diff',\n",
    "    'feat_auth_jaccard',\n",
    "    'feat_year_match',\n",
    "    'feat_id_match',\n",
    "    'feat_first_auth_match',\n",
    "]\n",
    "print(f\"üìù S·ª≠ d·ª•ng {len(SELECTED_FEATURES)} selected features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00f10b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Created submission directory: d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\23127011\\notebooks\\submission_final\\23127011\n",
      "üìÇ Dataset (labels): d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\dataset_final\n",
      "üìÇ Data (refs+references): d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\data_output_v2\n"
     ]
    }
   ],
   "source": [
    "# --- C·∫§U H√åNH QUAN TR·ªåNG ---\n",
    "\n",
    "# 1. Th√¥ng tin sinh vi√™n (ƒê·ªÉ t·∫°o folder n·ªôp b√†i)\n",
    "STUDENT_ID = \"23127011\" \n",
    "\n",
    "# 2. ƒê∆∞·ªùng d·∫´n d·ªØ li·ªáu\n",
    "DATASET_ROOT = '../../dataset_final'  # Ch·ª©a labels.json cho groundtruth\n",
    "DATA_FOLDER = '../../data_output_v2'  # Ch·ª©a refs.bib + references.json ƒë·∫ßy ƒë·ªß\n",
    "PARTITIONS = ['train', 'validation', 'test']\n",
    "\n",
    "# 3. ƒê∆∞·ªùng d·∫´n Model\n",
    "MODEL_PATH = '../../dataset_final/models/best_matcher.pkl'\n",
    "FEATURE_NAME_PATH = '../../dataset_final/models/feature_names.pkl'\n",
    "\n",
    "# 4. Th∆∞ m·ª•c Output\n",
    "SUBMISSION_DIR = f'submission_final/{STUDENT_ID}'\n",
    "\n",
    "if not os.path.exists(SUBMISSION_DIR):\n",
    "    os.makedirs(SUBMISSION_DIR)\n",
    "    print(f\"üìÅ Created submission directory: {os.path.abspath(SUBMISSION_DIR)}\")\n",
    "else:\n",
    "    print(f\"üìÅ Saving to existing directory: {os.path.abspath(SUBMISSION_DIR)}\")\n",
    "\n",
    "print(f\"üìÇ Dataset (labels): {os.path.abspath(DATASET_ROOT)}\")\n",
    "print(f\"üìÇ Data (refs+references): {os.path.abspath(DATA_FOLDER)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83de33a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SUPER Optimized functions ready (TF-IDF pre-computed per paper).\n"
     ]
    }
   ],
   "source": [
    "# --- OPTIMIZED HELPER FUNCTIONS WITH TIMING ---\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "\n",
    "# Global timing stats\n",
    "TIMING_STATS = {\n",
    "    'feature_compute': 0.0,\n",
    "    'tfidf_compute': 0.0,\n",
    "    'model_predict': 0.0,\n",
    "    'total_queries': 0\n",
    "}\n",
    "\n",
    "def batch_compute_features(pairs_list):\n",
    "    \"\"\"Compute features cho batch pairs - VECTORIZED.\"\"\"\n",
    "    if not pairs_list:\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    for row in pairs_list:\n",
    "        feats = compute_pairwise_features(row)\n",
    "        results.append(feats)\n",
    "    return results\n",
    "\n",
    "\n",
    "def rank_paper_fast(queries, candidates_list, model, feature_names):\n",
    "    \"\"\"\n",
    "    Ranking t·∫•t c·∫£ queries c·ªßa 1 paper c√πng l√∫c - SUPER OPTIMIZED.\n",
    "    \n",
    "    KEY OPTIMIZATION: \n",
    "    - Pre-compute TF-IDF vectors cho candidates 1 L·∫¶N\n",
    "    - Reuse cho t·∫•t c·∫£ queries\n",
    "    \"\"\"\n",
    "    global TIMING_STATS\n",
    "    \n",
    "    if not candidates_list:\n",
    "        return {}\n",
    "    \n",
    "    predictions = {}\n",
    "    \n",
    "    # Pre-normalize titles 1 l·∫ßn\n",
    "    cand_titles = [normalize_text_basic(c.get('cand_title', '')) for c in candidates_list]\n",
    "    cand_ids = [c['cand_id'] for c in candidates_list]\n",
    "    query_titles = [normalize_text_basic(q.get('bib_title', '')) for q in queries]\n",
    "    \n",
    "    # === PRE-COMPUTE TF-IDF CHO TO√ÄN B·ªò PAPER (1 L·∫¶N DUY NH·∫§T) ===\n",
    "    t_tfidf_start = time.time()\n",
    "    \n",
    "    # G·ªôp t·∫•t c·∫£ unique titles (queries + candidates)\n",
    "    all_titles = list(set(query_titles + cand_titles))\n",
    "    \n",
    "    try:\n",
    "        # Fit vectorizer 1 l·∫ßn duy nh·∫•t cho paper n√†y\n",
    "        vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 4), min_df=1)\n",
    "        vectorizer.fit(all_titles)\n",
    "        \n",
    "        # Transform candidates 1 l·∫ßn\n",
    "        cand_vecs = vectorizer.transform(cand_titles)\n",
    "        \n",
    "        # Transform queries 1 l·∫ßn  \n",
    "        query_vecs = vectorizer.transform(query_titles)\n",
    "        \n",
    "        # Pre-compute t·∫•t c·∫£ TF-IDF scores (queries x candidates)\n",
    "        # Shape: (n_queries, n_candidates)\n",
    "        tfidf_matrix = cosine_similarity(query_vecs, cand_vecs)\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Fallback n·∫øu l·ªói\n",
    "        tfidf_matrix = np.zeros((len(queries), len(candidates_list)))\n",
    "    \n",
    "    TIMING_STATS['tfidf_compute'] += time.time() - t_tfidf_start\n",
    "    \n",
    "    # === PROCESS T·ª™NG QUERY (ƒë√£ c√≥ TF-IDF s·∫µn) ===\n",
    "    for q_idx, query in enumerate(queries):\n",
    "        bib_key = query['key']\n",
    "        \n",
    "        # T·∫°o pairs\n",
    "        pairs = []\n",
    "        for cand in candidates_list:\n",
    "            row = {}\n",
    "            row.update(query)\n",
    "            row.update(cand)\n",
    "            pairs.append(row)\n",
    "        \n",
    "        # --- PHASE 1: Feature Compute (KH√îNG c√≥ TF-IDF) ---\n",
    "        t0 = time.time()\n",
    "        feats_list = batch_compute_features(pairs)\n",
    "        TIMING_STATS['feature_compute'] += time.time() - t0\n",
    "        \n",
    "        # Add TF-IDF t·ª´ pre-computed matrix\n",
    "        for c_idx, feats in enumerate(feats_list):\n",
    "            feats['feat_title_tfidf_cosine'] = tfidf_matrix[q_idx, c_idx]\n",
    "        \n",
    "        # Create DataFrame v√† predict\n",
    "        df_feats = pd.DataFrame(feats_list)\n",
    "        \n",
    "        # Fill missing cols\n",
    "        for col in feature_names:\n",
    "            if col not in df_feats.columns:\n",
    "                df_feats[col] = 0.0\n",
    "        \n",
    "        X_input = df_feats[feature_names].values\n",
    "        \n",
    "        # --- PHASE 2: Model Predict ---\n",
    "        t2 = time.time()\n",
    "        if model:\n",
    "            scores = model.predict_proba(X_input)[:, 1]\n",
    "        else:\n",
    "            scores = np.random.rand(len(pairs))\n",
    "        TIMING_STATS['model_predict'] += time.time() - t2\n",
    "        \n",
    "        # Ranking\n",
    "        ranked_idx = np.argsort(scores)[::-1][:5]\n",
    "        top_5 = [cand_ids[i] for i in ranked_idx]\n",
    "        \n",
    "        predictions[bib_key] = top_5\n",
    "        TIMING_STATS['total_queries'] += 1\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "def print_timing_stats():\n",
    "    \"\"\"In th·ªëng k√™ th·ªùi gian c·ªßa t·ª´ng phase.\"\"\"\n",
    "    total = TIMING_STATS['feature_compute'] + TIMING_STATS['tfidf_compute'] + TIMING_STATS['model_predict']\n",
    "    n_queries = max(TIMING_STATS['total_queries'], 1)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"‚è±Ô∏è  TIMING BREAKDOWN\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"{'Phase':<25} | {'Total (s)':<12} | {'Avg/Query (ms)':<15} | {'%':<8}\")\n",
    "    print(\"-\"*65)\n",
    "    \n",
    "    for phase, label in [\n",
    "        ('feature_compute', 'Feature Extraction'),\n",
    "        ('tfidf_compute', 'TF-IDF Cosine'),\n",
    "        ('model_predict', 'Model Prediction')\n",
    "    ]:\n",
    "        t = TIMING_STATS[phase]\n",
    "        pct = (t / total * 100) if total > 0 else 0\n",
    "        avg_ms = (t / n_queries) * 1000\n",
    "        print(f\"{label:<25} | {t:>10.2f}s | {avg_ms:>12.2f}ms | {pct:>6.1f}%\")\n",
    "    \n",
    "    print(\"-\"*65)\n",
    "    print(f\"{'TOTAL':<25} | {total:>10.2f}s | {(total/n_queries)*1000:>12.2f}ms | 100.0%\")\n",
    "    print(f\"\\nüìä Processed {n_queries} queries\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "\n",
    "def reset_timing_stats():\n",
    "    \"\"\"Reset timing stats.\"\"\"\n",
    "    global TIMING_STATS\n",
    "    TIMING_STATS = {\n",
    "        'feature_compute': 0.0,\n",
    "        'tfidf_compute': 0.0,\n",
    "        'model_predict': 0.0,\n",
    "        'total_queries': 0\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"‚úÖ SUPER Optimized functions ready (TF-IDF pre-computed per paper).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4270031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions for refs.bib + references.json loaded!\n"
     ]
    }
   ],
   "source": [
    "# --- NEW HELPER FUNCTIONS: Load refs.bib + references.json ---\n",
    "import bibtexparser\n",
    "\n",
    "def load_queries_from_bib(bib_path):\n",
    "    \"\"\"\n",
    "    ƒê·ªçc refs.bib v√† tr·∫£ v·ªÅ list queries (TARGET entries c·∫ßn match).\n",
    "    \n",
    "    Returns:\n",
    "        List[dict]: M·ªói query c√≥: key, bib_title, bib_authors, bib_id, bib_year\n",
    "    \"\"\"\n",
    "    if not os.path.exists(bib_path):\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        with open(bib_path, 'r', encoding='utf-8') as f:\n",
    "            bib_content = f.read()\n",
    "        \n",
    "        parser = bibtexparser.bparser.BibTexParser(common_strings=True)\n",
    "        parser.ignore_nonstandard_types = True\n",
    "        bib_db = bibtexparser.loads(bib_content, parser=parser)\n",
    "        \n",
    "        queries = []\n",
    "        for entry in bib_db.entries:\n",
    "            # L·∫•y raw text ƒë·ªÉ parse\n",
    "            raw_text = entry.get('ENTRYTYPE', '') + '{'\n",
    "            for k, v in entry.items():\n",
    "                if k not in ['ENTRYTYPE', 'ID']:\n",
    "                    raw_text += f'{k}={{{v}}}, '\n",
    "            raw_text += '}'\n",
    "            \n",
    "            # Parse b·∫±ng h√†m smart parser\n",
    "            parsed = parse_bibtex_smart(raw_text)\n",
    "            \n",
    "            queries.append({\n",
    "                'key': entry.get('ID', ''),\n",
    "                'bib_title': parsed.get('title', ''),\n",
    "                'bib_authors': parsed.get('authors', []),\n",
    "                'bib_id': parsed.get('extracted_id', ''),\n",
    "                'bib_year': parsed.get('year', '')\n",
    "            })\n",
    "        \n",
    "        return queries\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error parsing {bib_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def load_candidates_from_references(ref_json_path):\n",
    "    \"\"\"\n",
    "    ƒê·ªçc references.json v√† tr·∫£ v·ªÅ list candidates (CANDIDATES pool t·ª´ arXiv API).\n",
    "    \n",
    "    Returns:\n",
    "        List[dict]: M·ªói candidate c√≥: cand_id, cand_title, cand_authors, cand_year\n",
    "    \"\"\"\n",
    "    if not os.path.exists(ref_json_path):\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        ref_data = load_json(ref_json_path)\n",
    "        candidates = []\n",
    "        \n",
    "        for arxiv_id, meta in ref_data.items():\n",
    "            authors = meta.get('authors', [])\n",
    "            date = meta.get('submission_date', '')\n",
    "            year = str(date)[:4] if date and len(str(date)) >= 4 else ''\n",
    "            \n",
    "            candidates.append({\n",
    "                'cand_id': arxiv_id,\n",
    "                'cand_title': meta.get('title', '').lower().strip(),\n",
    "                'cand_authors': [str(a).lower().strip() for a in authors],\n",
    "                'cand_year': year\n",
    "            })\n",
    "        \n",
    "        return candidates\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading {ref_json_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def extract_groundtruth_from_labels(labels_data):\n",
    "    \"\"\"\n",
    "    Tr√≠ch xu·∫•t groundtruth t·ª´ labels.json.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {paper_id: {bib_key: true_arxiv_id}}\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    gt_map = defaultdict(dict)\n",
    "    \n",
    "    for item in labels_data:\n",
    "        paper_id = item.get('source_paper_id')\n",
    "        key = item.get('key')\n",
    "        true_id = item.get('ground_truth', {}).get('id')\n",
    "        \n",
    "        if paper_id and key and true_id:\n",
    "            gt_map[paper_id][key] = true_id\n",
    "    \n",
    "    return dict(gt_map)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helper functions for refs.bib + references.json loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fd07652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded: Pipeline\n",
      "üìù Expected features (6): ['feat_title_tfidf_cosine', 'feat_title_len_diff', 'feat_auth_jaccard', 'feat_year_match', 'feat_id_match', 'feat_first_auth_match']\n",
      "‚úÖ Features kh·ªõp v·ªõi SELECTED_FEATURES\n"
     ]
    }
   ],
   "source": [
    "# --- LOAD MODEL ---\n",
    "try:\n",
    "    model = joblib.load(MODEL_PATH)\n",
    "    feature_names = joblib.load(FEATURE_NAME_PATH)\n",
    "    print(f\"‚úÖ Model loaded: {type(model).__name__}\")\n",
    "    print(f\"üìù Expected features ({len(feature_names)}): {feature_names}\")\n",
    "    \n",
    "    # Verify features match SELECTED_FEATURES\n",
    "    if set(feature_names) == set(SELECTED_FEATURES):\n",
    "        print(\"‚úÖ Features kh·ªõp v·ªõi SELECTED_FEATURES\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è WARNING: Features kh√¥ng kh·ªõp! S·ª≠ d·ª•ng SELECTED_FEATURES\")\n",
    "        feature_names = SELECTED_FEATURES\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå CRITICAL ERROR: Model file not found. Cannot proceed.\")\n",
    "    model = None\n",
    "    feature_names = SELECTED_FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55902cd",
   "metadata": {},
   "source": [
    "\n",
    "## Main Inference Loop\n",
    "V√≤ng l·∫∑p n√†y s·∫Ω:\n",
    "1. Duy·ªát qua t·ª´ng Partition (Train -> Valid -> Test).\n",
    "2. Load `labels.json` c·ªßa partition ƒë√≥.\n",
    "3. Transform sang c·∫•u tr√∫c Paper-based.\n",
    "4. Ch·∫°y Ranking Model.\n",
    "5. L∆∞u file `pred.json` v√†o folder t∆∞∆°ng ·ª©ng v·ªõi Paper ID.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06aa648",
   "metadata": {},
   "source": [
    "## üîç Test & Validation\n",
    "Ki·ªÉm tra logic tr∆∞·ªõc khi ch·∫°y full inference:\n",
    "- Load sample papers t·ª´ labels.json\n",
    "- ƒê·ªçc refs.bib v√† references.json\n",
    "- Hi·ªÉn th·ªã pairs m·∫´u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53c329ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç TESTING DATA LOADING...\n",
      "\n",
      "‚úÖ Loaded 13348 entries from labels.json\n",
      "‚úÖ Extracted 604 unique papers\n",
      "   Sample paper IDs: ['2403-04225', '2403-03951', '2403-01980']\n",
      "\n",
      "üìÑ Testing with paper: 2403-04225\n",
      "   Groundtruth entries: 48\n",
      "   Sample groundtruth: {'ref_0': '2305-16213', 'ref_1': '2209-14988', 'ref_2': '2211-10440'}\n",
      "\n",
      "‚úÖ Loaded 54 queries from refs.bib\n",
      "\n",
      "üìã Sample Query (TARGET entry c·∫ßn match):\n",
      "   Key: ref_0\n",
      "   Title: Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation...\n",
      "   Authors: ['Wang, Zhengyi', 'Lu, Cheng', 'Wang, Yikai']\n",
      "   Year: 2024\n",
      "   ID: \n",
      "\n",
      "‚úÖ Loaded 48 candidates from references.json\n",
      "\n",
      "üéØ Sample Candidate (t·ª´ arXiv API):\n",
      "   arXiv ID: 2308-10490\n",
      "   Title: texture generation on 3d meshes with point-uv diffusion...\n",
      "   Authors: ['xin yu', 'peng dai', 'wenbo li']\n",
      "   Year: \n",
      "\n",
      "============================================================\n",
      "üìä PAIRING STATISTICS\n",
      "============================================================\n",
      "Total queries (refs.bib): 54\n",
      "Total candidates (references.json): 48\n",
      "Total pairs to rank: 54 √ó 48 = 2592\n",
      "\n",
      "Top-5 predictions per query ‚Üí 54 √ó 5 = 270 predictions\n",
      "============================================================\n",
      "\n",
      "üîó Example Pairing:\n",
      "Query Key: ref_0\n",
      "   ‚Üí Groundtruth: 2305-16213\n",
      "   ‚Üí Will rank against 48 candidates\n",
      "   ‚Üí Return top-5 arXiv IDs\n",
      "\n",
      "‚úÖ Test completed! Ready to run full inference.\n"
     ]
    }
   ],
   "source": [
    "# TEST: Load sample data t·ª´ partition ƒë·∫ßu ti√™n\n",
    "print(\"üîç TESTING DATA LOADING...\\n\")\n",
    "\n",
    "test_partition = 'train'\n",
    "label_file = os.path.join(DATASET_ROOT, test_partition, 'labels.json')\n",
    "\n",
    "if os.path.exists(label_file):\n",
    "    # 1. Load labels\n",
    "    labels_data = load_json(label_file)\n",
    "    print(f\"‚úÖ Loaded {len(labels_data)} entries from labels.json\")\n",
    "    \n",
    "    # 2. Extract groundtruth\n",
    "    groundtruth_map = extract_groundtruth_from_labels(labels_data)\n",
    "    paper_ids = list(groundtruth_map.keys())\n",
    "    \n",
    "    print(f\"‚úÖ Extracted {len(paper_ids)} unique papers\")\n",
    "    print(f\"   Sample paper IDs: {paper_ids[:3]}\\n\")\n",
    "    \n",
    "    # 3. Test v·ªõi paper ƒë·∫ßu ti√™n\n",
    "    test_paper_id = paper_ids[0]\n",
    "    print(f\"üìÑ Testing with paper: {test_paper_id}\")\n",
    "    print(f\"   Groundtruth entries: {len(groundtruth_map[test_paper_id])}\")\n",
    "    print(f\"   Sample groundtruth: {dict(list(groundtruth_map[test_paper_id].items())[:3])}\\n\")\n",
    "    \n",
    "    # 4. Load refs.bib\n",
    "    paper_dir = os.path.join(DATA_FOLDER, test_paper_id)\n",
    "    bib_path = os.path.join(paper_dir, 'refs.bib')\n",
    "    \n",
    "    if os.path.exists(bib_path):\n",
    "        queries = load_queries_from_bib(bib_path)\n",
    "        print(f\"‚úÖ Loaded {len(queries)} queries from refs.bib\")\n",
    "        \n",
    "        if queries:\n",
    "            print(f\"\\nüìã Sample Query (TARGET entry c·∫ßn match):\")\n",
    "            sample_q = queries[0]\n",
    "            print(f\"   Key: {sample_q['key']}\")\n",
    "            print(f\"   Title: {sample_q['bib_title'][:100]}...\")\n",
    "            print(f\"   Authors: {sample_q['bib_authors'][:3]}\")\n",
    "            print(f\"   Year: {sample_q['bib_year']}\")\n",
    "            print(f\"   ID: {sample_q['bib_id']}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è refs.bib not found at {bib_path}\")\n",
    "    \n",
    "    # 5. Load references.json\n",
    "    ref_json_path = os.path.join(paper_dir, 'references.json')\n",
    "    \n",
    "    if os.path.exists(ref_json_path):\n",
    "        candidates = load_candidates_from_references(ref_json_path)\n",
    "        print(f\"\\n‚úÖ Loaded {len(candidates)} candidates from references.json\")\n",
    "        \n",
    "        if candidates:\n",
    "            print(f\"\\nüéØ Sample Candidate (t·ª´ arXiv API):\")\n",
    "            sample_c = candidates[0]\n",
    "            print(f\"   arXiv ID: {sample_c['cand_id']}\")\n",
    "            print(f\"   Title: {sample_c['cand_title'][:100]}...\")\n",
    "            print(f\"   Authors: {sample_c['cand_authors'][:3]}\")\n",
    "            print(f\"   Year: {sample_c['cand_year']}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è references.json not found at {ref_json_path}\")\n",
    "    \n",
    "    # 6. Show pairing example\n",
    "    if queries and candidates:\n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(f\"üìä PAIRING STATISTICS\")\n",
    "        print(f\"=\"*60)\n",
    "        print(f\"Total queries (refs.bib): {len(queries)}\")\n",
    "        print(f\"Total candidates (references.json): {len(candidates)}\")\n",
    "        print(f\"Total pairs to rank: {len(queries)} √ó {len(candidates)} = {len(queries) * len(candidates)}\")\n",
    "        print(f\"\\nTop-5 predictions per query ‚Üí {len(queries)} √ó 5 = {len(queries) * 5} predictions\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Show matching example\n",
    "        print(f\"\\nüîó Example Pairing:\")\n",
    "        print(f\"Query Key: {queries[0]['key']}\")\n",
    "        print(f\"   ‚Üí Groundtruth: {groundtruth_map[test_paper_id].get(queries[0]['key'], 'N/A')}\")\n",
    "        print(f\"   ‚Üí Will rank against {len(candidates)} candidates\")\n",
    "        print(f\"   ‚Üí Return top-5 arXiv IDs\")\n",
    "        \n",
    "else:\n",
    "    print(f\"‚ùå Label file not found: {label_file}\")\n",
    "\n",
    "print(\"\\n‚úÖ Test completed! Ready to run full inference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8de2e697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ STARTING NEW INFERENCE PIPELINE (refs.bib + references.json)...\n",
      "\n",
      "üîµ Processing Partition: [TRAIN]\n",
      "   ‚è±Ô∏è Load labels: 0.14s\n",
      "   ‚è±Ô∏è Extract groundtruth: 0.01s\n",
      "   ‚Ü≥ Found 604 papers. Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Ranking train:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 406/604 [49:37<17:10,  5.21s/it]   WARNING: Error in configuration: macro '\\frac' failed its substitution!\n",
      "   Ranking train:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 502/604 [54:44<01:43,  1.02s/it]  WARNING: Error in configuration: macro '\\frac' failed its substitution!\n",
      "   Ranking train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 604/604 [1:08:24<00:00,  6.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Finished train. MRR: 0.9615\n",
      "   ‚è±Ô∏è Partition total: 4104.91s\n",
      "--------------------------------------------------\n",
      "üîµ Processing Partition: [VALIDATION]\n",
      "   ‚è±Ô∏è Load labels: 0.04s\n",
      "   ‚è±Ô∏è Extract groundtruth: 0.01s\n",
      "   ‚Ü≥ Found 2 papers. Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Ranking validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:41<00:00, 20.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Finished validation. MRR: 0.9657\n",
      "   ‚è±Ô∏è Partition total: 41.13s\n",
      "--------------------------------------------------\n",
      "üîµ Processing Partition: [TEST]\n",
      "   ‚è±Ô∏è Load labels: 0.02s\n",
      "   ‚è±Ô∏è Extract groundtruth: 0.00s\n",
      "   ‚Ü≥ Found 2 papers. Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Ranking test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:19<00:00,  9.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Finished test. MRR: 0.9324\n",
      "   ‚è±Ô∏è Partition total: 19.90s\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "‚è±Ô∏è  TIMING BREAKDOWN\n",
      "==================================================\n",
      "Phase                     | Total (s)    | Avg/Query (ms)  | %       \n",
      "-----------------------------------------------------------------\n",
      "Feature Extraction        |    3823.56s |       117.70ms |   98.8%\n",
      "TF-IDF Cosine             |      15.27s |         0.47ms |    0.4%\n",
      "Model Prediction          |      31.21s |         0.96ms |    0.8%\n",
      "-----------------------------------------------------------------\n",
      "TOTAL                     |    3870.04s |       119.13ms | 100.0%\n",
      "\n",
      "üìä Processed 32485 queries\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Bi·∫øn ƒë·ªÉ th·ªëng k√™ t·ªïng k·∫øt\n",
    "summary_stats = {\n",
    "    'train': {'papers': 0, 'mrr': 0, 'queries': 0},\n",
    "    'validation': {'papers': 0, 'mrr': 0, 'queries': 0},\n",
    "    'test': {'papers': 0, 'mrr': 0, 'queries': 0}\n",
    "}\n",
    "\n",
    "# Reset timing tr∆∞·ªõc khi ch·∫°y\n",
    "reset_timing_stats()\n",
    "\n",
    "print(\"üöÄ STARTING NEW INFERENCE PIPELINE (refs.bib + references.json)...\\n\")\n",
    "\n",
    "for partition in PARTITIONS:\n",
    "    print(f\"üîµ Processing Partition: [{partition.upper()}]\")\n",
    "    partition_start = time.time()\n",
    "    \n",
    "    # 1. Load labels.json ƒë·ªÉ l·∫•y groundtruth v√† paper_ids\n",
    "    t_load = time.time()\n",
    "    label_file = os.path.join(DATASET_ROOT, partition, 'labels.json')\n",
    "    if not os.path.exists(label_file):\n",
    "        print(f\"   ‚ö†Ô∏è Warning: File {label_file} not found. Skipping.\")\n",
    "        continue\n",
    "        \n",
    "    labels_data = load_json(label_file)\n",
    "    if not labels_data:\n",
    "        print(\"   ‚ö†Ô∏è Empty labels. Skipping.\")\n",
    "        continue\n",
    "    print(f\"   ‚è±Ô∏è Load labels: {time.time() - t_load:.2f}s\")\n",
    "\n",
    "    # 2. Extract groundtruth v√† paper_ids\n",
    "    t_transform = time.time()\n",
    "    groundtruth_map = extract_groundtruth_from_labels(labels_data)\n",
    "    paper_ids = list(groundtruth_map.keys())\n",
    "    \n",
    "    summary_stats[partition]['papers'] = len(paper_ids)\n",
    "    print(f\"   ‚è±Ô∏è Extract groundtruth: {time.time() - t_transform:.2f}s\")\n",
    "    print(f\"   ‚Ü≥ Found {len(paper_ids)} papers. Processing...\")\n",
    "    \n",
    "    # 3. NEW: Loop qua t·ª´ng paper v√† load refs.bib + references.json\n",
    "    part_mrr_sum = 0\n",
    "    part_query_count = 0\n",
    "    \n",
    "    for paper_id in tqdm(paper_ids, desc=f\"   Ranking {partition}\"):\n",
    "        # X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n paper folder trong data_output_v2\n",
    "        paper_dir = os.path.join(DATA_FOLDER, paper_id)\n",
    "        \n",
    "        if not os.path.exists(paper_dir):\n",
    "            print(f\"   ‚ö†Ô∏è Paper folder not found: {paper_id}\")\n",
    "            continue\n",
    "        \n",
    "        # 4. Load QUERIES t·ª´ refs.bib (TARGET entries)\n",
    "        bib_path = os.path.join(paper_dir, 'refs.bib')\n",
    "        queries = load_queries_from_bib(bib_path)\n",
    "        \n",
    "        if not queries:\n",
    "            print(f\"   ‚ö†Ô∏è No queries in {paper_id}/refs.bib\")\n",
    "            continue\n",
    "        \n",
    "        # 5. Load CANDIDATES t·ª´ references.json (Ground truth pool)\n",
    "        ref_json_path = os.path.join(paper_dir, 'references.json')\n",
    "        candidates = load_candidates_from_references(ref_json_path)\n",
    "        \n",
    "        if not candidates:\n",
    "            print(f\"   ‚ö†Ô∏è No candidates in {paper_id}/references.json\")\n",
    "            continue\n",
    "        \n",
    "        # 6. Run ranking\n",
    "        predictions = rank_paper_fast(queries, candidates, model, feature_names)\n",
    "        \n",
    "        # 7. Build pred.json output\n",
    "        groundtruth = groundtruth_map.get(paper_id, {})\n",
    "        \n",
    "        pred_output = {\n",
    "            \"partition\": partition,\n",
    "            \"groundtruth\": groundtruth,\n",
    "            \"prediction\": predictions\n",
    "        }\n",
    "        \n",
    "        # 8. Calculate MRR\n",
    "        for query in queries:\n",
    "            bib_key = query['key']\n",
    "            true_id = groundtruth.get(bib_key)\n",
    "            \n",
    "            if not true_id:\n",
    "                continue\n",
    "            \n",
    "            top_5 = predictions.get(bib_key, [])\n",
    "            \n",
    "            if true_id in top_5:\n",
    "                rank = top_5.index(true_id) + 1\n",
    "                part_mrr_sum += 1.0 / rank\n",
    "            \n",
    "            part_query_count += 1\n",
    "        \n",
    "        # 9. Save pred.json\n",
    "        safe_paper_id = str(paper_id).strip()\n",
    "        output_dir = os.path.join(SUBMISSION_DIR, safe_paper_id)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        save_path = os.path.join(output_dir, 'pred.json')\n",
    "        save_json(pred_output, save_path)\n",
    "    \n",
    "    # Update Stats\n",
    "    if part_query_count > 0:\n",
    "        summary_stats[partition]['mrr'] = part_mrr_sum / part_query_count\n",
    "        summary_stats[partition]['queries'] = part_query_count\n",
    "    \n",
    "    print(f\"   ‚úÖ Finished {partition}. MRR: {summary_stats[partition]['mrr']:.4f}\")\n",
    "    print(f\"   ‚è±Ô∏è Partition total: {time.time() - partition_start:.2f}s\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# In timing breakdown sau khi ch·∫°y xong\n",
    "print_timing_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "961486f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "üìä SUBMISSION GENERATION REPORT\n",
      "========================================\n",
      "Partition       | Papers     | Queries    | MRR       \n",
      "--------------------------------------------------\n",
      "TRAIN           | 604        | 13339      | 0.9615\n",
      "VALIDATION      | 2          | 107        | 0.9657\n",
      "TEST            | 2          | 85         | 0.9324\n",
      "========================================\n",
      "\n",
      "üìÅ All pred.json files are ready at: d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\23127011\\notebooks\\submission_final\\23127011\n"
     ]
    }
   ],
   "source": [
    "# --- FINAL REPORT ---\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"üìä SUBMISSION GENERATION REPORT\")\n",
    "print(\"=\"*40)\n",
    "print(f\"{'Partition':<15} | {'Papers':<10} | {'Queries':<10} | {'MRR':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for part in PARTITIONS:\n",
    "    stats = summary_stats[part]\n",
    "    print(f\"{part.upper():<15} | {stats['papers']:<10} | {stats['queries']:<10} | {stats['mrr']:.4f}\")\n",
    "\n",
    "print(\"=\"*40)\n",
    "print(f\"\\nüìÅ All pred.json files are ready at: {os.path.abspath(SUBMISSION_DIR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ee292d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ STARTING NEW INFERENCE PIPELINE (refs.bib + references.json)...\n",
      "\n",
      "üîµ Processing Partition: [VALIDATION]\n",
      "   ‚è±Ô∏è Load labels: 0.00s\n",
      "   ‚è±Ô∏è Extract groundtruth: 0.00s\n",
      "   ‚Ü≥ Found 2 papers. Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Ranking validation:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:10<00:10, 10.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip at partition  validation ref_6\n",
      "skip at partition  validation ref_8\n",
      "skip at partition  validation ref_13\n",
      "skip at partition  validation ref_14\n",
      "skip at partition  validation ref_17\n",
      "skip at partition  validation ref_22\n",
      "skip at partition  validation ref_33\n",
      "skip at partition  validation ref_35\n",
      "skip at partition  validation ref_42\n",
      "skip at partition  validation ref_48\n",
      "skip at partition  validation ref_49\n",
      "skip at partition  validation ref_50\n",
      "skip at partition  validation ref_51\n",
      "skip at partition  validation ref_52\n",
      "skip at partition  validation ref_53\n",
      "skip at partition  validation ref_60\n",
      "skip at partition  validation ref_61\n",
      "skip at partition  validation ref_62\n",
      "skip at partition  validation ref_63\n",
      "skip at partition  validation ref_75\n",
      "skip at partition  validation ref_88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Ranking validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:15<00:00,  7.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip at partition  validation ref_0\n",
      "skip at partition  validation ref_1\n",
      "skip at partition  validation ref_2\n",
      "skip at partition  validation ref_3\n",
      "skip at partition  validation ref_4\n",
      "skip at partition  validation ref_5\n",
      "skip at partition  validation ref_7\n",
      "skip at partition  validation ref_8\n",
      "skip at partition  validation ref_9\n",
      "skip at partition  validation ref_11\n",
      "skip at partition  validation ref_12\n",
      "skip at partition  validation ref_13\n",
      "skip at partition  validation ref_14\n",
      "skip at partition  validation ref_15\n",
      "skip at partition  validation ref_16\n",
      "skip at partition  validation ref_17\n",
      "skip at partition  validation ref_18\n",
      "skip at partition  validation ref_21\n",
      "skip at partition  validation ref_22\n",
      "skip at partition  validation ref_23\n",
      "skip at partition  validation ref_24\n",
      "skip at partition  validation ref_31\n",
      "skip at partition  validation ref_34\n",
      "skip at partition  validation ref_36\n",
      "skip at partition  validation ref_43\n",
      "skip at partition  validation ref_46\n",
      "skip at partition  validation ref_50\n",
      "skip at partition  validation ref_51\n",
      "skip at partition  validation ref_52\n",
      "skip at partition  validation ref_53\n",
      "skip at partition  validation ref_54\n",
      "skip at partition  validation ref_61\n",
      "skip at partition  validation ref_62\n",
      "skip at partition  validation ref_63\n",
      "skip at partition  validation ref_68\n",
      "skip at partition  validation ref_69\n",
      "skip at partition  validation ref_71\n",
      "skip at partition  validation ref_72\n",
      "skip at partition  validation ref_73\n",
      "skip at partition  validation ref_74\n",
      "   ‚úÖ Finished validation. MRR: 0.9657\n",
      "   ‚è±Ô∏è Partition total: 15.39s\n",
      "--------------------------------------------------\n",
      "üîµ Processing Partition: [TEST]\n",
      "   ‚è±Ô∏è Load labels: 0.00s\n",
      "   ‚è±Ô∏è Extract groundtruth: 0.00s\n",
      "   ‚Ü≥ Found 2 papers. Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Ranking test:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:01<00:01,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip at partition  test ref_9\n",
      "skip at partition  test ref_10\n",
      "skip at partition  test ref_11\n",
      "skip at partition  test ref_13\n",
      "skip at partition  test ref_14\n",
      "skip at partition  test ref_15\n",
      "skip at partition  test ref_17\n",
      "skip at partition  test ref_26\n",
      "skip at partition  test ref_29\n",
      "skip at partition  test ref_30\n",
      "skip at partition  test ref_32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Ranking test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip at partition  test ref_27\n",
      "skip at partition  test ref_36\n",
      "skip at partition  test ref_45\n",
      "skip at partition  test ref_65\n",
      "skip at partition  test ref_67\n",
      "   ‚úÖ Finished test. MRR: 0.9324\n",
      "   ‚è±Ô∏è Partition total: 6.41s\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "‚è±Ô∏è  TIMING BREAKDOWN\n",
      "==================================================\n",
      "Phase                     | Total (s)    | Avg/Query (ms)  | %       \n",
      "-----------------------------------------------------------------\n",
      "Feature Extraction        |      20.67s |        76.86ms |   99.1%\n",
      "TF-IDF Cosine             |       0.06s |         0.24ms |    0.3%\n",
      "Model Prediction          |       0.13s |         0.48ms |    0.6%\n",
      "-----------------------------------------------------------------\n",
      "TOTAL                     |      20.87s |        77.58ms | 100.0%\n",
      "\n",
      "üìä Processed 269 queries\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Bi·∫øn ƒë·ªÉ th·ªëng k√™ t·ªïng k·∫øt\n",
    "PARTITIONS = ['validation', 'test']\n",
    "summary_stats = {\n",
    "    'validation': {'papers': 0, 'mrr': 0, 'queries': 0},\n",
    "    'test': {'papers': 0, 'mrr': 0, 'queries': 0}\n",
    "}\n",
    "\n",
    "# Reset timing tr∆∞·ªõc khi ch·∫°y\n",
    "reset_timing_stats()\n",
    "\n",
    "print(\"üöÄ STARTING NEW INFERENCE PIPELINE (refs.bib + references.json)...\\n\")\n",
    "\n",
    "for partition in PARTITIONS:\n",
    "    print(f\"üîµ Processing Partition: [{partition.upper()}]\")\n",
    "    partition_start = time.time()\n",
    "    \n",
    "    # 1. Load labels.json ƒë·ªÉ l·∫•y groundtruth v√† paper_ids\n",
    "    t_load = time.time()\n",
    "    label_file = os.path.join(DATASET_ROOT, partition, 'labels.json')\n",
    "    if not os.path.exists(label_file):\n",
    "        print(f\"   ‚ö†Ô∏è Warning: File {label_file} not found. Skipping.\")\n",
    "        continue\n",
    "        \n",
    "    labels_data = load_json(label_file)\n",
    "    if not labels_data:\n",
    "        print(\"   ‚ö†Ô∏è Empty labels. Skipping.\")\n",
    "        continue\n",
    "    print(f\"   ‚è±Ô∏è Load labels: {time.time() - t_load:.2f}s\")\n",
    "\n",
    "    # 2. Extract groundtruth v√† paper_ids\n",
    "    t_transform = time.time()\n",
    "    groundtruth_map = extract_groundtruth_from_labels(labels_data)\n",
    "    paper_ids = list(groundtruth_map.keys())\n",
    "    \n",
    "    summary_stats[partition]['papers'] = len(paper_ids)\n",
    "    print(f\"   ‚è±Ô∏è Extract groundtruth: {time.time() - t_transform:.2f}s\")\n",
    "    print(f\"   ‚Ü≥ Found {len(paper_ids)} papers. Processing...\")\n",
    "    \n",
    "    # 3. NEW: Loop qua t·ª´ng paper v√† load refs.bib + references.json\n",
    "    part_mrr_sum = 0\n",
    "    part_query_count = 0\n",
    "    \n",
    "    for paper_id in tqdm(paper_ids, desc=f\"   Ranking {partition}\"):\n",
    "        # X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n paper folder trong data_output_v2\n",
    "        paper_dir = os.path.join(DATA_FOLDER, paper_id)\n",
    "        \n",
    "        if not os.path.exists(paper_dir):\n",
    "            print(f\"   ‚ö†Ô∏è Paper folder not found: {paper_id}\")\n",
    "            continue\n",
    "        \n",
    "        # 4. Load QUERIES t·ª´ refs.bib (TARGET entries)\n",
    "        bib_path = os.path.join(paper_dir, 'refs.bib')\n",
    "        queries = load_queries_from_bib(bib_path)\n",
    "        \n",
    "        if not queries:\n",
    "            print(f\"   ‚ö†Ô∏è No queries in {paper_id}/refs.bib\")\n",
    "            continue\n",
    "        \n",
    "        # 5. Load CANDIDATES t·ª´ references.json (Ground truth pool)\n",
    "        ref_json_path = os.path.join(paper_dir, 'references.json')\n",
    "        candidates = load_candidates_from_references(ref_json_path)\n",
    "        \n",
    "        if not candidates:\n",
    "            print(f\"   ‚ö†Ô∏è No candidates in {paper_id}/references.json\")\n",
    "            continue\n",
    "        \n",
    "        # 6. Run ranking\n",
    "        predictions = rank_paper_fast(queries, candidates, model, feature_names)\n",
    "        \n",
    "        # 7. Build pred.json output\n",
    "        groundtruth = groundtruth_map.get(paper_id, {})\n",
    "        \n",
    "        pred_output = {\n",
    "            \"partition\": partition,\n",
    "            \"groundtruth\": groundtruth,\n",
    "            \"prediction\": predictions\n",
    "        }\n",
    "        \n",
    "        # 8. Calculate MRR\n",
    "        for query in queries:\n",
    "            bib_key = query['key']\n",
    "            true_id = groundtruth.get(bib_key)\n",
    "            \n",
    "            if not true_id:\n",
    "                print(\"skip at partition \", partition, bib_key)\n",
    "                continue\n",
    "            \n",
    "            top_5 = predictions.get(bib_key, [])\n",
    "            \n",
    "            if true_id in top_5:\n",
    "                rank = top_5.index(true_id) + 1\n",
    "                part_mrr_sum += 1.0 / rank\n",
    "            \n",
    "            part_query_count += 1\n",
    "        \n",
    "        # 9. Save pred.json\n",
    "        safe_paper_id = str(paper_id).strip()\n",
    "        output_dir = os.path.join(SUBMISSION_DIR, safe_paper_id)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        save_path = os.path.join(output_dir, 'pred.json')\n",
    "        save_json(pred_output, save_path)\n",
    "    \n",
    "    # Update Stats\n",
    "    if part_query_count > 0:\n",
    "        summary_stats[partition]['mrr'] = part_mrr_sum / part_query_count\n",
    "        summary_stats[partition]['queries'] = part_query_count\n",
    "    \n",
    "    print(f\"   ‚úÖ Finished {partition}. MRR: {summary_stats[partition]['mrr']:.4f}\")\n",
    "    print(f\"   ‚è±Ô∏è Partition total: {time.time() - partition_start:.2f}s\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# In timing breakdown sau khi ch·∫°y xong\n",
    "print_timing_stats()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
