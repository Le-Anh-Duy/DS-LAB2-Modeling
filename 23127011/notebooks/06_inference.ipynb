{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa83b48",
   "metadata": {},
   "source": [
    "# Notebook 06: Full Inference for Submission\n",
    "**M·ª•c ti√™u:**\n",
    "1. Ch·∫°y pipeline d·ª± ƒëo√°n tr√™n **T·∫§T C·∫¢** c√°c t·∫≠p d·ªØ li·ªáu: Train, Validation, v√† Test.\n",
    "2. Sinh ra file `pred.json` cho t·ª´ng b√†i b√°o.\n",
    "3. S·∫Øp x·∫øp k·∫øt qu·∫£ v√†o ƒë√∫ng c·∫•u tr√∫c th∆∞ m·ª•c ƒë·ªÉ n·ªôp b√†i (Submission Ready).\n",
    "\n",
    "**Model:** Decision Tree/XGBoost v·ªõi **6 selected features**\n",
    "\n",
    "**C·∫•u tr√∫c Output:**\n",
    "```bash\n",
    "submission_final/\n",
    "‚îú‚îÄ‚îÄ <Student_ID>/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ <paper_id_1>/\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pred.json\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ <paper_id_2>/\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pred.json\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cb4b4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries v√† src.ml modules imported successfully!\n",
      "üìù S·ª≠ d·ª•ng 6 selected features\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup path ƒë·ªÉ import src module\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Import t·ª´ src.ml module (S·ª≠ d·ª•ng l·∫°i c√°c h√†m ƒë√£ refactor)\n",
    "from src.ml import (\n",
    "    load_json,\n",
    "    save_json,\n",
    "    normalize_text_basic,\n",
    "    compute_pairwise_features,\n",
    "    compute_tfidf_cosine_single,\n",
    "    parse_bibtex_smart,\n",
    "    transform_to_paper_based\n",
    ")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"‚úÖ Libraries v√† src.ml modules imported successfully!\")\n",
    "\n",
    "# --- DANH S√ÅCH 6 FEATURES ƒê√É CH·ªåN (ph·∫£i kh·ªõp v·ªõi Notebook 03/04) ---\n",
    "SELECTED_FEATURES = [\n",
    "    'feat_title_tfidf_cosine',\n",
    "    'feat_title_len_diff',\n",
    "    'feat_auth_jaccard',\n",
    "    'feat_year_match',\n",
    "    'feat_id_match',\n",
    "    'feat_first_auth_match',\n",
    "]\n",
    "print(f\"üìù S·ª≠ d·ª•ng {len(SELECTED_FEATURES)} selected features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00f10b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Saving to existing directory: d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\23127011\\notebooks\\submission_final\\23127011\n"
     ]
    }
   ],
   "source": [
    "# --- C·∫§U H√åNH QUAN TR·ªåNG ---\n",
    "\n",
    "# 1. Th√¥ng tin sinh vi√™n (ƒê·ªÉ t·∫°o folder n·ªôp b√†i)\n",
    "STUDENT_ID = \"23127011\" \n",
    "\n",
    "# 2. ƒê∆∞·ªùng d·∫´n d·ªØ li·ªáu\n",
    "DATASET_ROOT = '../../dataset_final'\n",
    "PARTITIONS = ['train', 'validation', 'test']\n",
    "\n",
    "# 3. ƒê∆∞·ªùng d·∫´n Model\n",
    "MODEL_PATH = '../../dataset_final/models/best_matcher.pkl'\n",
    "FEATURE_NAME_PATH = '../../dataset_final/models/feature_names.pkl'\n",
    "\n",
    "# 4. Th∆∞ m·ª•c Output\n",
    "SUBMISSION_DIR = f'submission_final/{STUDENT_ID}'\n",
    "\n",
    "if not os.path.exists(SUBMISSION_DIR):\n",
    "    os.makedirs(SUBMISSION_DIR)\n",
    "    print(f\"üìÅ Created submission directory: {os.path.abspath(SUBMISSION_DIR)}\")\n",
    "else:\n",
    "    print(f\"üìÅ Saving to existing directory: {os.path.abspath(SUBMISSION_DIR)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83de33a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SUPER Optimized functions ready (TF-IDF pre-computed per paper).\n"
     ]
    }
   ],
   "source": [
    "# --- OPTIMIZED HELPER FUNCTIONS WITH TIMING ---\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "\n",
    "# Global timing stats\n",
    "TIMING_STATS = {\n",
    "    'feature_compute': 0.0,\n",
    "    'tfidf_compute': 0.0,\n",
    "    'model_predict': 0.0,\n",
    "    'total_queries': 0\n",
    "}\n",
    "\n",
    "def batch_compute_features(pairs_list):\n",
    "    \"\"\"Compute features cho batch pairs - VECTORIZED.\"\"\"\n",
    "    if not pairs_list:\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    for row in pairs_list:\n",
    "        feats = compute_pairwise_features(row)\n",
    "        results.append(feats)\n",
    "    return results\n",
    "\n",
    "\n",
    "def rank_paper_fast(queries, candidates_list, model, feature_names):\n",
    "    \"\"\"\n",
    "    Ranking t·∫•t c·∫£ queries c·ªßa 1 paper c√πng l√∫c - SUPER OPTIMIZED.\n",
    "    \n",
    "    KEY OPTIMIZATION: \n",
    "    - Pre-compute TF-IDF vectors cho candidates 1 L·∫¶N\n",
    "    - Reuse cho t·∫•t c·∫£ queries\n",
    "    \"\"\"\n",
    "    global TIMING_STATS\n",
    "    \n",
    "    if not candidates_list:\n",
    "        return {}\n",
    "    \n",
    "    predictions = {}\n",
    "    \n",
    "    # Pre-normalize titles 1 l·∫ßn\n",
    "    cand_titles = [normalize_text_basic(c.get('cand_title', '')) for c in candidates_list]\n",
    "    cand_ids = [c['cand_id'] for c in candidates_list]\n",
    "    query_titles = [normalize_text_basic(q.get('bib_title', '')) for q in queries]\n",
    "    \n",
    "    # === PRE-COMPUTE TF-IDF CHO TO√ÄN B·ªò PAPER (1 L·∫¶N DUY NH·∫§T) ===\n",
    "    t_tfidf_start = time.time()\n",
    "    \n",
    "    # G·ªôp t·∫•t c·∫£ unique titles (queries + candidates)\n",
    "    all_titles = list(set(query_titles + cand_titles))\n",
    "    \n",
    "    try:\n",
    "        # Fit vectorizer 1 l·∫ßn duy nh·∫•t cho paper n√†y\n",
    "        vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 4), min_df=1)\n",
    "        vectorizer.fit(all_titles)\n",
    "        \n",
    "        # Transform candidates 1 l·∫ßn\n",
    "        cand_vecs = vectorizer.transform(cand_titles)\n",
    "        \n",
    "        # Transform queries 1 l·∫ßn  \n",
    "        query_vecs = vectorizer.transform(query_titles)\n",
    "        \n",
    "        # Pre-compute t·∫•t c·∫£ TF-IDF scores (queries x candidates)\n",
    "        # Shape: (n_queries, n_candidates)\n",
    "        tfidf_matrix = cosine_similarity(query_vecs, cand_vecs)\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Fallback n·∫øu l·ªói\n",
    "        tfidf_matrix = np.zeros((len(queries), len(candidates_list)))\n",
    "    \n",
    "    TIMING_STATS['tfidf_compute'] += time.time() - t_tfidf_start\n",
    "    \n",
    "    # === PROCESS T·ª™NG QUERY (ƒë√£ c√≥ TF-IDF s·∫µn) ===\n",
    "    for q_idx, query in enumerate(queries):\n",
    "        bib_key = query['key']\n",
    "        \n",
    "        # T·∫°o pairs\n",
    "        pairs = []\n",
    "        for cand in candidates_list:\n",
    "            row = {}\n",
    "            row.update(query)\n",
    "            row.update(cand)\n",
    "            pairs.append(row)\n",
    "        \n",
    "        # --- PHASE 1: Feature Compute (KH√îNG c√≥ TF-IDF) ---\n",
    "        t0 = time.time()\n",
    "        feats_list = batch_compute_features(pairs)\n",
    "        TIMING_STATS['feature_compute'] += time.time() - t0\n",
    "        \n",
    "        # Add TF-IDF t·ª´ pre-computed matrix\n",
    "        for c_idx, feats in enumerate(feats_list):\n",
    "            feats['feat_title_tfidf_cosine'] = tfidf_matrix[q_idx, c_idx]\n",
    "        \n",
    "        # Create DataFrame v√† predict\n",
    "        df_feats = pd.DataFrame(feats_list)\n",
    "        \n",
    "        # Fill missing cols\n",
    "        for col in feature_names:\n",
    "            if col not in df_feats.columns:\n",
    "                df_feats[col] = 0.0\n",
    "        \n",
    "        X_input = df_feats[feature_names].values\n",
    "        \n",
    "        # --- PHASE 2: Model Predict ---\n",
    "        t2 = time.time()\n",
    "        if model:\n",
    "            scores = model.predict_proba(X_input)[:, 1]\n",
    "        else:\n",
    "            scores = np.random.rand(len(pairs))\n",
    "        TIMING_STATS['model_predict'] += time.time() - t2\n",
    "        \n",
    "        # Ranking\n",
    "        ranked_idx = np.argsort(scores)[::-1][:5]\n",
    "        top_5 = [cand_ids[i] for i in ranked_idx]\n",
    "        \n",
    "        predictions[bib_key] = top_5\n",
    "        TIMING_STATS['total_queries'] += 1\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "def print_timing_stats():\n",
    "    \"\"\"In th·ªëng k√™ th·ªùi gian c·ªßa t·ª´ng phase.\"\"\"\n",
    "    total = TIMING_STATS['feature_compute'] + TIMING_STATS['tfidf_compute'] + TIMING_STATS['model_predict']\n",
    "    n_queries = max(TIMING_STATS['total_queries'], 1)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"‚è±Ô∏è  TIMING BREAKDOWN\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"{'Phase':<25} | {'Total (s)':<12} | {'Avg/Query (ms)':<15} | {'%':<8}\")\n",
    "    print(\"-\"*65)\n",
    "    \n",
    "    for phase, label in [\n",
    "        ('feature_compute', 'Feature Extraction'),\n",
    "        ('tfidf_compute', 'TF-IDF Cosine'),\n",
    "        ('model_predict', 'Model Prediction')\n",
    "    ]:\n",
    "        t = TIMING_STATS[phase]\n",
    "        pct = (t / total * 100) if total > 0 else 0\n",
    "        avg_ms = (t / n_queries) * 1000\n",
    "        print(f\"{label:<25} | {t:>10.2f}s | {avg_ms:>12.2f}ms | {pct:>6.1f}%\")\n",
    "    \n",
    "    print(\"-\"*65)\n",
    "    print(f\"{'TOTAL':<25} | {total:>10.2f}s | {(total/n_queries)*1000:>12.2f}ms | 100.0%\")\n",
    "    print(f\"\\nüìä Processed {n_queries} queries\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "\n",
    "def reset_timing_stats():\n",
    "    \"\"\"Reset timing stats.\"\"\"\n",
    "    global TIMING_STATS\n",
    "    TIMING_STATS = {\n",
    "        'feature_compute': 0.0,\n",
    "        'tfidf_compute': 0.0,\n",
    "        'model_predict': 0.0,\n",
    "        'total_queries': 0\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"‚úÖ SUPER Optimized functions ready (TF-IDF pre-computed per paper).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fd07652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded: Pipeline\n",
      "üìù Expected features (6): ['feat_title_tfidf_cosine', 'feat_title_len_diff', 'feat_auth_jaccard', 'feat_year_match', 'feat_id_match', 'feat_first_auth_match']\n",
      "‚úÖ Features kh·ªõp v·ªõi SELECTED_FEATURES\n"
     ]
    }
   ],
   "source": [
    "# --- LOAD MODEL ---\n",
    "try:\n",
    "    model = joblib.load(MODEL_PATH)\n",
    "    feature_names = joblib.load(FEATURE_NAME_PATH)\n",
    "    print(f\"‚úÖ Model loaded: {type(model).__name__}\")\n",
    "    print(f\"üìù Expected features ({len(feature_names)}): {feature_names}\")\n",
    "    \n",
    "    # Verify features match SELECTED_FEATURES\n",
    "    if set(feature_names) == set(SELECTED_FEATURES):\n",
    "        print(\"‚úÖ Features kh·ªõp v·ªõi SELECTED_FEATURES\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è WARNING: Features kh√¥ng kh·ªõp! S·ª≠ d·ª•ng SELECTED_FEATURES\")\n",
    "        feature_names = SELECTED_FEATURES\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå CRITICAL ERROR: Model file not found. Cannot proceed.\")\n",
    "    model = None\n",
    "    feature_names = SELECTED_FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55902cd",
   "metadata": {},
   "source": [
    "\n",
    "## Main Inference Loop\n",
    "V√≤ng l·∫∑p n√†y s·∫Ω:\n",
    "1. Duy·ªát qua t·ª´ng Partition (Train -> Valid -> Test).\n",
    "2. Load `labels.json` c·ªßa partition ƒë√≥.\n",
    "3. Transform sang c·∫•u tr√∫c Paper-based.\n",
    "4. Ch·∫°y Ranking Model.\n",
    "5. L∆∞u file `pred.json` v√†o folder t∆∞∆°ng ·ª©ng v·ªõi Paper ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8de2e697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ STARTING OPTIMIZED INFERENCE PIPELINE...\n",
      "\n",
      "üîµ Processing Partition: [TRAIN]\n",
      "   ‚è±Ô∏è Load data: 0.25s\n",
      "   ‚è±Ô∏è Transform data: 290.45s\n",
      "   ‚Ü≥ Found 605 papers. Ranking now...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Ranking train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 605/605 [39:16<00:00,  3.90s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Finished train. MRR: 0.9973\n",
      "   ‚è±Ô∏è Partition total: 2647.71s\n",
      "--------------------------------------------------\n",
      "üîµ Processing Partition: [VALIDATION]\n",
      "   ‚è±Ô∏è Load data: 0.02s\n",
      "   ‚è±Ô∏è Transform data: 1.12s\n",
      "   ‚Ü≥ Found 1 papers. Ranking now...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Ranking validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Finished validation. MRR: 1.0000\n",
      "   ‚è±Ô∏è Partition total: 8.33s\n",
      "--------------------------------------------------\n",
      "üîµ Processing Partition: [TEST]\n",
      "   ‚è±Ô∏è Load data: 0.00s\n",
      "   ‚è±Ô∏è Transform data: 0.14s\n",
      "   ‚Ü≥ Found 1 papers. Ranking now...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Ranking test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Finished test. MRR: 1.0000\n",
      "   ‚è±Ô∏è Partition total: 0.85s\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "‚è±Ô∏è  TIMING BREAKDOWN\n",
      "==================================================\n",
      "Phase                     | Total (s)    | Avg/Query (ms)  | %       \n",
      "-----------------------------------------------------------------\n",
      "Feature Extraction        |    2310.78s |       171.91ms |   99.0%\n",
      "TF-IDF Cosine             |      10.90s |         0.81ms |    0.5%\n",
      "Model Prediction          |      12.61s |         0.94ms |    0.5%\n",
      "-----------------------------------------------------------------\n",
      "TOTAL                     |    2334.29s |       173.66ms | 100.0%\n",
      "\n",
      "üìä Processed 13442 queries\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Bi·∫øn ƒë·ªÉ th·ªëng k√™ t·ªïng k·∫øt\n",
    "summary_stats = {\n",
    "    'train': {'papers': 0, 'mrr': 0, 'queries': 0},\n",
    "    'validation': {'papers': 0, 'mrr': 0, 'queries': 0},\n",
    "    'test': {'papers': 0, 'mrr': 0, 'queries': 0}\n",
    "}\n",
    "\n",
    "# Reset timing tr∆∞·ªõc khi ch·∫°y\n",
    "reset_timing_stats()\n",
    "\n",
    "print(\"üöÄ STARTING OPTIMIZED INFERENCE PIPELINE...\\n\")\n",
    "\n",
    "for partition in PARTITIONS:\n",
    "    print(f\"üîµ Processing Partition: [{partition.upper()}]\")\n",
    "    partition_start = time.time()\n",
    "    \n",
    "    # 1. Load Data\n",
    "    t_load = time.time()\n",
    "    label_file = os.path.join(DATASET_ROOT, partition, 'labels.json')\n",
    "    if not os.path.exists(label_file):\n",
    "        print(f\"   ‚ö†Ô∏è Warning: File {label_file} not found. Skipping.\")\n",
    "        continue\n",
    "        \n",
    "    raw_data = load_json(label_file)\n",
    "    if not raw_data:\n",
    "        print(\"   ‚ö†Ô∏è Empty data. Skipping.\")\n",
    "        continue\n",
    "    print(f\"   ‚è±Ô∏è Load data: {time.time() - t_load:.2f}s\")\n",
    "\n",
    "    # 2. Transform Data\n",
    "    t_transform = time.time()\n",
    "    papers_db = transform_to_paper_based(raw_data, parse_bibtex_smart)\n",
    "    summary_stats[partition]['papers'] = len(papers_db)\n",
    "    print(f\"   ‚è±Ô∏è Transform data: {time.time() - t_transform:.2f}s\")\n",
    "    \n",
    "    print(f\"   ‚Ü≥ Found {len(papers_db)} papers. Ranking now...\")\n",
    "    \n",
    "    # 3. OPTIMIZED Ranking Loop\n",
    "    part_mrr_sum = 0\n",
    "    part_query_count = 0\n",
    "    \n",
    "    for paper_id, data in tqdm(papers_db.items(), desc=f\"   Ranking {partition}\"):\n",
    "        queries = data['queries']\n",
    "        candidates_dict = data['candidates']\n",
    "        candidates_list = list(candidates_dict.values())\n",
    "        \n",
    "        if not candidates_list or not queries:\n",
    "            continue\n",
    "        \n",
    "        # --- FAST RANKING ---\n",
    "        predictions = rank_paper_fast(queries, candidates_list, model, feature_names)\n",
    "        \n",
    "        # Build Output\n",
    "        pred_output = {\n",
    "            \"partition\": partition,\n",
    "            \"groundtruth\": {q['key']: q['true_id'] for q in queries},\n",
    "            \"prediction\": predictions\n",
    "        }\n",
    "        \n",
    "        # MRR Calculation\n",
    "        for query in queries:\n",
    "            bib_key = query['key']\n",
    "            true_id = query['true_id']\n",
    "            top_5 = predictions.get(bib_key, [])\n",
    "            \n",
    "            if true_id in top_5:\n",
    "                rank = top_5.index(true_id) + 1\n",
    "                part_mrr_sum += 1.0 / rank\n",
    "            part_query_count += 1\n",
    "        \n",
    "        # --- SAVE pred.json ---\n",
    "        safe_paper_id = str(paper_id).strip()\n",
    "        paper_dir = os.path.join(SUBMISSION_DIR, safe_paper_id)\n",
    "        os.makedirs(paper_dir, exist_ok=True)\n",
    "        \n",
    "        save_path = os.path.join(paper_dir, 'pred.json')\n",
    "        save_json(pred_output, save_path)\n",
    "    \n",
    "    # Update Stats\n",
    "    if part_query_count > 0:\n",
    "        summary_stats[partition]['mrr'] = part_mrr_sum / part_query_count\n",
    "        summary_stats[partition]['queries'] = part_query_count\n",
    "    \n",
    "    print(f\"   ‚úÖ Finished {partition}. MRR: {summary_stats[partition]['mrr']:.4f}\")\n",
    "    print(f\"   ‚è±Ô∏è Partition total: {time.time() - partition_start:.2f}s\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# In timing breakdown sau khi ch·∫°y xong\n",
    "print_timing_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "961486f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "üìä SUBMISSION GENERATION REPORT\n",
      "========================================\n",
      "Partition       | Papers     | Queries    | MRR       \n",
      "--------------------------------------------------\n",
      "TRAIN           | 605        | 13348      | 0.9973\n",
      "VALIDATION      | 1          | 72         | 1.0000\n",
      "TEST            | 1          | 22         | 1.0000\n",
      "========================================\n",
      "\n",
      "üìÅ All pred.json files are ready at: d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\23127011\\notebooks\\submission_final\\23127011\n"
     ]
    }
   ],
   "source": [
    "# --- FINAL REPORT ---\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"üìä SUBMISSION GENERATION REPORT\")\n",
    "print(\"=\"*40)\n",
    "print(f\"{'Partition':<15} | {'Papers':<10} | {'Queries':<10} | {'MRR':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for part in PARTITIONS:\n",
    "    stats = summary_stats[part]\n",
    "    print(f\"{part.upper():<15} | {stats['papers']:<10} | {stats['queries']:<10} | {stats['mrr']:.4f}\")\n",
    "\n",
    "print(\"=\"*40)\n",
    "print(f\"\\nüìÅ All pred.json files are ready at: {os.path.abspath(SUBMISSION_DIR)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
