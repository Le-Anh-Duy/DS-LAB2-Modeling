{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa83b48",
   "metadata": {},
   "source": [
    "# Notebook 06: Full Inference for Submission\n",
    "**Má»¥c tiÃªu:**\n",
    "1. Cháº¡y pipeline dá»± Ä‘oÃ¡n trÃªn **Táº¤T Cáº¢** cÃ¡c táº­p dá»¯ liá»‡u: Train, Validation, vÃ  Test.\n",
    "2. Sinh ra file `pred.json` cho tá»«ng bÃ i bÃ¡o.\n",
    "3. Sáº¯p xáº¿p káº¿t quáº£ vÃ o Ä‘Ãºng cáº¥u trÃºc thÆ° má»¥c Ä‘á»ƒ ná»™p bÃ i (Submission Ready).\n",
    "\n",
    "**Cáº¥u trÃºc Output:**\n",
    "```bash\n",
    "submission_final/\n",
    "â”œâ”€â”€ <Student_ID>/\n",
    "â”‚   â”œâ”€â”€ <paper_id_1>/\n",
    "â”‚   â”‚   â””â”€â”€ pred.json\n",
    "â”‚   â”œâ”€â”€ <paper_id_2>/\n",
    "â”‚   â”‚   â””â”€â”€ pred.json\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cb4b4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries vÃ  src.ml modules imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup path Ä‘á»ƒ import src module\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Import tá»« src.ml module (Sá»­ dá»¥ng láº¡i cÃ¡c hÃ m Ä‘Ã£ refactor)\n",
    "from src.ml import (\n",
    "    load_json,\n",
    "    save_json,\n",
    "    normalize_text_basic,\n",
    "    compute_pairwise_features,\n",
    "    compute_tfidf_cosine_single,\n",
    "    parse_bibtex_smart,\n",
    "    transform_to_paper_based\n",
    ")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"âœ… Libraries vÃ  src.ml modules imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00f10b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Created submission directory: d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\23127011\\notebooks\\submission_final\\23127011\n"
     ]
    }
   ],
   "source": [
    "# --- Cáº¤U HÃŒNH QUAN TRá»ŒNG ---\n",
    "\n",
    "# 1. ThÃ´ng tin sinh viÃªn (Äá»ƒ táº¡o folder ná»™p bÃ i)\n",
    "STUDENT_ID = \"23127011\" \n",
    "\n",
    "# 2. ÄÆ°á»ng dáº«n dá»¯ liá»‡u\n",
    "DATASET_ROOT = '../../dataset_final'\n",
    "PARTITIONS = ['train', 'validation', 'test']\n",
    "\n",
    "# 3. ÄÆ°á»ng dáº«n Model\n",
    "MODEL_PATH = '../../dataset_final/models/best_matcher.pkl'\n",
    "FEATURE_NAME_PATH = '../../dataset_final/models/feature_names.pkl'\n",
    "\n",
    "# 4. ThÆ° má»¥c Output\n",
    "SUBMISSION_DIR = f'submission_final/{STUDENT_ID}'\n",
    "\n",
    "if not os.path.exists(SUBMISSION_DIR):\n",
    "    os.makedirs(SUBMISSION_DIR)\n",
    "    print(f\"ğŸ“ Created submission directory: {os.path.abspath(SUBMISSION_DIR)}\")\n",
    "else:\n",
    "    print(f\"ğŸ“ Saving to existing directory: {os.path.abspath(SUBMISSION_DIR)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83de33a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Feature wrapper ready.\n"
     ]
    }
   ],
   "source": [
    "# --- HELPER FUNCTION: Feature Calculation Wrapper ---\n",
    "def compute_features_with_tfidf(row):\n",
    "    \"\"\"Compute features bao gá»“m TF-IDF cosine.\"\"\"\n",
    "    feats = compute_pairwise_features(row)\n",
    "    \n",
    "    # ThÃªm TF-IDF cosine cho single pair\n",
    "    q_tit = normalize_text_basic(row.get('bib_title', ''))\n",
    "    c_tit = normalize_text_basic(row.get('cand_title', ''))\n",
    "    feats['feat_title_tfidf_cosine'] = compute_tfidf_cosine_single(q_tit, c_tit)\n",
    "    \n",
    "    return feats\n",
    "\n",
    "print(\"âœ… Feature wrapper ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fd07652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded: XGBClassifier\n",
      "ğŸ“ Expected features: 13\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- LOAD MODEL ---\n",
    "try:\n",
    "    model = joblib.load(MODEL_PATH)\n",
    "    feature_names = joblib.load(FEATURE_NAME_PATH)\n",
    "    print(f\"âœ… Model loaded: {type(model).__name__}\")\n",
    "    print(f\"ğŸ“ Expected features: {len(feature_names)}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ CRITICAL ERROR: Model file not found. Cannot proceed.\")\n",
    "    model = None\n",
    "    feature_names = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55902cd",
   "metadata": {},
   "source": [
    "\n",
    "## Main Inference Loop\n",
    "VÃ²ng láº·p nÃ y sáº½:\n",
    "1. Duyá»‡t qua tá»«ng Partition (Train -> Valid -> Test).\n",
    "2. Load `labels.json` cá»§a partition Ä‘Ã³.\n",
    "3. Transform sang cáº¥u trÃºc Paper-based.\n",
    "4. Cháº¡y Ranking Model.\n",
    "5. LÆ°u file `pred.json` vÃ o folder tÆ°Æ¡ng á»©ng vá»›i Paper ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8de2e697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ STARTING FULL INFERENCE PIPELINE...\n",
      "\n",
      "ğŸ”µ Processing Partition: [TRAIN]\n",
      "   â†³ Found 292 papers. Ranking now...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Ranking train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 292/292 [33:16<00:00,  6.84s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Finished train. Saved pred.json files.\n",
      "--------------------------------------------------\n",
      "ğŸ”µ Processing Partition: [VALIDATION]\n",
      "   â†³ Found 39 papers. Ranking now...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Ranking validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [04:00<00:00,  6.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Finished validation. Saved pred.json files.\n",
      "--------------------------------------------------\n",
      "ğŸ”µ Processing Partition: [TEST]\n",
      "   â†³ Found 39 papers. Ranking now...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Ranking test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:15<00:00,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Finished test. Saved pred.json files.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Biáº¿n Ä‘á»ƒ thá»‘ng kÃª tá»•ng káº¿t\n",
    "summary_stats = {\n",
    "    'train': {'papers': 0, 'mrr': 0, 'queries': 0},\n",
    "    'validation': {'papers': 0, 'mrr': 0, 'queries': 0},\n",
    "    'test': {'papers': 0, 'mrr': 0, 'queries': 0}\n",
    "}\n",
    "\n",
    "print(\"ğŸš€ STARTING FULL INFERENCE PIPELINE...\\n\")\n",
    "\n",
    "for partition in PARTITIONS:\n",
    "    print(f\"ğŸ”µ Processing Partition: [{partition.upper()}]\")\n",
    "    \n",
    "    # 1. Load Data\n",
    "    label_file = os.path.join(DATASET_ROOT, partition, 'labels.json')\n",
    "    if not os.path.exists(label_file):\n",
    "        print(f\"   âš ï¸ Warning: File {label_file} not found. Skipping.\")\n",
    "        continue\n",
    "        \n",
    "    raw_data = load_json(label_file)\n",
    "    if not raw_data:\n",
    "        print(\"   âš ï¸ Empty data. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # 2. Transform Data\n",
    "    # Sá»­ dá»¥ng hÃ m tá»« module src.ml\n",
    "    papers_db = transform_to_paper_based(raw_data, parse_bibtex_smart)\n",
    "    summary_stats[partition]['papers'] = len(papers_db)\n",
    "    \n",
    "    print(f\"   â†³ Found {len(papers_db)} papers. Ranking now...\")\n",
    "    \n",
    "    # 3. Ranking Loop\n",
    "    part_mrr_sum = 0\n",
    "    part_query_count = 0\n",
    "    \n",
    "    for paper_id, data in tqdm(papers_db.items(), desc=f\"   Ranking {partition}\"):\n",
    "        queries = data['queries']\n",
    "        candidates_dict = data['candidates']\n",
    "        candidates_list = list(candidates_dict.values())\n",
    "        \n",
    "        # Náº¿u paper khÃ´ng cÃ³ candidate nÃ o (edge case)\n",
    "        if not candidates_list:\n",
    "            continue\n",
    "            \n",
    "        # Init Output Structure cho pred.json\n",
    "        pred_output = {\n",
    "            \"partition\": partition,\n",
    "            \"groundtruth\": {},\n",
    "            \"prediction\": {}\n",
    "        }\n",
    "        \n",
    "        # Duyá»‡t tá»«ng Query\n",
    "        for query in queries:\n",
    "            bib_key = query['key']\n",
    "            true_id = query['true_id']\n",
    "            \n",
    "            # Groundtruth\n",
    "            pred_output['groundtruth'][bib_key] = true_id\n",
    "            \n",
    "            # Pairing\n",
    "            pairs = []\n",
    "            for cand in candidates_list:\n",
    "                row = {}\n",
    "                row.update(query)\n",
    "                row.update(cand)\n",
    "                pairs.append(row)\n",
    "            \n",
    "            # Feature Engineering\n",
    "            feats_list = [compute_features_with_tfidf(p) for p in pairs]\n",
    "            df_feats = pd.DataFrame(feats_list)\n",
    "            \n",
    "            # Prediction\n",
    "            scores = []\n",
    "            if model:\n",
    "                # Fill missing cols\n",
    "                for col in feature_names:\n",
    "                    if col not in df_feats.columns: df_feats[col] = 0.0\n",
    "                \n",
    "                X_input = df_feats[feature_names]\n",
    "                scores = model.predict_proba(X_input)[:, 1]\n",
    "            else:\n",
    "                scores = np.random.rand(len(pairs))\n",
    "            \n",
    "            # Ranking & Top 5\n",
    "            ranked_candidates = []\n",
    "            for idx, score in enumerate(scores):\n",
    "                ranked_candidates.append((candidates_list[idx]['cand_id'], score))\n",
    "            \n",
    "            ranked_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_5 = [x[0] for x in ranked_candidates[:5]]\n",
    "            \n",
    "            # Save Prediction\n",
    "            pred_output['prediction'][bib_key] = top_5\n",
    "            \n",
    "            # MRR Calculation (Internal check)\n",
    "            if true_id in top_5:\n",
    "                rank = top_5.index(true_id) + 1\n",
    "                part_mrr_sum += 1.0 / rank\n",
    "            part_query_count += 1\n",
    "            \n",
    "        # --- 4. SAVE pred.json ---\n",
    "        # Táº¡o folder cho Paper ID: submission_final/<Student_ID>/<paper_id>/\n",
    "        # LÆ°u Ã½: paper_id cÃ³ thá»ƒ chá»©a kÃ½ tá»± '/' (vÃ­ dá»¥: 2005.12345), cáº§n xá»­ lÃ½ náº¿u há»‡ Ä‘iá»u hÃ nh khÃ´ng chá»‹u\n",
    "        # Tuy nhiÃªn Arxiv ID thÆ°á»ng an toÃ n hoáº·c lÃ  yymm-id.\n",
    "        safe_paper_id = str(paper_id).strip() \n",
    "        paper_dir = os.path.join(SUBMISSION_DIR, safe_paper_id)\n",
    "        \n",
    "        if not os.path.exists(paper_dir):\n",
    "            os.makedirs(paper_dir)\n",
    "            \n",
    "        save_path = os.path.join(paper_dir, 'pred.json')\n",
    "        save_json(pred_output, save_path)\n",
    "    \n",
    "    # Update Stats\n",
    "    if part_query_count > 0:\n",
    "        summary_stats[partition]['mrr'] = part_mrr_sum / part_query_count\n",
    "        summary_stats[partition]['queries'] = part_query_count\n",
    "    \n",
    "    print(f\"   âœ… Finished {partition}. Saved pred.json files.\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "961486f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "ğŸ“Š SUBMISSION GENERATION REPORT\n",
      "========================================\n",
      "Partition       | Papers     | Queries    | MRR       \n",
      "--------------------------------------------------\n",
      "TRAIN           | 292        | 6722       | 0.7942\n",
      "VALIDATION      | 39         | 1127       | 0.8005\n",
      "TEST            | 39         | 812        | 0.8349\n",
      "========================================\n",
      "\n",
      "ğŸ“ All pred.json files are ready at: d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\23127011\\notebooks\\submission_final\\23127011\n"
     ]
    }
   ],
   "source": [
    "# --- FINAL REPORT ---\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"ğŸ“Š SUBMISSION GENERATION REPORT\")\n",
    "print(\"=\"*40)\n",
    "print(f\"{'Partition':<15} | {'Papers':<10} | {'Queries':<10} | {'MRR':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for part in PARTITIONS:\n",
    "    stats = summary_stats[part]\n",
    "    print(f\"{part.upper():<15} | {stats['papers']:<10} | {stats['queries']:<10} | {stats['mrr']:.4f}\")\n",
    "\n",
    "print(\"=\"*40)\n",
    "print(f\"\\nğŸ“ All pred.json files are ready at: {os.path.abspath(SUBMISSION_DIR)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
