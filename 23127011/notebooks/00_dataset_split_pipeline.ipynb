{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "772d96bb",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "244c7960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup ho√†n t·∫•t!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from typing import List, Dict\n",
    "\n",
    "# Setup path\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "print(\"‚úÖ Setup ho√†n t·∫•t!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "237aafd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Input:  d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\data_output_v2\n",
      "üìÇ Output: d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\dataset_final\n",
      "üìÖ Prefix: 2403\n",
      "üî¢ Limit:  700\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# C·∫§U H√åNH - THAY ƒê·ªîI ·ªû ƒê√ÇY\n",
    "# =============================================================================\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n Input (th∆∞ m·ª•c ch·ª©a c√°c folder paper v·ªõi labels.json)\n",
    "INPUT_DIR = os.path.abspath('../../data_output_v2')\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n Output (th∆∞ m·ª•c xu·∫•t dataset)\n",
    "OUTPUT_DIR = os.path.abspath('../../dataset_final')\n",
    "\n",
    "# Prefix nƒÉm-th√°ng c·ªßa papers (VD: 2403 = March 2024)\n",
    "YYMM_PREFIX = '2403'\n",
    "\n",
    "# Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng papers (None = l·∫•y t·∫•t c·∫£)\n",
    "LIMIT = 700  # ƒê·∫∑t None ƒë·ªÉ l·∫•y full dataset\n",
    "\n",
    "# Ho·∫∑c d√πng Range (∆∞u ti√™n h∆°n LIMIT n·∫øu c·∫£ 2 ƒë·ªÅu set)\n",
    "USE_RANGE = False\n",
    "RANGE_START = 0\n",
    "RANGE_END = 100\n",
    "\n",
    "# Random seed\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# =============================================================================\n",
    "print(f\"üìÇ Input:  {INPUT_DIR}\")\n",
    "print(f\"üìÇ Output: {OUTPUT_DIR}\")\n",
    "print(f\"üìÖ Prefix: {YYMM_PREFIX}\")\n",
    "print(f\"üî¢ Limit:  {LIMIT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae71d5e",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7483013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions defined!\n"
     ]
    }
   ],
   "source": [
    "def load_selected_papers(input_dir: str, folder_names: List[str]) -> Dict[str, List[dict]]:\n",
    "    \"\"\"\n",
    "    ƒê·ªçc d·ªØ li·ªáu t·ª´ danh s√°ch folder.\n",
    "    \n",
    "    Returns:\n",
    "        Dict {paper_id: [list of references]}\n",
    "    \"\"\"\n",
    "    papers_map = {}\n",
    "    print(f\"üîÑ Loading data from {len(folder_names)} folders...\")\n",
    "\n",
    "    for folder_name in folder_names:\n",
    "        file_path = os.path.join(input_dir, folder_name, 'labels.json')\n",
    "        if not os.path.exists(file_path):\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            if isinstance(data, list) and len(data) > 0:\n",
    "                papers_map[folder_name] = data\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Error loading {folder_name}: {e}\")\n",
    "            \n",
    "    print(f\"   ‚úÖ Loaded {len(papers_map)} papers with valid labels\")\n",
    "    return papers_map\n",
    "\n",
    "\n",
    "def save_json(data_list: List, filepath: str):\n",
    "    \"\"\"L∆∞u list ra file JSON.\"\"\"\n",
    "    if not data_list:\n",
    "        return\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data_list, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"   üíæ Saved: {os.path.basename(filepath)} ({len(data_list)} items)\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87af018",
   "metadata": {},
   "source": [
    "## 3. Scan & Filter Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e864362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Found 5000 folders matching prefix '2403'\n",
      "   First: 2403-00530\n",
      "   Last:  2403-05529\n"
     ]
    }
   ],
   "source": [
    "# Ki·ªÉm tra input directory\n",
    "if not os.path.exists(INPUT_DIR):\n",
    "    raise FileNotFoundError(f\"‚ùå Input directory not found: {INPUT_DIR}\")\n",
    "\n",
    "# Qu√©t t·∫•t c·∫£ folders\n",
    "all_items = os.listdir(INPUT_DIR)\n",
    "valid_folders = [\n",
    "    name for name in all_items \n",
    "    if os.path.isdir(os.path.join(INPUT_DIR, name)) \n",
    "    and name.startswith(YYMM_PREFIX)\n",
    "]\n",
    "valid_folders.sort()\n",
    "\n",
    "if not valid_folders:\n",
    "    raise ValueError(f\"‚ùå No folders found with prefix '{YYMM_PREFIX}'\")\n",
    "\n",
    "print(f\"üìã Found {len(valid_folders)} folders matching prefix '{YYMM_PREFIX}'\")\n",
    "print(f\"   First: {valid_folders[0]}\")\n",
    "print(f\"   Last:  {valid_folders[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75bb5b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé≤ Mode: RANDOM LIMIT (700 papers)\n",
      "   üëâ Will process 700 papers\n"
     ]
    }
   ],
   "source": [
    "# √Åp d·ª•ng Limit ho·∫∑c Range\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "if USE_RANGE:\n",
    "    target_folders = valid_folders[RANGE_START:RANGE_END]\n",
    "    print(f\"‚úÇÔ∏è  Mode: RANGE [{RANGE_START} -> {RANGE_END}]\")\n",
    "elif LIMIT and LIMIT < len(valid_folders):\n",
    "    temp_list = list(valid_folders)\n",
    "    random.shuffle(temp_list)\n",
    "    target_folders = temp_list[:LIMIT]\n",
    "    print(f\"üé≤ Mode: RANDOM LIMIT ({LIMIT} papers)\")\n",
    "else:\n",
    "    target_folders = valid_folders\n",
    "    print(f\"üöÄ Mode: FULL DATASET\")\n",
    "\n",
    "print(f\"   üëâ Will process {len(target_folders)} papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f569d9",
   "metadata": {},
   "source": [
    "## 4. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aee7f191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading data from 700 folders...\n",
      "   ‚úÖ Loaded 512 papers with valid labels\n",
      "\n",
      "üìä Statistics:\n",
      "   Papers loaded: 512\n",
      "   Total refs:    11545\n",
      "   Avg refs/paper: 22.5\n"
     ]
    }
   ],
   "source": [
    "# Load papers\n",
    "papers_db = load_selected_papers(INPUT_DIR, target_folders)\n",
    "all_loaded_ids = list(papers_db.keys())\n",
    "\n",
    "if not all_loaded_ids:\n",
    "    raise ValueError(\"‚ùå No valid labels.json found in selected folders!\")\n",
    "\n",
    "# Th·ªëng k√™\n",
    "total_refs = sum(len(refs) for refs in papers_db.values())\n",
    "print(f\"\\nüìä Statistics:\")\n",
    "print(f\"   Papers loaded: {len(papers_db)}\")\n",
    "print(f\"   Total refs:    {total_refs}\")\n",
    "print(f\"   Avg refs/paper: {total_refs/len(papers_db):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdabe444",
   "metadata": {},
   "source": [
    "## 5. Select Manual Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2376f666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Candidates for Manual (>= 20 refs): 215 papers\n",
      "\n",
      "   Top 10:\n",
      "   - 2403-01387: 163 refs\n",
      "   - 2403-02695: 154 refs\n",
      "   - 2403-03256: 154 refs\n",
      "   - 2403-01202: 148 refs\n",
      "   - 2403-04317: 142 refs\n",
      "   - 2403-02332: 141 refs\n",
      "   - 2403-03001: 118 refs\n",
      "   - 2403-01528: 113 refs\n",
      "   - 2403-02308: 104 refs\n",
      "   - 2403-02774: 97 refs\n"
     ]
    }
   ],
   "source": [
    "# T√¨m papers c√≥ >= 20 refs cho Manual\n",
    "MIN_REFS_FOR_MANUAL = 20\n",
    "\n",
    "candidates_manual = []\n",
    "for pid, refs in papers_db.items():\n",
    "    if len(refs) >= MIN_REFS_FOR_MANUAL:\n",
    "        candidates_manual.append((pid, len(refs)))\n",
    "\n",
    "candidates_manual.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"üéØ Candidates for Manual (>= {MIN_REFS_FOR_MANUAL} refs): {len(candidates_manual)} papers\")\n",
    "print(\"\\n   Top 10:\")\n",
    "for pid, count in candidates_manual[:10]:\n",
    "    print(f\"   - {pid}: {count} refs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae95fdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê·ªß candidates cho Manual selection!\n"
     ]
    }
   ],
   "source": [
    "# Ki·ªÉm tra ƒë·ªß candidates\n",
    "if len(candidates_manual) < 5:\n",
    "    print(\"‚ùå ERROR: Not enough candidates for manual selection!\")\n",
    "    print(f\"   Required: 5 papers with >= {MIN_REFS_FOR_MANUAL} refs\")\n",
    "    print(f\"   Found: {len(candidates_manual)}\")\n",
    "    print(\"   üí° Suggestion: Increase LIMIT or use full dataset\")\n",
    "else:\n",
    "    print(\"‚úÖ ƒê·ªß candidates cho Manual selection!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "572aff0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úçÔ∏è  SELECTED MANUAL PAPERS:\n",
      "   Train (3): ['2403-03883', '2403-02386', '2403-04386']\n",
      "   Val   (1): ['2403-02845']\n",
      "   Test  (1): ['2403-03022']\n"
     ]
    }
   ],
   "source": [
    "# Ch·ªçn 5 b√†i Manual (3 Train, 1 Val, 1 Test)\n",
    "random.seed(RANDOM_SEED)\n",
    "candidate_ids = [x[0] for x in candidates_manual]\n",
    "random.shuffle(candidate_ids)\n",
    "\n",
    "selected_manual = candidate_ids[:5]\n",
    "\n",
    "manual_train_ids = selected_manual[:3]\n",
    "manual_val_ids = selected_manual[3:4]\n",
    "manual_test_ids = selected_manual[4:5]\n",
    "\n",
    "print(\"‚úçÔ∏è  SELECTED MANUAL PAPERS:\")\n",
    "print(f\"   Train (3): {manual_train_ids}\")\n",
    "print(f\"   Val   (1): {manual_val_ids}\")\n",
    "print(f\"   Test  (1): {manual_test_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb516d9",
   "metadata": {},
   "source": [
    "## 6. Split Auto Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7accc261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ AUTO DATASET SPLIT (507 papers):\n",
      "   Train: 405 papers\n",
      "   Val:   51 papers\n",
      "   Test:  51 papers\n"
     ]
    }
   ],
   "source": [
    "# Auto = (T·∫•t c·∫£) - (5 b√†i manual)\n",
    "auto_pool_ids = [pid for pid in all_loaded_ids if pid not in selected_manual]\n",
    "\n",
    "random.shuffle(auto_pool_ids)\n",
    "total_auto = len(auto_pool_ids)\n",
    "\n",
    "# Chia 80/10/10\n",
    "tr_end = int(total_auto * 0.8)\n",
    "val_end = int(total_auto * 0.9)\n",
    "\n",
    "if total_auto < 10:\n",
    "    print(\"‚ö†Ô∏è Small auto dataset. Putting all into Train.\")\n",
    "    auto_train_ids = auto_pool_ids\n",
    "    auto_val_ids = []\n",
    "    auto_test_ids = []\n",
    "else:\n",
    "    auto_train_ids = auto_pool_ids[:tr_end]\n",
    "    auto_val_ids = auto_pool_ids[tr_end:val_end]\n",
    "    auto_test_ids = auto_pool_ids[val_end:]\n",
    "\n",
    "print(f\"\\nü§ñ AUTO DATASET SPLIT ({total_auto} papers):\")\n",
    "print(f\"   Train: {len(auto_train_ids)} papers\")\n",
    "print(f\"   Val:   {len(auto_val_ids)} papers\")\n",
    "print(f\"   Test:  {len(auto_test_ids)} papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafd0729",
   "metadata": {},
   "source": [
    "## 7. Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5d05fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save(subset_name: str, auto_ids: List[str], manual_ids: List[str]):\n",
    "    \"\"\"G·ªôp data v√† l∆∞u file.\"\"\"\n",
    "    # G·ªôp Auto data\n",
    "    auto_data = []\n",
    "    for pid in auto_ids:\n",
    "        auto_data.extend(papers_db[pid])\n",
    "    \n",
    "    # G·ªôp Manual data\n",
    "    manual_data = []\n",
    "    for pid in manual_ids:\n",
    "        manual_data.extend(papers_db[pid])\n",
    "        \n",
    "    base_path = os.path.join(OUTPUT_DIR, subset_name)\n",
    "    print(f\"\\nüìÇ Writing {subset_name.upper()}...\")\n",
    "    \n",
    "    if auto_data:\n",
    "        save_json(auto_data, os.path.join(base_path, 'auto.json'))\n",
    "    if manual_data:\n",
    "        save_json(manual_data, os.path.join(base_path, 'manual.json'))\n",
    "    \n",
    "    return len(auto_data), len(manual_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8372c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Writing TRAIN...\n",
      "   üíæ Saved: auto.json (8844 items)\n",
      "   üíæ Saved: manual.json (143 items)\n",
      "\n",
      "üìÇ Writing VALIDATION...\n",
      "   üíæ Saved: auto.json (1247 items)\n",
      "   üíæ Saved: manual.json (41 items)\n",
      "\n",
      "üìÇ Writing TEST...\n",
      "   üíæ Saved: auto.json (1240 items)\n",
      "   üíæ Saved: manual.json (30 items)\n",
      "\n",
      "üéâ DONE! Dataset saved to: d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\dataset_final\n"
     ]
    }
   ],
   "source": [
    "# T·∫°o output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# L∆∞u t·ª´ng partition\n",
    "train_auto, train_manual = process_and_save('train', auto_train_ids, manual_train_ids)\n",
    "val_auto, val_manual = process_and_save('validation', auto_val_ids, manual_val_ids)\n",
    "test_auto, test_manual = process_and_save('test', auto_test_ids, manual_test_ids)\n",
    "\n",
    "print(f\"\\nüéâ DONE! Dataset saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40070bb2",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9250f072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìä DATASET SPLIT SUMMARY\n",
      "============================================================\n",
      "\n",
      "Partition    Auto Refs    Manual Refs  Total     \n",
      "--------------------------------------------------\n",
      "Train        8844         143          8987      \n",
      "Validation   1247         41           1288      \n",
      "Test         1240         30           1270      \n",
      "--------------------------------------------------\n",
      "TOTAL        11331        214          11545     \n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# T·ªïng k·∫øt\n",
    "print(\"=\"*60)\n",
    "print(\"üìä DATASET SPLIT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Partition':<12} {'Auto Refs':<12} {'Manual Refs':<12} {'Total':<10}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"{'Train':<12} {train_auto:<12} {train_manual:<12} {train_auto+train_manual:<10}\")\n",
    "print(f\"{'Validation':<12} {val_auto:<12} {val_manual:<12} {val_auto+val_manual:<10}\")\n",
    "print(f\"{'Test':<12} {test_auto:<12} {test_manual:<12} {test_auto+test_manual:<10}\")\n",
    "print(\"-\"*50)\n",
    "total = train_auto + train_manual + val_auto + val_manual + test_auto + test_manual\n",
    "print(f\"{'TOTAL':<12} {train_auto+val_auto+test_auto:<12} {train_manual+val_manual+test_manual:<12} {total:<10}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f80c889e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ Output Files:\n",
      "\n",
      "   train/\n",
      "      ‚îî‚îÄ‚îÄ auto.json: 8,087,712 bytes\n",
      "      ‚îî‚îÄ‚îÄ labels.json: 6,261,472 bytes\n",
      "      ‚îî‚îÄ‚îÄ manual.json: 226,942 bytes\n",
      "\n",
      "   validation/\n",
      "      ‚îî‚îÄ‚îÄ auto.json: 1,042,362 bytes\n",
      "      ‚îî‚îÄ‚îÄ labels.json: 1,009,496 bytes\n",
      "      ‚îî‚îÄ‚îÄ manual.json: 36,600 bytes\n",
      "\n",
      "   test/\n",
      "      ‚îî‚îÄ‚îÄ auto.json: 1,178,756 bytes\n",
      "      ‚îî‚îÄ‚îÄ labels.json: 717,933 bytes\n",
      "      ‚îî‚îÄ‚îÄ manual.json: 30,391 bytes\n"
     ]
    }
   ],
   "source": [
    "# Verify output files\n",
    "print(\"\\nüìÅ Output Files:\")\n",
    "for partition in ['train', 'validation', 'test']:\n",
    "    partition_path = os.path.join(OUTPUT_DIR, partition)\n",
    "    if os.path.exists(partition_path):\n",
    "        files = os.listdir(partition_path)\n",
    "        print(f\"\\n   {partition}/\")\n",
    "        for f in files:\n",
    "            fpath = os.path.join(partition_path, f)\n",
    "            size = os.path.getsize(fpath)\n",
    "            print(f\"      ‚îî‚îÄ‚îÄ {f}: {size:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e583002",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "Sau khi ch·∫°y notebook n√†y, b·∫°n c√≥ th·ªÉ:\n",
    "\n",
    "1. **Notebook 01:** `01_completeness_check_data_cleaning.ipynb` - Ki·ªÉm tra v√† l√†m s·∫°ch d·ªØ li·ªáu\n",
    "2. **Notebook 02:** `02_data_augmentation.ipynb` - Sinh negative samples\n",
    "3. **Notebook 03:** `03_feature_engineering.ipynb` - Tr√≠ch xu·∫•t features\n",
    "4. **Notebook 04:** `04_modeling.ipynb` - Hu·∫•n luy·ªán model\n",
    "5. **Notebook 05:** `05_evaluation_ranking.ipynb` - ƒê√°nh gi√° v√† ranking\n",
    "\n",
    "**Ho·∫∑c ch·∫°y qua CLI:**\n",
    "```bash\n",
    "python -m src.merge_labels --yymm 2403 --limit 50 --input ./data_output_v2 --output ./dataset_final\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
