{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "772d96bb",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "244c7960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Setup hoÃ n táº¥t!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from typing import List, Dict\n",
    "\n",
    "# Setup path\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "print(\"âœ… Setup hoÃ n táº¥t!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "237aafd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Input:  d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\data_output_v2\n",
      "ğŸ“‚ Output: d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\dataset_final\n",
      "ğŸ“… Prefix: 2403\n",
      "ğŸ”¢ Limit:  780\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cáº¤U HÃŒNH - THAY Äá»”I á» ÄÃ‚Y\n",
    "# =============================================================================\n",
    "\n",
    "# ÄÆ°á»ng dáº«n Input (thÆ° má»¥c chá»©a cÃ¡c folder paper vá»›i labels.json)\n",
    "INPUT_DIR = os.path.abspath('../../data_output_v2')\n",
    "\n",
    "# ÄÆ°á»ng dáº«n Output (thÆ° má»¥c xuáº¥t dataset)\n",
    "OUTPUT_DIR = os.path.abspath('../../dataset_final')\n",
    "\n",
    "# Prefix nÄƒm-thÃ¡ng cá»§a papers (VD: 2403 = March 2024)\n",
    "YYMM_PREFIX = '2403'\n",
    "\n",
    "# Giá»›i háº¡n sá»‘ lÆ°á»£ng papers (None = láº¥y táº¥t cáº£)\n",
    "LIMIT = 780  # Äáº·t None Ä‘á»ƒ láº¥y full dataset\n",
    "\n",
    "# Hoáº·c dÃ¹ng Range (Æ°u tiÃªn hÆ¡n LIMIT náº¿u cáº£ 2 Ä‘á»u set)\n",
    "USE_RANGE = False\n",
    "RANGE_START = 0\n",
    "RANGE_END = 100\n",
    "\n",
    "# Random seed\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# =============================================================================\n",
    "print(f\"ğŸ“‚ Input:  {INPUT_DIR}\")\n",
    "print(f\"ğŸ“‚ Output: {OUTPUT_DIR}\")\n",
    "print(f\"ğŸ“… Prefix: {YYMM_PREFIX}\")\n",
    "print(f\"ğŸ”¢ Limit:  {LIMIT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae71d5e",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b7483013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Helper functions defined!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def check_bibtex_format(input_dir: str, folder_name: str) -> bool:\n",
    "    \"\"\"\n",
    "    Kiá»ƒm tra xem paper cÃ³ labels.json vá»›i Táº¤T Cáº¢ content lÃ  BibTeX chuáº©n khÃ´ng.\n",
    "    \n",
    "    Kiá»ƒm tra trÆ°á»ng 'content' trong labels.json:\n",
    "    - True náº¿u Táº¤T Cáº¢ items cÃ³ content báº¯t Ä‘áº§u vá»›i @article, @misc, etc.\n",
    "    - False náº¿u cÃ³ Báº¤T Ká»² item nÃ o lÃ  flat text (\\newblock) hoáº·c khÃ´ng cÃ³ file\n",
    "    \"\"\"\n",
    "    labels_path = os.path.join(input_dir, folder_name, 'labels.json')\n",
    "    \n",
    "    if not os.path.exists(labels_path):\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        with open(labels_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if not isinstance(data, list) or len(data) == 0:\n",
    "            return False\n",
    "        \n",
    "        # Kiá»ƒm tra Táº¤T Cáº¢ items pháº£i cÃ³ content báº¯t Ä‘áº§u vá»›i @\n",
    "        bibtex_pattern = re.compile(r'^\\s*@\\w+\\s*\\{', re.IGNORECASE)\n",
    "        \n",
    "        for item in data:\n",
    "            content = item.get('content', '')\n",
    "            if not content or not bibtex_pattern.match(content):\n",
    "                # CÃ³ Ã­t nháº¥t 1 item khÃ´ng pháº£i BibTeX chuáº©n -> skip paper nÃ y\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def load_selected_papers(input_dir: str, folder_names: List[str]) -> Dict[str, List[dict]]:\n",
    "    \"\"\"\n",
    "    Äá»c dá»¯ liá»‡u tá»« danh sÃ¡ch folder.\n",
    "    \n",
    "    Returns:\n",
    "        Dict {paper_id: [list of references]}\n",
    "    \"\"\"\n",
    "    papers_map = {}\n",
    "    print(f\"ğŸ”„ Loading data from {len(folder_names)} folders...\")\n",
    "\n",
    "    for folder_name in folder_names:\n",
    "        file_path = os.path.join(input_dir, folder_name, 'labels.json')\n",
    "        if not os.path.exists(file_path):\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            if isinstance(data, list) and len(data) > 0:\n",
    "                papers_map[folder_name] = data\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ Error loading {folder_name}: {e}\")\n",
    "            \n",
    "    print(f\"   âœ… Loaded {len(papers_map)} papers with valid labels\")\n",
    "    return papers_map\n",
    "\n",
    "\n",
    "def save_json(data_list: List, filepath: str):\n",
    "    \"\"\"LÆ°u list ra file JSON.\"\"\"\n",
    "    if not data_list:\n",
    "        return\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data_list, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"   ğŸ’¾ Saved: {os.path.basename(filepath)} ({len(data_list)} items)\")\n",
    "\n",
    "\n",
    "print(\"âœ… Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87af018",
   "metadata": {},
   "source": [
    "## 3. Scan & Filter Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4e864362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Found 5000 folders matching prefix '2403'\n",
      "\n",
      "ğŸ” Checking labels.json content format...\n",
      "   (Only papers where ALL refs start with @article/@misc/...)\n",
      "\n",
      "   âœ… Valid (all BibTeX format): 762 papers\n",
      "   â­ï¸  Skipped (has flat text refs): 4238 papers\n",
      "\n",
      "   Sample skipped: ['2403-00532', '2403-00534', '2403-00535', '2403-00536', '2403-00537']\n",
      "\n",
      "   First valid: 2403-00530\n",
      "   Last valid:  2403-05519\n"
     ]
    }
   ],
   "source": [
    "# Kiá»ƒm tra input directory\n",
    "if not os.path.exists(INPUT_DIR):\n",
    "    raise FileNotFoundError(f\"âŒ Input directory not found: {INPUT_DIR}\")\n",
    "\n",
    "# QuÃ©t táº¥t cáº£ folders\n",
    "all_items = os.listdir(INPUT_DIR)\n",
    "all_folders = [\n",
    "    name for name in all_items \n",
    "    if os.path.isdir(os.path.join(INPUT_DIR, name)) \n",
    "    and name.startswith(YYMM_PREFIX)\n",
    "]\n",
    "all_folders.sort()\n",
    "\n",
    "if not all_folders:\n",
    "    raise ValueError(f\"âŒ No folders found with prefix '{YYMM_PREFIX}'\")\n",
    "\n",
    "print(f\"ğŸ“‹ Found {len(all_folders)} folders matching prefix '{YYMM_PREFIX}'\")\n",
    "\n",
    "# --- Lá»ŒC CHá»ˆ Láº¤Y PAPERS CÃ“ Táº¤T Cáº¢ REFS LÃ€ BIBTEX FORMAT CHUáº¨N ---\n",
    "print(f\"\\nğŸ” Checking labels.json content format...\")\n",
    "print(f\"   (Only papers where ALL refs start with @article/@misc/...)\")\n",
    "valid_folders = []\n",
    "skipped_folders = []\n",
    "\n",
    "for folder in all_folders:\n",
    "    if check_bibtex_format(INPUT_DIR, folder):\n",
    "        valid_folders.append(folder)\n",
    "    else:\n",
    "        skipped_folders.append(folder)\n",
    "\n",
    "print(f\"\\n   âœ… Valid (all BibTeX format): {len(valid_folders)} papers\")\n",
    "print(f\"   â­ï¸  Skipped (has flat text refs): {len(skipped_folders)} papers\")\n",
    "\n",
    "if skipped_folders[:5]:\n",
    "    print(f\"\\n   Sample skipped: {skipped_folders[:5]}\")\n",
    "\n",
    "if not valid_folders:\n",
    "    raise ValueError(f\"âŒ No papers with valid BibTeX format found!\")\n",
    "\n",
    "print(f\"\\n   First valid: {valid_folders[0]}\")\n",
    "print(f\"   Last valid:  {valid_folders[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "75bb5b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Mode: FULL DATASET\n",
      "   ğŸ‘‰ Will process 762 papers\n"
     ]
    }
   ],
   "source": [
    "# Ãp dá»¥ng Limit hoáº·c Range\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "if USE_RANGE:\n",
    "    target_folders = valid_folders[RANGE_START:RANGE_END]\n",
    "    print(f\"âœ‚ï¸  Mode: RANGE [{RANGE_START} -> {RANGE_END}]\")\n",
    "elif LIMIT and LIMIT < len(valid_folders):\n",
    "    temp_list = list(valid_folders)\n",
    "    random.shuffle(temp_list)\n",
    "    target_folders = temp_list[:LIMIT]\n",
    "    print(f\"ğŸ² Mode: RANDOM LIMIT ({LIMIT} papers)\")\n",
    "else:\n",
    "    target_folders = valid_folders\n",
    "    print(f\"ğŸš€ Mode: FULL DATASET\")\n",
    "\n",
    "print(f\"   ğŸ‘‰ Will process {len(target_folders)} papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f569d9",
   "metadata": {},
   "source": [
    "## 4. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aee7f191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading data from 762 folders...\n",
      "   âœ… Loaded 762 papers with valid labels\n",
      "\n",
      "ğŸ“Š Statistics:\n",
      "   Papers loaded: 762\n",
      "   Total refs:    16489\n",
      "   Avg refs/paper: 21.6\n"
     ]
    }
   ],
   "source": [
    "# Load papers\n",
    "papers_db = load_selected_papers(INPUT_DIR, target_folders)\n",
    "all_loaded_ids = list(papers_db.keys())\n",
    "\n",
    "if not all_loaded_ids:\n",
    "    raise ValueError(\"âŒ No valid labels.json found in selected folders!\")\n",
    "\n",
    "# Thá»‘ng kÃª\n",
    "total_refs = sum(len(refs) for refs in papers_db.values())\n",
    "print(f\"\\nğŸ“Š Statistics:\")\n",
    "print(f\"   Papers loaded: {len(papers_db)}\")\n",
    "print(f\"   Total refs:    {total_refs}\")\n",
    "print(f\"   Avg refs/paper: {total_refs/len(papers_db):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdabe444",
   "metadata": {},
   "source": [
    "## 5. Select Manual Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2376f666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Candidates for Manual (>= 20 refs): 306 papers\n",
      "\n",
      "   Top 10:\n",
      "   - 2403-04279: 214 refs\n",
      "   - 2403-05131: 185 refs\n",
      "   - 2403-01313: 180 refs\n",
      "   - 2403-02619: 180 refs\n",
      "   - 2403-01202: 148 refs\n",
      "   - 2403-00533: 132 refs\n",
      "   - 2403-02469: 126 refs\n",
      "   - 2403-05175: 119 refs\n",
      "   - 2403-05030: 100 refs\n",
      "   - 2403-02203: 99 refs\n"
     ]
    }
   ],
   "source": [
    "# TÃ¬m papers cÃ³ >= 20 refs cho Manual\n",
    "MIN_REFS_FOR_MANUAL = 20\n",
    "\n",
    "candidates_manual = []\n",
    "for pid, refs in papers_db.items():\n",
    "    if len(refs) >= MIN_REFS_FOR_MANUAL:\n",
    "        candidates_manual.append((pid, len(refs)))\n",
    "\n",
    "candidates_manual.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"ğŸ¯ Candidates for Manual (>= {MIN_REFS_FOR_MANUAL} refs): {len(candidates_manual)} papers\")\n",
    "print(\"\\n   Top 10:\")\n",
    "for pid, count in candidates_manual[:10]:\n",
    "    print(f\"   - {pid}: {count} refs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ae95fdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Äá»§ candidates cho Manual selection!\n"
     ]
    }
   ],
   "source": [
    "# Kiá»ƒm tra Ä‘á»§ candidates\n",
    "if len(candidates_manual) < 5:\n",
    "    print(\"âŒ ERROR: Not enough candidates for manual selection!\")\n",
    "    print(f\"   Required: 5 papers with >= {MIN_REFS_FOR_MANUAL} refs\")\n",
    "    print(f\"   Found: {len(candidates_manual)}\")\n",
    "    print(\"   ğŸ’¡ Suggestion: Increase LIMIT or use full dataset\")\n",
    "else:\n",
    "    print(\"âœ… Äá»§ candidates cho Manual selection!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "572aff0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœï¸  SELECTED MANUAL PAPERS:\n",
      "   Train (3): ['2403-04225', '2403-03951', '2403-01980']\n",
      "   Val   (1): ['2403-04970']\n",
      "   Test  (1): ['2403-00803']\n"
     ]
    }
   ],
   "source": [
    "# Chá»n 5 bÃ i Manual (3 Train, 1 Val, 1 Test)\n",
    "random.seed(RANDOM_SEED)\n",
    "candidate_ids = [x[0] for x in candidates_manual]\n",
    "random.shuffle(candidate_ids)\n",
    "\n",
    "selected_manual = candidate_ids[:5]\n",
    "\n",
    "manual_train_ids = selected_manual[:3]\n",
    "manual_val_ids = selected_manual[3:4]\n",
    "manual_test_ids = selected_manual[4:5]\n",
    "\n",
    "print(\"âœï¸  SELECTED MANUAL PAPERS:\")\n",
    "print(f\"   Train (3): {manual_train_ids}\")\n",
    "print(f\"   Val   (1): {manual_val_ids}\")\n",
    "print(f\"   Test  (1): {manual_test_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb516d9",
   "metadata": {},
   "source": [
    "## 6. Split Auto Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7accc261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¤– AUTO DATASET SPLIT (757 papers):\n",
      "   Train: 605 papers\n",
      "   Val:   76 papers\n",
      "   Test:  76 papers\n"
     ]
    }
   ],
   "source": [
    "# Auto = (Táº¥t cáº£) - (5 bÃ i manual)\n",
    "auto_pool_ids = [pid for pid in all_loaded_ids if pid not in selected_manual]\n",
    "\n",
    "random.shuffle(auto_pool_ids)\n",
    "total_auto = len(auto_pool_ids)\n",
    "\n",
    "# Chia 80/10/10\n",
    "tr_end = int(total_auto * 0.8)\n",
    "val_end = int(total_auto * 0.9)\n",
    "\n",
    "if total_auto < 10:\n",
    "    print(\"âš ï¸ Small auto dataset. Putting all into Train.\")\n",
    "    auto_train_ids = auto_pool_ids\n",
    "    auto_val_ids = []\n",
    "    auto_test_ids = []\n",
    "else:\n",
    "    auto_train_ids = auto_pool_ids[:tr_end]\n",
    "    auto_val_ids = auto_pool_ids[tr_end:val_end]\n",
    "    auto_test_ids = auto_pool_ids[val_end:]\n",
    "\n",
    "print(f\"\\nğŸ¤– AUTO DATASET SPLIT ({total_auto} papers):\")\n",
    "print(f\"   Train: {len(auto_train_ids)} papers\")\n",
    "print(f\"   Val:   {len(auto_val_ids)} papers\")\n",
    "print(f\"   Test:  {len(auto_test_ids)} papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafd0729",
   "metadata": {},
   "source": [
    "## 7. Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d5d05fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save(subset_name: str, auto_ids: List[str], manual_ids: List[str]):\n",
    "    \"\"\"Gá»™p data vÃ  lÆ°u file.\"\"\"\n",
    "    # Gá»™p Auto data\n",
    "    auto_data = []\n",
    "    for pid in auto_ids:\n",
    "        auto_data.extend(papers_db[pid])\n",
    "    \n",
    "    # Gá»™p Manual data\n",
    "    manual_data = []\n",
    "    for pid in manual_ids:\n",
    "        manual_data.extend(papers_db[pid])\n",
    "        \n",
    "    base_path = os.path.join(OUTPUT_DIR, subset_name)\n",
    "    print(f\"\\nğŸ“‚ Writing {subset_name.upper()}...\")\n",
    "    \n",
    "    if auto_data:\n",
    "        save_json(auto_data, os.path.join(base_path, 'auto.json'))\n",
    "    if manual_data:\n",
    "        save_json(manual_data, os.path.join(base_path, 'manual.json'))\n",
    "    \n",
    "    return len(auto_data), len(manual_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4337a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_before_process(auto_ids: List[str]):\n",
    "    d = []\n",
    "    for pid in auto_ids:\n",
    "        d.extend(papers_db[pid])\n",
    "    return len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a8372c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_val_id = 0\n",
    "auto_test_id = 0\n",
    "\n",
    "for id in range(len(auto_val_ids)):\n",
    "    if count_before_process(auto_val_ids[id:id + 1]) >= 20:\n",
    "        auto_val_id = id\n",
    "\n",
    "\n",
    "for id in range(len(auto_test_ids)):\n",
    "    if count_before_process(auto_test_ids[id:id + 1]) >= 20:\n",
    "        auto_test_id = id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a8372c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‚ Writing TRAIN...\n",
      "   ğŸ’¾ Saved: auto.json (13222 items)\n",
      "   ğŸ’¾ Saved: manual.json (126 items)\n",
      "\n",
      "ğŸ“‚ Writing VALIDATION...\n",
      "   ğŸ’¾ Saved: auto.json (35 items)\n",
      "   ğŸ’¾ Saved: manual.json (72 items)\n",
      "\n",
      "ğŸ“‚ Writing TEST...\n",
      "   ğŸ’¾ Saved: auto.json (63 items)\n",
      "   ğŸ’¾ Saved: manual.json (22 items)\n",
      "\n",
      "ğŸ‰ DONE! Dataset saved to: d:\\Coding\\School\\Y3-K1\\Intro2DS\\DS - LAB 2\\Milestone2_Project\\dataset_final\n"
     ]
    }
   ],
   "source": [
    "# Táº¡o output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# LÆ°u tá»«ng partition\n",
    "train_auto, train_manual = process_and_save('train', auto_train_ids, manual_train_ids)\n",
    "\n",
    "# chá»‰ láº¥y 1 tá»‡p\n",
    "val_auto, val_manual = process_and_save('validation', auto_val_ids[auto_val_id:auto_val_id + 1], manual_val_ids)\n",
    "test_auto, test_manual = process_and_save('test', auto_test_ids[auto_test_id:auto_test_id + 1], manual_test_ids)\n",
    "\n",
    "print(f\"\\nğŸ‰ DONE! Dataset saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40070bb2",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9250f072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ“Š DATASET SPLIT SUMMARY\n",
      "============================================================\n",
      "\n",
      "Partition    Auto Refs    Manual Refs  Total     \n",
      "--------------------------------------------------\n",
      "Train        13222        126          13348     \n",
      "Validation   35           72           107       \n",
      "Test         63           22           85        \n",
      "--------------------------------------------------\n",
      "TOTAL        13320        220          13540     \n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Tá»•ng káº¿t\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“Š DATASET SPLIT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Partition':<12} {'Auto Refs':<12} {'Manual Refs':<12} {'Total':<10}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"{'Train':<12} {train_auto:<12} {train_manual:<12} {train_auto+train_manual:<10}\")\n",
    "print(f\"{'Validation':<12} {val_auto:<12} {val_manual:<12} {val_auto+val_manual:<10}\")\n",
    "print(f\"{'Test':<12} {test_auto:<12} {test_manual:<12} {test_auto+test_manual:<10}\")\n",
    "print(\"-\"*50)\n",
    "total = train_auto + train_manual + val_auto + val_manual + test_auto + test_manual\n",
    "print(f\"{'TOTAL':<12} {train_auto+val_auto+test_auto:<12} {train_manual+val_manual+test_manual:<12} {total:<10}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f80c889e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Output Files:\n",
      "\n",
      "   train/\n",
      "      â””â”€â”€ auto.json: 16,205,452 bytes\n",
      "      â””â”€â”€ labels.json: 16,324,211 bytes\n",
      "      â””â”€â”€ manual.json: 118,762 bytes\n",
      "\n",
      "   validation/\n",
      "      â””â”€â”€ auto.json: 32,561 bytes\n",
      "      â””â”€â”€ labels.json: 96,074 bytes\n",
      "      â””â”€â”€ manual.json: 96,074 bytes\n",
      "\n",
      "   test/\n",
      "      â””â”€â”€ auto.json: 58,352 bytes\n",
      "      â””â”€â”€ labels.json: 21,747 bytes\n",
      "      â””â”€â”€ manual.json: 21,747 bytes\n"
     ]
    }
   ],
   "source": [
    "# Verify output files\n",
    "print(\"\\nğŸ“ Output Files:\")\n",
    "for partition in ['train', 'validation', 'test']:\n",
    "    partition_path = os.path.join(OUTPUT_DIR, partition)\n",
    "    if os.path.exists(partition_path):\n",
    "        files = os.listdir(partition_path)\n",
    "        print(f\"\\n   {partition}/\")\n",
    "        for f in files:\n",
    "            fpath = os.path.join(partition_path, f)\n",
    "            size = os.path.getsize(fpath)\n",
    "            print(f\"      â””â”€â”€ {f}: {size:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e583002",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "Sau khi cháº¡y notebook nÃ y, báº¡n cÃ³ thá»ƒ:\n",
    "\n",
    "1. **Notebook 01:** `01_completeness_check_data_cleaning.ipynb` - Kiá»ƒm tra vÃ  lÃ m sáº¡ch dá»¯ liá»‡u\n",
    "2. **Notebook 02:** `02_data_augmentation.ipynb` - Sinh negative samples\n",
    "3. **Notebook 03:** `03_feature_engineering.ipynb` - TrÃ­ch xuáº¥t features\n",
    "4. **Notebook 04:** `04_modeling.ipynb` - Huáº¥n luyá»‡n model\n",
    "5. **Notebook 05:** `05_evaluation_ranking.ipynb` - ÄÃ¡nh giÃ¡ vÃ  ranking\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
