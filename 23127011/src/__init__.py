"""
LaTeX Paper Processing Pipeline
===============================

Pipeline x·ª≠ l√Ω v√† tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ LaTeX papers.

Modules ch√≠nh:
- pipeline: ƒêi·ªÅu ph·ªëi to√†n b·ªô qu√° tr√¨nh x·ª≠ l√Ω
- parser: Ph√¢n t√≠ch c·∫•u tr√∫c LaTeX (flatten, build tree, process content)
- processing: X·ª≠ l√Ω v√† lo·∫°i b·ªè tr√πng l·∫∑p (dedup)
- matching: So kh·ªõp reference v·ªõi ground truth
- utils: Ti·ªán √≠ch h·ªó tr·ª£ (I/O, cleaner)

S·ª≠ d·ª•ng nhanh:
    from src import run_full_pipeline
    run_full_pipeline(data_raw="path/to/raw", data_output="path/to/output")

Ho·∫∑c t·ª´ng phase ri√™ng:
    from src import run_processing_pipeline, run_matching_pipeline, run_merge_pipeline
"""

# =============================================================================
# PHASE 1: Pre-processing & Parsing Pipeline
# =============================================================================
from .pipeline import (
    run_processing_pipeline,
    process_single_paper
)

# =============================================================================
# PHASE 2: Reference Matching Pipeline  
# =============================================================================
from .run_matching import run_matching_pipeline

# =============================================================================
# PHASE 3: Dataset Merging
# =============================================================================
from .merge_labels import (
    load_selected_papers,
    save_json
)

# =============================================================================
# Core Components (for advanced usage)
# =============================================================================
from .parser import (
    LatexFlattener,
    LatexStructureBuilder, 
    LatexContentProcessor,
    find_root_tex_file
)

# Import t·ª´ submodules ƒë√£ refactor
from .processing import (
    ReferenceProcessor,
    ReferenceDeduplicator,
    ContentDeduplicator,
    replace_citations_in_text
)

from .matching import ReferenceMatcher

# =============================================================================
# Convenience Function: Run Full Pipeline
# =============================================================================
def run_full_pipeline(
    data_raw: str,
    data_output: str,
    parallel: bool = True,
    max_workers: int = None,
    run_matching: bool = True,
    verbose: bool = True
) -> dict:
    """
    Ch·∫°y to√†n b·ªô pipeline t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi.
    
    Args:
        data_raw: ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c ch·ª©a LaTeX papers th√¥
        data_output: ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c xu·∫•t k·∫øt qu·∫£
        parallel: S·ª≠ d·ª•ng x·ª≠ l√Ω song song (m·∫∑c ƒë·ªãnh: True)
        max_workers: S·ªë lu·ªìng t·ªëi ƒëa (m·∫∑c ƒë·ªãnh: s·ªë CPU)
        run_matching: Ch·∫°y phase matching sau khi x·ª≠ l√Ω (m·∫∑c ƒë·ªãnh: True)
        verbose: In th√¥ng tin ti·∫øn tr√¨nh (m·∫∑c ƒë·ªãnh: True)
    
    Returns:
        dict: Th·ªëng k√™ k·∫øt qu·∫£ x·ª≠ l√Ω
            - processed: S·ªë papers ƒë√£ x·ª≠ l√Ω
            - matched: S·ªë papers ƒë√£ match (n·∫øu run_matching=True)
            - output_path: ƒê∆∞·ªùng d·∫´n output
    
    Example:
        >>> from src import run_full_pipeline
        >>> result = run_full_pipeline(
        ...     data_raw="./data_raw",
        ...     data_output="./data_output",
        ...     parallel=True
        ... )
        >>> print(f"Processed {result['processed']} papers")
    """
    import os
    
    stats = {
        "processed": 0,
        "matched": 0,
        "output_path": data_output
    }
    
    if verbose:
        print("=" * 60)
        print("üöÄ PHASE 1: Pre-processing & Parsing")
        print("=" * 60)
    
    # Phase 1: Processing
    run_processing_pipeline(
        data_raw_path=data_raw,
        data_output_path=data_output,
        parallel=parallel,
        max_workers=max_workers
    )
    
    # Count processed
    if os.path.exists(data_output):
        stats["processed"] = len([
            f for f in os.listdir(data_output) 
            if os.path.isdir(os.path.join(data_output, f))
        ])
    
    if verbose:
        print(f"\n‚úÖ Phase 1 Complete: {stats['processed']} papers processed")
    
    # Phase 2: Matching (optional)
    if run_matching:
        if verbose:
            print("\n" + "=" * 60)
            print("üîç PHASE 2: Reference Matching")
            print("=" * 60)
        
        run_matching_pipeline(data_output)
        
        # Count matched
        for folder in os.listdir(data_output):
            labels_path = os.path.join(data_output, folder, "labels.json")
            if os.path.exists(labels_path):
                stats["matched"] += 1
        
        if verbose:
            print(f"\n‚úÖ Phase 2 Complete: {stats['matched']} papers matched")
    
    if verbose:
        print("\n" + "=" * 60)
        print("üéâ PIPELINE COMPLETE!")
        print(f"   Output: {data_output}")
        print("=" * 60)
    
    return stats


# =============================================================================
# Version Info
# =============================================================================
__version__ = "1.0.0"
__author__ = "23127011"

__all__ = [
    # Main pipeline functions
    "run_full_pipeline",
    "run_processing_pipeline",
    "run_matching_pipeline",
    
    # Core processing
    "process_single_paper",
    
    # Parser components
    "LatexFlattener",
    "LatexStructureBuilder",
    "LatexContentProcessor",
    "find_root_tex_file",
    
    # Processors
    "ReferenceProcessor",
    "ReferenceMatcher",
    
    # Deduplicators
    "ReferenceDeduplicator",
    "ContentDeduplicator",
    "replace_citations_in_text",
    
    # Merge utilities
    "load_selected_papers",
    "save_json",
]
