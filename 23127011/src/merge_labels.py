import json
import os
import argparse
import random
from typing import List, Dict

# ==============================================================================
# C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N M·∫∂C ƒê·ªäNH
# ==============================================================================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DEFAULT_INPUT_DIR = os.path.join(BASE_DIR, '../../data_output_v2') 
DEFAULT_OUTPUT_DIR = os.path.join(BASE_DIR, '../../dataset_final')
# ==============================================================================

def load_selected_papers(input_dir: str, folder_names: List[str]) -> Dict[str, List[dict]]:
    """
    ƒê·ªçc d·ªØ li·ªáu t·ª´ danh s√°ch folder ƒë√£ ƒë∆∞·ª£c l·ªçc (Limit/Range).
    
    Args:
        input_dir: ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c ch·ª©a data
        folder_names: Danh s√°ch t√™n folder c·∫ßn load
    """
    papers_map = {}
    print(f"üîÑ Loading data from {len(folder_names)} folders...")

    for folder_name in folder_names:
        file_path = os.path.join(input_dir, folder_name, 'labels.json')
        if not os.path.exists(file_path): continue
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            if isinstance(data, list) and len(data) > 0:
                papers_map[folder_name] = data
        except Exception:
            pass
            
    return papers_map

def save_json(data_list, filepath):
    if not data_list: return
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data_list, f, ensure_ascii=False, indent=4)
    print(f"   üíæ Saved: {os.path.basename(filepath)} ({len(data_list)} items)")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Split dataset into Auto/Manual with Limits.")
    parser.add_argument("--yymm", type=str, required=True, help="yymm prefix (e.g., 2403)")
    
    # --- OPTION ƒê∆Ø·ªúNG D·∫™N ---
    parser.add_argument("--input", type=str, default=DEFAULT_INPUT_DIR,
                        help=f"Input directory path (default: {DEFAULT_INPUT_DIR})")
    parser.add_argument("--output", type=str, default=DEFAULT_OUTPUT_DIR,
                        help=f"Output directory path (default: {DEFAULT_OUTPUT_DIR})")
    
    # --- OPTION GI·ªöI H·∫†N S·ªê L∆Ø·ª¢NG ---
    parser.add_argument("--limit", type=int, help="Ch·ªâ l·∫•y ng·∫´u nhi√™n N b√†i (VD: 50)")
    parser.add_argument("--range", type=int, nargs=2, help="L·∫•y t·ª´ index A ƒë·∫øn B (VD: 0 100)")
    
    args = parser.parse_args()
    
    # S·ª≠ d·ª•ng paths t·ª´ arguments
    INPUT_DIR_PATH = os.path.abspath(args.input)
    OUTPUT_DIR = os.path.abspath(args.output)

    print(f"üìÇ Input Directory:  {INPUT_DIR_PATH}")
    print(f"üìÇ Output Directory: {OUTPUT_DIR}")
    
    if not os.path.exists(INPUT_DIR_PATH):
        print(f"‚ùå Error: Path not found!")
        exit(1)

    # 1. QU√âT T·∫§T C·∫¢ FOLDER
    all_items = os.listdir(INPUT_DIR_PATH)
    valid_folders = [
        name for name in all_items 
        if os.path.isdir(os.path.join(INPUT_DIR_PATH, name)) 
        and name.startswith(args.yymm)
    ]
    valid_folders.sort() # S·∫Øp x·∫øp ƒë·ªÉ ƒë·∫£m b·∫£o th·ª© t·ª± cho --range
    
    if not valid_folders:
        print(f"‚ùå No folders found starting with {args.yymm}")
        exit(1)

    print(f"üìã Found total {len(valid_folders)} folders matching prefix.")

    # 2. √ÅP D·ª§NG LIMIT / RANGE
    target_folders = []
    
    if args.range:
        start, end = args.range
        target_folders = valid_folders[start:end]
        print(f"‚úÇÔ∏è  Mode: RANGE [{start} -> {end}]")
    elif args.limit:
        if args.limit < len(valid_folders):
            print(f"üé≤ Mode: RANDOM LIMIT ({args.limit} papers)")
            random.seed(42)
            # Shuffle to√†n b·ªô r·ªìi l·∫•y N ph·∫ßn t·ª≠ ƒë·∫ßu
            # Copy list ƒë·ªÉ kh√¥ng ·∫£nh h∆∞·ªüng list g·ªëc
            temp_list = list(valid_folders)
            random.shuffle(temp_list)
            target_folders = temp_list[:args.limit]
        else:
            target_folders = valid_folders
            print(f"‚ö†Ô∏è Limit ({args.limit}) > Total. Taking all.")
    else:
        target_folders = valid_folders
        print(f"üöÄ Mode: FULL DATASET")

    if not target_folders:
        print("‚ùå Error: Target list is empty after filtering.")
        exit(1)

    print(f"   üëâ Processing subset of {len(target_folders)} papers.")

    # 3. LOAD D·ªÆ LI·ªÜU (Ch·ªâ load nh·ªØng b√†i ƒë√£ l·ªçc)
    papers_db = load_selected_papers(INPUT_DIR_PATH, target_folders)
    all_loaded_ids = list(papers_db.keys())
    
    if not all_loaded_ids:
        print("‚ùå Error: No valid labels.json found in the selected subset.")
        exit(1)

    # 4. T√åM ·ª®NG VI√äN CHO MANUAL (Ref >= 20)
    candidates_manual = []
    for pid, refs in papers_db.items():
        if len(refs) >= 20:
            candidates_manual.append(pid)
            
    print(f"üéØ Candidates for Manual (>= 20 refs): {len(candidates_manual)} papers")
    
    if len(candidates_manual) < 5:
        print("‚ùå ERROR: Not enough candidates for manual selection!")
        print(f"   Requirement: 5 papers with >= 20 refs.")
        print(f"   Found: {len(candidates_manual)}")
        print("   üí° Suggestion: Increase your --limit or expand your --range.")
        exit(1)

    # 5. CH·ªåN 5 B√ÄI MANUAL (3 Train, 1 Val, 1 Test)
    random.seed(42)
    random.shuffle(candidates_manual)
    
    selected_manual = candidates_manual[:5]
    
    manual_train_ids = selected_manual[:3]
    manual_val_ids   = selected_manual[3:4]
    manual_test_ids  = selected_manual[4:5]
    
    print("\n‚úçÔ∏è  SELECTED MANUAL PAPERS:")
    print(f"   Train (3): {manual_train_ids}")
    print(f"   Val   (1): {manual_val_ids}")
    print(f"   Test  (1): {manual_test_ids}")

    # 6. PH√ÇN LO·∫†I AUTO (C√≤n l·∫°i)
    # Auto = (T·∫•t c·∫£ b√†i ƒë√£ load) - (5 b√†i manual)
    auto_pool_ids = [pid for pid in all_loaded_ids if pid not in selected_manual]
    
    random.shuffle(auto_pool_ids)
    total_auto = len(auto_pool_ids)
    
    # Chia 80/10/10
    tr_end = int(total_auto * 0.8)
    val_end = int(total_auto * 0.9)
    
    # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p t·∫≠p d·ªØ li·ªáu qu√° nh·ªè (<10 b√†i)
    if total_auto < 10:
        print("‚ö†Ô∏è Small auto dataset. Putting all into Train.")
        auto_train_ids = auto_pool_ids
        auto_val_ids = []
        auto_test_ids = []
    else:
        auto_train_ids = auto_pool_ids[:tr_end]
        auto_val_ids   = auto_pool_ids[tr_end:val_end]
        auto_test_ids  = auto_pool_ids[val_end:]
    
    print(f"\nü§ñ AUTO DATASET SPLIT ({total_auto} papers):")
    print(f"   Train: {len(auto_train_ids)} | Val: {len(auto_val_ids)} | Test: {len(auto_test_ids)}")

    # 7. GHI FILE
    def process_and_save(subset_name, auto_ids, manual_ids):
        auto_data = []
        for pid in auto_ids: auto_data.extend(papers_db[pid])
        
        manual_data = []
        for pid in manual_ids: manual_data.extend(papers_db[pid])
            
        base_path = os.path.join(OUTPUT_DIR, subset_name)
        print(f"\nüìÇ Writing {subset_name.upper()}...")
        
        if auto_data: save_json(auto_data, os.path.join(base_path, 'auto.json'))
        if manual_data: save_json(manual_data, os.path.join(base_path, 'manual.json'))

    process_and_save('train', auto_train_ids, manual_train_ids)
    process_and_save('validation', auto_val_ids, manual_val_ids)
    process_and_save('test', auto_test_ids, manual_test_ids)

    print(f"\nüéâ DONE! Saved to: {OUTPUT_DIR}")