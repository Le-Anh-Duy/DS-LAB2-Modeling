import os
import re
import uuid
import json
from src.utils.tex_cleaner import LatexCleaner
class LatexFlattener:
    """
    A class to flatten LaTeX documents by recursively merging all included files into a single structure.
    This class processes a root LaTeX file and recursively resolves all \input, \include, and \subfile
    commands to create a flattened representation of the entire document. It also handles circular
    dependencies, missing files, and removes comments and bibliography sections.
    Attributes:
        root_path (str): Absolute path to the root LaTeX file.
        root_dir (str): Directory containing the root LaTeX file.
        paper_id (str): Identifier for the paper being processed.
        version (str): Version identifier for the paper.
        remove_references (bool): Flag to control whether to remove bibliography sections.
        merged_files (list): List of relative paths of files successfully merged.
        missing_files (list): List of relative paths of files that could not be found.
    Methods:
        flatten():
            Main method that processes the root file and returns a dictionary containing
            the flattened content along with metadata.
            Returns:
                dict: A dictionary with keys:
                    - paper_id: Identifier of the paper
                    - version: Version of the paper
                    - root_file_path: Absolute path to root file
                    - metadata: Dictionary containing processing statistics
                    - content: Flattened LaTeX content as string
        _read_file(path):
            Reads content from a file with UTF-8 encoding.
            Args:
                path (str): Path to the file to read.
            Returns:
                str or None: File content if successful, None otherwise.
        _remove_comments(text):
            Removes LaTeX comments (lines starting with %) while preserving escaped percent signs.
            Args:
                text (str): LaTeX content to process.
            Returns:
                str: Content with comments removed.
        _remove_bibliography(text):
            Removes bibliography-related commands and environments from LaTeX content.
            Args:
                text (str): LaTeX content to process.
            Returns:
                str: Content with bibliography sections removed.
        _process_file(current_path, visited=None):
            Recursively processes a LaTeX file and all its dependencies.
            Args:
                current_path (str): Path to the current file being processed.
                visited (set, optional): Set of already visited file paths to detect circular dependencies.
            Returns:
                str: Flattened content with markers indicating file boundaries.
    Example:
        >>> flattener = LatexFlattener('/path/to/main.tex', 'paper123', 'v1', remove_references=True)
        >>> result = flattener.flatten()
        >>> print(result['metadata']['merged_count'])
    """
    def __init__(self, root_file_path, paper_id, version, remove_references=True):
        self.root_path = os.path.abspath(root_file_path)
        self.root_dir = os.path.dirname(self.root_path)
        self.paper_id = paper_id
        self.version = version
        self.remove_references = remove_references
        print(f"üìù Kh·ªüi t·∫°o LatexFlattener cho Paper: {self.paper_id}, Version: {self.version}")
        print(f"   Remove references: {'Yes' if self.remove_references else 'No'}")
        self.merged_files = [] # Danh s√°ch c√°c file ƒë√£ g·ªôp th√†nh c√¥ng
        self.missing_files = [] # Danh s√°ch c√°c file b·ªã thi·∫øu

    def flatten(self):
        """
        H√†m ch√≠nh: Th·ª±c hi·ªán g·ªôp v√† tr·∫£ v·ªÅ c·∫•u tr√∫c Dictionary (JSON object)
        """
        # B·∫Øt ƒë·∫ßu ƒë·ªá quy t·ª´ root
        full_content = self._process_file(self.root_path)
        
        # T·∫°o object k·∫øt qu·∫£
        result_object = {
            "paper_id": self.paper_id,
            "version": self.version,
            "root_file_path": self.root_path,
            "metadata": {
                "total_length": len(full_content),
                "merged_count": len(self.merged_files),
                "merged_files": self.merged_files,
                "missing_files": self.missing_files,
                "remove_references": self.remove_references
            },
            "content": full_content
        }
        return result_object

    def _read_file(self, path):
        if not os.path.exists(path): return None
        try:
            with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                return f.read()
        except: return None

    def _remove_comments(self, text):
        """X√≥a comment g·ªëc c·ªßa t√°c gi·∫£ ƒë·ªÉ gi·∫£m nhi·ªÖu, nh∆∞ng gi·ªØ l·∫°i marker c·ªßa m√¨nh sau n√†y"""
        # Regex: T√¨m k√Ω t·ª± % kh√¥ng ƒëi sau d·∫•u \
        return re.sub(r'(?<!\\)%.*', '', text)

    def _remove_bibliography(self, text):
        """Lo·∫°i b·ªè ph·∫ßn t√†i li·ªáu tham kh·∫£o theo y√™u c·∫ßu"""
        if not self.remove_references:
            return text
        
        text = re.sub(r'\\bibliography\{[^}]+\}', '', text)
        text = re.sub(r'\\printbibliography', '', text)
        text = re.sub(r'\\begin\{thebibliography\}.*?\\end\{thebibliography\}', '', text, flags=re.DOTALL)
        return text

    def _process_file(self, current_path, visited=None):
        if visited is None: visited = set()
        
        abs_path = os.path.abspath(current_path)
        rel_path = os.path.relpath(abs_path, self.root_dir).replace('\\', '/') # Chu·∫©n h√≥a ƒë∆∞·ªùng d·∫´n
        
        # 1. Check v√≤ng l·∫∑p
        if abs_path in visited:
            return f"\n% <WARNING: Circular dependency detected for {rel_path}>\n"
        visited.add(abs_path)

        # 2. ƒê·ªçc n·ªôi dung
        raw_content = self._read_file(abs_path)
        if raw_content is None:
            self.missing_files.append(rel_path)
            return f"\n% <WARNING: File not found: {rel_path}>\n"
        
        self.merged_files.append(rel_path)

        # 3. L√†m s·∫°ch s∆° b·ªô (X√≥a comment g·ªëc + X√≥a Bib n·∫øu c·ªù b·∫≠t)
        content = self._remove_comments(raw_content)
        content = self._remove_bibliography(content)

        # 4. T√¨m v√† thay th·∫ø ƒë·ªá quy c√°c file con
        # Regex h·ªó tr·ª£: \input{file}, \include{file}, \subfile{file}, \input file
        pattern = re.compile(r'\\(?:input|include|subfile)(?:(?:\s*\{([^}]+)\})|(?:\s+([^\s%]+)))')

        def replace_match(match):
            fname = match.group(1) or match.group(2)
            if not fname: return ""
            fname = fname.strip()
            if not fname.lower().endswith('.tex'): fname += '.tex'
            
            # Resolve path
            child_path = os.path.join(self.root_dir, fname)
            if not os.path.exists(child_path):
                child_path = os.path.join(os.path.dirname(abs_path), fname)
            
            # ƒê·ªá quy
            child_content = self._process_file(child_path, visited)
            
            # QUAN TR·ªåNG: K·∫πp n·ªôi dung gi·ªØa 2 Marker
            return (f"\n% <BEGIN_FILE: {fname}>\n"
                    f"{child_content}"
                    f"\n% <END_FILE: {fname}>\n")

        flattened_content = pattern.sub(replace_match, content)
        
        return flattened_content

class LatexStructureBuilder:
    def __init__(self, flattened_content, paper_id, version):
        self.content = flattened_content
        self.paper_id = paper_id
        self.version = version
        # ƒê·ªãnh nghƒ©a th·ª© t·ª± c·∫•p b·∫≠c (nh·ªè h∆°n l√† c·∫•p cao h∆°n/cha)
        self.HIERARCHY_LEVELS = {
            'document': 0,      # Root
            'part': 1,
            'chapter': 2,
            'section': 3,
            'subsection': 4,
            'subsubsection': 5,
            'paragraph': 6,
            'subparagraph': 7
        }
        # Regex ƒë·ªÉ b·∫Øt c√°c header: \section{Title}, \section*{Title}, \chapter{...}
        # Group 1: command (section, chapter...)
        # Group 2: * (n·∫øu c√≥)
        # Group 3: Title
        self.SECTION_START_REGEX = re.compile(
            r'\\(part|chapter|section|subsection|subsubsection|paragraph|subparagraph)(\*?)\s*\{', 
            re.IGNORECASE
        )

    def _extract_balanced_title(self, start_idx):
        """
        H√†m ph·ª• tr·ª£ ƒë·ªÉ l·∫•y n·ªôi dung trong ngo·∫∑c nh·ªçn {} c√≥ l·ªìng nhau.
        start_idx: V·ªã tr√≠ ngay sau d·∫•u '{' m·ªü ƒë·∫ßu.
        Returns: (title_content, end_idx)
        """
        depth = 1
        current_idx = start_idx
        max_len = len(self.content)
        
        while current_idx < max_len and depth > 0:
            char = self.content[current_idx]
            if char == '{':
                depth += 1
            elif char == '}':
                depth -= 1
            
            if depth > 0:
                current_idx += 1
        
        # current_idx l√∫c n√†y ƒëang ·ªü d·∫•u '}' ƒë√≥ng cu·ªëi c√πng
        title = self.content[start_idx:current_idx]
        return title, current_idx + 1  # +1 ƒë·ªÉ nh·∫£y qua d·∫•u '}'

    def build_coarse_tree(self):
        root = {
            'id': f'{self.paper_id}-{self.version}-document-{uuid.uuid4()}',
            'type': 'document',
            'title': 'Root Document',
            'level': 0,
            'raw_content': "",
            'children': []
        }
        stack = [root]
        
        cleaner = LatexCleaner()
        # S·ª¨A 2: Logic l·∫∑p thay ƒë·ªïi ƒë·ªÉ k·∫øt h·ª£p Regex + Manual Counting
        cursor = 0
        
        # T√¨m t·∫•t c·∫£ c√°c ƒëi·ªÉm b·∫Øt ƒë·∫ßu
        # L∆∞u √Ω: finditer s·∫Ω t√¨m c√°c match.
        matches = list(self.SECTION_START_REGEX.finditer(self.content))
        
        for match in matches:
            match_start = match.start()
            match_end = match.end() # V·ªã tr√≠ ngay sau d·∫•u '{'
            
            # N·∫øu match n·∫±m tr∆∞·ªõc cursor (ƒë√£ b·ªã x·ª≠ l√Ω b·ªüi logic l·ªìng nhau n√†o ƒë√≥ - hi·∫øm g·∫∑p nh∆∞ng c·ª© check), b·ªè qua
            if match_start < cursor: 
                continue

            command = match.group(1)
            is_starred = match.group(2) == '*'
            
            # S·ª¨A 3: D√πng h√†m ƒë·∫øm ngo·∫∑c ƒë·ªÉ l·∫•y title ch√≠nh x√°c
            # title_raw s·∫Ω ch·ª©a: "\textbf{Spiral-type galaxies}" (bao g·ªìm c·∫£ command b√™n trong)
            title_raw, end_idx = self._extract_balanced_title(match_end)
            
            # S·ª¨A 4: Clean title ngay t·∫°i ƒë√¢y (D√πng LatexCleaner ƒë√£ vi·∫øt ·ªü c√¢u tr∆∞·ªõc)
            # B∆∞·ªõc n√†y c·ª±c quan tr·ªçng ƒë·ªÉ bi·∫øn "\textbf{Spiral...}" th√†nh "Spiral..."
            # Gi·∫£ s·ª≠ b·∫°n ƒë√£ import class LatexCleaner
            # title_clean = LatexCleaner.clean_latex(title_raw) 
            title_clean = cleaner.clean_latex(title_raw) # T·∫°m th·ªùi ƒë·ªÉ raw n·∫øu ch∆∞a t√≠ch h·ª£p Cleaner

            # --- Logic G√°n Content & T·∫°o Node (nh∆∞ c≈©) ---
            current_level = self.HIERARCHY_LEVELS.get(command, 100)
            
            # L·∫•y text ƒëo·∫°n tr∆∞·ªõc header n√†y g√°n cho node tr∆∞·ªõc ƒë√≥
            text_segment = self.content[cursor:match_start]
            if text_segment.strip():
                if 'raw_content' not in stack[-1]: stack[-1]['raw_content'] = ""
                stack[-1]['raw_content'] += text_segment

            # Adjust Stack
            while len(stack) > 1 and stack[-1]['level'] >= current_level:
                stack.pop()
            
            parent = stack[-1]

            # B·ªè qua References (Optional)
            if 'references' in title_clean.lower() or 't√†i li·ªáu tham kh·∫£o' in title_clean.lower():
                cursor = end_idx # Nh·∫£y qua header n√†y
                # T√πy logic c·ªßa b·∫°n: c√≥ th·ªÉ mu·ªën g√°n references v√†o content c·ªßa parent
                # ho·∫∑c t·∫°o m·ªôt node ri√™ng. ·ªû ƒë√¢y ta pass ƒë·ªÉ x·ª≠ l√Ω ti·∫øp content
                # pass 
            
            new_node = {
                'id': f'{self.paper_id}-{self.version}-{command}-{uuid.uuid4()}',
                'type': command,
                'title': title_clean.strip(), # Title ƒë√£ s·∫°ch
                'level': current_level,
                'is_starred': is_starred,
                'raw_content': "", 
                'children': []
            }

            parent['children'].append(new_node)
            stack.append(new_node)
            
            # C·∫≠p nh·∫≠t cursor ƒë·∫øn h·∫øt ph·∫ßn header v·ª´a x·ª≠ l√Ω (bao g·ªìm c·∫£ d·∫•u ƒë√≥ng ngo·∫∑c ƒë√∫ng)
            cursor = end_idx

        # X·ª≠ l√Ω ph·∫ßn d∆∞ cu·ªëi c√πng
        remaining_text = self.content[cursor:]
        if remaining_text.strip():
            if 'raw_content' not in stack[-1]: stack[-1]['raw_content'] = ""
            stack[-1]['raw_content'] += remaining_text

        return root

    def print_tree(self, node, indent=0):
        """H√†m helper ƒë·ªÉ in c√¢y ra console ki·ªÉm tra"""
        prefix = "  " * indent
        preview = (node.get('raw_content', '')[:50] + '...') if node.get('raw_content') else "[Empty]"
        print(f"{prefix}- [{node['type'].upper()}] {node['title']} (ID: {node['id'][:8]})")
        print(f"{prefix}  Content Preview: {preview}")
        
        for child in node['children']:
            self.print_tree(child, indent + 1)
    
    def print_tree_to_file(self, root_node, output_path):
        """
        In c√¢y ra file JSON v·ªõi danh s√°ch nodes v√† edges
        
        Args:
            root_node: Node g·ªëc c·ªßa c√¢y
            output_path: ƒê∆∞·ªùng d·∫´n file JSON ƒë·ªÉ l∆∞u
        
        Output format:
        {
            "nodes": [
                {
                    "id": "uuid",
                    "type": "section",
                    "title": "Section Title",
                    "level": 3,
                    "content": "raw content..."
                }
            ],
            "edges": [
                {
                    "from": "parent_id",
                    "to": "child_id"
                }
            ]
        }
        """
        nodes = []
        edges = []
        
        def traverse(node, parent_id=None):
            # Th√™m node hi·ªán t·∫°i v√†o danh s√°ch
            node_data = {
                "id": node['id'],
                "type": node['type'],
                "title": node['title'],
                "level": node['level'],
                "content": node.get('raw_content', '')
            }
            nodes.append(node_data)
            
            # N·∫øu c√≥ parent, t·∫°o edge
            if parent_id is not None:
                edges.append({
                    "from": parent_id,
                    "to": node['id']
                })
            
            # ƒê·ªá quy cho c√°c children
            for child in node.get('children', []):
                traverse(child, node['id'])
        
        # B·∫Øt ƒë·∫ßu traverse t·ª´ root
        traverse(root_node)
        
        # T·∫°o c·∫•u tr√∫c d·ªØ li·ªáu cu·ªëi c√πng
        output_data = {
            "nodes": nodes,
            "edges": edges,
            "metadata": {
                "total_nodes": len(nodes),
                "total_edges": len(edges)
            }
        }
        
        # Ghi ra file
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ ƒê√£ l∆∞u c·∫•u tr√∫c c√¢y v√†o: {output_path}")
        print(f"   - T·ªïng s·ªë nodes: {len(nodes)}")
        print(f"   - T·ªïng s·ªë edges: {len(edges)}")

    def export_to_markdown(self, root_node):
        """
        Export the parsed tree to Markdown format.
        
        Args:
            root_node: The root node of the parsed tree.
            
        Returns:
            str: The content in Markdown format.
        """
        def traverse_and_build(node, depth=0):
            md = ""
            
            # --- HANDLE METADATA NODES ---
            if node['type'] == 'title':
                return f"# {node['title']}\n\n"
            
            if node['type'] == 'author':
                # L·∫•y n·ªôi dung author (c√≥ th·ªÉ l∆∞u ·ªü title ho·∫∑c content/raw_content)
                val = node.get('content', node.get('raw_content', node['title']))
                return f"**Authors:** {val}\n\n"

            if node['type'] == 'abstract':
                md += "## Abstract\n\n"
                # Abstract children are sentences, handled by recursion
            
            # 1. Add Header with appropriate Markdown level
            # Skip header for metadata nodes handled above
            elif node['level'] > 0 and node['level'] < 99:
                # Map LaTeX levels to Markdown headers
                md_level = min(node['level'], 6)
                md += f"\n{'#' * md_level} {node['title']}\n\n"
            
            # 2. Add Content based on type
            raw_content = node.get('raw_content', '').strip()
            
            if node['type'] == 'equation':
                md += f"$$\n{raw_content}\n$$\n\n"
            
            elif node['type'] == 'figure':
                md += f"> **[{node['title']}]**\n> {raw_content}\n\n"
            
            elif node['type'] == 'list':
                md += "\n"  # Lists handled by children
            
            elif node['type'] == 'list_item':
                indent = "  " * depth
                md += f"{indent}- {raw_content}\n"
            
            elif node['type'] == 'sentence':
                md += f"{raw_content}\n\n"
            
            elif node['type'] not in ['abstract'] and raw_content:
                # For other types, just add raw content if exists
                md += f"{raw_content}\n\n"
            
            # 3. Recurse Children
            for child in node.get('children', []):
                # Increase depth for list items
                child_depth = depth + 1 if node['type'] == 'list' else depth
                md += traverse_and_build(child, child_depth)
            
            return md
        
        markdown_content = traverse_and_build(root_node)
        return markdown_content.strip()

    def export_to_html(self, root_node):
        """
        Export the parsed tree to HTML format.
        
        Args:
            root_node: The root node of the parsed tree.
            
        Returns:
            str: The content in HTML format.
        """
        def traverse_and_build(node):
            html = ""
            
            # --- HANDLE METADATA NODES ---
            if node['type'] == 'title':
                return f"<h1 class='paper-title'>{node['title']}</h1>\n"
            
            if node['type'] == 'author':
                 val = node.get('content', node.get('raw_content', node['title']))
                 return f"<div class='authors'><strong>Authors:</strong> {val}</div>\n"
            
            if node['type'] == 'abstract':
                html += "<section class='abstract'>\n<h2>Abstract</h2>\n"
                for child in node.get('children', []):
                    html += traverse_and_build(child)
                html += "</section>\n"
                return html

            # 1. Add Header with appropriate HTML tag
            if node['level'] > 0 and node['level'] < 99:
                html_level = min(node['level'], 6)
                html += f"<h{html_level}>{node['title']}</h{html_level}>\n"
            
            # 2. Add Content based on type
            raw_content = node.get('raw_content', '').strip()
            
            if node['type'] == 'equation':
                # Use MathJax/KaTeX compatible format
                html += f'<div class="equation">\n$$\n{raw_content}\n$$\n</div>\n'
            
            elif node['type'] == 'figure':
                html += f'<figure>\n<figcaption>{node["title"]}</figcaption>\n<blockquote>{raw_content}</blockquote>\n</figure>\n'
            
            elif node['type'] == 'list':
                # Determine list type
                list_tag = "ol" if "enumerate" in node.get('title', '').lower() else "ul"
                html += f"<{list_tag}>\n"
                
                # Process children
                for child in node.get('children', []):
                    html += traverse_and_build(child)
                
                html += f"</{list_tag}>\n"
                return html  # Return early to avoid duplicate child processing
            
            elif node['type'] == 'list_item':
                html += f"<li>{raw_content}</li>\n"
            
            elif node['type'] == 'sentence':
                html += f"<p>{raw_content}</p>\n"
            
            elif raw_content:
                # For other types, wrap in div or paragraph
                html += f"<div>{raw_content}</div>\n"
            
            # 3. Recurse Children (skip if already handled like in list or abstract)
            if node['type'] != 'list' and node['type'] != 'abstract':
                for child in node.get('children', []):
                    html += traverse_and_build(child)
            
            return html
        
        html_content = f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>{self.paper_id} - {self.version}</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body {{ font-family: 'Segoe UI', Arial, sans-serif; max-width: 900px; margin: 0 auto; padding: 40px; line-height: 1.6; color: #333; }}
        h1.paper-title {{ text-align: center; color: #2c3e50; margin-bottom: 10px; }}
        .authors {{ text-align: center; font-style: italic; color: #555; margin-bottom: 30px; }}
        .abstract {{ background: #f9f9f9; padding: 20px; border-radius: 8px; margin-bottom: 30px; border-left: 5px solid #3498db; }}
        h1, h2, h3, h4, h5, h6 {{ color: #2c3e50; margin-top: 24px; }}
        .equation {{ margin: 20px 0; text-align: center; background: #fff; padding: 10px; overflow-x: auto; }}
        figure {{ border: 1px solid #ddd; padding: 10px; margin: 20px 0; border-radius: 4px; background: #fafafa; }}
        figcaption {{ font-weight: bold; margin-bottom: 5px; color: #666; }}
        blockquote {{ margin: 0; color: #777; font-style: italic; }}
        p {{ margin-bottom: 12px; }}
        li {{ margin-bottom: 5px; }}
    </style>
</head>
<body>
{traverse_and_build(root_node)}
</body>
</html>"""
        
        return html_content

    def export_cleaned_paper(self, root_node):
        """
        Reconstruct the cleaned LaTeX content from the tree.
        This allows checking if the parsing logic preserved the content integrity.
        
        Args:
            root_node: The root node of the parsed tree.
            
        Returns:
            str: The reconstructed LaTeX string.
        """
        def traverse_and_build(node):
            text = ""
            
            # 1. Reconstruct Header (if not Document root)
            if node['level'] > 0 and node['level'] < 99:
                lat_sections = {'part', 'chapter', 'section', 'subsection', 'subsubsection', 'paragraph', 'subparagraph'}
                if node['type'] in lat_sections:
                    star = "*" if node.get('is_starred') else ""
                    text += f"\n\\{node['type']}{star}{{{node['title']}}}\n\n"
            
            # 2. Add Content based on type
            raw_content = node.get('raw_content', '').strip()
            
            if node['type'] == 'equation':
                text += f"\n{raw_content}\n\n"
            elif node['type'] == 'figure':
                text += f"\n{raw_content}\n\n"
            elif node['type'] == 'list_item':
                text += f"\\item {raw_content}\n"
            elif node['type'] == 'list':
                list_type = "itemize"
                if "enumerate" in node.get('title', '').lower():
                    list_type = "enumerate"
                text += f"\n\\begin{{{list_type}}}\n"
                for child in node.get('children', []):
                    text += traverse_and_build(child)
                text += f"\\end{{{list_type}}}\n\n"
                return text  # Return early to avoid duplicate processing
            elif raw_content:
                text += f"{raw_content}\n\n"
            
            # 3. Recurse Children
            if node['type'] != 'list':
                for child in node.get('children', []):
                    text += traverse_and_build(child)
                
            return text

        return traverse_and_build(root_node).strip()

class LatexContentProcessor:
    def __init__(self, paper_id, version):
        self.paper_id = paper_id
        self.version = version
        
        # --- REGEX PATTERNS ---
        
        # 1. Block Math: $$...$$, \[...\], \begin{equation}...
        self.REGEX_MATH_BLOCK = re.compile(
            r'\\begin\{equation\*?\}.*?\\end\{equation\*?\}|\\\[.*?\\\]|\$\$.*?\$\$', 
            re.DOTALL
        )
        
        # 2. Figures/Tables: \begin{figure/table}...
        self.REGEX_FIGURE = re.compile(
            r'\\begin\{(?:figure|table)\*?\}.*?\\end\{(?:figure|table)\*?\}', 
            re.DOTALL | re.IGNORECASE
        )
        
        # 3. Lists: \begin{itemize/enumerate}...
        self.REGEX_LIST = re.compile(
            r'(\\begin\{(itemize|enumerate)\}.*?\\end\{(itemize|enumerate)\})', 
            re.DOTALL
        )
        
        # 4. Sentence Splitter: T√¨m d·∫•u ch·∫•m/h·ªèi/th√°n k·∫øt th√∫c c√¢u
        # X·ª≠ l√Ω c√°c tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát: abbreviations, s·ªë th·∫≠p ph√¢n, tr√≠ch d·∫´n
        
        # Danh s√°ch abbreviations ph·ªï bi·∫øn trong paper khoa h·ªçc
        abbrev_pattern = r'(?:Fig|Eq|Eqs|Tab|Sec|Ref|Vol|No|Ch|Dr|Prof|Ph\.D|' \
                        r'et al|i\.e|e\.g|vs|cf|etc|approx|ca|viz)'
        
        # Pattern ch√≠nh:
        # - Kh√¥ng ph·∫£i sau abbreviations
        # - Kh√¥ng ph·∫£i gi·ªØa ch·ªØ c√°i ƒë∆°n (U.S.)
        # - Kh√¥ng ph·∫£i s·ªë th·∫≠p ph√¢n (3.14)
        # - Cho ph√©p d·∫•u ngo·∫∑c k√©p/ƒë∆°n sau d·∫•u c√¢u
        self.REGEX_SENTENCE = re.compile(
                    r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|\!)\s+(?=[A-Z\(])'
                )

    def process_tree(self, node):
        """
        Duy·ªát ƒë·ªá quy c√¢y c·∫•u tr√∫c th√¥ ƒë·ªÉ "m·ªï x·∫ª" raw_content th√†nh c√°c elements.
        """
        # 1. X·ª≠ l√Ω raw_content c·ªßa node hi·ªán t·∫°i (n·∫øu c√≥)
        if node.get('raw_content') and node['raw_content'].strip():
            # T√°ch n·ªôi dung th√†nh c√°c node con chi ti·∫øt (c√¢u, h√¨nh, c√¥ng th·ª©c...)

            if node.get('level') == 0 and node['type'] == 'document':
                # V·ªõi document root, ta c√≥ th·ªÉ mu·ªën x·ª≠ l√Ω preamble ri√™ng
                # Gi·∫£ s·ª≠ ta c√≥ h√†m _process_preamble ƒë·ªÉ tr√≠ch xu·∫•t title, author, abstract
                preamble_nodes = self._process_preamble(node['raw_content'])
                
                # Ch√®n c√°c node preamble v√†o ƒë·∫ßu danh s√°ch children
                node['children'] = preamble_nodes + node['children']
                
                # X√≥a raw_content ƒë·ªÉ gi·∫£i ph√≥ng b·ªô nh·ªõ v√† ƒë√°nh d·∫•u l√† ƒë√£ x·ª≠ l√Ω
                del node['raw_content']
            else:
                fine_grained_nodes = self.parse_content_blocks(node['raw_content'])
                # print(fine_grained_nodes)
                # QUAN TR·ªåNG: Ch√®n c√°c node n·ªôi dung v√†o ƒê·∫¶U danh s√°ch children
                # L√Ω do: Trong LaTeX, text c·ªßa Section lu√¥n n·∫±m tr∆∞·ªõc Subsection con.
                node['children'] = fine_grained_nodes + node['children']
                
                # X√≥a raw_content ƒë·ªÉ gi·∫£i ph√≥ng b·ªô nh·ªõ v√† ƒë√°nh d·∫•u l√† ƒë√£ x·ª≠ l√Ω
                del node['raw_content']

        # 2. ƒê·ªá quy x·ª≠ l√Ω c√°c con (bao g·ªìm c·∫£ c√°c Subsection c≈© v√† c√°c List m·ªõi t·∫°o)
        # L∆∞u √Ω: Ta ch·ªâ ƒë·ªá quy v√†o c√°c node c·∫•u tr√∫c (part, chapter, section...) 
        # ho·∫∑c list, kh√¥ng c·∫ßn ƒë·ªá quy v√†o sentence/equation (node l√°).
        for child in node['children']:
            # Ch·ªâ ƒë·ªá quy n·∫øu node con ƒë√≥ c√≥ th·ªÉ ch·ª©a content con (v√≠ d·ª• List ho·∫∑c Section con)
            if child['type'] not in ['sentence', 'equation', 'figure', 'list_item']:
                self.process_tree(child)

    def parse_content_blocks(self, text):
        """
        C·∫Øt chu·ªói text h·ªón h·ª£p th√†nh danh s√°ch c√°c Node Elements
        """
        elements = []
        
        # Pattern t·ªïng h·ª£p ƒë·ªÉ split: Math OR Figure OR List

        combined_pattern_str = f"({self.REGEX_MATH_BLOCK.pattern}|{self.REGEX_FIGURE.pattern}|{self.REGEX_LIST.pattern})"

        pattern = re.compile(combined_pattern_str, re.DOTALL | re.IGNORECASE)
        # pattern = re.compile(
        #     f"({self.REGEX_MATH_BLOCK.pattern}|{self.REGEX_FIGURE.pattern}|{self.REGEX_LIST.pattern})",
        #     re.DOTALL | re.IGNORECASE
        # )
        # print(text)
        # Split text, gi·ªØ l·∫°i delimiter (ch√≠nh l√† n·ªôi dung block)
        parts = pattern.split(text)
        cleaner = LatexCleaner()


        for part in parts:
            if not part: continue
            part = part.strip()
            if not part: continue

            # print(part)
            # --- PH√ÇN LO·∫†I & T·∫†O NODE ---
            
            # 1. Math Block
            if self.REGEX_MATH_BLOCK.fullmatch(part):
                elements.append(self._create_node(
                    type_name='equation',
                    title='Equation Block',
                    raw_content= cleaner.clean_equation(part)
                ))
            
            # 2. Figure/Table
            elif self.REGEX_FIGURE.fullmatch(part):
                elements.append(self._create_node(
                    type_name='figure',
                    title='Figure/Table',
                    raw_content= cleaner.clean_figure_table(part) #self._clean_latex(part)
                ))
            
            # 3. List (Itemize/Enumerate) -> T·∫°o c·∫•u tr√∫c l·ªìng nhau
            elif self.REGEX_LIST.fullmatch(part):
                list_node = self._process_list_block(part)
                elements.append(list_node)
            
            # 4. Text thu·∫ßn -> T√°ch th√†nh Sentence Nodes
            else:
                sentences = self._split_sentences(part)
                for sent in sentences:
                    elements.append(self._create_node(
                        type_name='sentence',
                        title=sent[:30] + "...", # Title xem tr∆∞·ªõc
                        raw_content=cleaner.clean_latex(sent)
                    ))
                    
        return elements
    
    def _process_preamble(self, preamble_text):
        """
        Input: Text v√πng preamble.
        Output: List c√°c Node (Title Node, Author Node, Abstract Node)
        """
        print("üîç X·ª≠ l√Ω Preamble ƒë·ªÉ tr√≠ch xu·∫•t Title, Authors, Abstract...")
        nodes = []
        cleaner = LatexCleaner()
        # 1. Tr√≠ch xu·∫•t Title (Leaf Node)
        title_match = re.search(r'\\title(?:\s*\[.*?\])?\s*\{((?:[^{}]|{[^{}]*})*)\}', preamble_text, re.DOTALL | re.IGNORECASE)
        if title_match:
            clean_title = cleaner.clean_latex(title_match.group(1))
            nodes.append({
                "id": f"{self.paper_id}-{self.version}-title-{uuid.uuid4()}",
                "title": clean_title,
                "content": clean_title,
                "type": "title",
                "level": 99,
                "children": [] # Title l√† l√°
            })

        # 2. Tr√≠ch xu·∫•t Authors (Leaf Node)
        # Gom t·∫•t c·∫£ author th√†nh 1 chu·ªói ho·∫∑c t·∫°o list
        authors = []
        for match in re.finditer(r'\\author(?:\s*\[.*?\])?\s*\{((?:[^{}]|{[^{}]*})*)\}', preamble_text, re.DOTALL | re.IGNORECASE):
            clean_auth = cleaner.clean_latex(match.group(1))
            if clean_auth:
                authors.append(clean_auth)
        
        if authors:
            nodes.append({
                "id": f"{self.paper_id}-{self.version}-authors-{uuid.uuid4()}",
                "title": ", ".join(authors), # N·ªëi l·∫°i ho·∫∑c ƒë·ªÉ array t√πy c·∫•u tr√∫c c·ªßa b·∫°n
                "content": ", ".join(authors),
                "type": "author",
                "level": 99,
                "children": []
            })

        # 3. Tr√≠ch xu·∫•t Abstract (Component Node - C√≥ con l√† sentences) co level cung voi paragraph
        # T√¨m abstract environment
        abs_match = re.search(r'\\begin\s*\{abstract\}(.*?)\\end\s*\{abstract\}', preamble_text, re.DOTALL | re.IGNORECASE)
        if not abs_match:
             # Fallback t√¨m l·ªánh \abstract{}
             abs_match = re.search(r'\\abstract\s*\{((?:[^{}]|{[^{}]*})*)\}', preamble_text, re.DOTALL | re.IGNORECASE)

        if abs_match:
            raw_abstract = abs_match.group(1)
            
            # QUAN TR·ªåNG: D√πng LatexContentProcessor ƒë·ªÉ t√°ch c√¢u cho Abstract
            # C·∫Øt chu·ªói text h·ªón h·ª£p th√†nh danh s√°ch c√°c Node Elements
            abstract_sentences = self.parse_content_blocks(raw_abstract)
            
            # G√°n ID cha cho c√°c c√¢u n√†y l√† abstract
            for sent in abstract_sentences:
                sent['parent'] = "abstract"

            nodes.append({
                "id": f"{self.paper_id}-{self.version}-abstract-{uuid.uuid4()}",
                "title": "Abstract",
                "content": "Abstract",
                "type": "abstract", # ƒê√°nh d·∫•u n√≥ l√† 1 section ƒë·∫∑c bi·ªát
                "level": 2,
                "children": abstract_sentences
            })

        return nodes

    def _process_list_block(self, list_content):
        """X·ª≠ l√Ω ri√™ng cho Itemize/Enumerate ƒë·ªÉ t√°ch c√°c \item"""
        # 1. X√°c ƒë·ªãnh lo·∫°i list (itemize hay enumerate)
        # Group 1 s·∫Ω b·∫Øt ƒë∆∞·ª£c t√™n m√¥i tr∆∞·ªùng (itemize/enumerate)
        match = re.match(r'\\begin\{(itemize|enumerate)\}', list_content, re.IGNORECASE)
        list_type = match.group(1) if match else "itemize"

        list_node = self._create_node(
            type_name='list',
            title=f'List ({list_type})',
            raw_content="" 
        )
        
        # 2. B√≥c v·ªè (Unwrap) an to√†n
        # X√≥a th·∫ª m·ªü ƒë·∫ßu ti√™n (ch·ªâ x√≥a 1 l·∫ßn - count=1)
        # Regex b·∫Øt: \begin{type} theo sau c√≥ th·ªÉ l√† [options]
        content_inner = re.sub(r'^\\begin\{' + list_type + r'\}(\[.*?\])?', '', list_content, count=1, flags=re.IGNORECASE).strip()
        
        # X√≥a th·∫ª ƒë√≥ng cu·ªëi c√πng (Neo v√†o cu·ªëi chu·ªói $)
        content_inner = re.sub(r'\\end\{' + list_type + r'\}\s*$', '', content_inner, count=1, flags=re.IGNORECASE).strip()

        # 3. T√°ch c√°c \item
        # L√∫c n√†y content_inner ch·ªâ c√≤n n·ªôi dung ru·ªôt, c√°c list con (n·∫øu c√≥) v·∫´n nguy√™n v·∫πn
        items = re.split(r'\\item\s+', content_inner)
        
        for item in items:
            # B·ªè qua ph·∫ßn text r√°c tr∆∞·ªõc \item ƒë·∫ßu ti√™n (th∆∞·ªùng l√† kho·∫£ng tr·∫Øng)
            if not item.strip(): 
                continue
                
            # Clean n·ªôi dung item
            # L∆∞u √Ω: KH√îNG d√πng replace \end n·ªØa v√¨ ta ƒë√£ b√≥c v·ªè ·ªü b∆∞·ªõc 2 r·ªìi
            clean_content = self._clean_latex(item)
            
            if clean_content:
                item_node = self._create_node(
                    type_name='list_item',
                    title='List Item',
                    raw_content=clean_content
                )
                list_node['children'].append(item_node)
                
        return list_node
    

    def _create_node(self, type_name, title, raw_content):
        """Helper t·∫°o node chu·∫©n theo format ID c·ªßa b·∫°n"""
        return {
            'id': f'{self.paper_id}-{self.version}-{type_name}-{uuid.uuid4()}',
            'type': type_name,
            'title': title,
            'level': 99, # Level th·∫•p nh·∫•t (l√°)
            'raw_content': raw_content,
            'children': []
        }

    def _split_sentences(self, text):
        """T√°ch c√¢u"""
        text = re.sub(r'\s+', ' ', text) # G·ªôp newline th√†nh space
        sentences = self.REGEX_SENTENCE.split(text)
        return [s.strip() for s in sentences if s.strip()]

    def _normalize_math(self, content):
        """Chu·∫©n h√≥a to√°n h·ªçc: Convert $$ -> equation"""
        if content.startswith('$$') and content.endswith('$$'):
            inner = content[2:-2].strip()
            return f"\\begin{{equation}}{inner}\\end{{equation}}"
        elif content.startswith(r'\['):
            inner = content[2:-2].strip()
            return f"\\begin{{equation}}{inner}\\end{{equation}}"
        
        cleaner = LatexCleaner()
        content = cleaner.clean_equation(content)
        return content

    def _clean_latex(self, text):
        """X√≥a c√°c l·ªánh format r√°c"""
        # X√≥a \centering, \hfill, label, cite, ref... t√πy nhu c·∫ßu
        text = re.sub(r'\\(centering|hfill|vfill|noindent|small|tiny|large)', '', text)
        # X√≥a optional params [htbp] c·ªßa figure
        text = re.sub(r'\\begin\{(figure|table)\}\[.*?\]', r'\\begin{\1}', text)
        return text.strip()
